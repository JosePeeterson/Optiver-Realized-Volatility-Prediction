{"cells":[{"cell_type":"code","execution_count":null,"id":"N0Dnw4c6tG64","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37451,"status":"ok","timestamp":1727057884872,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"N0Dnw4c6tG64","outputId":"bfbf2469-b034-4701-ca48-2dd66f604be7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Mon Sep 23 02:17:51 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n","Collecting plotly_express\n","  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (2.1.4)\n","Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (5.15.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (0.14.3)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.13.1)\n","Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (0.5.6)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2024.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5->plotly_express) (1.16.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly_express) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly_express) (24.1)\n","Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n","Installing collected packages: plotly_express\n","Successfully installed plotly_express-0.4.1\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n","Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n","Collecting optuna\n","  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.35)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-4.0.0\n","Collecting shap\n","  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.3.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.1.4)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n","Collecting slicer==0.0.8 (from shap)\n","  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n","Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.46.0 slicer-0.0.8\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/optiver_real_vol')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","\n","!pip install plotly_express\n","!pip install numba\n","!pip install optuna\n","!pip install shap"]},{"cell_type":"code","execution_count":null,"id":"3c4fec69","metadata":{"id":"3c4fec69"},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","import statsmodels.api as sm\n","import plotly.subplots as sub_plots\n","import plotly.graph_objects as go\n","import statsmodels.api as sm\n","import scipy.stats as stats\n","\n","from sklearn.cluster import KMeans\n","import re\n","\n","import warnings\n","#warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import confusion_matrix\n","#from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","\n","import glob\n","import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","from numba import jit, njit\n","import numba as nb\n","import plotly_express as px\n","from itertools import combinations, permutations, product, combinations_with_replacement\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from scipy.signal import find_peaks\n","import pickle\n","from joblib import Parallel, delayed\n","import seaborn as sns\n","from sklearn import model_selection\n","from sklearn.metrics import r2_score\n","import gc\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","import scipy as sp\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from sklearn.cluster import SpectralClustering, MiniBatchKMeans, MeanShift, AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from scipy.spatial.distance import squareform\n","from scipy.stats import skew, kurtosis\n","import shap\n","from datetime import datetime\n","import ipywidgets as widgets\n","from matplotlib.patches import Rectangle\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from xgboost import plot_tree, plot_importance\n","from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.genmod.generalized_linear_model import GLM\n","import warnings\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","import re\n","\n","from matplotlib.pyplot import cm\n","\n","\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import minmax_scale\n","\n"]},{"cell_type":"code","execution_count":null,"id":"0792a582","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1278,"status":"ok","timestamp":1727057891466,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"0792a582","outputId":"d12b7de9-25b6-478b-b15b-8fbed874b734"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/optiver_real_vol/kaggle/input/optiver-realized-volatility-prediction'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/kaggle/input/optiver-realized-volatility-prediction')\n","#os.chdir('/kaggle/input/optiver-realized-volatility-prediction')\n","\n","with open('correct_time_id_order.pkl','rb') as f:\n","  correct_time_id_order = pickle.load(f)\n","\n","data_dir = os.getcwd()\n","os.getcwd()"]},{"cell_type":"code","execution_count":null,"id":"nYjTnvn1yf_c","metadata":{"id":"nYjTnvn1yf_c"},"outputs":[],"source":["test = pd.read_csv('test.csv')['time_id']\n","train = pd.read_csv('train_full.csv')['time_id']\n","unordered_train_set = pd.DataFrame({'time_id':list(set(train) - set(test))})\n","unordered_train_set['time_id'] = unordered_train_set['time_id'].astype(int)\n","correct_time_id_order['time_id'] = correct_time_id_order['time_id'].astype(int)\n","\n","# Merge the two DataFrames to reorder the smaller list\n","new_correct_time_id_order = pd.merge(correct_time_id_order, unordered_train_set, on='time_id', how='inner')\n","del correct_time_id_order\n","\n","# Extract the reordered smaller list\n","correct_time_id_order = pd.DataFrame(new_correct_time_id_order['time_id'])\n","\n","\n","# for dir in sorted(glob.glob('trade_train_partial.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1])):\n","#   x = pd.read_parquet(dir)\n","#   print(x)\n","#   x = x[x['time_id'].isin( correct_time_id_order)]\n","#   parquet_file = glob.glob(os.path.join(dir, '*.parquet'))\n","#   x.to_parquet(parquet_file[0])\n","#   print(x)\n","\n","# for dir in sorted(glob.glob('book_train_partial.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1])):\n","#   x = pd.read_parquet(dir)\n","#   print(x)\n","#   x = x[x['time_id'].isin( correct_time_id_order)]\n","#   parquet_file = glob.glob(os.path.join(dir, '*.parquet'))\n","#   x.to_parquet(parquet_file[0])\n","#   print(x)\n"]},{"cell_type":"code","execution_count":null,"id":"bc2cbc97","metadata":{"id":"bc2cbc97"},"outputs":[],"source":["## Naming convention has t for transformation, e.g. tlog_1p for log transformation, texp for exp transformation\n","\n","class Train_Test_FeatureTransformation():\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def log_3p(self, col_name):\n","        \"\"\"log epsilon1 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 3)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_3p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_3p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_3p_test(self, x, mean, std,col_name):\n","        \"\"\"log epsilon1 transformation for test data\"\"\"\n","        x = np.log(x+ 3)\n","        #x = (x - mean) / std\n","        return x, 'tlog_3p_' + col_name\n","\n","    def log_1p(self, col_name):\n","        \"\"\"log epsilon1 transformation\"\"\"\n","        self.data[col_name] = np.log1p(self.data[col_name])\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_1p_test(self, x, mean, std,col_name):\n","        \"\"\"log epsilon1 transformation for test data\"\"\"\n","        x = np.log1p(x)\n","        #x = (x - mean) / std\n","        return x,'tlog_1p_' + col_name\n","\n","    def log_eps5e3(self, col_name):\n","        \"\"\"log epsilon3 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 0.005)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_eps523_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_eps523_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_eps5e3_test(self, x, mean, std,col_name):\n","        \"\"\"log epsilon3 transformation for test data\"\"\"\n","        x = np.log(x+ 0.005)\n","        #x = (x - mean) / std\n","        return x, 'tlog_eps523_' + col_name\n","\n","    def log_eps1e4(self, col_name):\n","        \"\"\"log epsilon4 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 0.0001)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def log_eps1e4_test(self, x, mean, std, col_name):\n","        \"\"\"log epsilon4 transformation for test data\"\"\"\n","        x = np.log(x+ 0.0001)\n","        #x = (x - mean) / std\n","        return x, 'tlog_eps1e4_' + col_name\n","\n","\n","    def log(self, col_name):\n","        \"\"\"log transformation\"\"\"\n","        val = np.log(self.data[col_name])\n","        val1 = (val - val.mean()) / val.std()\n","        #return val1, 'tlog_' + col_name , val.mean(), val.std()\n","        return val, 'tlog_' + col_name , val.mean(), val.std()\n","\n","    def log_test(self, x, mean, std, col_name):\n","        \"\"\"log transformation for test data\"\"\"\n","        x = np.log(x)\n","        #x = (x - mean) / std\n","        return x, 'tlog_' + col_name\n","\n","    def log_10p(self, col_name):\n","        \"\"\"log epsilon2 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 10)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_10p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_10p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_10p_test(self, x, mean, std, col_name):\n","        \"\"\"log epsilon2 transformation for test data\"\"\"\n","        x = np.log(x+ 10)\n","        #x = (x - mean) / std\n","        return x, 'tlog_10p_' + col_name\n","\n","    def log_log1p(self, col_name):\n","        \"\"\"log log epsilon1 transformation\"\"\"\n","        self.data[col_name] = np.log(np.log1p(self.data[col_name]))\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_tlog1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_tlog1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def log_log1p_test(self, x, mean, std, col_name):\n","        \"\"\"log log epsilon1 transformation for test data\"\"\"\n","        x = np.log(np.log1p(x))\n","        #x = (x - mean) / std\n","        return x, 'tlog_tlog1p_' + col_name\n","\n","    def log_log1p_eps1e4(self, col_name):\n","        \"\"\"log log epsilon4 transformation\"\"\"\n","        self.data[col_name] = np.log(np.log1p(self.data[col_name]+ 0.0001))\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_tlog1p_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name] , 'tlog_tlog1p_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def log_log1p_eps1e4_test(self, x, mean, std, col_name):\n","        \"\"\"log log epsilon4 transformation for test data\"\"\"\n","        x = np.log(np.log1p(x+ 0.0001))\n","        #x = (x - mean) / std\n","        return x, 'tlog_tlog1p_eps1e4_' + col_name\n","\n","\n","    def log_lin100_1(self, col_name):\n","        \"\"\"log of linear transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]*100 + 1)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_tlinear_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_tlinear_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_lin100_1_test(self, x, mean, std, col_name):\n","        \"\"\"log of linear transformation for test data\"\"\"\n","        x = np.log(x*100 + 1)\n","        #x = (x - mean) / std\n","        return x, 'tlog_tlinear_' + col_name\n","\n","    def standard_scaling(self, col_name):\n","        \"\"\"standard scaling transformation\"\"\"\n","        #self.data[col_name] = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return self.data[col_name], self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def standard_scaling_test(self, x, mean, std, col_name):\n","        \"\"\"standard scaling transformation for test data\"\"\"\n","        #x = (x - mean) / std\n","        return x, col_name\n","\n","    def exp(self, col_name):\n","        \"\"\"exp transformation\"\"\"\n","        self.data[col_name] = np.exp(self.data[col_name])\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def exp_test(self, x, mean, std, col_name):\n","        \"\"\"exp transformation for test data\"\"\"\n","        x = np.exp(x)\n","        #x = (x - mean) / std\n","        return x, 'texp_' + col_name\n","\n","    def exp_exp(self, col_name):\n","        \"\"\"exp exp transformation\"\"\"\n","        self.data[col_name] = np.exp(np.exp(self.data[col_name]))\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'texp_texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'texp_texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def exp_exp_test(self, x, mean, std, col_name):\n","        \"\"\"exp exp transformation for test data\"\"\"\n","        x = np.exp(np.exp(x))\n","        #x = (x - mean) / std\n","        return x, 'texp_texp_' + col_name\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ee5b8c31","metadata":{"id":"ee5b8c31"},"outputs":[],"source":["class create_training_n_inference_general_features( object  ):\n","    def __init__(self,ml_stage='training' ,):\n","        self.ml_stage = ml_stage\n","        if self.ml_stage == 'training':\n","            self.trade_paths = sorted(glob.glob('trade_train_partial.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.book_paths = sorted(glob.glob('book_train_partial.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.unique_stock_ids = []\n","            for path in self.book_paths:\n","                self.unique_stock_ids.append(int(path.split('=')[1]))\n","            self.train = pd.read_csv('train_partial.csv')\n","            self.train_target = pd.read_csv('train_partial.csv')\n","            self.all_uniq_time_ids = pd.DataFrame({'time_id':self.train['time_id'].unique()})\n","        elif self.ml_stage == 'inference':\n","            self.trade_paths = sorted(glob.glob('trade_test.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.book_paths = sorted(glob.glob('book_test.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.test = pd.read_csv('test.csv') #test1_sub.csv , test_sub.csv\n","            self.all_uniq_time_ids = pd.DataFrame({'time_id':self.test['time_id'].unique()})\n","            self.unique_stock_ids = []\n","            for path in self.book_paths:\n","                self.unique_stock_ids.append(int(path.split('=')[1]))\n","            self.train = pd.read_csv('train_partial.csv')\n","            self.train_target = pd.read_csv('train_partial.csv')\n","        else:\n","            print('invalid ml_stage param')\n","        return\n","\n","\n","\n","    def create_bk_level1_2_size_imbalance_feat(self,):\n","\n","        \"\"\"\n","        New features start\n","        \"\"\"\n","\n","        subset_paths = self.book_paths\n","        level = 1  # set level 1 or 2 for book_train data\n","        corr_method = 'spearman'  # set 'pearson' or 'spearman'\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","        bk_level1_2_size_imbalance_feat = {\n","            'bid_lvl2_min_lvl1_size_feat': pd.DataFrame(),\n","            'ask_lvl2_min_lvl1_size_feat': pd.DataFrame(),\n","            'lvl2_minus_lvl1_bid_n_ask_size_feat': pd.DataFrame()\n","        }\n","\n","        @njit\n","        def calculate_features_numba(bid_size1, bid_size2, ask_size1, ask_size2):\n","            bid_lvl2_min_lvl1_size_feat = np.minimum(bid_size2 - bid_size1, 0)\n","            ask_lvl2_min_lvl1_size_feat = np.minimum(ask_size2 - ask_size1, 0)\n","            lvl2_minus_lvl1_bid_n_ask_size_feat = np.minimum((bid_size2 + ask_size2) - (bid_size1 + ask_size1), 0)\n","            return bid_lvl2_min_lvl1_size_feat, ask_lvl2_min_lvl1_size_feat, lvl2_minus_lvl1_bid_n_ask_size_feat\n","\n","        def calculate_features(book_train_st):\n","            # Extract the necessary numpy arrays\n","            bid_size1 = book_train_st[\"bid_size1\"].values\n","            bid_size2 = book_train_st[\"bid_size2\"].values\n","            ask_size1 = book_train_st[\"ask_size1\"].values\n","            ask_size2 = book_train_st[\"ask_size2\"].values\n","\n","            # Perform the calculations using numba\n","            bid_feat, ask_feat, level_feat = calculate_features_numba(bid_size1, bid_size2, ask_size1, ask_size2)\n","\n","            # Add the results back to the DataFrame\n","            book_train_st[\"bid_lvl2_min_lvl1_size_feat\"] = bid_feat\n","            book_train_st[\"ask_lvl2_min_lvl1_size_feat\"] = ask_feat\n","            book_train_st[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = level_feat\n","\n","            # Aggregate the features by time_id\n","            bid_sum = book_train_st.groupby('time_id')[\"bid_lvl2_min_lvl1_size_feat\"].sum()\n","            ask_sum = book_train_st.groupby('time_id')[\"ask_lvl2_min_lvl1_size_feat\"].sum()\n","            level_sum = book_train_st.groupby('time_id')[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"].sum()\n","\n","            # Transformation to make the data more normal\n","            bid_sum = np.log1p((-bid_sum) ** 0.5)\n","            ask_sum = np.log1p((-ask_sum) ** 0.5)\n","            level_sum = np.log1p((-level_sum) ** 0.5)\n","\n","            return bid_sum, ask_sum, level_sum\n","\n","        def process_path_train(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            target_st = self.train_target.loc[self.train_target['stock_id'] == st_id].set_index(\"time_id\")\n","            book_train_st = pd.read_parquet(path)\n","            bid_sum, ask_sum, level_sum = calculate_features(book_train_st)\n","            result = {\n","                'bid_lvl2_min_lvl1_size_feat': bid_sum.reindex(target_st.index).ffill().bfill(),\n","                'ask_lvl2_min_lvl1_size_feat': ask_sum.reindex(target_st.index).ffill().bfill(),\n","                'lvl2_minus_lvl1_bid_n_ask_size_feat': level_sum.reindex(target_st.index).ffill().bfill(),\n","            }\n","            return result\n","\n","        def process_path_inference(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","            book_train_st = pd.read_parquet(path)\n","            bid_sum, ask_sum, level_sum = calculate_features(book_train_st)\n","            result = {\n","                'bid_lvl2_min_lvl1_size_feat': bid_sum.reindex(target_st.index),\n","                'ask_lvl2_min_lvl1_size_feat': ask_sum.reindex(target_st.index),\n","                'lvl2_minus_lvl1_bid_n_ask_size_feat': level_sum.reindex(target_st.index),\n","            }\n","            return result\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_path_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_path_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_level1_2_size_imbalance_feat[key] = pd.concat([bk_level1_2_size_imbalance_feat[key], value], axis=0)\n","\n","        return bk_level1_2_size_imbalance_feat\n","\n","\n","\n","    def create_trade_sum_size_sum_order_count_sum_size_per_order_count(self,):\n","\n","\n","        subset_paths = self.trade_paths\n","        # Set parameters\n","        corr_method = 'spearman'  # set 'pearson' or 'spearman'\n","        file = 'trade_train'  # set 'book_train' or 'trade_train'\n","\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","\n","        trade_sum_size_sum_order_count_sum_size_per_order_count = {\n","            'sum_size': pd.DataFrame(),\n","            'sum_order_count': pd.DataFrame(),\n","            'sum_size_per_order_count': pd.DataFrame()\n","        }\n","\n","        @njit\n","        def calculate_sum_and_normalize(values):\n","            sum_values = np.sum(values)\n","            normalized_sum = np.log(sum_values)\n","            return normalized_sum\n","\n","        def process_stock_data_train(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Compute sum and normalize\n","            st_sum_size = trade_train_st.groupby(by='time_id')[size].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            st_sum_order_count = trade_train_st.groupby(by='time_id')[order_count].apply(lambda x: calculate_sum_and_normalize(x.values))\n","\n","            trade_train_st[\"size_per_order_count\"] = trade_train_st[\"size\"] / trade_train_st[\"order_count\"]\n","            st_size_per_order_count = trade_train_st.groupby(by='time_id')[\"size_per_order_count\"].apply(lambda x: calculate_sum_and_normalize(x.values))\n","\n","            result = {\n","                'sum_size': st_sum_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'sum_order_count': st_sum_order_count.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'sum_size_per_order_count': st_size_per_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","        def process_stock_data_inference(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            print(path)\n","            trade_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","            # Compute sum and normalize\n","            st_sum_size = trade_train_st.groupby(by='time_id')[size].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            st_sum_order_count = trade_train_st.groupby(by='time_id')[order_count].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            trade_train_st[\"size_per_order_count\"] = trade_train_st[\"size\"] / trade_train_st[\"order_count\"]\n","            st_size_per_order_count = trade_train_st.groupby(by='time_id')[\"size_per_order_count\"].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            result = {\n","                'sum_size': st_sum_size.reindex(target_st.index),\n","                'sum_order_count': st_sum_order_count.reindex(target_st.index),\n","                'sum_size_per_order_count': st_size_per_order_count.reindex(target_st.index)\n","            }\n","            return result\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_sum_size_sum_order_count_sum_size_per_order_count[key] = pd.concat([trade_sum_size_sum_order_count_sum_size_per_order_count[key], value], axis=0)\n","\n","        ####### Saving File #######\n","        ## The trade_sum_size_sum_order_count_sum_size_per_order_count is saved later below\n","        return trade_sum_size_sum_order_count_sum_size_per_order_count\n","        \"\"\"\n","        New features end\n","        \"\"\"\n","\n","\n","\n","\n","    def create_bk_price_size_min_max_range(self,):\n","\n","        # \"\"\"\n","        # old features start\n","        # \"\"\"\n","\n","        # ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","\n","        level = 1\n","        subset_paths = self.book_paths\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","        # Define your custom range function for price and size\n","        @njit\n","        def my_range_price(values):\n","            return np.max(values) - np.min(values)\n","\n","        # Initialize dictionaries\n","        bk_price_size_min_max_range = {\n","            'st_min_max_bid_price'+str(level): pd.DataFrame(),\n","            'st_min_max_ask_price'+str(level): pd.DataFrame(),\n","            'st_min_max_bid_size'+str(level): pd.DataFrame(),\n","            'st_min_max_ask_size'+str(level): pd.DataFrame(),\n","            'st_range_ask_price'+str(level): pd.DataFrame(),\n","            'st_range_bid_price'+str(level): pd.DataFrame(),\n","            'st_range_ask_size'+str(level): pd.DataFrame(),\n","            'st_range_bid_size'+str(level): pd.DataFrame()\n","        }\n","\n","        @njit\n","        def calculate_min_max_range(values):\n","            min_val = np.min(values)\n","            max_val = np.max(values)\n","            return min_val, max_val\n","\n","        def process_book_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate min and max\n","            bid_price_min_max = book_train_st.groupby(by='time_id')[bid_price].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_price_min_max = book_train_st.groupby(by='time_id')[ask_price].apply(lambda x: calculate_min_max_range(x.values))\n","            bid_size_min_max = book_train_st.groupby(by='time_id')[bid_size].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_size_min_max = book_train_st.groupby(by='time_id')[ask_size].apply(lambda x: calculate_min_max_range(x.values))\n","\n","            min_max_bid_price = pd.DataFrame(bid_price_min_max.tolist(), index=bid_price_min_max.index, columns=['min_bid_price', 'max_bid_price'])\n","            min_max_ask_price = pd.DataFrame(ask_price_min_max.tolist(), index=ask_price_min_max.index, columns=['min_ask_price', 'max_ask_price'])\n","            min_max_bid_size = pd.DataFrame(bid_size_min_max.tolist(), index=bid_size_min_max.index, columns=['min_bid_size', 'max_bid_size'])\n","            min_max_ask_size = pd.DataFrame(ask_size_min_max.tolist(), index=ask_size_min_max.index, columns=['min_ask_size', 'max_ask_size'])\n","\n","            # Calculate ranges\n","            range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: lambda x: my_range_price(x.values)}).rename(columns={ask_price: 'range_ask_price'})\n","            range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: lambda x: my_range_price(x.values)}).rename(columns={bid_price: 'range_bid_price'})\n","            range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: lambda x: my_range_price(x.values)}).rename(columns={ask_size: 'range_ask_size'})\n","            range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: lambda x: my_range_price(x.values)}).rename(columns={bid_size: 'range_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_max_bid_price'+str(level): min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_max_ask_price'+str(level): min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_max_bid_size'+str(level): min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_max_ask_size'+str(level): min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_ask_price'+str(level): range_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_bid_price'+str(level): range_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_ask_size'+str(level): range_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_bid_size'+str(level): range_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","        def process_book_data_inference(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","            # Calculate min and max\n","            bid_price_min_max = book_train_st.groupby(by='time_id')[bid_price].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_price_min_max = book_train_st.groupby(by='time_id')[ask_price].apply(lambda x: calculate_min_max_range(x.values))\n","            bid_size_min_max = book_train_st.groupby(by='time_id')[bid_size].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_size_min_max = book_train_st.groupby(by='time_id')[ask_size].apply(lambda x: calculate_min_max_range(x.values))\n","\n","            min_max_bid_price = pd.DataFrame(bid_price_min_max.tolist(), index=bid_price_min_max.index, columns=['min_bid_price', 'max_bid_price'])\n","            min_max_ask_price = pd.DataFrame(ask_price_min_max.tolist(), index=ask_price_min_max.index, columns=['min_ask_price', 'max_ask_price'])\n","            min_max_bid_size = pd.DataFrame(bid_size_min_max.tolist(), index=bid_size_min_max.index, columns=['min_bid_size', 'max_bid_size'])\n","            min_max_ask_size = pd.DataFrame(ask_size_min_max.tolist(), index=ask_size_min_max.index, columns=['min_ask_size', 'max_ask_size'])\n","\n","            # Calculate ranges\n","            range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: lambda x: my_range_price(x.values)}).rename(columns={ask_price: 'range_ask_price'})\n","            range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: lambda x: my_range_price(x.values)}).rename(columns={bid_price: 'range_bid_price'})\n","            range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: lambda x: my_range_price(x.values)}).rename(columns={ask_size: 'range_ask_size'})\n","            range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: lambda x: my_range_price(x.values)}).rename(columns={bid_size: 'range_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_max_bid_price'+str(level): min_max_bid_price.reindex(target_st.index),\n","                'st_min_max_ask_price'+str(level): min_max_ask_price.reindex(target_st.index),\n","                'st_min_max_bid_size'+str(level): min_max_bid_size.reindex(target_st.index),\n","                'st_min_max_ask_size'+str(level): min_max_ask_size.reindex(target_st.index),\n","                'st_range_ask_price'+str(level): range_ask_price.reindex(target_st.index),\n","                'st_range_bid_price'+str(level): range_bid_price.reindex(target_st.index),\n","                'st_range_ask_size'+str(level): range_ask_size.reindex(target_st.index),\n","                'st_range_bid_size'+str(level): range_bid_size.reindex(target_st.index)\n","            }\n","\n","            return result\n","\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_price_size_min_max_range[key] = pd.concat([bk_price_size_min_max_range[key], value], axis=0)\n","\n","        \"\"\"\n","        Old features end\n","        \"\"\"\n","        return bk_price_size_min_max_range\n","\n","\n","\n","\n","\n","\n","    def create_bk_price_size_sad(self,):\n","\n","        ## 7f) - 7i) Check if  the sum of absolute differences is correlated with target\n","\n","\n","        subset_paths = self.book_paths\n","        level = 1\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","        # Define your custom SAD function\n","        @njit\n","        def my_sum_abs_diff(values):\n","            return np.sum(np.abs(np.diff(values)))\n","\n","        # Initialize dictionaries\n","        bk_price_size_sad = {\n","            'st_sad_ask_price'+str(level): pd.DataFrame(),\n","            'st_sad_ask_size'+str(level): pd.DataFrame(),\n","            'st_sad_bid_price'+str(level): pd.DataFrame(),\n","            'st_sad_bid_size'+str(level): pd.DataFrame()\n","        }\n","\n","        def process_book_data_sad_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate SAD\n","            def calculate_sad(series):\n","                return my_sum_abs_diff(series.values)\n","\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: calculate_sad}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: calculate_sad}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: calculate_sad}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: calculate_sad}).rename(columns={bid_size: 'sad_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_ask_price'+str(level): st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_ask_size'+str(level): st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_bid_price'+str(level): st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_bid_size'+str(level): st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","        def process_book_data_sad_inference(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","            # Calculate SAD\n","            def calculate_sad(series):\n","                return my_sum_abs_diff(series.values)\n","\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: calculate_sad}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: calculate_sad}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: calculate_sad}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: calculate_sad}).rename(columns={bid_size: 'sad_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_ask_price'+str(level): st_sad_ask_price.reindex(target_st.index),\n","                'st_sad_ask_size'+str(level): st_sad_ask_size.reindex(target_st.index),\n","                'st_sad_bid_price'+str(level): st_sad_bid_price.reindex(target_st.index),\n","                'st_sad_bid_size'+str(level): st_sad_bid_size.reindex(target_st.index)\n","            }\n","\n","            return result\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_sad_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_sad_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_price_size_sad[key] = pd.concat([bk_price_size_sad[key], value], axis=0)\n","\n","        ######## SAVING FILE\n","        # This bk_price_size_sad is saved later below\n","\n","        return bk_price_size_sad\n","\n","\n","\n","\n","\n","    def create_bk_size_price_corr(self,):\n","\n","        ## 7g) - 7j) Check if the correlation of any pair of bidsize1,bid_price1,asksize1,ask_price1 is correlated with target realized volatitlity for all the time_ids?\n","\n","        level=1\n","        subset_paths = self.book_paths\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","\n","        def process_book_data_corr_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            # Reindex with target_st time_ids and apply forward and backward fill\n","            for key in corr_dict.keys():\n","                corr_dict[key] = corr_dict[key].reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return corr_dict\n","\n","\n","        def process_book_data_corr_inference(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            # Reindex with target_st time_ids and apply forward and backward fill\n","            for key in corr_dict.keys():\n","                corr_dict[key] = corr_dict[key].reindex(target_st.index)\n","\n","            return corr_dict\n","\n","\n","        # Initialize dictionaries\n","        bk_size_price_corr = {\n","            'st_bs_bp_corr'+str(level): pd.DataFrame(),\n","            'st_bs_as_corr'+str(level): pd.DataFrame(),\n","            'st_bs_ap_corr'+str(level): pd.DataFrame(),\n","            'st_bp_as_corr'+str(level): pd.DataFrame(),\n","            'st_bp_ap_corr'+str(level): pd.DataFrame(),\n","            'st_as_ap_corr'+str(level): pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_corr_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_corr_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_size_price_corr[key+str(level)] = pd.concat([bk_size_price_corr[key+str(level)], value], axis=0)\n","\n","        ######## saving FILE\n","        # This bk_size_price_corr is saved later below\n","\n","        return bk_size_price_corr\n","\n","\n","\n","\n","\n","    def create_trade_price_size_order_count_min_max_range(self,):\n","\n","        ## SET PARAMETERS HERE for trade_train.parquet files ONLY!!\n","        ## set the Correlation method\n","\n","        corr_method = 'pearson' # set 'pearson' or 'spearman'\n","        file = 'trade_train' # set 'book_train' or 'trade_train'\n","\n","        price = \"price\"\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","        ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","        subset_paths = self.trade_paths\n","\n","        # Define custom range function\n","\n","        def my_range_price(values):\n","            return np.max(values) - np.min(values)\n","\n","        # Define a function to calculate min, max, and range\n","        def calculate_min_max_range(df, column, range_func):\n","            min_max = df.groupby(by='time_id')[column].agg(['min', 'max']).rename(columns={'min': f'min_{column}', 'max': f'max_{column}'})\n","            range_df = df.groupby(by='time_id').agg({column: [range_func]}).rename(columns={column: f'range_{column}'})\n","            range_df.columns = range_df.columns.droplevel(1)\n","            return min_max, range_df\n","\n","        def process_trade_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate min, max, and range for each column\n","            min_max_price, range_price = calculate_min_max_range(trade_train_st, price, my_range_price)\n","            min_max_size, range_size = calculate_min_max_range(trade_train_st, size, my_range_price)\n","            min_max_order_count, range_order_count = calculate_min_max_range(trade_train_st, order_count, my_range_price)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_price': min_max_price['min_price'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_max_price': min_max_price['max_price'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_size': min_max_size['min_size'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_max_size': min_max_size['max_size'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_order_count': min_max_order_count['min_order_count'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_max_order_count': min_max_order_count['max_order_count'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_price': range_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_size': range_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_order_count': range_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","\n","        def process_trade_data_inference(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            trade_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","\n","            # Calculate min, max, and range for each column\n","            min_max_price, range_price = calculate_min_max_range(trade_train_st, price, my_range_price)\n","            min_max_size, range_size = calculate_min_max_range(trade_train_st, size, my_range_price)\n","            min_max_order_count, range_order_count = calculate_min_max_range(trade_train_st, order_count, my_range_price)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_price': min_max_price['min_price'].reindex(target_st.index),\n","                'st_max_price': min_max_price['max_price'].reindex(target_st.index),\n","                'st_min_size': min_max_size['min_size'].reindex(target_st.index),\n","                'st_max_size': min_max_size['max_size'].reindex(target_st.index),\n","                'st_min_order_count': min_max_order_count['min_order_count'].reindex(target_st.index),\n","                'st_max_order_count': min_max_order_count['max_order_count'].reindex(target_st.index),\n","                'st_range_price': range_price.reindex(target_st.index),\n","                'st_range_size': range_size.reindex(target_st.index),\n","                'st_range_order_count': range_order_count.reindex(target_st.index)\n","            }\n","\n","            return result\n","\n","        # Initialize dictionaries\n","        trade_price_size_order_count_min_max_range = {\n","            'st_min_price': pd.DataFrame(),\n","            'st_max_price': pd.DataFrame(),\n","            'st_min_size': pd.DataFrame(),\n","            'st_max_size': pd.DataFrame(),\n","            'st_min_order_count': pd.DataFrame(),\n","            'st_max_order_count': pd.DataFrame(),\n","            'st_range_price': pd.DataFrame(),\n","            'st_range_size': pd.DataFrame(),\n","            'st_range_order_count': pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_price_size_order_count_min_max_range[key] = pd.concat([trade_price_size_order_count_min_max_range[key], value], axis=0)\n","\n","        ######## Saving File\n","        # This trade_price_size_order_count_min_max_range is saved below\n","\n","        return trade_price_size_order_count_min_max_range\n","\n","\n","\n","\n","\n","    def create_trade_price_size_order_count_sad(self,):\n","\n","        subset_paths = self.trade_paths\n","\n","        price = \"price\"\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","        # Define custom sum of absolute differences function\n","\n","        def my_sum_abs_diff(values):\n","            return np.sum(np.abs(np.diff(values)))\n","\n","        # Define a function to calculate sum of absolute differences\n","        def calculate_sad(df, column, sad_func):\n","            sad_df = df.groupby(by='time_id').agg({column: [sad_func]}).rename(columns={column: f'sad_{column}'})\n","            sad_df.columns = sad_df.columns.droplevel(1)\n","            return sad_df\n","\n","        def process_trade_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate sum of absolute differences for each column\n","            sad_price = calculate_sad(trade_train_st, price, my_sum_abs_diff)\n","            sad_size = calculate_sad(trade_train_st, size, my_sum_abs_diff)\n","            sad_order_count = calculate_sad(trade_train_st, order_count, my_sum_abs_diff)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_price': sad_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_size': sad_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_order_count': sad_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","\n","        def process_trade_data_inference(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","\n","            trade_train_st = pd.read_parquet(path)\n","            # Calculate sum of absolute differences for each column\n","            sad_price = calculate_sad(trade_train_st, price, my_sum_abs_diff)\n","            sad_size = calculate_sad(trade_train_st, size, my_sum_abs_diff)\n","            sad_order_count = calculate_sad(trade_train_st, order_count, my_sum_abs_diff)\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_price': sad_price.reindex(target_st.index),\n","                'st_sad_size': sad_size.reindex(target_st.index),\n","                'st_sad_order_count': sad_order_count.reindex(target_st.index)\n","            }\n","            return result\n","\n","        # Initialize dictionaries\n","        trade_price_size_order_count_sad = {\n","            'st_sad_price': pd.DataFrame(),\n","            'st_sad_size': pd.DataFrame(),\n","            'st_sad_order_count': pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_price_size_order_count_sad[key] = pd.concat([trade_price_size_order_count_sad[key], value], axis=0)\n","\n","        ######## Saving File\n","        # This trade_price_size_order_count_sad is saved below\n","\n","        return trade_price_size_order_count_sad\n","\n","\n","\n","\n","    def create_trade_price_size_order_count_corr(self,):\n","\n","        subset_paths = self.trade_paths\n","\n","        price = \"price\"\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","        # Define a function to calculate correlation for given columns\n","        def calculate_corr(df, col1, col2):\n","            corr_df = df.groupby('time_id')[[col1, col2]].corr().iloc[0::2, -1]\n","            corr_df.index = corr_df.index.droplevel(1)\n","            return corr_df\n","\n","        def process_trade_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations\n","            size_order_count_corr = calculate_corr(trade_train_st, size, order_count)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_size_order_count_corr': size_order_count_corr.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","            return result\n","\n","        def process_trade_data_inference(path):\n","            trade_train_st = pd.read_parquet(path)\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","\n","            # Calculate correlations\n","            size_order_count_corr = calculate_corr(trade_train_st, size, order_count)\n","            # Reindex and concatenate\n","            result = {\n","                'st_size_order_count_corr': size_order_count_corr.reindex(target_st.index)\n","            }\n","            return result\n","\n","        # Initialize dictionary\n","        trade_price_size_order_count_corr = {\n","            'st_size_order_count_corr': pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_price_size_order_count_corr[key] = pd.concat([trade_price_size_order_count_corr[key], value], axis=0)\n","\n","        return trade_price_size_order_count_corr\n","    \"\"\"old features end\"\"\"\n","\n","\n","\n","\n","    def create_trade_price_n_wap1_deviation_df_AND_trade_price_n_wap_eqi_price0_deviation_df(self,):\n","\n","        # Define the Numba-accelerated function for WAP1 price calculation\n","        @njit\n","        def compute_wap1(bid_price1, ask_price1, bid_size1, ask_size1):\n","            return (bid_price1 * ask_size1 + ask_price1 * bid_size1) / (bid_size1 + ask_size1)\n","\n","        # Function to process trade parquet files\n","        def process_trade_file(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            trade_train_st = pd.read_parquet(path)\n","            trade_train_st['stock_id'] = st_id\n","            return trade_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'price']].rename(columns={'price': 'trade_price'})\n","\n","        # Function to process book parquet files\n","        def process_book_file(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            book_train_st['stock_id'] = st_id\n","            # Apply Numba-accelerated WAP1 calculation\n","            book_train_st['wap1_price'] = compute_wap1(\n","                book_train_st['bid_price1'].values,\n","                book_train_st['ask_price1'].values,\n","                book_train_st['bid_size1'].values,\n","                book_train_st['ask_size1'].values\n","            )\n","            return book_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'wap1_price']]\n","\n","\n","        # Use joblib's Parallel and delayed to process trade files in parallel\n","        trade_dfs = Parallel(n_jobs=-1)(delayed(process_trade_file)(path) for path in self.trade_paths)\n","\n","        # Use joblib's Parallel and delayed to process book files in parallel\n","        book_dfs = Parallel(n_jobs=-1)(delayed(process_book_file)(path) for path in self.book_paths)\n","\n","        # Concatenate the DataFrames into the final DataFrames\n","        trade_price_df = pd.concat(trade_dfs, axis=0)\n","        book_price_df = pd.concat(book_dfs, axis=0)\n","\n","        ##### merge trade_price and book_price data and calculate trade_price_n_wap1_deviation #####\n","        trade_price_n_wap1_deviation = pd.DataFrame()\n","\n","        # Group by 'stock_id' and perform operations in one go\n","        for st, trade_price_st in trade_price_df.groupby('stock_id'):\n","            book_price_st = book_price_df[book_price_df['stock_id'] == st]\n","\n","            # Ensure consistent types for merging using .loc to avoid SettingWithCopyWarning\n","            trade_price_st.loc[:, 'time_id'] = trade_price_st['time_id'].astype(int)\n","            book_price_st.loc[:, 'time_id'] = book_price_st['time_id'].astype(int)\n","            trade_price_st.loc[:, 'seconds_in_bucket'] = trade_price_st['seconds_in_bucket'].astype(int)\n","            book_price_st.loc[:, 'seconds_in_bucket'] = book_price_st['seconds_in_bucket'].astype(int)\n","\n","            # Merge trade and book data on 'stock_id', 'time_id', 'seconds_in_bucket'\n","            merged = trade_price_st.merge(book_price_st, on=['stock_id', 'time_id', 'seconds_in_bucket'], how='inner')\n","\n","            # Calculate the ratio with vectorized operations\n","            merged['ratio'] = (merged['wap1_price'] / merged['trade_price'])**0.5\n","\n","            # Group by 'stock_id' and 'time_id', and calculate standard deviation of 'ratio'\n","            temp_df1 = merged.groupby(['stock_id', 'time_id'])['ratio'].std().reset_index()\n","\n","            # Append to the final DataFrame\n","            trade_price_n_wap1_deviation = pd.concat([trade_price_n_wap1_deviation, temp_df1], axis=0)\n","\n","        ####### Calculate correlation between trade_price_n_wap1_deviation and target ########\n","        #merged_df = train.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","        if self.ml_stage == 'training':\n","          trade_price_n_wap1_deviation_df = self.train.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","        elif self.ml_stage == 'inference':\n","          #trade_price_n_wap1_deviation_df = trade_price_n_wap1_deviation\n","          trade_price_n_wap1_deviation_df = self.test.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left')\n","        del book_price_df, trade_dfs, book_dfs, trade_price_st, book_price_st, merged\n","\n","\n","        # Assuming find_equilibrium_price is already defined elsewhere\n","        # Function to process book parquet files and calculate WAP equilibrium price\n","        def process_book_eqi_file(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            book_train_st['stock_id'] = st_id\n","\n","            # Avoid log(0) errors with a small adjustment to prices if necessary\n","            book_train_st['log_ask_price2'] = book_train_st['ask_price2'] #.replace(0, np.nan)\n","            book_train_st['log_bid_price2'] = book_train_st['bid_price2'] #.replace(0, np.nan)\n","            book_train_st['log_ask_price1'] = book_train_st['ask_price1'] #.replace(0, np.nan)\n","            book_train_st['log_bid_price1'] = book_train_st['bid_price1'] #.replace(0, np.nan)\n","\n","            # Calculate the equilibrium price\n","            book_train_st['wap_eqi_price0'] = find_equilibrium_price(book_train_st, 0)\n","\n","            return book_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'wap_eqi_price0']]\n","\n","\n","        # Use joblib's Parallel and delayed to process book files in parallel\n","        book_eqi_dfs = Parallel(n_jobs=-1)(delayed(process_book_eqi_file)(path) for path in self.book_paths)\n","\n","        # Concatenate the DataFrames into the final DataFrame\n","        book_eqi_price_df = pd.concat(book_eqi_dfs, axis=0)\n","\n","        ##### Merge trade_price and book_price data and calculate trade_price_n_wap_eqi_price0_deviation #####\n","        trade_price_n_wap_eqi_price0_deviation = pd.DataFrame()\n","\n","        # Group by 'stock_id' and perform operations in one go\n","        for st, trade_price_st in trade_price_df.groupby('stock_id'):\n","            book_price_st = book_eqi_price_df[book_eqi_price_df['stock_id'] == st]\n","\n","            # Ensure consistent types for merging using .loc to avoid SettingWithCopyWarning\n","            trade_price_st.loc[:, 'time_id'] = trade_price_st['time_id'].astype(int)\n","            book_price_st.loc[:, 'time_id'] = book_price_st['time_id'].astype(int)\n","            trade_price_st.loc[:, 'seconds_in_bucket'] = trade_price_st['seconds_in_bucket'].astype(int)\n","            book_price_st.loc[:, 'seconds_in_bucket'] = book_price_st['seconds_in_bucket'].astype(int)\n","\n","            # Merge trade and book data on 'stock_id', 'time_id', 'seconds_in_bucket'\n","            merged1 = trade_price_st.merge(book_price_st, on=['stock_id', 'time_id', 'seconds_in_bucket'], how='inner')\n","\n","            # Calculate the ratio with vectorized operations\n","            merged1['ratio'] = (merged1['wap_eqi_price0'] / (merged1['trade_price'] ) )**0.5\n","\n","            # Group by 'stock_id' and 'time_id', and calculate standard deviation of 'ratio'\n","            temp_df1 = merged1.groupby(['stock_id', 'time_id'])['ratio'].std().reset_index()\n","\n","            # Append to the final DataFrame\n","            trade_price_n_wap_eqi_price0_deviation = pd.concat([trade_price_n_wap_eqi_price0_deviation, temp_df1], axis=0)\n","\n","        ####### Calculate correlation between trade_price_n_wap_eqi_price0_deviation and target ########\n","        if self.ml_stage == 'training':\n","          trade_price_n_wap_eqi_price0_deviation_df = self.train.merge(trade_price_n_wap_eqi_price0_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","        elif self.ml_stage == 'inference':\n","          #trade_price_n_wap_eqi_price0_deviation_df = trade_price_n_wap_eqi_price0_deviation\n","          trade_price_n_wap_eqi_price0_deviation_df = self.test.merge(trade_price_n_wap_eqi_price0_deviation, on=['stock_id', 'time_id'], how='left')\n","        del trade_price_df, trade_price_st, book_price_st, merged1, temp_df1, book_eqi_price_df, book_eqi_dfs\n","\n","        return trade_price_n_wap1_deviation_df,  trade_price_n_wap_eqi_price0_deviation_df\n","\n","\n","\n","\n","    def merge_trade_price_n_wap_eqi_price0_deviation_df(self,bk_level1_2_size_imbalance_feat,trade_sum_size_sum_order_count_sum_size_per_order_count,trade_price_n_wap1_deviation_df,trade_price_n_wap_eqi_price0_deviation_df, feat_df):\n","        for key in bk_level1_2_size_imbalance_feat.keys():\n","            bk_level1_2_size_imbalance_feat[key].index = range(bk_level1_2_size_imbalance_feat[key].shape[0])\n","            bk_level1_2_size_imbalance_feat[key].columns = [key]\n","            feat_df = feat_df.merge(bk_level1_2_size_imbalance_feat[key], left_index=True, right_index=True)\n","        del bk_level1_2_size_imbalance_feat\n","\n","        for key in trade_sum_size_sum_order_count_sum_size_per_order_count.keys():\n","            trade_sum_size_sum_order_count_sum_size_per_order_count[key].index = range(trade_sum_size_sum_order_count_sum_size_per_order_count[key].shape[0])\n","            trade_sum_size_sum_order_count_sum_size_per_order_count[key].columns = [key]\n","            feat_df = feat_df.merge(trade_sum_size_sum_order_count_sum_size_per_order_count[key], left_index=True, right_index=True)\n","        del trade_sum_size_sum_order_count_sum_size_per_order_count\n","\n","        trade_price_n_wap1_deviation_df[\"trade_price_n_wap1_dev\"] = trade_price_n_wap1_deviation_df[\"ratio\"]**0.5\n","        trade_price_n_wap1_deviation_df.drop(columns=[\"ratio\"], inplace=True)\n","        feat_df = feat_df.merge(trade_price_n_wap1_deviation_df, on=['stock_id', 'time_id'], how='left')\n","        del trade_price_n_wap1_deviation_df\n","\n","        trade_price_n_wap_eqi_price0_deviation_df[\"trade_price_n_wap_eqi_price0_dev\"] = trade_price_n_wap_eqi_price0_deviation_df[\"ratio\"]**0.5\n","        trade_price_n_wap_eqi_price0_deviation_df.drop(columns=[\"ratio\"], inplace=True)\n","        feat_df = feat_df.merge(trade_price_n_wap_eqi_price0_deviation_df, on=['stock_id', 'time_id'], how='left')\n","        del trade_price_n_wap_eqi_price0_deviation_df\n","\n","        return feat_df\n","\n","\n","    def log_return(self,series):\n","        return np.log(series).diff()\n","\n","    def realized_volatility(self,series_log_return):\n","        return np.sqrt(np.sum(series_log_return**2))\n","\n","\n","    def create_df_20_min_volatility(self,):\n","\n","        # def log_return(series):\n","        #     return np.log(series).diff()\n","\n","        # def realized_volatility(series_log_return):\n","        #     return np.sqrt(np.sum(series_log_return**2))\n","\n","        def realized_volatility_per_time_id(file_path, prediction_column_name):\n","            df_book_data = pd.read_parquet(file_path)\n","\n","            # Calculate WAP\n","            df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1'] +\n","                                  df_book_data['ask_price1'] * df_book_data['bid_size1']) / (\n","                                  df_book_data['bid_size1'] + df_book_data['ask_size1'])\n","\n","            # Calculate log return\n","            df_book_data['log_return'] = df_book_data.groupby('time_id')['wap'].transform(self.log_return)\n","\n","            # Remove rows with NaN log returns\n","            df_book_data = df_book_data.dropna(subset=['log_return'])\n","\n","            # Calculate realized volatility per time_id\n","            df_realized_vol_per_stock = (df_book_data.groupby('time_id')['log_return']\n","                                        .agg(self.realized_volatility)\n","                                        .reset_index(name=prediction_column_name))\n","\n","            stock_id = file_path.split('=')[1]\n","            df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x: f'{stock_id}-{x}')\n","\n","            return df_realized_vol_per_stock[['row_id', prediction_column_name]]\n","\n","        def past_realized_volatility_per_stock(list_file, prediction_column_name):\n","            # Parallel processing of files\n","            results = Parallel(n_jobs=-1)(delayed(realized_volatility_per_time_id)(file, prediction_column_name) for file in list_file)\n","            df_past_realized = pd.concat(results, ignore_index=True)\n","            return df_past_realized\n","\n","        list_order_book_file_train = self.book_paths\n","\n","        df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n","                                                                  prediction_column_name='first_10_min_vol')\n","\n","        if self.ml_stage == 'training':\n","\n","          # Prepare self.train DataFrame\n","          self.train['row_id'] = self.train['stock_id'].astype(str) + '-' + self.train['time_id'].astype(str)\n","          # Merge with past realized volatility\n","          df_20_min_volatility = self.train[['row_id', 'target']].merge(df_past_realized_train[['row_id', 'first_10_min_vol']],\n","                                            on='row_id', how='left')\n","\n","        elif self.ml_stage == 'inference':\n","          # Merge with past realized volatility\n","          #df_20_min_volatility = df_past_realized_train\n","          #pd.DataFrame(self.test['row_id'])\n","          df_20_min_volatility = pd.DataFrame(self.test['row_id']).merge(df_past_realized_train[['row_id', 'first_10_min_vol']],\n","                                    on='row_id', how='left')\n","\n","        # Extract stock_id and time_id\n","        df_20_min_volatility[['stock_id', 'time_id']] = df_20_min_volatility['row_id'].str.split('-', expand=True)\n","\n","        return df_20_min_volatility\n","\n","\n","\n","\n","\n","\n","    def create_n_merge_trade_price_std_df(self,feat_df):\n","\n","        subset_paths = self.trade_paths\n","\n","\n","        def process_trade_data(path, idx):\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate statistics for price\n","            std_price = trade_train_st.groupby('time_id')['price'].std()\n","            std_price = pd.merge(idx, std_price, left_index=True, right_index=True, how='left')\n","            std_price.fillna(std_price['price'].mean(), inplace=True)\n","\n","            log_ret_price = trade_train_st.groupby('time_id')['price'].apply(self.log_return)\n","            trade_train_st['log_ret_price'] = trade_train_st.groupby('time_id')['price'].transform(self.log_return)\n","            trade_train_st = trade_train_st[~trade_train_st['log_ret_price'].isnull()]\n","            real_vol_price = trade_train_st.groupby('time_id')['log_ret_price'].agg(self.realized_volatility)\n","            real_vol_price = pd.merge(idx, real_vol_price, left_index=True, right_index=True, how='left')\n","            real_vol_price.fillna(real_vol_price['log_ret_price'].mean(), inplace=True)\n","\n","            # Calculate statistics for size\n","            std_size = trade_train_st.groupby('time_id')['size'].std()\n","            std_size = pd.merge(idx, std_size, left_index=True, right_index=True, how='left')\n","            std_size.fillna(std_size['size'].mean(), inplace=True)\n","\n","            mean_size = trade_train_st.groupby('time_id')['size'].mean()\n","            mean_size = pd.merge(idx, mean_size, left_index=True, right_index=True, how='left')\n","            mean_size.fillna(mean_size['size'].mean(), inplace=True)\n","\n","            # Calculate statistics for order_count\n","            std_order_count = trade_train_st.groupby('time_id')['order_count'].std()\n","            std_order_count = pd.merge(idx, std_order_count, left_index=True, right_index=True, how='left')\n","            std_order_count.fillna(std_order_count['order_count'].mean(), inplace=True)\n","\n","            mean_order_count = trade_train_st.groupby('time_id')['order_count'].mean()\n","            mean_order_count = pd.merge(idx, mean_order_count, left_index=True, right_index=True, how='left')\n","            mean_order_count.fillna(mean_order_count['order_count'].mean(), inplace=True)\n","\n","            # Return results\n","            return {\n","                'trade_price_std': std_price['price'].values,\n","                'trade_price_real_vol': real_vol_price['log_ret_price'].values,\n","                'trade_size_std': std_size['size'].values,\n","                'trade_size_mean': mean_size['size'].values,\n","                'trade_order_count_std': std_order_count['order_count'].values,\n","                'trade_order_count_mean': mean_order_count['order_count'].values\n","            }\n","\n","        if self.ml_stage == 'training':\n","          subset_paths = self.trade_paths\n","          train_idx = self.train.set_index('time_id')[['stock_id']]\n","          results = []\n","          for path in subset_paths:\n","              st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","              if st_id in self.train['stock_id'].values:\n","                  result = process_trade_data(path, train_idx[train_idx['stock_id'] == st_id])\n","                  results.append(result)\n","\n","        elif self.ml_stage == 'inference':\n","          subset_paths = self.trade_paths\n","          test_idx = self.test.set_index('time_id')[['stock_id']]\n","          results = []\n","          for path in subset_paths:\n","              st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","              if st_id in self.test['stock_id'].values:\n","                  result = process_trade_data(path, test_idx[test_idx['stock_id'] == st_id])\n","                  results.append(result)\n","\n","        # Concatenate results\n","        trade_price_std_arr = np.concatenate([result['trade_price_std'] for result in results])\n","        trade_price_real_vol_arr = np.concatenate([result['trade_price_real_vol'] for result in results])\n","        trade_size_std_arr = np.concatenate([result['trade_size_std'] for result in results])\n","        trade_size_mean_arr = np.concatenate([result['trade_size_mean'] for result in results])\n","        trade_order_count_std_arr = np.concatenate([result['trade_order_count_std'] for result in results])\n","        trade_order_count_mean_arr = np.concatenate([result['trade_order_count_mean'] for result in results])\n","\n","        # Create DataFrame and merge\n","        trade_price_std_df = pd.DataFrame({\n","            'trade_price_std': trade_price_std_arr,\n","            'trade_price_real_vol': trade_price_real_vol_arr,\n","            'trade_size_std': trade_size_std_arr,\n","            'trade_size_mean': trade_size_mean_arr,\n","            'trade_order_count_std': trade_order_count_std_arr,\n","            'trade_order_count_mean': trade_order_count_mean_arr\n","        })\n","\n","        feat_df = feat_df.merge(trade_price_std_df, left_index=True, right_index=True)\n","        del trade_price_std_df\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","    def create_all_stocks_first_10_min_vol_df_AND_merge_first_10_min_vol_df(self,feat_df,df_20_min_volatility):\n","\n","        first_10_min_vol_df = df_20_min_volatility\n","        first_10_min_vol_df['time_id'] = first_10_min_vol_df['time_id'].astype(int)\n","        first_10_min_vol_df['stock_id'] = first_10_min_vol_df['stock_id'].astype(int)\n","\n","        # # Initialize an array with NaN values\n","        # all_stocks_first_10_min_vol_array = np.full((len(self.all_uniq_time_ids), len(self.unique_stock_ids)), np.nan)\n","\n","        # # Create a dictionary for quick access to stock index\n","        # stock_id_to_index = {st_id: idx for idx, st_id in enumerate(self.unique_stock_ids)}\n","\n","        # # Fill the array with data\n","        # for st_id, subset in first_10_min_vol_df.groupby('stock_id'):\n","        #     time_id_indices = self.all_uniq_time_ids['time_id'].searchsorted(subset['time_id'].astype(int))\n","        #     all_stocks_first_10_min_vol_array[range(len(self.all_uniq_time_ids)), stock_id_to_index[int(st_id)]] = subset['first_10_min_vol'].values\n","\n","        # # Initialize an array with NaN values\n","        # all_stocks_first_10_min_vol_array = np.full((len(self.all_uniq_time_ids), len(self.unique_stock_ids)), np.nan)\n","\n","        # # Create a dictionary for quick access to stock index\n","        # stock_id_to_index = {st_id: idx for idx, st_id in enumerate(self.unique_stock_ids)}\n","\n","        # time_id_to_index = {time_id: idx for idx, time_id in enumerate(self.all_uniq_time_ids['time_id'])}\n","\n","\n","        # # Iterate over each stock_id\n","        # for st_id, subset in first_10_min_vol_df.groupby('stock_id'):\n","        #     # Map the stock_id to the correct index in the array\n","        #     stock_index = stock_id_to_index[int(st_id)]\n","\n","        #     subset = pd.merge(self.all_uniq_time_ids, subset, left_on='time_id', right_on='time_id', how='left')\n","\n","        #     time_indices = subset['time_id'].map(time_id_to_index)\n","\n","        #     # Fill the array with the first_10_min_vol values\n","        #     all_stocks_first_10_min_vol_array[time_indices, stock_index] = subset['first_10_min_vol'].values\n","\n","\n","\n","        # # Forward fill and backward fill missing values\n","        # all_stocks_first_10_min_vol_array = pd.DataFrame(all_stocks_first_10_min_vol_array).ffill().bfill().to_numpy()\n","\n","        pivot_table = df_20_min_volatility.pivot(columns= 'stock_id', values='first_10_min_vol', index='time_id')\n","        all_stocks_first_10_min_vol_array = pivot_table.values\n","        all_unique_time_ids = pivot_table.index.values\n","        all_unique_stock_ids = pivot_table.columns.values\n","        # Reshape to the desired shape\n","        all_stocks_first_10_min_vol_array = all_stocks_first_10_min_vol_array[:, :, np.newaxis]\n","\n","        all_stocks_first_10_min_vol_df = all_stocks_first_10_min_vol_array\n","\n","        feat_df['first_10_min_vol'] = first_10_min_vol_df['first_10_min_vol']\n","        del first_10_min_vol_df\n","\n","        return feat_df, all_stocks_first_10_min_vol_df, all_unique_time_ids, all_unique_stock_ids\n","\n","\n","\n","\n","\n","\n","\n","    def merge_bk_price_size_min_max_range(self,feat_df,bk_price_size_min_max_range):\n","        bk_price_size_min_max_range['st_min_max_bid_price1'].rename(columns={'min_bid_price':'min_bid_price1', 'max_bid_price':'max_bid_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_bid_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_bid_price1']], axis=1)\n","        feat_df.columns\n","\n","        bk_price_size_min_max_range['st_min_max_ask_price1'].rename(columns={'min_ask_price':'min_ask_price1', 'max_ask_price':'max_ask_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_ask_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_ask_price1']], axis=1)\n","        feat_df.columns\n","\n","        bk_price_size_min_max_range['st_min_max_bid_size1'].rename(columns={'min_bid_size':'min_bid_size1', 'max_bid_size':'max_bid_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_bid_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_bid_size1']], axis=1)\n","        feat_df.columns\n","\n","        bk_price_size_min_max_range['st_min_max_ask_size1'].rename(columns={'min_ask_size':'min_ask_size1', 'max_ask_size':'max_ask_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_ask_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_ask_size1']], axis=1)\n","\n","        bk_price_size_min_max_range['st_range_ask_price1'].rename(columns={'range_ask_price':'range_ask_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_ask_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_ask_price1']], axis=1)\n","\n","        bk_price_size_min_max_range['st_range_bid_price1'].rename(columns={'range_bid_price':'range_bid_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_bid_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_bid_price1']], axis=1)\n","\n","\n","        bk_price_size_min_max_range['st_range_ask_size1'].rename(columns={'range_ask_size':'range_ask_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_ask_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_ask_size1']], axis=1)\n","\n","        bk_price_size_min_max_range['st_range_bid_size1'].rename(columns={'range_bid_size':'range_bid_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_bid_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_bid_size1']], axis=1)\n","        feat_df.columns\n","        del bk_price_size_min_max_range\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","    def merge_bk_price_size_sad(self,feat_df,bk_price_size_sad):\n","\n","        ####### bk_price_size_sad\n","\n","        bk_price_size_sad['st_sad_ask_price1'].rename(columns={'sad_ask_price':'sad_ask_price1'}, inplace=True)\n","        bk_price_size_sad['st_sad_ask_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_ask_price1']], axis=1)\n","\n","        bk_price_size_sad['st_sad_ask_size1'].rename(columns={'sad_ask_size':'sad_ask_size1'}, inplace=True)\n","        bk_price_size_sad['st_sad_ask_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_ask_size1']], axis=1)\n","\n","        bk_price_size_sad['st_sad_bid_price1'].rename(columns={'sad_bid_price':'sad_bid_price1'}, inplace=True)\n","        bk_price_size_sad['st_sad_bid_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_bid_price1']], axis=1)\n","\n","        bk_price_size_sad['st_sad_bid_size1'].rename(columns={'sad_bid_size':'sad_bid_size1'}, inplace=True)\n","        bk_price_size_sad['st_sad_bid_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_bid_size1']], axis=1)\n","        del bk_price_size_sad\n","\n","        return feat_df\n","\n","\n","    def merge_bk_size_price_corr(self,feat_df,bk_size_price_corr):\n","\n","        ## bk_size_price_corr\n","\n","        #bk_size_price_corr['st_bs_bp_corr1'].rename(columns={0:'bs_bp_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_bp_corr1'].rename(columns={bk_size_price_corr['st_bs_bp_corr1'].columns[0]:'bs_bp_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_bp_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bs_bp_corr1']], axis=1)\n","\n","        #bk_size_price_corr['st_bs_as_corr1'].rename(columns={0:'bs_as_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_as_corr1'].rename(columns={bk_size_price_corr['st_bs_as_corr1'].columns[0]:'bs_as_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_as_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bs_as_corr1']], axis=1)\n","\n","        #bk_size_price_corr['st_bs_ap_corr1'].rename(columns={0:'bs_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_ap_corr1'].rename(columns={bk_size_price_corr['st_bs_ap_corr1'].columns[0]:'bs_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_ap_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bs_ap_corr1']], axis=1)\n","\n","        #bk_size_price_corr['st_bp_as_corr1'].rename(columns={0:'bp_as_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bp_as_corr1'].rename(columns={bk_size_price_corr['st_bp_as_corr1'].columns[0]:'bp_as_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bp_as_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bp_as_corr1']], axis=1)\n","\n","        #bk_size_price_corr['st_bp_ap_corr1'].rename(columns={0:'bp_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bp_ap_corr1'].rename(columns={bk_size_price_corr['st_bp_ap_corr1'].columns[0]:'bp_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bp_ap_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bp_ap_corr1']], axis=1)\n","\n","        #bk_size_price_corr['st_as_ap_corr1'].rename(columns={0:'as_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_as_ap_corr1'].rename(columns={bk_size_price_corr['st_as_ap_corr1'].columns[0]:'as_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_as_ap_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_as_ap_corr1']], axis=1)\n","        del bk_size_price_corr\n","        return feat_df\n","\n","\n","    def merge_trade_price_size_order_count_min_max_range(self,feat_df,trade_price_size_order_count_min_max_range):\n","        ## trade_price_size_order_count_min_max_range\n","\n","        #trade_price_size_order_count_min_max_range['st_min_price'].rename(columns={0:'min_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_price'].rename(columns={trade_price_size_order_count_min_max_range['st_min_price'].columns[0]:'min_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_min_price']], axis=1)\n","\n","        #trade_price_size_order_count_min_max_range['st_max_price'].rename(columns={0:'max_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_price'].rename(columns={trade_price_size_order_count_min_max_range['st_max_price'].columns[0]:'max_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_max_price']], axis=1)\n","\n","        #trade_price_size_order_count_min_max_range['st_min_size'].rename(columns={0:'min_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_size'].rename(columns={trade_price_size_order_count_min_max_range['st_min_size'].columns[0]:'min_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_min_size']], axis=1)\n","\n","        #trade_price_size_order_count_min_max_range['st_max_size'].rename(columns={0:'max_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_size'].rename(columns={trade_price_size_order_count_min_max_range['st_max_size'].columns[0]:'max_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_max_size']], axis=1)\n","\n","        #trade_price_size_order_count_min_max_range['st_min_order_count'].rename(columns={0:'min_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_order_count'].rename(columns={trade_price_size_order_count_min_max_range['st_min_order_count'].columns[0]:'min_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_min_order_count']], axis=1)\n","\n","        #trade_price_size_order_count_min_max_range['st_max_order_count'].rename(columns={0:'max_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_order_count'].rename(columns={trade_price_size_order_count_min_max_range['st_max_order_count'].columns[0]:'max_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_max_order_count']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_range_price'].rename(columns={'range_price':'range_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_range_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_range_price']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_range_size'].rename(columns={'range_size':'range_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_range_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_range_size']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_range_order_count'].rename(columns={'range_order_count':'range_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_range_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_range_order_count']], axis=1)\n","        del trade_price_size_order_count_min_max_range\n","\n","        return feat_df\n","\n","\n","    def merge_trade_price_size_order_count_sad(self,feat_df,trade_price_size_order_count_sad):\n","        ## trade_price_size_order_count_sad\n","\n","        trade_price_size_order_count_sad['st_sad_price'].rename(columns={'sad_price':'sad_price1'}, inplace=True)\n","        trade_price_size_order_count_sad['st_sad_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_sad['st_sad_price']], axis=1)\n","\n","        trade_price_size_order_count_sad['st_sad_size'].rename(columns={'sad_size':'sad_size1'}, inplace=True)\n","        trade_price_size_order_count_sad['st_sad_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_sad['st_sad_size']], axis=1)\n","\n","        trade_price_size_order_count_sad['st_sad_order_count'].rename(columns={'sad_order_count':'sad_order_count1'}, inplace=True)\n","        trade_price_size_order_count_sad['st_sad_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_sad['st_sad_order_count']], axis=1)\n","        del trade_price_size_order_count_sad\n","\n","        return feat_df\n","\n","\n","    def merge_trade_price_size_order_count_corr(self,feat_df,trade_price_size_order_count_corr):\n","\n","        ## trade_price_size_order_count_corr\n","\n","        #trade_price_size_order_count_corr['st_size_order_count_corr'].rename(columns={0:'size_order_count_corr1'}, inplace=True)\n","        trade_price_size_order_count_corr['st_size_order_count_corr'].rename(columns={trade_price_size_order_count_corr['st_size_order_count_corr'].columns[0]:'size_order_count_corr1'}, inplace=True)\n","        trade_price_size_order_count_corr['st_size_order_count_corr'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_corr['st_size_order_count_corr']], axis=1)\n","        del trade_price_size_order_count_corr\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","    ## apply summary statistics and pearson correlated cluster labels to stock ids, similar labels represent similarity in stocks\n","    ## These will be considered as categorical features\n","    ## we can also input our own feature clustering this way as well instead of taking median of the first_10_min_target_volatility for each cluster\n","\n","    def merge_final_sum_stats_target_vol_clusters(self,feat_df,final_sum_stats_target_vol_clusters,unique_stock_ids):\n","\n","        for c in final_sum_stats_target_vol_clusters.columns:\n","            labels = final_sum_stats_target_vol_clusters[c]\n","            feat_df['sum_stats_'+c+'_labels'] = feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","\n","        return feat_df\n","\n","\n","    def merge_final_robust_sum_stats_target_vol_clusters(self,feat_df,final_robust_sum_stats_target_vol_clusters,unique_stock_ids):\n","        for c in final_robust_sum_stats_target_vol_clusters.columns:\n","            labels = final_robust_sum_stats_target_vol_clusters[c]\n","            feat_df['robust_sum_stats_'+c+'_labels'] = feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","\n","        return feat_df\n","\n","\n","    def merge_final_pear_corr_target_vol_clusters(self,feat_df,final_pear_corr_target_vol_clusters,unique_stock_ids):\n","\n","        for c in final_pear_corr_target_vol_clusters.columns:\n","            labels = final_pear_corr_target_vol_clusters[c]\n","            feat_df['pear_corr_'+c+'_labels'] = feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","\n","    def create_bk_price_size_min_max_range_2(self,):\n","\n","        level = 2\n","        subset_paths = self.book_paths\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","\n","        train_target = self.train\n","\n","        def my_range_price(series):\n","            return series.max()-series.min()\n","\n","        def process_stock_data_train(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = train_target[train_target['stock_id'] == st_id]\n","            target_st.index = target_st['time_id']\n","\n","            book_train_st = pd.read_parquet(path)\n","\n","            result = {}\n","\n","            # Min-Max Bid Price\n","            st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min', 'max']).rename(columns={'min': 'min_bid_price', 'max': 'max_bid_price'})\n","            result['st_min_max_bid_price'] = st_min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Min-Max Ask Price\n","            st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min', 'max']).rename(columns={'min': 'min_ask_price', 'max': 'max_ask_price'})\n","            result['st_min_max_ask_price'] = st_min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Min-Max Bid Size\n","            st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min', 'max']).rename(columns={'min': 'min_bid_size', 'max': 'max_bid_size'})\n","            result['st_min_max_bid_size'] = st_min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Min-Max Ask Size\n","            st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min', 'max']).rename(columns={'min': 'min_ask_size', 'max': 'max_ask_size'})\n","            result['st_min_max_ask_size'] = st_min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Ask Price\n","            st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_range_price]}).rename(columns={ask_price: 'range_ask_price'})\n","            st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","            result['st_range_ask_price'] = st_range_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Bid Price\n","            st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_range_price]}).rename(columns={bid_price: 'range_bid_price'})\n","            st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","            result['st_range_bid_price'] = st_range_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Ask Size\n","            st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_range_price]}).rename(columns={ask_size: 'range_ask_size'})\n","            st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","            result['st_range_ask_size'] = st_range_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Bid Size\n","            st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_range_price]}).rename(columns={bid_size: 'range_bid_size'})\n","            st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","            result['st_range_bid_size'] = st_range_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return result\n","\n","\n","        def process_stock_data_inference(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","\n","            result = {}\n","\n","            # Min-Max Bid Price\n","            st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min', 'max']).rename(columns={'min': 'min_bid_price', 'max': 'max_bid_price'})\n","            result['st_min_max_bid_price'] = st_min_max_bid_price.reindex(target_st.index)\n","\n","            # Min-Max Ask Price\n","            st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min', 'max']).rename(columns={'min': 'min_ask_price', 'max': 'max_ask_price'})\n","            result['st_min_max_ask_price'] = st_min_max_ask_price.reindex(target_st.index)\n","\n","            # Min-Max Bid Size\n","            st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min', 'max']).rename(columns={'min': 'min_bid_size', 'max': 'max_bid_size'})\n","            result['st_min_max_bid_size'] = st_min_max_bid_size.reindex(target_st.index)\n","\n","            # Min-Max Ask Size\n","            st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min', 'max']).rename(columns={'min': 'min_ask_size', 'max': 'max_ask_size'})\n","            result['st_min_max_ask_size'] = st_min_max_ask_size.reindex(target_st.index)\n","\n","            # Range Ask Price\n","            st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_range_price]}).rename(columns={ask_price: 'range_ask_price'})\n","            st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","            result['st_range_ask_price'] = st_range_ask_price.reindex(target_st.index)\n","\n","            # Range Bid Price\n","            st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_range_price]}).rename(columns={bid_price: 'range_bid_price'})\n","            st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","            result['st_range_bid_price'] = st_range_bid_price.reindex(target_st.index)\n","\n","            # Range Ask Size\n","            st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_range_price]}).rename(columns={ask_size: 'range_ask_size'})\n","            st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","            result['st_range_ask_size'] = st_range_ask_size.reindex(target_st.index)\n","\n","            # Range Bid Size\n","            st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_range_price]}).rename(columns={bid_size: 'range_bid_size'})\n","            st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","            result['st_range_bid_size'] = st_range_bid_size.reindex(target_st.index)\n","\n","            return result\n","\n","\n","        if self.ml_stage == 'training':\n","          # Parallel processing\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_train)(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          # Sequential processing\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_inference)(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price) for path in subset_paths)\n","\n","        # Combine the results\n","        bk_price_size_min_max_range_2 = {\n","            'st_min_max_bid_price'+str(level): pd.concat([res['st_min_max_bid_price'] for res in results if res is not None], axis=0),\n","            'st_min_max_ask_price'+str(level): pd.concat([res['st_min_max_ask_price'] for res in results if res is not None], axis=0),\n","            'st_min_max_bid_size'+str(level): pd.concat([res['st_min_max_bid_size'] for res in results if res is not None], axis=0),\n","            'st_min_max_ask_size'+str(level): pd.concat([res['st_min_max_ask_size'] for res in results if res is not None], axis=0),\n","            'st_range_ask_price'+str(level): pd.concat([res['st_range_ask_price'] for res in results if res is not None], axis=0),\n","            'st_range_bid_price'+str(level): pd.concat([res['st_range_bid_price'] for res in results if res is not None], axis=0),\n","            'st_range_ask_size'+str(level): pd.concat([res['st_range_ask_size'] for res in results if res is not None], axis=0),\n","            'st_range_bid_size'+str(level): pd.concat([res['st_range_bid_size'] for res in results if res is not None], axis=0),\n","        }\n","        return bk_price_size_min_max_range_2\n","\n","\n","    def merge_bk_price_size_min_max_range_2(self,bk_price_size_min_max_range_2,feat_df):\n","\n","        ####### bk_price_size_min_max_range_2\n","\n","        bk_price_size_min_max_range_2['st_min_max_bid_price2'].rename(columns={'min_bid_price':'min_bid_price2', 'max_bid_price':'max_bid_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_bid_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_bid_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_min_max_ask_price2'].rename(columns={'min_ask_price':'min_ask_price2', 'max_ask_price':'max_ask_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_ask_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_ask_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_min_max_bid_size2'].rename(columns={'min_bid_size':'min_bid_size2', 'max_bid_size':'max_bid_size2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_bid_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_bid_size2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_min_max_ask_size2'].rename(columns={'min_ask_size':'min_ask_size2', 'max_ask_size':'max_ask_size2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_ask_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_ask_size2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_range_ask_price2'].rename(columns={'range_ask_price':'range_ask_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_range_ask_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_range_ask_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_range_bid_price2'].rename(columns={'range_bid_price':'range_bid_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_range_bid_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_range_bid_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_range_ask_size2'].rename(columns={'range_ask_size':'range_ask_size2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_range_ask_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_range_ask_size2']], axis=1)\n","        del bk_price_size_min_max_range_2\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    def create_bk_price_size_sad_2(self,):\n","\n","        level = 2\n","        subset_paths = self.book_paths\n","        train_target = self.train\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","        def my_sum_abs_diff(values):\n","            return np.sum(np.abs(np.diff(values)))\n","\n","        # Function to process each stock ID\n","        def process_stock_train(path, level):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = train_target[train_target['stock_id'] == st_id]\n","            target_st.index = target_st[\"time_id\"]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Aggregate features\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_sum_abs_diff]}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","            st_sad_ask_price = st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_sum_abs_diff]}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","            st_sad_ask_size = st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_sum_abs_diff]}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","            st_sad_bid_price = st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_sum_abs_diff]}).rename(columns={bid_size: 'sad_bid_size'})\n","            st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","            st_sad_bid_size = st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return (st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size)\n","\n","\n","        # Function to process each stock ID\n","        def process_stock_inference(path, level):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","\n","            # Aggregate features\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_sum_abs_diff]}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","            st_sad_ask_price = st_sad_ask_price.reindex(target_st.index)\n","\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_sum_abs_diff]}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","            st_sad_ask_size = st_sad_ask_size.reindex(target_st.index)\n","\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_sum_abs_diff]}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","            st_sad_bid_price = st_sad_bid_price.reindex(target_st.index)\n","\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_sum_abs_diff]}).rename(columns={bid_size: 'sad_bid_size'})\n","            st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","            st_sad_bid_size = st_sad_bid_size.reindex(target_st.index)\n","\n","            return (st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size)\n","\n","\n","\n","        if self.ml_stage == 'training':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_train)(path, level) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_inference)(path, level) for path in subset_paths)\n","\n","        # Initialize empty DataFrames to store the results\n","        bk_price_size_sad_2 = {\n","            'st_sad_ask_price' + str(level): pd.DataFrame(),\n","            'st_sad_ask_size' + str(level): pd.DataFrame(),\n","            'st_sad_bid_price' + str(level): pd.DataFrame(),\n","            'st_sad_bid_size' + str(level): pd.DataFrame()\n","        }\n","\n","        # Collect results into the final DataFrames\n","        for result in results:\n","            if result is not None:\n","                st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size = result\n","                bk_price_size_sad_2['st_sad_ask_price' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_ask_price' + str(level)], st_sad_ask_price], axis=0)\n","                bk_price_size_sad_2['st_sad_ask_size' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_ask_size' + str(level)], st_sad_ask_size], axis=0)\n","                bk_price_size_sad_2['st_sad_bid_price' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_bid_price' + str(level)], st_sad_bid_price], axis=0)\n","                bk_price_size_sad_2['st_sad_bid_size' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_bid_size' + str(level)], st_sad_bid_size], axis=0)\n","\n","        # The result is stored in bk_price_size_sad\n","        return bk_price_size_sad_2\n","\n","\n","\n","    def merge_bk_price_size_sad_2(self,feat_df,bk_price_size_sad_2):\n","\n","        ####### bk_price_size_sad_2\n","\n","        bk_price_size_sad_2['st_sad_ask_price2'].rename(columns={'sad_ask_price':'sad_ask_price2'}, inplace=True)\n","        bk_price_size_sad_2['st_sad_ask_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad_2['st_sad_ask_price2']], axis=1)\n","\n","        bk_price_size_sad_2['st_sad_ask_size2'].rename(columns={'sad_ask_size':'sad_ask_size2'}, inplace=True)\n","        bk_price_size_sad_2['st_sad_ask_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad_2['st_sad_ask_size2']], axis=1)\n","\n","        bk_price_size_sad_2['st_sad_bid_price2'].rename(columns={'sad_bid_price':'sad_bid_price2'}, inplace=True)\n","        bk_price_size_sad_2['st_sad_bid_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad_2['st_sad_bid_price2']], axis=1)\n","        del bk_price_size_sad_2\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","    def create_bk_size_price_corr_2(self,):\n","\n","        level=2\n","        subset_paths = self.book_paths\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","\n","        train_target = self.train\n","\n","        from joblib import Parallel, delayed\n","        import pandas as pd\n","\n","        # Function to process each stock ID\n","        def process_stock_train(path, level):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = train_target[train_target['stock_id'] == st_id]\n","            target_st.index = target_st[\"time_id\"]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            # Reindex with target_st time_ids and apply forward and backward fill\n","            for key in corr_dict.keys():\n","                corr_dict[key] = corr_dict[key].reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return corr_dict\n","\n","\n","\n","        # Function to process each stock ID\n","        def process_stock_inference(path, level):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            # Reindex with target_st time_ids and apply forward and backward fill\n","            for key in corr_dict.keys():\n","                corr_dict[key] = corr_dict[key].reindex(target_st.index)\n","\n","            return corr_dict\n","\n","\n","        if self.ml_stage == 'training':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_train)(path, level) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_inference)(path, level) for path in subset_paths)\n","\n","        # Initialize empty DataFrames to store the results\n","        bk_size_price_corr_2 = {\n","            'st_bs_bp_corr' + str(level): pd.DataFrame(),\n","            'st_bs_as_corr' + str(level): pd.DataFrame(),\n","            'st_bs_ap_corr' + str(level): pd.DataFrame(),\n","            'st_bp_as_corr' + str(level): pd.DataFrame(),\n","            'st_bp_ap_corr' + str(level): pd.DataFrame(),\n","            'st_as_ap_corr' + str(level): pd.DataFrame()\n","        }\n","\n","        # Collect results into the final DataFrames\n","        for result in results:\n","            if result is not None:\n","                for key in result.keys():\n","                    bk_size_price_corr_2[key + str(level)] = pd.concat([bk_size_price_corr_2[key + str(level)], result[key]], axis=0)\n","\n","        return bk_size_price_corr_2\n","\n","\n","\n","\n","    def merge_bk_size_price_corr_2(self,feat_df,bk_size_price_corr_2):\n","\n","        ###### bk_size_price_corr_2\n","\n","        #bk_size_price_corr_2['st_bs_bp_corr2'].rename(columns={0:'bs_bp_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_bp_corr2'].rename(columns={ bk_size_price_corr_2['st_bs_bp_corr2'].columns[0]:'bs_bp_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_bp_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bs_bp_corr2']], axis=1)\n","\n","        #bk_size_price_corr_2['st_bs_as_corr2'].rename(columns={0:'bs_as_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_as_corr2'].rename(columns={bk_size_price_corr_2['st_bs_as_corr2'].columns[0]:'bs_as_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_as_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bs_as_corr2']], axis=1)\n","\n","        #bk_size_price_corr_2['st_bs_ap_corr2'].rename(columns={0:'bs_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_ap_corr2'].rename(columns={bk_size_price_corr_2['st_bs_ap_corr2'].columns[0]:'bs_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_ap_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bs_ap_corr2']], axis=1)\n","\n","        #bk_size_price_corr_2['st_bp_as_corr2'].rename(columns={0:'bp_as_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bp_as_corr2'].rename(columns={bk_size_price_corr_2['st_bp_as_corr2'].columns[0]:'bp_as_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bp_as_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bp_as_corr2']], axis=1)\n","\n","        #bk_size_price_corr_2['st_bp_ap_corr2'].rename(columns={0:'bp_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bp_ap_corr2'].rename(columns={bk_size_price_corr_2['st_bp_ap_corr2'].columns[0]:'bp_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bp_ap_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bp_ap_corr2']], axis=1)\n","\n","        #bk_size_price_corr_2['st_as_ap_corr2'].rename(columns={0:'as_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_as_ap_corr2'].rename(columns={bk_size_price_corr_2['st_as_ap_corr2'].columns[0]:'as_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_as_ap_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_as_ap_corr2']], axis=1)\n","        del bk_size_price_corr_2\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","\n","    def merge_clustering_features_to_training_data(self,feat_df,final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df ):\n","        # Merge clustering features with training data\n","\n","        feat_df = pd.concat([feat_df, final_sum_stats_target_vol_df], axis=1)\n","        feat_df = pd.concat([feat_df, final_robust_sum_stats_target_vol_df], axis=1)\n","        feat_df = pd.concat([feat_df, final_pear_corr_target_vol_df], axis=1)\n","\n","        return feat_df\n","\n"]},{"cell_type":"code","execution_count":null,"id":"4cbd696b","metadata":{"id":"4cbd696b"},"outputs":[],"source":["\n","def find_equilibrium_price(book_data, lvl, iterations=22):\n","    loga2 = np.array(book_data['log_ask_price2'])\n","    loga1 = np.array(book_data['log_ask_price1'])\n","    logb1 = np.array(book_data['log_bid_price1'])\n","    logb2 = np.array(book_data['log_bid_price2'])\n","\n","    asize2 = np.array(book_data['ask_size2'])\n","    asize1 = np.array(book_data['ask_size1'])\n","    bsize1 = np.array(book_data['bid_size1'])\n","    bsize2 = np.array(book_data['bid_size2'])\n","\n","    ub = loga1\n","    lb = logb1\n","\n","    s = (-1)**lvl\n","    for iter in range(iterations):\n","        mid_price = (ub + lb)/2.0\n","        inv_diff_a2 = 1.0/( 1000*( mid_price - loga2 ) )\n","        inv_diff_a1 = 1.0/( 1000*( mid_price - loga1 ) )\n","        inv_diff_b1 = 1.0/( 1000*( mid_price - logb1 ) ) # negative\n","        inv_diff_b2 = 1.0/( 1000*( mid_price - logb2 ) ) # negative\n","\n","        f  = -(   ( bsize2*inv_diff_b2**(lvl+1) + bsize1*inv_diff_b1**(lvl+1) )\n","              + s*( asize1*inv_diff_a1**(lvl+1) + asize2*inv_diff_a2**(lvl+1) ) )\n","\n","        # when lvl = even, f is positive when buy side missing volume is larger than sell side missing volume\n","        # when lvl = even, f is negative when sell side missing volume is larger than buy side missing volume\n","        # when lvl = odd, f is positive when sell side missing volume is larger than buy side missing volume and vice versa\n","\n","        dub = - (ub-lb)/2.0*(f>=0)\n","        dlb =   (ub-lb)/2.0*(f< 0)\n","\n","        # when f is positive, mid price moves towards buy side (bid_price) by reducing the upper bound\n","        # when f is negative, mid price moves towards sell side (ask_price) by increasing the lower bound\n","        ub = ub + dub\n","        lb = lb + dlb\n","\n","    equilibrium_price = (ub + lb)/2.0\n","\n","    return equilibrium_price\n","\n","\n","\n","def diff(list_stock_prices):\n","    return list_stock_prices.diff()\n","\n","\n","\n","@jit(nopython=True)\n","def bucketized_summed_data(seconds_arr, time_id, data, buk_width, n_buks, time_ids_size):\n","    z = np.zeros( (time_ids_size,n_buks) ) # 30 buckets for 600 seconds (10 minutes)\n","\n","    t_id  = 0\n","    for s in range(seconds_arr.shape[0]): # seconds.shape[0] is total size of the seconds column i.e. total rows in seconds column\n","\n","        if time_id[s] != time_id[max(s-1,0)]:\n","            t_id = t_id + 1\n","\n","        z[t_id, int(seconds_arr[s]//buk_width)] += data[s]\n","\n","    return z\n","\n","\n","@jit(nopython=True)\n","def end_bucket(buk_width, buk_sum:float, buk_weight:float, last_val, last_weight, last_time)->float:\n","    dt = buk_width - last_time%buk_width\n","\n","    buk_weight += 1.0*last_weight*dt\n","    buk_sum    += 1.0*last_weight*dt*last_val\n","\n","    return float(buk_sum/(buk_weight + 1e-8))\n","\n","\n","\n","@jit(nopython=True)\n","def bucketized_time_weighted_avg_data(seconds_arr, time_id_arr, data,weights, buk_width, n_buks, time_ids_size):\n","\n","    z = np.zeros( (time_ids_size,n_buks) )\n","\n","    prev_time   = 0\n","    prev_weight = 0.0\n","    prev_val    = 0.0\n","\n","    buk_sum = 0.0\n","    buk_weight = 0.0\n","\n","    t_id  = 0  # time id\n","    buk = 0  # bucket id\n","    for idx in range(seconds_arr.shape[0]): # seconds.shape[0] is total size of the seconds column i.e. total rows in seconds column\n","\n","        if time_id_arr[idx] != time_id_arr[max(idx-1,0)]: # transition to new time id\n","            z[t_id, buk] = float(end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time))\n","            buk += 1\n","\n","            while buk < z.shape[1]:\n","                z[t_id, buk] = prev_val\n","                buk += 1\n","            t_id += 1\n","            buk = 0\n","\n","            prev_time  = 0\n","            buk_sum    = 0.0\n","            buk_weight = 0.0\n","\n","        if int(seconds_arr[idx]//buk_width) != int(prev_time//buk_width): # transition to new bucket\n","\n","            z[t_id, buk] = float(end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time)) # end the previous bucket\n","            buk += 1 # move to next bucket\n","\n","            while buk < seconds_arr[idx]//buk_width:\n","                z[t_id, buk] = prev_val\n","                buk += 1\n","\n","            prev_time  = buk_width*(seconds_arr[idx]//buk_width)\n","            buk_sum    = 0.0\n","            buk_weight = 0.0\n","\n","        buk_sum    += prev_val*prev_weight*(seconds_arr[idx] - prev_time)  # in the same bucket\n","        buk_weight +=          prev_weight*(seconds_arr[idx] - prev_time)  # in the same bucket\n","\n","        prev_time   = seconds_arr[idx] # in the same bucket\n","        prev_val    = data[idx] # in the same bucket\n","        prev_weight = weights[idx] # in the same bucket\n","\n","    z[t_id, buk] = end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time)\n","\n","    for buk in range(buk+1, z.shape[1]): # all buckets of the last time id\n","        z[t_id, buk] = prev_val\n","\n","    return z\n","\n","\n","\n","def create_stock_data(st_id, dset):\n","\n","    cols = ['st_id', 'time_id', 'seconds_in_bucket']\n","\n","    ############################## BOOK DATA ##########################################\n","\n","    book_data = pd.read_parquet(os.path.join(data_dir, 'book_{}.parquet/stock_id={}/'.format(dset, st_id)))\n","\n","    book_data['st_id'] = st_id\n","\n","    columns = cols + [col for col in book_data.columns if col not in cols]\n","    book_data = book_data[columns]\n","    # columns = 'st_id', 'time_id', 'seconds_in_bucket', 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2'\n","\n","    # volume\n","    book_data['ask_volume1'] = book_data['ask_price1']*book_data['ask_size1']\n","    book_data['ask_volume2'] = book_data['ask_price2']*book_data['ask_size2']\n","    book_data['bid_volume1'] = book_data['bid_price1']*book_data['bid_size1']\n","    book_data['bid_volume2'] = book_data['bid_price2']*book_data['bid_size2']\n","\n","    # becomes zero centered, reduces skew and kurtosis slightly bringing it slightly closer to normal for right skewed data, It is increases/worsens\n","    # skew and kurtosis for left-skewed data\n","    # correct way is to use box-cox transformation, variance stabilization\n","    # Most stocks are right skewed only a few are left skewed?\n","    book_data['log_ask_price1'] = np.log( book_data['ask_price1'] )\n","    book_data['log_ask_price2'] = np.log( book_data['ask_price2'] )\n","    book_data['log_bid_price1'] = np.log( book_data['bid_price1'] )\n","    book_data['log_bid_price2'] = np.log( book_data['bid_price2'] )\n","\n","    # redefining WAP using log prices\n","    book_data['wap1_log_price'] = ( book_data['log_bid_price1'] * book_data['ask_size1'] + book_data['log_ask_price1'] * book_data['bid_size1'] ) / (book_data['bid_size1'] + book_data['ask_size1'])\n","    book_data['wap2_log_price'] = ( book_data['log_bid_price2'] * book_data['ask_size2'] + book_data['log_ask_price2'] * book_data['bid_size2'] ) / (book_data['bid_size2'] + book_data['ask_size2'])\n","\n","    # Find equilibrium price at which trades are likely to happen\n","    # This price minimizes the missing total volume from buy and sell side\n","    book_data['wap_eqi_price0'] = find_equilibrium_price( book_data, lvl=0)\n","    book_data['wap_eqi_price1'] = find_equilibrium_price( book_data, lvl=1)\n","    book_data['wap_eqi_price2'] = find_equilibrium_price( book_data, lvl=2)\n","    book_data['wap_eqi_price3'] = find_equilibrium_price( book_data, lvl=3)\n","    # book_data['wap_eqi_price4'] = find_equilibrium_price( book_data, lvl=4)\n","\n","\n","    # equilibrium price has converged closer to\n","    book_data['liquidity0'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_bid_price1']) )\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_bid_price2']) )\n","                - book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_ask_price1']) )\n","                - book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_ask_price2']) )\n","    )\n","\n","    # liquidity 0 and liquidity 1 are negatively correlated with each other, if one has prices moving towards buy side, the other has price moving towards sell side\n","    book_data['liquidity1'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price1']) )\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price2']) )\n","                - book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price1']) )\n","                - book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price2']) )\n","    )\n","\n","    book_data['liquidity2'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_bid_price1']) )**2\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_bid_price2']) )**2\n","                + book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_ask_price1']) )**2\n","                + book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_ask_price2']) )**2\n","    )\n","\n","    book_data['liquidity3'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price3'] - book_data['log_bid_price1']) )**3\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price3'] - book_data['log_bid_price2']) )**3\n","                + book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price3'] - book_data['log_ask_price1']) )**3\n","                + book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price3'] - book_data['log_ask_price2']) )**3\n","    )\n","\n","    book_data['spread']     = book_data['log_ask_price1'] - book_data['log_bid_price1']\n","    book_data['inv_spread'] = (book_data['log_ask_price1'] - book_data['log_bid_price1'])**-2 # inverse of spread has the effect of amplifying low values and diminishing high values\n","    book_data['log_spread'] = book_data['spread'].apply(np.log) # log of spread has the effect of amplifying low values and diminishing high values. It can normalize right skewed data\n","    book_data['log_spread2'] = np.log(book_data['log_ask_price2'] - book_data['log_bid_price2'])\n","\n","    book_data['book_size1'] = book_data['ask_volume1'] + book_data['bid_volume1']\n","    book_data['book_size'] = book_data['ask_volume1'] + book_data['bid_volume1'] + book_data['ask_volume2'] + book_data['bid_volume2']\n","\n","    # difference betweeen ask's level 1 and level 2 liquidity\n","    # posiitive means level 2 ask liquidity is higher than level 1 ask liquidity\n","    book_data['ask_liq1_diff'] = (\n","                  book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price1']) )**1\n","              -  book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price2']) )**1\n","    )\n","\n","    # difference betweeen bid's level 1 and level 2 liquidity\n","    # posiitive means level 1 bid liquidity is higher than level 2 bid liquidity\n","    book_data['bid_liq1_diff'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price1']) )**1\n","              -  book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price2']) )**1\n","    )\n","\n","    # simple returns on prices wap1_log_price,wap2_log_price,wap_eqi_price0,wap_eqi_price1\n","    book_data['wap1_log_price_ret'] = book_data.groupby(by = ['time_id'])['wap1_log_price'].apply(diff).fillna(0).values\n","    book_data['wap2_log_price_ret'] = book_data.groupby(by = ['time_id'])['wap2_log_price'].apply(diff).fillna(0).values\n","    book_data['wap_eqi_price0_ret'] = book_data.groupby(by = ['time_id'])['wap_eqi_price0'].apply(diff).fillna(0).values\n","    book_data['wap_eqi_price1_ret'] = book_data.groupby(by = ['time_id'])['wap_eqi_price1'].apply(diff).fillna(0).values\n","\n","    # this indicates the changes in level 2 wap when level 1 wap does NOT change\n","    # This happens when all orders in level 1 are filled and new orders are placed in level 2\n","    # indication of liquidity as prices in level 2 are moving towards level 1\n","    # Aggressive Market Orders, Imbalance in Market Depth, Execution of Large Orders, Liquidity Changes:\n","    book_data['wap2_log_price_ret_changes_n_wap1_log_price_ret_constant'] = book_data['wap2_log_price_ret' ]*(book_data['wap1_log_price_ret' ]==0)\n","\n","    # variance stabilization of right skewed data.\n","    book_data['log_liquidity1'] = np.log(book_data['liquidity1'])\n","    book_data['log_liquidity2'] = np.log(book_data['liquidity2'])\n","    book_data['log_liquidity3'] = np.log(book_data['liquidity3'])\n","\n","    # simple returns on liquidity / first order changes in liquidity\n","    book_data['log_liquidity1_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity1'].apply(diff).fillna(0).values\n","    book_data['log_liquidity2_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity2'].apply(diff).fillna(0).values\n","    book_data['log_liquidity3_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity3'].apply(diff).fillna(0).values\n","    # simple returns on log_spread / first order changes in log_spread\n","    book_data['log_spread_ret'] = book_data.groupby(by = ['time_id'])['log_spread'].apply(diff).fillna(0).values\n","\n","    # wap1 price returns when liquidity1 is positive/increases and negative/decreases\n","    book_data['wap1_log_price_ret_pos_log_liq_ret'] = (book_data['log_liquidity1_ret']>0)*book_data['wap1_log_price_ret']\n","    book_data['wap1_log_price_ret_neg_log_liq_ret'] = (book_data['log_liquidity1_ret']<0)*book_data['wap1_log_price_ret']\n","\n","\n","    ids = np.array(book_data[['st_id', 'time_id']]) # single stock and all time_ids and seconda_in_bucket\n","    ids = np.unique(ids, axis=0)\n","    book_n_trade_data = {}\n","    book_n_trade_data['time_id'] = ids[:,1:2]\n","\n","    # bucketized data for book data\n","    # Amount of wap1 price movements in a time bucket of 30 seconds, i.e. wap1 returns volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap1_log_price_ret']),\n","                                    20, 30, ids.shape[0])\n","\n","    # Amount of absolute wap1 price movements in a time bucket of 30 seconds, i.e. ahsolute wap1 returns volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.abs(np.array(book_data['wap1_log_price_ret'])),\n","                                    20, 30, ids.shape[0])\n","\n","    # Amount of absolute wap2 price movements in a time bucket of 30 seconds, i.e.  ahsolute wap2 returns volatitlity in bucket\n","    book_n_trade_data['wap2_log_price_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.abs(np.array(book_data['wap2_log_price_ret'])),\n","                                    20, 30, ids.shape[0])\n","\n","      # wap1 returns variance/ squared volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap1_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])\n","    # wap2 returns variance/ squared volatitlity in bucket\n","    book_n_trade_data['wap2_log_price_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap2_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])\n","\n","    # squared wap1 returns volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap1_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])**0.5\n","    \n","\n","    book_n_trade_data['wap1_log_price_ret_quart_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                       np.array(book_data['time_id']),\n","                                       np.array(book_data['wap1_log_price_ret'])**4,\n","                                       20, 30, ids.shape[0])**0.25\n","\n","    # squared wap2 returns volatitlity in bucket\n","    book_n_trade_data['wap2_log_price_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap2_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])**0.5\n","\n","    # squared wap2_log_price_ret_changes_n_wap1_log_price_ret_constant volatitlity in bucket\n","    book_n_trade_data['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap2_log_price_ret_changes_n_wap1_log_price_ret_constant'])**2,\n","                                    20, 30, ids.shape[0])**0.5\n","\n","    # equilibrium price returns absolute volatitlity in bucket\n","    book_n_trade_data['wap_eqi_price0_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.abs(np.array(book_data['wap_eqi_price0_ret'])),\n","                                      20, 30, ids.shape[0])\n","\n","    # squared equilibrium price returns volatitlity in bucket\n","    book_n_trade_data['wap_eqi_price0_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap_eqi_price0_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","\n","    # volatitlity in wap1_log_price_ret when liquidity1 return is positive. i.e. increases\n","    book_n_trade_data['wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap1_log_price_ret_pos_log_liq_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","    # volatitlity in wap1_log_price_ret when liquidity1 is negative. i.e. decreases\n","    book_n_trade_data['wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap1_log_price_ret_neg_log_liq_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","\n","    # squared wap equilibrium price 1 returns volatitlity in bucket\n","    book_n_trade_data['wap_eqi_price1_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap_eqi_price1_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","\n","    book_n_trade_data['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array((book_data['log_liquidity2_ret']*book_data['wap_eqi_price1_ret'])**2 ),\n","                                      20, 30, ids.shape[0])\n","\n","    # squared wap equilibrium price 1 returns volatitlity in bucket amplified (> 1) by positive/increasing liquidity returns (through exponent)\n","    # and diminished ( < 1) by negative/decreasing liquidity returns (through exponent)\n","    book_n_trade_data['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(( np.exp(book_data['log_liquidity1_ret'])*book_data['wap_eqi_price1_ret'])**2 ),\n","                                      20, 30, ids.shape[0])\n","    # copy of above\n","    book_n_trade_data['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(( np.exp(book_data['log_liquidity1_ret'])*book_data['wap_eqi_price1_ret'])**2 ),\n","                                      20, 30, ids.shape[0])\n","    # variance/ squared volatitliy of wap1 price returns per unit of spread\n","    # large value indicates volatilty\n","    book_n_trade_data['wap1_log_price_ret_per_spread_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(( book_data['wap1_log_price_ret']/book_data['spread'])**2 ),\n","                                      20, 30, ids.shape[0])\n","    # variance/ squared volatitliy of wap1 price returns per unit of liquidity\n","    # small value indicates volatilty?\n","    book_n_trade_data['wap1_log_price_ret_per_liq2_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array( ( book_data['wap1_log_price_ret'])**2/book_data['liquidity2'] ),\n","                                      20, 30, ids.shape[0])\n","\n","    # measure of variance/ squared volatility of liquidity1 returns\n","    book_n_trade_data['log_liquidity1_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['log_liquidity1_ret'])**2,\n","                                      20, 30, ids.shape[0])\n","\n","    # measure of variance/ squared volatility of liquidity2 returns\n","    book_n_trade_data['log_liquidity2_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['log_liquidity2_ret'])**2,\n","                                      20, 30, ids.shape[0])\n","\n","    book_n_trade_data['log_liquidity3_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                       np.array(book_data['time_id']),\n","                                       np.array(book_data['log_liquidity3_ret'])**2,\n","                                       20, 30, ids.shape[0])\n","\n","    # measure of variance/ squared volatility of log spread returns\n","    book_n_trade_data['log_spread_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['log_spread_ret'])**2,\n","                                      20, 30, ids.shape[0])\n","\n","    # counting number of data points available in each time bucket\n","    book_n_trade_data['book_delta_count_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array(book_data['wap1_log_price_ret']*0+1.0),\n","                                        20, 30, ids.shape[0])\n","\n","    # time weighted average of wap1_log_price in each time bucket\n","    book_n_trade_data['wap1_log_price_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap1_log_price']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","    # time weighted average of wap2_log_price in each time bucket\n","    book_n_trade_data['wap2_log_price_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap2_log_price']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","    # time weighted average of wap_eqi_price0 equilibrium price in each time bucket\n","    book_n_trade_data['wap_eqi_price0_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap_eqi_price0']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","    # time weighted average of wap_eqi_price1 equilibrium price in each time bucket\n","    book_n_trade_data['wap_eqi_price1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap_eqi_price1']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","\n","    # filter out the extremely high and low prices of wap1_log_price by amplifying with postiive and negative exponential of wap1_log_price\n","    # apply time weighted average to the amplified wap1_log_price\n","    # what may be the physical meaning?\n","    book_n_trade_data['wap1_log_price_amp_max_wavg'] = np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp( 4000*np.array(book_data['wap1_log_price'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","    book_n_trade_data['wap1_log_price_amp_min_wavg'] = -np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp(-4000*np.array(book_data['wap1_log_price'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","    # amplification of the difference between max and min\n","    book_n_trade_data['wavg_wap1_log_price_amp_diff']  = np.exp(book_n_trade_data['wap1_log_price_amp_max_wavg'] - book_n_trade_data['wap1_log_price_amp_min_wavg'])\n","\n","    # filter out the extremely high and low prices of wap_eqi_price0 by amplifying with postiive and negative exponential of wap_eqi_price0\n","    # apply time weighted average to the amplified wap_eqi_price0\n","    book_n_trade_data['wap_eqi_price0_amp_max_wavg'] = np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp( 4000*np.array(book_data['wap_eqi_price0'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","\n","    book_n_trade_data['wap_eqi_price0_amp_min_wavg'] = -np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp(-4000*np.array(book_data['wap_eqi_price0'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","    # amplification of the difference between max and min\n","    book_n_trade_data['wavg_wap_eqi_price0_amp_diff']  = np.exp(book_n_trade_data['wap_eqi_price0_amp_max_wavg'] - book_n_trade_data['wap_eqi_price0_amp_min_wavg'])\n","\n","    del book_n_trade_data['wap1_log_price_amp_max_wavg'], book_n_trade_data['wap1_log_price_amp_min_wavg']\n","    del book_n_trade_data['wap_eqi_price0_amp_max_wavg'], book_n_trade_data['wap_eqi_price0_amp_min_wavg']\n","\n","    book_n_trade_data['liquidity1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity1'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    book_n_trade_data['liquidity2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity2'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    book_n_trade_data['root_liquidity2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity2']))**0.5,\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    book_n_trade_data['liquidity3_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity3'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    book_n_trade_data['root_liquidity3_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                       np.array(book_data['time_id']),\n","                                         np.array((book_data['liquidity3']**0.5)),\n","                                         np.ones((book_data.shape[0])),\n","                                         20, 30, ids.shape[0])\n","    \n","    book_n_trade_data['quart_root_liquidity3_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                       np.array(book_data['time_id']),\n","                                         np.array((book_data['liquidity3']**0.25)),\n","                                         np.ones((book_data.shape[0])),\n","                                         20, 30, ids.shape[0])\n","\n","    # time weighted average of spread in each time bucket\n","    book_n_trade_data['spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['spread'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of inverse spread in each time bucket\n","    book_n_trade_data['inv_spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['spread']))**-1,\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of log spread in each time bucket\n","    book_n_trade_data['log_spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.log(np.array((book_data['spread']))),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of log spread 2 in each time bucket\n","    book_n_trade_data['log_spread2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['log_spread2'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of book size1 in each time bucket\n","    book_n_trade_data['book_size1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['book_size1'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of book size in each time bucket\n","    book_n_trade_data['book_size_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['book_size'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","\n","    ############################## TRADE DATA ##########################################\n","\n","    trade_data =  pd.read_parquet(os.path.join(data_dir,'trade_{}.parquet/stock_id={}'.format( dset, st_id)))\n","    trade_data['trade_volume'] = trade_data['size']*trade_data['price']\n","\n","    # bucketized trade volume\n","    book_n_trade_data['trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']),\n","                                        20, 30, ids.shape[0])\n","    # bucketized root of trade volume\n","    book_n_trade_data['sqrt_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**.5),\n","                                        20, 30, ids.shape[0])\n","    # bucketized cube root of trade volume\n","    book_n_trade_data['cube_root_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**(1/3)),\n","                                        20, 30, ids.shape[0])\n","\n","    # bucketized square of cube root of trade volume\n","    book_n_trade_data['trade_volume_p2/3_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**(2/3)),\n","                                        20, 30, ids.shape[0])\n","\n","    # bucketized quart root of trade volume\n","    book_n_trade_data['quart_root_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**.25),\n","                                        20, 30, ids.shape[0])\n","\n","    # count the number of trades in each time bucket\n","    book_n_trade_data['trade_count_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**0),\n","                                        20, 30, ids.shape[0])\n","\n","\n","    # trade volume per unit of liquidity1\n","    book_n_trade_data['trade_volume_per_liquidity1_wavg_buks'] = book_n_trade_data['trade_volume_buks']/book_n_trade_data['liquidity1_wavg']\n","    book_n_trade_data['trade_volume_per_liquidity2_wavg_buks'] = book_n_trade_data['trade_volume_buks']/book_n_trade_data['liquidity2_wavg']\n","\n","    # time weighted average of difference betweeen ask's level 1 and level 2 liquidity\n","    book_n_trade_data['ask_liq1_diff_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                        np.array(book_data['time_id']),\n","                                        np.array((book_data['ask_liq1_diff'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of difference betweeen bid's level 1 and level 2 liquidity\n","    book_n_trade_data['bid_liq1_diff_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                        np.array(book_data['time_id']),\n","                                        np.array((book_data['bid_liq1_diff'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    return book_n_trade_data\n","\n","\n","\n","\n","@jit(nopython=True)\n","def identify_missing_time_ids(all_time_ids, st_time_ids): # all_time_ids is all unique time_ids from all stocks, st_time_ids is time_ids for particular single stock\n","    j = 0\n","    z = 1 == np.zeros(  all_time_ids.shape[0]) # set all unique time_ids to False\n","    for i in range(st_time_ids.shape[0]):\n","        while all_time_ids[j] != st_time_ids[i]: # missing time id in the stock\n","            z[j] = False # set the missing time id index in all unique time ids array z to False\n","            j = j+1\n","            if j >= all_time_ids.shape[0]:\n","                return z\n","        z[j] = True\n","        j = j+1\n","    return z\n","\n","\n","\n","\n","def create_dataSet(st_ids,dset):\n","\n","\n","    st_ids = sorted(st_ids)\n","\n","    print('st_ids',st_ids)\n","\n","    # a list contains all stock data each element of list is a dictionary of features for a particular stock\n","    all_stock_data = Parallel(n_jobs = os.cpu_count() - 5)( delayed(create_stock_data)(st_id, dset) for st_id in st_ids)\n","\n","    final_data = {}\n","\n","    # get all unique time ids from all stocks. This is helpful to fill missing time ids.\n","    t_ids = sum([list(ss['time_id']) for ss in all_stock_data], [] )\n","    t_ids = list(np.unique(t_ids))\n","\n","    num_buks = 30\n","    t_ids_size = len(t_ids)\n","    st_ids_size = len(st_ids)\n","\n","    final_data['time_ids' ] = np.array(t_ids)\n","    final_data['stock_ids'] = np.array(st_ids)\n","\n","\n","    for key in all_stock_data[0].keys(): # common columns (features) to all stocks\n","        if key == 'time_id':\n","            continue\n","\n","        Z = np.zeros(( t_ids_size, st_ids_size, num_buks))\n","\n","        for st in range(st_ids_size):\n","            ss = all_stock_data[st]\n","\n","            #ts = index_into_set(np.array(time_ids), ss['time_id']).astype(int)\n","\n","            b = identify_missing_time_ids(np.array(t_ids), ss['time_id']) # all unique time ids from all stocks and time ids of a particular stock are input\n","\n","            #print(b)\n","            #print(b.shape)\n","\n","            Z[ b, st, :] = ss[key] # fill with features for avaialble time ids\n","\n","            Z[~b, st, :] = np.nanmean(ss[key]) # fill with mean of features for missing time ids\n","\n","            Z[:,st,:][np.isnan(Z[:,st,:])] = np.nanmean(Z[:,st,:]) # fill with mean of features for missing time ids and any missing bins\n","\n","            #del ss[key]\n","\n","        final_data[key] = Z\n","\n","        gc.collect()\n","\n","\n","    del all_stock_data\n","    gc.collect()\n","\n","\n","    # arbitrarily weighted average of wap1_log_price_ret_abs_vol_buks and wap2_log_price_ret_abs_vol_buks\n","    final_data['wap1_log_price_ret_vol_buks'] = ( final_data['wap1_log_price_ret_vol_buks']**2 + .25*final_data['wap2_log_price_ret_vol_buks']**2)**0.5\n","\n","    return final_data\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e43384eb","metadata":{"id":"e43384eb"},"outputs":[],"source":["def get_cohesion_features(train_buckets, final_features, ffrom=0):\n","    wap1_log_price_ret_buks = train_buckets['wap1_log_price_ret_buks'] # shape of (3830,112,30)\n","\n","    if ffrom > 0:\n","        buk = f'_:{ffrom}'\n","    else:\n","        buk = '_:0'\n","\n","    # variance along time_id axis, mean along bucket axis and then square root\n","    # basically standard deviation of wap1_log_price_ret_buks in each stock. This is like (overall) volatility over entire time period for each stock.\n","    stocks_overall_wap1_log_price_ret_vol = np.mean( np.var(wap1_log_price_ret_buks, 0,keepdims=True), 2, keepdims=True)**0.5 # shape of (1,112,1)\n","\n","    # normalize the variance of wap1_log_price_ret_buks by overall volatility, assume that mean of wap1_log_price_ret_buks is zero\n","    wap1_log_price_ret_normalized = wap1_log_price_ret_buks[:,:,ffrom:]/stocks_overall_wap1_log_price_ret_vol # shape of (3830,112,30)\n","\n","    # variance of wap1_log_price_ret_normalized along stock id axis, mean along bucket axis and then square root, shape of (3830,1,1) and then normalized by wap1_log_price_ret_vol (shape 3830 x 112, 1). dim 1 is broadcasted to dim 112\n","    # multiply by stocks_overall_wap1_log_price_ret_vol (shape = (1,112,1) ) to get the original variance of wap1_log_price_ret_buks to get final shape of (3830,112,1)\n","    # multiplication by stocks_overall_wap1_log_price_ret_vol (overall volatility) is the reverse of normalization\n","    # Volatility across stocks at each time id divided by volatility at each stocks and time id = factor of overall volatility across stocks contributed by each stock in each time id. above average results in > 1, below average results in < 1\n","    # This is multiplied by overall volatility across time for each stock.\n","    # It is just a scalar giving importance to amount of variance across all time.\n","    # ati = across time, ast = across stock,     # shape of (3830,112,1),\n","    final_features['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol'+ buk] = stocks_overall_wap1_log_price_ret_vol*np.mean(  np.var( wap1_log_price_ret_normalized, 1, keepdims=True), 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","    # shape of (3830,112,1), market is np.mean(wap1_log_price_ret_normalized, 1, keepdims=True). IT is just mean over all stock ids.\n","    final_features['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol'+ buk] = stocks_overall_wap1_log_price_ret_vol*np.mean( (np.mean(wap1_log_price_ret_normalized, 1, keepdims=True) )**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","\n","    # deviation from market is np.mean(wap1_log_price_ret_normalized, 1, keepdims=True)  minus wap1_log_price_ret_normalized\n","    final_features['wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol' + buk] = stocks_overall_wap1_log_price_ret_vol*np.mean( (np.mean(wap1_log_price_ret_normalized, 1, keepdims=True) - wap1_log_price_ret_normalized)**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","\n","\n","\n","\n","\n","\n","# def cluster_agg_for_inference(x, clusters, agg_fun,test,train_all_unique_stock_ids):\n","#     r = 0*x\n","\n","#     train_all_unique_stock_ids = pd.DataFrame(train_all_unique_stock_ids)\n","#     train_all_unique_stock_ids['labels'] = clusters.values\n","#     clusters_inference = test.merge(train_all_unique_stock_ids, how='left', on='stock_id')['labels']\n","\n","#     for k in np.unique(clusters_inference):\n","#         print('k',k)\n","#         z = agg_fun(x[:,clusters_inference==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","#         r[:,clusters_inference==k,:] = np.repeat(z, repeats=int(np.sum(clusters_inference==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","#     return r\n","\n","def cluster_agg_for_inference(x, clusters, agg_fun,test,train_all_unique_stock_ids):\n","    r = 0*x\n","\n","    train_all_unique_stock_ids = pd.DataFrame(train_all_unique_stock_ids)\n","    train_all_unique_stock_ids['labels'] = clusters.values\n","    unique_test_stock_ids_df = pd.DataFrame(test['stock_id'].unique())\n","    unique_test_stock_ids_df.columns = ['stock_id']\n","    clusters_inference = unique_test_stock_ids_df.merge(train_all_unique_stock_ids, how='left', on='stock_id')['labels']\n","\n","    for k in clusters_inference.unique():\n","        z = agg_fun(x[:,clusters_inference==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","        r[:,clusters_inference==k,:] = np.repeat(z, repeats=int(np.sum(clusters_inference==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","    return r\n","\n","\n","\n","\n","\n","def get_misc_features(train_buckets, final_features):\n","\n","    trade_volume_buks    = train_buckets['trade_volume_buks']\n","    wap1_log_price_ret_vol_buks    = train_buckets['wap1_log_price_ret_vol_buks']\n","    sqrt_trade_volume_buks   = train_buckets['sqrt_trade_volume_buks']\n","    liquidity2_wavg   = train_buckets['liquidity2_wavg']\n","    log_spread2_wavg  = train_buckets['log_spread2_wavg']\n","    liquidity3_wavg   = train_buckets['liquidity3_wavg']    \n","    quart_root_liquidity3_wavg = train_buckets['quart_root_liquidity3_wavg']\n","\n","    # average out along the time ids and buckets axis\n","    stocks_overall_trade_volume  = np.nanmean( trade_volume_buks, (0,2), keepdims=True) # shape of (1,112,1)\n","    stocks_overall_sqrt_trade_volume = np.nanmean(sqrt_trade_volume_buks, (0,2), keepdims=True) # shape of (1,112,1)\n","    stocks_overall_liquidity2 = np.nanmean(liquidity2_wavg, (0,2), keepdims=True) # shape of (1,112,1)\n","    # average out along the buckets axis\n","    stocks_overall_wap1_log_price_ret_vol = np.nanmean(wap1_log_price_ret_vol_buks**2, 2, keepdims=True)**.5 # shape of (3830,112,1)\n","\n","    # (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)  = (vol[:,:, 0:] / s ) / (liq2[:,:, 0:]) / l2), standardized volume divided by standardized liquidity2\n","    # (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8) , diminish the effect of outliers, or reduce large values\n","    # np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True) , average over stock_id and buckets axis\n","    # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8) , destandardize and then take the power of 8 to undo the effect of 1/8\n","    # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5 , take the square root as it is liquidity of level 2\n","    # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1 , divide by wap1_log_price_ret_vol to check > 1\n","    # for greater than wap1_log_price_ret_vol or wap1_log_price_ret_vol < 1 for less than wap1_log_price_ret_vol\n","    # log detects > 1 or < 1\n","    final_features['soft_stock_mean_tvpl2_:0'    ] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    # using only the last 20 buckets\n","    final_features['soft_stock_mean_tvpl2_:10'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    # using only the last 10 buckets\n","    final_features['soft_stock_mean_tvpl2_:20'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","    # same as above but only using the last bucket of liquidity2\n","    final_features['soft_stock_mean_tvpl2_liqf'       ] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['soft_stock_mean_tvpl2_liqf_volf10'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,10:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['soft_stock_mean_tvpl2_liqf_volf20'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,20:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","\n","\n","    tvpl3lf20 = np.log( np.mean( sqrt_trade_volume_buks, 2, keepdims=True)**(1/2)/np.mean( (quart_root_liquidity3_wavg[:,:, 20:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_rmed2v1lf20'] = np.log( np.median(tvpl3lf20/stocks_overall_wap1_log_price_ret_vol,1, keepdims=True))\n","\n","\n","\n","    # np.mean(vol1[:,:,25:]**2,2,keepdims=True) ,squared wap1_log_price_ret_vol_buks average for last 5 buckets\n","    # np.mean(vol1[:,:,:15]**2,2,keepdims=True) , squared wap1_log_price_ret_vol_buks average for first 15 buckets\n","    # np.nanmedian( np.mean(vol1[:,:,25:]**2,2,keepdims=True) / np.mean(vol1[:,:,:15]**2,2,keepdims=True),1,keepdims=True) , median of ratio  along stock_id axis , shape of (3830,1,1)\n","    # square root to get back standard deviation / volatility\n","    # log to detect > 1 or < 1\n","    final_features['v1proj_25_15'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True) / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # same as above but only for wap1_log_price_ret high correlation stocks\n","    # final_features['v1proj_25_15_lr1_high_corr_stocks'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                             / np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","    # same as above but only for wap1_log_price_ret_vol high correlation stocks\n","    #final_features['v1proj_25_15_vol1_high_corr_stocks'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                             / np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","    # same as above but ratio of average of last 5 buckets to average of all buckets\n","    #final_features['v1proj_25_lr1_high_corr_stocks']     = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                         / np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","    # same as above but ratio of average of last 5 buckets to average of all buckets\n","    #final_features['v1proj_25_vol1_high_corr_stocks']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                          / np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # average of log_spread_ret_sqr_vol_buks over all buckets\n","    final_features['lsvol'] = np.log( np.nanmean(train_buckets['log_spread_ret_sqr_vol_buks'], 2, keepdims=True))\n","\n","\n","    # average of log_liquidity1_ret_sqr_vol_buks over all buckets\n","    final_features['liqvol1'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], 2, keepdims=True))\n","    # average of log_liquidity2_ret_sqr_vol_buks over all stock ids and buckets\n","    final_features['liqvol1_smean'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], (1,2), keepdims=True)) # shape of (3830,1,1)\n","\n","    # average of log_liquidity1_ret_sqr_vol_buks over all buckets for each cluster\n","    # is grouped by cluster and then median is taken. repeat the median value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['liqvol1_smean_c3'] = np.log( cluster_agg(np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], 2, keepdims=True), wap1_log_price_ret_cluster3 ,np.nanmedian))\n","\n","    # average of log_liquidity2_ret_sqr_vol_buks over all buckets\n","    final_features['liqvol2'] = np.log( np.nanmean(train_buckets['log_liquidity2_ret_sqr_vol_buks'], 2, keepdims=True))\n","    final_features['liqvol3'] = np.log( np.nanmean(train_buckets['log_liquidity3_ret_sqr_vol_buks'], 2, keepdims=True))\n","\n","\n","    # ratio of average log_liquidity1_ret_sqr_vol_buks in last 15 buckets to first 15 buckets\n","    final_features['liqvol1_15_15'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'][:,:,15:  ], 2, keepdims=True)\n","                                            /np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'][:,:,  :15], 2, keepdims=True))\n","\n","    # average of trade_count over all buckets\n","    final_features['trade_count']      = np.log( np.nanmean(train_buckets['trade_count_buks']    , 2, keepdims=True))\n","    # average of squre root of trade_count over all buckets\n","    final_features['root_trade_count'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","\n","    # average of squre root of trade_count over all stock ids and all buckets\n","    final_features['root_trade_count_smean'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, (1,2), keepdims=True))\n","\n","    # average of squre root of number of data points in a bucket over all buckets\n","    final_features['root_book_delta_count'] = np.log( np.nanmean(train_buckets['book_delta_count_buks']**.5, 2, keepdims=True))\n","\n","    # average of square root of trade_count over all buckets for each cluster\n","    # is grouped by cluster 1 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['root_trade_count_smean_c1'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","    ####                                                                 wap1_log_price_ret_cluster1,np.nanmean))\n","\n","    # average of square root of trade_count over all buckets for each cluster\n","    # is grouped by cluster 2 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['root_trade_count_smean_c2'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","    ####                                                                 wap1_log_price_ret_cluster2,np.nanmean))\n","\n","    # average of square root of trade_count over all buckets for each cluster\n","    # is grouped by cluster 3 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['root_trade_count_smean_c3'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","    ####                                                                 wap1_log_price_ret_cluster3,np.nanmean))\n","\n","    # variance of square root of trade_count over all buckets for each cluster\n","    final_features['root_trade_count_var'] = np.log( np.nanvar(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","\n","    # ratio of average trade_count in last 15 buckets to first 15 buckets\n","    final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","                                                      np.nanmean(train_buckets['trade_count_buks'][:,:,  :15], 2, keepdims=True))\n","\n","    # ratio of average square root trade_count in last 15 buckets to first 15 buckets\n","    final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","                                                       np.nanmean(train_buckets['trade_count_buks'][:,:,  :15]**.5, 2, keepdims=True))\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last bucket to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,29:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True),1,keepdims=True)**.5)\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last 10 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_20']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,20:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_15_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_15_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                               / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last bucket to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,29:]**2,2,keepdims=True)\n","                                                               / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 10 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_20_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,20:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","    # difference between 75% and 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_q3q1'] = np.log(np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True))\n","                                                    ,0.75, axis=1,keepdims=True)\n","                                                -\n","                                                np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True))\n","                                                    ,0.25, axis=1,keepdims=True))\n","\n","\n","    # CHECK IF there are more than 100 stocks\n","    # Basically, all features calcualted above are repeated and aggregated for each cluster using mean, median and std etc..\n","#     if wap1_log_price_ret_vol_buks.shape[1]>100:\n","\n","\n","#         final_features['v1proj_25_c1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                              / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), wap1_log_price_ret_cluster1, np.median)**0.5 )\n","\n","\n","#         final_features['v1proj_25_c2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         wap1_log_price_ret_cluster2, np.median)**0.5 )\n","#         final_features['v1proj_25_c3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         wap1_log_price_ret_cluster3, np.median)**0.5 )\n","#         final_features['v1proj_25_c4'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                        wap1_log_price_ret_cluster4, np.median)**0.5 )\n","#         final_features['v1proj_25_c5'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         wap1_log_price_ret_cluster5, np.median)**0.5 )\n","\n","\n","\n","#         final_features['soft_stock_mean_tvpl2_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","#         final_features['soft_stock_mean_tvpl2_10_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_10_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_10_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","#         final_features['soft_stock_mean_tvpl2_20_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_20_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_20_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","\n","#         final_features['v1proj_25_c1_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster1, np.nanstd)**0.5  )\n","#         final_features['v1proj_25_c2_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster2, np.nanstd)**0.5 )\n","#         final_features['v1proj_25_c3_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster3, np.nanstd)**0.5 )\n","#         final_features['v1proj_25_c4_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster4, np.nanstd)**0.5 )\n","#         final_features['v1proj_25_c5_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster5, np.nanstd)**0.5 )\n","\n","\n","#         final_features['v1proj_25_vc1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters1, np.median)**0.5  )\n","#         final_features['v1proj_25_vc2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters2, np.median)**0.5 )\n","#         final_features['v1proj_25_vc3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters3, np.median)**0.5 )\n","\n","#         final_features['v1proj_25_vc4'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters4, np.median)**0.5 )\n","\n","\n","\n","\n","#         final_features['v1proj_25_vvc1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_quart_trade_volume_clusters1, np.median)**0.5  )\n","#         final_features['v1proj_25_vvc2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_quart_trade_volume_clusters2, np.median)**0.5 )\n","#         final_features['v1proj_25_vvc3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_quart_trade_volume_clusters3, np.median)**0.5 )\n","\n","\n","\n","#         final_features['v1spprojt15f25_c1'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster1, np.median) )\n","#         final_features['v1spprojt15f25_c2'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster2, np.median) )\n","#         final_features['v1spprojt15f25_c3'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster3, np.median) )\n","#         final_features['v1spprojt15f25_c4'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster4, np.median) )\n","\n","#         final_features['v1spprojt15f25_vc1'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            log_wap1_log_price_ret_vol_clusters1, np.median) )\n","#         final_features['v1spprojt15f25_vc2'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            log_wap1_log_price_ret_vol_clusters2, np.median) )\n","#         final_features['v1spprojt15f25_vc3'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            log_wap1_log_price_ret_vol_clusters3, np.median) )\n","\n","\n","    # ratio of average of all buckets trade_volume to average of all buckets liquidity2, shape of (3830,1,1)\n","    final_features['tvpl2_rmed2v1']     = np.log( np.median( ( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","                                                    / np.mean( liquidity2_wavg[:,:, 0:]**.5, 2, keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","    # ratio of average of all buckets trade_volume to average of last 5 buckets liquidity2, shape of (3830,1,1)\n","    final_features['tvpl2_rmed2v1lf25'] = np.log( np.median(( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","                                                    / np.mean( (liquidity2_wavg[:,:, 25:])**.5, (2), keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","    # ratio of average of all buckets trade_volume to average of last 1 buckets liquidity2, shape of (3830,1,1)\n","    final_features['tvpl2_rmed2v1lf29'] = np.log( np.median(( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","                                                    / np.mean( (liquidity2_wavg[:,:, 29:])**.5, (2), keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","\n","\n","\n","    # ratio of average of all buckets trade_volume to average of    all buckets liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2']        = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg)**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all buckets trade_volume to average of last 20 buckets liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2_liqf10'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all buckets trade_volume to average of last 10 buckets liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2_liqf20'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all buckets trade_volume to average of last 1 bucket liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2_liqf29'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","\n","    # ratio of average of all time ids and buckets trade_volume to average of all bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol'       ] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:, 0:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all time ids and buckets trade_volume to average of last 20 bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol_liqf10'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all time ids and buckets trade_volume to average of last 10 bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol_liqf20'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all time ids and buckets trade_volume to average of last 1 bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol_liqf29'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","\n","    final_features['tvpl3'       ] = np.log(np.mean( sqrt_trade_volume_buks, 2, keepdims=True)/np.mean( (quart_root_liquidity3_wavg[:,:, 0:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_liqf10'] = np.log(np.mean( sqrt_trade_volume_buks, 2, keepdims=True)/np.mean( (quart_root_liquidity3_wavg[:,:,10:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_liqf20'] = np.log(np.mean( sqrt_trade_volume_buks, 2, keepdims=True)/np.mean( (quart_root_liquidity3_wavg[:,:,20:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_liqf29'] = np.log(np.mean( sqrt_trade_volume_buks, 2, keepdims=True)/np.mean( (quart_root_liquidity3_wavg[:,:,29:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","\n","    final_features['tvpl3_smean_vol'       ] = np.log(np.mean( sqrt_trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (quart_root_liquidity3_wavg[:,:, 0:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_smean_vol_liqf10'] = np.log(np.mean( sqrt_trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (quart_root_liquidity3_wavg[:,:,10:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_smean_vol_liqf20'] = np.log(np.mean( sqrt_trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (quart_root_liquidity3_wavg[:,:,20:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['tvpl3_smean_vol_liqf29'] = np.log(np.mean( sqrt_trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (quart_root_liquidity3_wavg[:,:,29:]), (2), keepdims=True)**(4/3)/stocks_overall_wap1_log_price_ret_vol)\n","\n","    #\n","    final_features['v1liq2projt5'] = np.log( ( np.mean( liquidity2_wavg[:,:,  : 5]**(1/8), 2, keepdims=True)**8\n","                                            / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","    #\n","    final_features['v1liq2projt10'] = np.log( ( np.mean( liquidity2_wavg[:,:,  :10]**(1/8), 2, keepdims=True)**8\n","                                             / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","\n","    final_features['v1liq2projt20'] = np.log( ( np.mean( liquidity2_wavg[:,:,  :20]**(1/8), 2, keepdims=True)**8\n","                                             / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","    # ratio of average of first 10 buckets liquidity2_wavg to average of last 1 buckets liquidity2, shape of (3830,112,1)\n","    final_features['liqt10rf29'] = np.log( np.mean( liquidity2_wavg[:,:,:10]**.5, (2), keepdims=True)**2 / liquidity2_wavg[:,:,29:] )\n","    # ratio of average of first 20 buckets liquidity2_wavg to average of last 1 buckets liquidity2, shape of (3830,112,1)\n","    final_features['liqt20rf29'] = np.log( np.mean( liquidity2_wavg[:,:,:20]**.5, (2), keepdims=True)**2 / liquidity2_wavg[:,:,29:] )\n","\n","\n","\n","    # median along stock id axis, how liquidity2_wavg changes/ratio in first 10 bins to last 5 bins. shape of (3830,1,1)\n","    final_features['v1liq2sprojt10f25'] = np.log( np.median(\n","                          np.mean(liquidity2_wavg[:,:,:10]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity2_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/2) )\n","\n","    final_features['v1liq2sprojt5f25'] = np.log( np.median(\n","                          np.mean(liquidity2_wavg[:,:,  : 5]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity2_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/2) )\n","\n","    final_features['v1liq3sprojt10f25'] = np.log( np.median(\n","                          np.mean(liquidity3_wavg[:,:,  : ]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity3_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/3) )\n","\n","    final_features['v1liq3sprojt15f25'] = np.log( np.median(\n","                          np.mean(liquidity3_wavg[:,:,  :15 ]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity3_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/3) )\n","\n","    final_features['v1liq3sprojt5f25'] = np.log( np.median(\n","                          np.mean(liquidity3_wavg[:,:,  : 5]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity3_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/3) )\n","\n","    # median along stock id axis, of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt10f29'] = np.median( - np.mean(log_spread2_wavg[:,:,  :  ], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:  ], (2),keepdims=True) , 1, keepdims=True)\n","    # median along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f25'] = np.median( - np.mean(log_spread2_wavg[:,:,  :15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:  ], (2),keepdims=True) , 1, keepdims=True)\n","    # median along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f29'] = np.median( - np.mean(log_spread2_wavg[:,:,  :15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:  ], (2),keepdims=True), 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f29_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f29_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f25_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis,of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f25_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf29_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf29_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf25_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf25_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","\n","    return\n","\n","\n","\n","\n","\n","def get_simple_features(binned_features, final_features, name, ffrom=0):\n","    wap1_log_price_ret_vol = binned_features[name+'_buks']\n","\n","    suffix = f'_from_{ffrom}'\n","\n","    # average of wap1_log_price_ret_buks in all buckets of a time id. Then average over all time ids. Then take square root, shape of (1,112,1)\n","    stocks_overall_wap1_log_price_ret_mean  = np.mean(np.mean(wap1_log_price_ret_vol**2, (2), keepdims=True)**.5, 0, keepdims=True)\n","\n","    # wap1_log_price_ret_vol and then normalized by wap1_log_price_ret_vol (shape 3830 x 112, 1), is it 1 for ffrom=0?\n","    final_features[name + suffix] =            np.log(             np.mean(wap1_log_price_ret_vol[:,:,ffrom:]**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","    #\n","    final_features[name+'stock_mean'+suffix] = np.log(stocks_overall_wap1_log_price_ret_mean*np.median( np.mean( wap1_log_price_ret_vol[:,:,ffrom:]/stocks_overall_wap1_log_price_ret_mean, 2, keepdims=True), 1, keepdims=True)/final_features['wap1_log_price_ret_vol'])\n","\n","\n","\n","\n","\n","\n","\n","def generate_liquidity_features(train_buckets):\n","\n","    final_features = {}\n","\n","    final_features['time_ids'] = (1*train_buckets['time_ids'][:,np.newaxis] + 0*train_buckets['stock_ids'][np.newaxis,:])[:,:,np.newaxis] # (3830, 112, 1) containing only repeated time ids along stock axis\n","    final_features['stock_ids'] = (0*train_buckets['time_ids'][:,np.newaxis] + 1*train_buckets['stock_ids'][np.newaxis,:])[:,:,np.newaxis] # (3830, 112, 1) containing only repeated stock ids along time axis\n","\n","    # average out along the buckets axis\n","    final_features['wap1_log_price_ret_vol'] = np.mean(train_buckets['wap1_log_price_ret_vol_buks']**2, 2, keepdims=True)**0.5\n","\n","    # (log liquidity ret x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","    # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","    # check if greater than or less than wap1 price volatility\n","    final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol'] = np.log(np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol'] = np.log(np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2'] = np.log(np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","\n","    # average out along the buckets axis\n","    final_features['wap1_log_price_ret_per_liq2_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks']**2, 2, keepdims=True)**0.5)\n","    final_features['wap1_log_price_ret_per_spread_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks']**2, 2, keepdims=True)**0.5)\n","\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = np.log( np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    final_features['wap1_log_price_ret_per_liq2_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = np.log( np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio'] = np.log( np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    # median across all stocks (dimension 1) for that time id.\n","    final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                                    / np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    # median across all stocks (dimension 1) for that time id.\n","    final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                                    / np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    # median across all stocks (dimension 1) for that time id.\n","    final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                                    / np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","\n","    # (liquidity x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","    # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","    # check if greater than or less than wap1 price volatility\n","    final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol'] = np.log(np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","\n","    # (liquidity x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","    # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","    # check if greater than or less than wap1 price volatility\n","    # volatitlity in wap1_log_price_ret when liquidity1 is negative. i.e. decreases divided by wap1_log_price_ret_vol\n","    final_features['wap1_log_price_ret_neg_log_liq_ret_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'] )\n","    # volatitlity in wap1_log_price_ret when liquidity1 is positive. i.e. increases divided by wap1_log_price_ret_vol\n","    final_features['wap1_log_price_ret_pos_log_liq_ret_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'] )\n","    # difference between volatitlity in wap1_log_price_ret when liquidity1 is positive and negative. i.e. increases minus decreases\n","    final_features['wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol'] = final_features['wap1_log_price_ret_pos_log_liq_ret_sqr_vol'] - final_features['wap1_log_price_ret_neg_log_liq_ret_sqr_vol']\n","\n","\n","    get_cohesion_features(train_buckets, final_features, ffrom=0) # uses wap1_log_price_ret_buks from bucket 0 to 30 of a time id, all\n","    get_cohesion_features(train_buckets, final_features, ffrom=10) # uses wap1_log_price_ret_buks from bucket 10 to 30 of a time id, last two thirds\n","    get_cohesion_features(train_buckets, final_features, ffrom=20) # uses wap1_log_price_ret_buks from bucket 20 to 30 of a time id, last one third\n","\n","    get_misc_features(train_buckets, final_features)\n","\n","\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=0)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=10)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=20)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=25)\n","\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_quart_vol', ffrom=0)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_quart_vol', ffrom=10)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_quart_vol', ffrom=20)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_quart_vol', ffrom=25)\n","\n","    # standardize wap1_log_price_ret_vol by dividing by mean of wap1_log_price_ret_vol\n","    final_features['vol1_mean'] = np.log(final_features['wap1_log_price_ret_vol']/np.nanmean(final_features['wap1_log_price_ret_vol'], 0, keepdims=True))\n","\n","    # std of ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend, shape of (1,112,1)\n","    final_features['mean_half_delta'] = np.nanstd( np.log( np.mean( train_buckets['wap1_log_price_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)\n","                                                         / np.mean( train_buckets['wap1_log_price_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True) ) , 0, keepdims=True)\n","\n","    # std of difference between last bin and first bin of log_spread_wavg,  shape of (1,112,1)\n","    final_features['mean_half_delta_lsprd'] = np.log(np.nanstd( (  train_buckets['log_spread_wavg'][:,:,-1: ]\n","                                                         -  train_buckets['log_spread_wavg'][:,:,  :1] ), 0, keepdims=True) )\n","\n","    # take log\n","    final_features['log_wap1_log_price_ret_vol'] = np.log(final_features['wap1_log_price_ret_vol'])\n","\n","    return final_features\n","\n","\n","\n","\n","\n","\n","\n","def read_targets_from_df(df, s, t):\n","    # returns a matrix of shape (len(t), len(s)) with target values as entries\n","    t = list(t)\n","    s = list(s)\n","\n","\n","    Z = np.zeros((len(t), len(s)))\n","\n","    dft = np.array(df['time_id'])\n","    dfs = np.array(df['stock_id'])\n","    dfr = np.array(df['target'])\n","\n","\n","    for k in range(df.shape[0]):\n","        Z[t.index(dft[k]), s.index(dfs[k])] = dfr[k]\n","    return Z\n","\n","\n","\n","\n","\n","\n","\n","def merge_features_to_df(fdict, df, features):\n","\n","    T = fdict['time_ids'][:,:,0]\n","    S = fdict['stock_ids'][:,:,0]\n","\n","    T = np.reshape(T, [T.shape[0]*T.shape[1]])\n","    S = np.reshape(S, [S.shape[0]*S.shape[1]])\n","\n","    sh = np.max( [fdict[f].shape[1] for f in features] )\n","    sq = np.max( [fdict[f].shape[0] for f in features] )\n","\n","    for f in features:\n","        if fdict[f].shape[1]==1:\n","            print(f, 'shape[1]==1')\n","            fdict[f] = np.repeat(fdict[f], repeats=sh, axis=1)\n","        if fdict[f].shape[0]==1:\n","            print(f, 'shape[0]==1')\n","            fdict[f] = np.repeat(fdict[f], repeats=sq, axis=0)\n","\n","    reshaped_features = [np.reshape( fdict[feature], [fdict[feature].shape[0]*fdict[feature].shape[1]] )\n","                         for feature in features]\n","\n","    dfz = pd.DataFrame(data=np.vstack([S,T] + reshaped_features  ).T, columns=['stock_id', 'time_id']+features)\n","    dfz['time_id' ] = dfz['time_id' ].astype(int)\n","    dfz['stock_id'] = dfz['stock_id'].astype(int)\n","\n","    dfz = df.merge(dfz, on=['stock_id', 'time_id'], how='left')\n","    # some time ids are missing in the df, so df is smaller than dfz\n","    # 428960 to 428932, 151 to 152\n","    # suffix _x and _y are added for clashing columns during merge\n","    return dfz\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a0eabed3","metadata":{"id":"a0eabed3"},"outputs":[],"source":["\"\"\"\n","Clustering on training data to get categorical features. This is NOT done for test data\n","\"\"\"\n","\n","\n","class FeatureTransformation:\n","    def __init__(self):\n","        self.scaler = None\n","\n","    def feature_normalization(self, X_train, transform_type='minmax'):\n","        \"\"\"\n","        Normalize features using MinMaxScaler or StandardScaler.\n","        \"\"\"\n","        self.scaler = StandardScaler() if transform_type == 'standard' else MinMaxScaler()\n","        X_train_normalized = self.scaler.fit_transform(X_train)\n","        return X_train_normalized\n","\n","    def inv_feature_normalization(self, X_train_normalized):\n","        \"\"\"\n","        Inverse transform normalized features to their original scale.\n","        \"\"\"\n","        return self.scaler.inverse_transform(X_train_normalized)\n","\n","\n","def clustering_on_training_data(all_stocks_first_10_min_vol_df):\n","\n","\n","    train_trade_paths = sorted(glob.glob('trade_train_partial.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","    train_book_paths = sorted(glob.glob('book_train_partial.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","\n","    unique_stock_ids = []\n","    for path in train_book_paths:\n","        unique_stock_ids.append(int(path.split('=')[1]))\n","\n","    train = pd.read_csv('train_partial.csv')\n","    train_target = train\n","\n","    all_uniq_time_ids = pd.DataFrame({'time_id':train['time_id'].unique()})\n","\n","\n","\n","\n","    def get_alternating_cluster_coloring(labels):\n","\n","        sorted_labels = np.sort(labels)\n","        colors = ['yellow', 'magenta']\n","        coloring = []\n","\n","        sorted_labels = np.insert(sorted_labels, 0, sorted_labels[0], axis=0)\n","        for i in range(len(sorted_labels)-1):\n","            if sorted_labels[i+1] == sorted_labels[i]:\n","                coloring.append(colors[0])\n","            else:\n","                colors = colors[::-1]\n","                coloring.append(colors[0])\n","        return coloring\n","\n","\n","\n","\n","\n","\n","\n","        ## verify the clustering using silhouette score for particular clustring parameters\n","    def plot_silhouette_scores(X_train,labels,metric,linkage,clustering_type):\n","        unique_clusters = np.unique(labels)\n","        num_clusters = len(unique_clusters)\n","        sample_silhouette_values = silhouette_samples(X=X_train, labels=labels,metric=metric)\n","        silhouette_avg = silhouette_score(X=X_train, labels=labels,metric=metric)\n","        fig, ax1 = plt.subplots()\n","        y_lower = 10\n","        mean_num_zones_dev_in_clusters = []\n","\n","        mean_num_zones_in_clusters = X_train.shape[0] /num_clusters\n","\n","        for i in range(num_clusters): # exclude the\n","            # Aggregate the silhouette scores for samples belonging to\n","            # cluster i, and sort them\n","            ith_cluster_silhouette_values = sample_silhouette_values[labels == i+1]\n","            ith_cluster_silhouette_values.sort()\n","            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","            y_upper = y_lower + size_cluster_i\n","            color = cm.nipy_spectral(float(i+1) / num_clusters)\n","            ax1.fill_betweenx(\n","                np.arange(y_lower, y_upper),\n","                0,\n","                ith_cluster_silhouette_values,\n","                facecolor=color,\n","                edgecolor=color,\n","                alpha=0.7,\n","            )\n","            # Label the silhouette plots with their cluster numbers at the middle\n","            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n","            # Compute the new y_lower for next plot\n","            y_lower = y_upper + 10  # 10 for the 0 samples\n","            mean_num_zones_dev_in_clusters.append(abs(mean_num_zones_in_clusters - np.sum(labels == i+1) ))\n","\n","        num_neg_silhouette_scores = np.sum(sample_silhouette_values < 0 )\n","        min_silhouette_score = min(sample_silhouette_values)\n","        dev_from_mean_num_zone = np.sum(np.array(mean_num_zones_dev_in_clusters))\n","\n","        print(\"[\",num_clusters,',',silhouette_avg,',',num_neg_silhouette_scores,',',min_silhouette_score,\"]\")\n","        ax1.set_title(\"met: \"+metric+\", link: \"+linkage+\", sil_avg: \"+str(silhouette_avg)+\",\\n num_neg_sil_score: \"+str(num_neg_silhouette_scores)+\", min_sil_score: \"+str(min_silhouette_score)+\",\\n num_clust: \"+str(num_clusters)+\", Clust_type : \"+clustering_type )\n","        ax1.set_xlabel(\"The silhouette coefficient values\")\n","        ax1.set_ylabel(\"Cluster label\")\n","        # The vertical line for average silhouette score of all the values\n","        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","        return silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone\n","\n","\n","\n","\n","\n","\n","\n","\n","    def find_unique_time_ids_in_all_stocks(train):\n","        all_time_ids = np.array([])\n","        for st_id in unique_stock_ids:\n","            all_time_ids = np.append(all_time_ids,train[train['stock_id'] == st_id]['time_id'])\n","        return np.unique(all_time_ids)\n","\n","    unique_time_ids = find_unique_time_ids_in_all_stocks(train)\n","\n","\n","    train_common_time_ids_df = pd.DataFrame()\n","    for st_id in unique_stock_ids:\n","        temp_df = pd.DataFrame()\n","        temp_df[str(st_id)] = train[train['stock_id'] == st_id]['target']\n","        temp_df.index = train[train['stock_id'] == st_id]['time_id']\n","        temp_df = temp_df.reindex(unique_time_ids).ffill().bfill() ## forward and backward fill the missing values so that data is available at all time_id\n","        train_common_time_ids_df = pd.concat([train_common_time_ids_df,temp_df],axis=1)\n","\n","    train_common_time_ids_df\n","\n","\n","    target_corr_mat = train_common_time_ids_df.corr()\n","    target_corr_mat\n","\n","\n","    if ~(np.any(np.where(target_corr_mat < 0))):\n","        dissimilarity = 1 - abs(target_corr_mat)\n","\n","    # create map of labels_order (0,1,..., 111, 112) to stock_id_dict (0,1,..., 125, 126)\n","    map_labels_order_to_stock_id_dict = {x: target_corr_mat.columns[x] for x in range(112)}\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using complete for best small number of clusters\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","    metrics = ['null']\n","\n","    final_pear_corr_target_vol_clusters = pd.DataFrame(columns=['stock_id'])\n","    final_pear_corr_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","\n","\n","    c = [3] # number of clusters\n","    i=0\n","    for metric in metrics:\n","        for method in ['complete']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.5]: #[0.2,0.45]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order,ax= ax)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix with weighted linkage\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","    metrics = ['null']\n","\n","    c = [49] # number of clusters\n","    i=0\n","    for metric in metrics:\n","        for method in ['weighted']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.15]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using ward\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","\n","    c=[90] # number of clusters\n","    i=0\n","    metrics = ['nil']\n","    for metric in metrics:\n","        for method in ['ward']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.1]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using median\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","\n","    c=[10] # number of clusters\n","    i=0\n","    metrics = ['nil']\n","    for metric in metrics:\n","        for method in ['median']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.25]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using average\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","\n","    c=[26] # number of clusters\n","    i=0\n","    metrics = ['nil']\n","    for metric in metrics:\n","        for method in ['average']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.2]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    def create_feature_matrix(feature_dicts):\n","        \"\"\"\n","        Create a feature matrix from a list of dictionaries containing feature values.\n","        \"\"\"\n","        n_samples = len(feature_dicts[0])\n","        n_features = len(feature_dicts)\n","        X_feat = np.zeros((n_samples, n_features))\n","\n","        for i, feature_dict in enumerate(feature_dicts):\n","            X_feat[:, i] = np.array(list(feature_dict.values()))\n","\n","        return X_feat\n","\n","\n","\n","\n","\n","\n","\n","\n","    def calculate_summary_statistics(train_df):\n","        \"\"\"\n","        Calculate summary statistics for each stock's target column.\n","        \"\"\"\n","        summary_stats = {\n","            'total_time_id': {},\n","            'mean_vol': {},\n","            'std_vol': {},\n","            'min_vol': {},\n","            'p25_vol': {},\n","            'median_vol': {},\n","            'p75_vol': {},\n","            'max_vol': {},\n","            'skew_vol': {},\n","            'kurt_vol': {}\n","        }\n","\n","        for st_id, group in train_df.groupby('stock_id'):\n","            target = group['target']\n","            summary_stats['total_time_id'][st_id] = len(target)\n","            summary_stats['mean_vol'][st_id] = target.mean()\n","            summary_stats['std_vol'][st_id] = target.std()\n","            summary_stats['min_vol'][st_id] = target.min()\n","            summary_stats['p25_vol'][st_id] = target.quantile(0.25)\n","            summary_stats['median_vol'][st_id] = target.median()\n","            summary_stats['p75_vol'][st_id] = target.quantile(0.75)\n","            summary_stats['max_vol'][st_id] = target.max()\n","            summary_stats['skew_vol'][st_id] = skew(target)\n","            summary_stats['kurt_vol'][st_id] = kurtosis(target)\n","\n","        return summary_stats\n","\n","\n","    # Usage\n","    # train = pd.read_csv('your_train_data.csv')  # Assuming 'train' is your DataFrame\n","    summary_stats = calculate_summary_statistics(train)\n","\n","\n","    std_vol = summary_stats['std_vol']\n","    mean_vol = summary_stats['mean_vol']\n","    min_vol = summary_stats['min_vol']\n","    median_vol = summary_stats['median_vol']\n","    max_vol = summary_stats['max_vol']\n","    kurt_vol = summary_stats['kurt_vol']\n","    p25_vol = summary_stats['p25_vol']\n","    p75_vol = summary_stats['p75_vol']\n","    skew_vol = summary_stats['skew_vol']\n","\n","\n","\n","\n","\n","\n","\n","\n","    ####### NON-ROBUST features #######\n","\n","\n","    new_X_feat = [std_vol,min_vol,median_vol,max_vol,kurt_vol]\n","    X_feat = create_feature_matrix(new_X_feat)\n","\n","    feat_transform = FeatureTransformation()\n","    X_feat_normalized = feat_transform.feature_normalization(X_feat)\n","\n","\n","    ## Select the BEST agglomerative hierarchical clustering for features: std_vol,min_vol,median_vol,max_vol,kurt_vol\n","\n","    #['single','complete','average','weighted','centroid','median','ward']\n","\n","    final_sum_stats_target_vol_clusters = pd.DataFrame(columns=['stock_id','4_clusters', '10_clusters', '16_clusters', '30_clusters'])\n","    final_sum_stats_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","    c = [4,10,16,30]\n","    clustering_type = 'Agg. Hier. Clustering'\n","    metric = 'euclidean'\n","    for method in ['complete']:\n","        Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","        i=0\n","\n","        for t in [1,0.5,0.4,0.25]:#np.arange(0.2,0.3,0.1):\n","            # Calculate the cluster\n","            labels = fcluster(Z, t, criterion='distance')\n","            # Keep the indices to sort labels\n","            final_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","            i+=1\n","            labels_order = np.argsort(labels)\n","\n","\n","            plt.figure(figsize=(20,5))\n","            plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","            dendrogram(Z,color_threshold=t)\n","            plt.show()\n","\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","    print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","\n","\n","\n","    # # others that can be tried are\n","    # Num_clusters = 10, linkage = ward,threshold = 0.7\n","    # Num_clusters = 15, linkage = ward,threshold = 0.5\n","    # Num_clusters = 5, linkage = average,threshold = 0.5\n","    # Num_clusters = 20, linkage = average,threshold = 0.25\n","    # Num_clusters = 4, linkage =weighted,threshold = 0.6\n","    # Num_clusters = 8, linkage =weighted,threshold = 0.4\n","    # Num_clusters = 16, linkage =weighted,threshold = 0.3\n","\n","\n","\n","\n","\n","\n","\n","\n","    ####### ROBUST features #######\n","\n","    new_X_feat = [std_vol,p25_vol,median_vol,p75_vol,skew_vol,kurt_vol]\n","    X_feat = create_feature_matrix(new_X_feat)\n","\n","    feat_transform = FeatureTransformation()\n","    X_feat_normalized = feat_transform.feature_normalization(X_feat, transform_type=\"standard\")\n","\n","\n","    ## Select the BEST agglomerative hierarchical clustering for features : std_vol,p25_vol,median_vol,p75_vol,skew_vol,kurt_vol\n","\n","    #['single','complete','average','weighted','centroid','median','ward']\n","\n","    final_robust_sum_stats_target_vol_clusters = pd.DataFrame(columns=['stock_id','2_clusters', '4_clusters', '14_clusters','20_clusters','32_clusters', '60_clusters' ])\n","    final_robust_sum_stats_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","    c = [20,32,60]\n","    clustering_type = 'Agg. Hier. Clustering'\n","    metric = 'euclidean'\n","    for method in ['complete']:\n","        Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","        i=0\n","\n","        for t in [1.5,1,0.5]:#np.arange(0.2,0.3,0.1):\n","            # Calculate the cluster\n","            labels = fcluster(Z, t, criterion='distance')\n","            # Keep the indices to sort labels\n","            final_robust_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","            i+=1\n","            labels_order = np.argsort(labels)\n","\n","\n","            plt.figure(figsize=(20,5))\n","            plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","            dendrogram(Z,color_threshold=t)\n","            plt.show()\n","\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","    print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","    c = [2,4,14]\n","    metric = 'euclidean'\n","    for method in ['ward']:\n","        Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","        i=0\n","\n","        for t in [20,10,3]:#np.arange(0.2,0.3,0.1):\n","            # Calculate the cluster\n","            labels = fcluster(Z, t, criterion='distance')\n","            # Keep the indices to sort labels\n","            final_robust_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","            i+=1\n","            labels_order = np.argsort(labels)\n","\n","\n","            plt.figure(figsize=(20,5))\n","            plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","            dendrogram(Z,color_threshold=t)\n","            plt.show()\n","\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","    print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","\n","    # # others that can be tried are\n","    # Num_clusters = 10, linkage = ward,threshold = 0.7\n","    # Num_clusters = 15, linkage = ward,threshold = 0.5\n","    # Num_clusters = 5, linkage = average,threshold = 0.5\n","    # Num_clusters = 20, linkage = average,threshold = 0.25\n","    # Num_clusters = 4, linkage =weighted,threshold = 0.6\n","    # Num_clusters = 8, linkage =weighted,threshold = 0.4\n","    # Num_clusters = 16, linkage =weighted,threshold = 0.3\n","\n","\n","\n","\n","\n","\n","\n","    def cluster_agg(x, clusters, agg_fun):\n","        r = 0*x\n","\n","        for k in range(np.max(clusters)+1):\n","            print('k',k)\n","            z = agg_fun(x[:,clusters==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","            r[:,clusters==k,:] = np.repeat(z, repeats=int(np.sum(clusters==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","        return r\n","\n","\n","\n","\n","    train_all_unique_stock_ids = final_sum_stats_target_vol_clusters['stock_id']\n","    final_sum_stats_target_vol_clusters = final_sum_stats_target_vol_clusters.drop(columns=['stock_id'])\n","    final_robust_sum_stats_target_vol_clusters = final_robust_sum_stats_target_vol_clusters.drop(columns=['stock_id'])\n","\n","    final_pear_corr_target_vol_clusters = final_pear_corr_target_vol_clusters.drop(columns=['stock_id'])\n","\n","\n","\n","    final_sum_stats_target_vol_dict = {}\n","    final_robust_sum_stats_target_vol_clusters_dict = {}\n","    final_pear_corr_target_vol_dict = {}\n","\n","    for c in final_sum_stats_target_vol_clusters.columns:\n","        labels = final_sum_stats_target_vol_clusters[c]\n","        labels = labels - 1 # shift to add 0 label as well\n","        final_sum_stats_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","    for c in final_robust_sum_stats_target_vol_clusters.columns:\n","        labels = final_robust_sum_stats_target_vol_clusters[c]\n","        labels = labels - 1 # shift to add 0 label as well\n","        print('final_robust_sum_stats_target_vol_clusters[c]:', labels)\n","        final_robust_sum_stats_target_vol_clusters_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","    for c in final_pear_corr_target_vol_clusters.columns:\n","        labels = final_pear_corr_target_vol_clusters[c]\n","        labels = labels - 1 # shift to add 0 label as well\n","        final_pear_corr_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","\n","\n","\n","    final_sum_stats_target_vol_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","    # final_sum_stats_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","    final_sum_stats_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=2681, axis=0)[:,:,np.newaxis]\n","\n","    sum_stats_df = merge_features_to_df(final_sum_stats_target_vol_dict, train,   [f for f in list(final_sum_stats_target_vol_dict.keys()) ]   )\n","    sum_stats_df.shape\n","\n","    final_robust_sum_stats_target_vol_clusters_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","    # final_robust_sum_stats_target_vol_clusters_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","    final_robust_sum_stats_target_vol_clusters_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=2681, axis=0)[:,:,np.newaxis]\n","\n","    robust_sum_stats_df = merge_features_to_df(final_robust_sum_stats_target_vol_clusters_dict, train,   [f for f in list(final_robust_sum_stats_target_vol_clusters_dict.keys()) ]   )\n","    robust_sum_stats_df.shape\n","\n","\n","\n","    final_pear_corr_target_vol_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","    # final_pear_corr_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","    final_pear_corr_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=2681, axis=0)[:,:,np.newaxis]\n","\n","    pear_corr_df = merge_features_to_df(final_pear_corr_target_vol_dict, train,   [f for f in list(final_pear_corr_target_vol_dict.keys()) ]   )\n","    pear_corr_df.shape\n","\n","\n","\n","    final_sum_stats_target_vol_df = sum_stats_df[['4_clusters','10_clusters','16_clusters','30_clusters']]\n","    final_sum_stats_target_vol_df.rename(columns={'4_clusters':'target_vol_sum_stats_4_clusters','10_clusters':'target_vol_sum_stats_10_clusters',\n","                                                  '16_clusters':'target_vol_sum_stats_16_clusters','30_clusters':'target_vol_sum_stats_30_clusters'}, inplace=True)\n","\n","    final_robust_sum_stats_target_vol_df = robust_sum_stats_df[[\"2_clusters\",\"4_clusters\",\"14_clusters\",\"20_clusters\",\"32_clusters\",\"60_clusters\"]]\n","    final_robust_sum_stats_target_vol_df.rename(columns={'2_clusters':'target_vol_robust_sum_stats_2_clusters','4_clusters':'target_vol_robust_sum_stats_4_clusters',\n","                                                          '14_clusters':'target_vol_robust_sum_stats_14_clusters','20_clusters':'target_vol_robust_sum_stats_20_clusters',\n","                                                          '32_clusters':'target_vol_robust_sum_stats_32_clusters','60_clusters':'target_vol_robust_sum_stats_60_clusters'}, inplace=True)\n","\n","    final_pear_corr_target_vol_df = pear_corr_df[['3_clusters','49_clusters','90_clusters','10_clusters','26_clusters']]\n","    final_pear_corr_target_vol_df.rename(columns={'3_clusters':'target_vol_pcorr_3_clusters','49_clusters': 'target_vol_pcorr_49_clusters',\n","                                                    '90_clusters':'target_vol_pcorr_90_clusters','10_clusters':'target_vol_pcorr_10_clusters',\n","                                                    '26_clusters':'target_vol_pcorr_26_clusters'}, inplace=True)\n","\n","\n","\n","    return train_all_unique_stock_ids,final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df , \\\n","         final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"2d6739d6","metadata":{"id":"2d6739d6"},"outputs":[],"source":["def apply_trained_clusters_to_inference(train_all_unique_stock_ids,test,all_unique_time_ids, all_unique_stock_ids,feat_df,all_stocks_first_10_min_vol_df,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters):\n","\n","    final_sum_stats_target_vol_dict = {}\n","    final_robust_sum_stats_target_vol_clusters_dict = {}\n","    final_pear_corr_target_vol_dict = {}\n","\n","    for c in final_sum_stats_target_vol_clusters.columns:\n","        labels = final_sum_stats_target_vol_clusters[c]\n","        #labels = labels + 1 # shift to remove 0 labels\n","        final_sum_stats_target_vol_dict[c] = cluster_agg_for_inference(all_stocks_first_10_min_vol_df, labels, np.nanmean,test,train_all_unique_stock_ids) # can try np.nanmedian\n","\n","    for c in final_robust_sum_stats_target_vol_clusters.columns:\n","        labels = final_robust_sum_stats_target_vol_clusters[c]\n","        #labels = labels + 1 # shift to remove 0 labels\n","        print('final_robust_sum_stats_target_vol_clusters[c]:', labels)\n","        final_robust_sum_stats_target_vol_clusters_dict[c] = cluster_agg_for_inference(all_stocks_first_10_min_vol_df, labels, np.nanmean,test,train_all_unique_stock_ids) # can try np.nanmedian\n","\n","    for c in final_pear_corr_target_vol_clusters.columns:\n","        labels = final_pear_corr_target_vol_clusters[c]\n","        #labels = labels + 1 # shift to remove 0 labels\n","        final_pear_corr_target_vol_dict[c] = cluster_agg_for_inference(all_stocks_first_10_min_vol_df, labels, np.nanmean,test,train_all_unique_stock_ids) # can try np.nanmedian\n","\n","\n","    all_unique_time_ids = pd.DataFrame({'time_id':test['time_id'].unique()})\n","    all_unique_stock_ids = pd.DataFrame({'stock_id':test['stock_id'].unique()})\n","\n","    final_sum_stats_target_vol_dict['time_ids'] = np.repeat(all_unique_time_ids, repeats=len(all_unique_stock_ids), axis=1)[:,:,np.newaxis]\n","    final_sum_stats_target_vol_dict['stock_ids'] = np.repeat([all_unique_stock_ids], repeats=len(all_unique_time_ids), axis=0)[:,:,np.newaxis]\n","\n","    sum_stats_df = merge_features_to_df(final_sum_stats_target_vol_dict, test,   [f for f in list(final_sum_stats_target_vol_dict.keys()) ]   )\n","    sum_stats_df.shape\n","\n","    final_robust_sum_stats_target_vol_clusters_dict['time_ids'] = np.repeat(all_unique_time_ids, repeats=len(all_unique_stock_ids), axis=1)[:,:,np.newaxis]\n","    final_robust_sum_stats_target_vol_clusters_dict['stock_ids'] = np.repeat([all_unique_stock_ids], repeats=len(all_unique_time_ids), axis=0)[:,:,np.newaxis]\n","\n","    robust_sum_stats_df = merge_features_to_df(final_robust_sum_stats_target_vol_clusters_dict, test,   [f for f in list(final_robust_sum_stats_target_vol_clusters_dict.keys()) ]   )\n","    robust_sum_stats_df.shape\n","\n","\n","\n","    final_pear_corr_target_vol_dict['time_ids'] = np.repeat(all_unique_time_ids, repeats=len(all_unique_stock_ids), axis=1)[:,:,np.newaxis]\n","    final_pear_corr_target_vol_dict['stock_ids'] = np.repeat([all_unique_stock_ids], repeats=len(all_unique_time_ids), axis=0)[:,:,np.newaxis]\n","\n","    pear_corr_df = merge_features_to_df(final_pear_corr_target_vol_dict, test,   [f for f in list(final_pear_corr_target_vol_dict.keys()) ]   )\n","    pear_corr_df.shape\n","\n","\n","\n","    final_sum_stats_target_vol_df = sum_stats_df[['4_clusters','10_clusters','16_clusters','30_clusters']]\n","    final_sum_stats_target_vol_df.rename(columns={'4_clusters':'target_vol_sum_stats_4_clusters','10_clusters':'target_vol_sum_stats_10_clusters',\n","                                                  '16_clusters':'target_vol_sum_stats_16_clusters','30_clusters':'target_vol_sum_stats_30_clusters'}, inplace=True)\n","\n","    final_robust_sum_stats_target_vol_df = robust_sum_stats_df[[\"2_clusters\",\"4_clusters\",\"14_clusters\",\"20_clusters\",\"32_clusters\",\"60_clusters\"]]\n","    final_robust_sum_stats_target_vol_df.rename(columns={'2_clusters':'target_vol_robust_sum_stats_2_clusters','4_clusters':'target_vol_robust_sum_stats_4_clusters',\n","                                                          '14_clusters':'target_vol_robust_sum_stats_14_clusters','20_clusters':'target_vol_robust_sum_stats_20_clusters',\n","                                                          '32_clusters':'target_vol_robust_sum_stats_32_clusters','60_clusters':'target_vol_robust_sum_stats_60_clusters'}, inplace=True)\n","\n","    final_pear_corr_target_vol_df = pear_corr_df[['3_clusters','49_clusters','90_clusters','10_clusters','26_clusters']]\n","    final_pear_corr_target_vol_df.rename(columns={'3_clusters':'target_vol_pcorr_3_clusters','49_clusters': 'target_vol_pcorr_49_clusters',\n","                                                    '90_clusters':'target_vol_pcorr_90_clusters','10_clusters':'target_vol_pcorr_10_clusters',\n","                                                    '26_clusters':'target_vol_pcorr_26_clusters'}, inplace=True)\n","\n","\n","    feat_df = pd.concat([feat_df, final_sum_stats_target_vol_df], axis=1)\n","    feat_df = pd.concat([feat_df, final_robust_sum_stats_target_vol_df], axis=1)\n","    feat_df = pd.concat([feat_df, final_pear_corr_target_vol_df], axis=1)\n","\n","    return feat_df\n","\n"]},{"cell_type":"code","execution_count":null,"id":"add64824","metadata":{"id":"add64824"},"outputs":[],"source":["\n","def perform_ml_stage(ml_stage,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters,feat_normalization_mu_std_df,train_all_unique_stock_ids):\n","\n","    train_or_inference = ml_stage # 'training' or 'inference'\n","\n","\n","    if ml_stage == 'training':\n","      data_dir = os.getcwd()\n","      train = pd.read_csv('train_partial.csv')\n","      train_buckets = create_dataSet(st_ids = list(np.unique(train['stock_id'])), dset = 'train_partial')\n","    elif ml_stage == 'inference':\n","      data_dir = os.getcwd()\n","      test = pd.read_csv('test.csv')\n","      test_buckets = create_dataSet(st_ids = list(np.unique(test['stock_id'])), dset = 'test')\n","\n","\n","\n","    if ml_stage == 'training':\n","      buckets_type = train_buckets\n","      del train_buckets\n","    elif ml_stage == 'inference':\n","      buckets_type = test_buckets\n","      del test_buckets\n","\n","\n","\n","    features = generate_liquidity_features(buckets_type)\n","\n","\n","    if ml_stage == 'training':\n","      train = pd.read_csv('train_partial.csv')\n","      features['target'] = read_targets_from_df(train, buckets_type['stock_ids'], buckets_type['time_ids'])\n","      feat_df = merge_features_to_df(features, train, [f for f in list(features.keys()) if ('time_id' not in f)])\n","      del features, buckets_type\n","      feat_df['target'] = feat_df['target_y']\n","      feat_df.drop(columns=['target_x', 'target_y'], inplace=True)\n","\n","\n","    elif ml_stage == 'inference':\n","      test = pd.read_csv('test.csv')\n","      feat_df = merge_features_to_df(features, test, [f for f in list(features.keys()) if ('time_id' not in f)])\n","      del features\n","\n","\n","\n","\n","\n","\n","\n","    # ##### Training and Inference features generation #####\n","    general_features = create_training_n_inference_general_features(ml_stage=train_or_inference)\n","\n","    bk_level1_2_size_imbalance_feat = general_features.create_bk_level1_2_size_imbalance_feat()\n","    trade_sum_size_sum_order_count_sum_size_per_order_count = general_features.create_trade_sum_size_sum_order_count_sum_size_per_order_count()\n","    bk_price_size_min_max_range = general_features.create_bk_price_size_min_max_range()\n","    bk_price_size_sad = general_features.create_bk_price_size_sad()\n","    bk_size_price_corr = general_features.create_bk_size_price_corr()\n","    trade_price_size_order_count_min_max_range = general_features.create_trade_price_size_order_count_min_max_range()\n","    trade_price_size_order_count_sad = general_features.create_trade_price_size_order_count_sad()\n","    trade_price_size_order_count_corr = general_features.create_trade_price_size_order_count_corr()\n","    trade_price_n_wap1_deviation_df, trade_price_n_wap_eqi_price0_deviation_df = general_features.create_trade_price_n_wap1_deviation_df_AND_trade_price_n_wap_eqi_price0_deviation_df()\n","\n","    feat_df = general_features.merge_trade_price_n_wap_eqi_price0_deviation_df(bk_level1_2_size_imbalance_feat,trade_sum_size_sum_order_count_sum_size_per_order_count,trade_price_n_wap1_deviation_df,trade_price_n_wap_eqi_price0_deviation_df, feat_df)\n","\n","    feat_df.drop(columns=['stock_ids'], inplace=True) # drop this redundant column\n","\n","    if ml_stage == 'inference':\n","      feat_df.drop(columns=['row_id'], inplace=True) # drop this redundant column\n","\n","\n","\n","    df_20_min_volatility = general_features.create_df_20_min_volatility()\n","\n","    feat_df = general_features.create_n_merge_trade_price_std_df(feat_df)\n","\n","    feat_df,all_stocks_first_10_min_vol_df, all_unique_time_ids, all_unique_stock_ids = general_features.create_all_stocks_first_10_min_vol_df_AND_merge_first_10_min_vol_df(feat_df,df_20_min_volatility)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    if ml_stage == 'training':\n","      train_all_unique_stock_ids,final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df, \\\n","      final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters  = clustering_on_training_data(all_stocks_first_10_min_vol_df)\n","      feat_df = general_features.merge_clustering_features_to_training_data(feat_df,final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df )\n","\n","    elif ml_stage == 'inference':\n","\n","\n","      feat_df = apply_trained_clusters_to_inference( train_all_unique_stock_ids,test,all_unique_time_ids, all_unique_stock_ids,feat_df,all_stocks_first_10_min_vol_df,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters)\n","\n","    feat_df = general_features.merge_bk_price_size_min_max_range(feat_df,bk_price_size_min_max_range)\n","\n","    feat_df = general_features.merge_bk_price_size_sad(feat_df,bk_price_size_sad)\n","\n","\n","    feat_df = general_features.merge_bk_size_price_corr(feat_df,bk_size_price_corr)\n","\n","    #print(\"bk_size_price_corr['st_bs_bp_corr1']\")\n","    #print(bk_size_price_corr['bs_bp_corr1'])\n","\n","    feat_df = general_features.merge_trade_price_size_order_count_min_max_range(feat_df,trade_price_size_order_count_min_max_range)\n","\n","    feat_df = general_features.merge_trade_price_size_order_count_sad(feat_df,trade_price_size_order_count_sad)\n","\n","\n","    feat_df = general_features.merge_trade_price_size_order_count_corr(feat_df,trade_price_size_order_count_corr)\n","\n","\n","    if ml_stage == 'training':\n","      unique_stock_ids = np.unique(train['stock_id'])\n","    elif ml_stage == 'inference':\n","      unique_stock_ids = np.unique(test['stock_id'])\n","\n","    feat_df = general_features.merge_final_sum_stats_target_vol_clusters(feat_df,final_sum_stats_target_vol_clusters,unique_stock_ids)\n","\n","    feat_df = general_features.merge_final_robust_sum_stats_target_vol_clusters(feat_df,final_robust_sum_stats_target_vol_clusters,unique_stock_ids)\n","\n","    feat_df = general_features.merge_final_pear_corr_target_vol_clusters(feat_df,final_pear_corr_target_vol_clusters,unique_stock_ids)\n","\n","\n","    bk_price_size_min_max_range_2 = general_features.create_bk_price_size_min_max_range_2()\n","\n","    feat_df = general_features.merge_bk_price_size_min_max_range_2(bk_price_size_min_max_range_2,feat_df)\n","\n","    bk_price_size_sad_2 = general_features.create_bk_price_size_sad_2()\n","\n","    feat_df = general_features.merge_bk_price_size_sad_2(feat_df,bk_price_size_sad_2)\n","\n","    bk_size_price_corr_2 = general_features.create_bk_size_price_corr_2()\n","\n","    feat_df = general_features.merge_bk_size_price_corr_2(feat_df,bk_size_price_corr_2)\n","\n","    if ml_stage == 'inference':\n","      feat_df.drop(columns=['row_id_x','row_id_y'], inplace=True) # drop this redundant column\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    drop_cols_for_training = [ 'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\n","    'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio',\n","    'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio',\n","     'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock',\n","     'v1proj_29_15_std', 'v1proj_29_std', 'target_y' , 'target_x']\n","\n","    if ml_stage == 'training':\n","      feat_df.drop(columns=drop_cols_for_training, inplace=True)\n","\n","\n","    drop_cols_for_inference = ['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\n","     'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio',\n","     'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio',\n","     'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock',\n","     'v1proj_29_15_std',\n","     'v1proj_29_std']\n","\n","    if ml_stage == 'inference':\n","      feat_df.drop(columns=drop_cols_for_inference, inplace=True)\n","      feat_df\n","\n","\n","    print(\"\\n feat_df columns\")\n","    for c in feat_df.columns:\n","        print(c)\n","    print(\"\\n\")\n","\n","    # Check for NULL and INF values in feat_df\n","\n","    pos_inf_cols = []\n","    neg_inf_cols = []\n","\n","    #float_cols = [ c for c in feat_df.columns if feat_df[c].dtypes != 'category']\n","\n","    float_cols = ['stock_id',\n","     'time_id',\n","     'wap1_log_price_ret_vol',\n","     'log_liq2_ret_*_wap_eqi_price1_ret_vol',\n","     'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol',\n","     'wap1_log_price_ret_per_liq2_vol',\n","     'wap1_log_price_ret_per_spread_sqr_vol',\n","     'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio',\n","     'wap1_log_price_ret_per_liq2_vol_15_ratio',\n","     'wap1_log_price_ret_per_spread_sqr_vol_15_ratio',\n","     'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio',\n","     'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock',\n","     'wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock',\n","     'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol',\n","     'wap1_log_price_ret_neg_log_liq_ret_sqr_vol',\n","     'wap1_log_price_ret_pos_log_liq_ret_sqr_vol',\n","     'wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol',\n","     'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0',\n","     'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0',\n","     'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0',\n","     'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10',\n","     'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10',\n","     'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10',\n","     'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n","     'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20',\n","     'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20',\n","     'soft_stock_mean_tvpl2_:0',\n","     'soft_stock_mean_tvpl2_:10',\n","     'soft_stock_mean_tvpl2_:20',\n","     'soft_stock_mean_tvpl2_liqf',\n","     'soft_stock_mean_tvpl2_liqf_volf10',\n","     'soft_stock_mean_tvpl2_liqf_volf20',\n","     'v1proj_25_15',\n","     'lsvol',\n","     'liqvol1',\n","     'liqvol1_smean',\n","     'liqvol2',\n","     'liqvol1_15_15',\n","     'trade_count',\n","     'root_trade_count',\n","     'root_trade_count_smean',\n","     'root_book_delta_count',\n","     'root_trade_count_var',\n","     'trade_count_15_15',\n","     'root_trade_count_15_15',\n","     'v1proj_29_15',\n","     'v1proj_20',\n","     'v1proj_25',\n","     'v1proj_29',\n","     'v1proj_29_q1',\n","     'v1proj_29_q3',\n","     'v1proj_25_q1',\n","     'v1proj_25_q3',\n","     'v1proj_29_15_q1',\n","     'v1proj_29_15_q3',\n","     'v1proj_25_15_q1',\n","     'v1proj_25_15_q3',\n","     'v1proj_25_15_std',\n","     'v1proj_20_std',\n","     'v1proj_25_std',\n","     'v1proj_29_q3q1',\n","     'tvpl2_rmed2v1',\n","     'tvpl2_rmed2v1lf25',\n","     'tvpl2_rmed2v1lf29',\n","     'tvpl2',\n","     'tvpl2_liqf10',\n","     'tvpl2_liqf20',\n","     'tvpl2_liqf29',\n","     'tvpl2_smean_vol',\n","     'tvpl2_smean_vol_liqf10',\n","     'tvpl2_smean_vol_liqf20',\n","     'tvpl2_smean_vol_liqf29',\n","     'v1liq2projt5',\n","     'v1liq2projt10',\n","     'v1liq2projt20',\n","     'liqt10rf29',\n","     'liqt20rf29',\n","     'v1liq2sprojt10f25',\n","     'v1liq2sprojt5f25',\n","     'v1spprojt10f29',\n","     'v1spprojt15f25',\n","     'v1spprojt15f29',\n","     'v1spprojt15f29_q1',\n","     'v1spprojt15f29_q3',\n","     'v1spprojt15f25_q1',\n","     'v1spprojt15f25_q3',\n","     'v1spprojtf29_q1',\n","     'v1spprojtf29_q3',\n","     'v1spprojtf25_q1',\n","     'v1spprojtf25_q3',\n","     'wap1_log_price_ret_vol_from_0',\n","     'wap1_log_price_ret_volstock_mean_from_0',\n","     'wap1_log_price_ret_vol_from_10',\n","     'wap1_log_price_ret_volstock_mean_from_10',\n","     'wap1_log_price_ret_vol_from_20',\n","     'wap1_log_price_ret_volstock_mean_from_20',\n","     'wap1_log_price_ret_vol_from_25',\n","     'wap1_log_price_ret_volstock_mean_from_25',\n","     'vol1_mean',\n","     'mean_half_delta',\n","     'mean_half_delta_lsprd',\n","     'log_wap1_log_price_ret_vol',\n","     'bid_lvl2_min_lvl1_size_feat',\n","     'ask_lvl2_min_lvl1_size_feat',\n","     'lvl2_minus_lvl1_bid_n_ask_size_feat',\n","     'sum_size',\n","     'sum_order_count',\n","     'sum_size_per_order_count',\n","     'trade_price_n_wap1_dev',\n","     'trade_price_n_wap_eqi_price0_dev',\n","     'trade_price_std',\n","     'trade_price_real_vol',\n","     'trade_size_std',\n","     'trade_size_mean',\n","     'trade_order_count_std',\n","     'trade_order_count_mean',\n","     'first_10_min_vol',\n","     'target_vol_sum_stats_4_clusters',\n","     'target_vol_sum_stats_10_clusters',\n","     'target_vol_sum_stats_16_clusters',\n","     'target_vol_sum_stats_30_clusters',\n","     'target_vol_robust_sum_stats_2_clusters',\n","     'target_vol_robust_sum_stats_4_clusters',\n","     'target_vol_robust_sum_stats_14_clusters',\n","     'target_vol_robust_sum_stats_20_clusters',\n","     'target_vol_robust_sum_stats_32_clusters',\n","     'target_vol_robust_sum_stats_60_clusters',\n","     'target_vol_pcorr_3_clusters',\n","     'target_vol_pcorr_49_clusters',\n","     'target_vol_pcorr_90_clusters',\n","     'target_vol_pcorr_10_clusters',\n","     'target_vol_pcorr_26_clusters',\n","     'min_bid_price1',\n","     'max_bid_price1',\n","     'min_ask_price1',\n","     'max_ask_price1',\n","     'min_bid_size1',\n","     'max_bid_size1',\n","     'min_ask_size1',\n","     'max_ask_size1',\n","     'range_ask_price1',\n","     'range_bid_price1',\n","     'range_ask_size1',\n","     'range_bid_size1',\n","     'sad_ask_price1',\n","     'sad_ask_size1',\n","     'sad_bid_price1',\n","     'sad_bid_size1',\n","     'bs_bp_corr1',\n","     'bs_as_corr1',\n","     'bs_ap_corr1',\n","     'bp_as_corr1',\n","     'bp_ap_corr1',\n","     'as_ap_corr1',\n","     'min_price1',\n","     'max_price1',\n","     'min_size1',\n","     'max_size1',\n","     'min_order_count1',\n","     'max_order_count1',\n","     'range_price1',\n","     'range_size1',\n","     'range_order_count1',\n","     'sad_price1',\n","     'sad_size1',\n","     'sad_order_count1',\n","     'size_order_count_corr1',\n","     'sum_stats_4_clusters_labels',\n","     'sum_stats_10_clusters_labels',\n","     'sum_stats_16_clusters_labels',\n","     'sum_stats_30_clusters_labels',\n","     'robust_sum_stats_2_clusters_labels',\n","     'robust_sum_stats_4_clusters_labels',\n","     'robust_sum_stats_14_clusters_labels',\n","     'robust_sum_stats_20_clusters_labels',\n","     'robust_sum_stats_32_clusters_labels',\n","     'robust_sum_stats_60_clusters_labels',\n","     'pear_corr_3_clusters_labels',\n","     'pear_corr_49_clusters_labels',\n","     'pear_corr_90_clusters_labels',\n","     'pear_corr_10_clusters_labels',\n","     'pear_corr_26_clusters_labels',\n","     'min_bid_price2',\n","     'max_bid_price2',\n","     'min_ask_price2',\n","     'max_ask_price2',\n","     'min_bid_size2',\n","     'max_bid_size2',\n","     'min_ask_size2',\n","     'max_ask_size2',\n","     'range_ask_price2',\n","     'range_bid_price2',\n","     'range_ask_size2',\n","     'sad_ask_price2',\n","     'sad_ask_size2',\n","     'sad_bid_price2',\n","     'bs_bp_corr2',\n","     'bs_as_corr2',\n","     'bs_ap_corr2',\n","     'bp_as_corr2',\n","     'bp_ap_corr2',\n","     'as_ap_corr2']\n","\n","\n","    for c in float_cols:\n","        if np.isinf(feat_df[c]).any():\n","            print(\"pos INF: \",c,': ', np.isinf(feat_df[c]).sum())\n","            pos_inf_cols.append(c)\n","\n","        if np.isneginf(feat_df[c]).any():\n","            print(\"neg INF: \",c,': ', np.isneginf(feat_df[c]).sum())\n","            neg_inf_cols.append(c)\n","\n","\n","\n","\n","\n","\n","    ### groupby stock id and fill positive infinity values with max of the stock id or 1e8\n","    for c in pos_inf_cols:\n","        #feat_df[c] = feat_df.groupby('stock_id')[c].transform(lambda x: x.replace([np.inf], x.loc[~np.isinf(x)].max()))\n","        feat_df[c].replace(np.inf, 1e8, inplace=True)\n","\n","    ### groupby stock id and fill negative infinity values with min of the stock id or -1e8\n","    for c in neg_inf_cols:\n","        #feat_df[c] = feat_df.groupby('stock_id')[c].transform(lambda x: x.replace([-np.inf], x.loc[~np.isneginf(x)].min()))\n","        feat_df[c].replace(-np.inf, -1e8, inplace=True)\n","\n","\n","    del bk_level1_2_size_imbalance_feat, \\\n","    trade_sum_size_sum_order_count_sum_size_per_order_count, \\\n","    bk_price_size_min_max_range, \\\n","    bk_price_size_sad, \\\n","    bk_size_price_corr, \\\n","    trade_price_size_order_count_min_max_range, \\\n","    trade_price_size_order_count_sad, \\\n","    trade_price_size_order_count_corr, \\\n","    trade_price_n_wap1_deviation_df, \\\n","    df_20_min_volatility,\\\n","    bk_price_size_min_max_range_2, \\\n","    bk_price_size_sad_2,\\\n","    bk_size_price_corr_2\n","\n","\n","\n","    \"\"\"\n","    ######## Feature TRANSFORMATION of the TRAINING data ########\n","    \"\"\"\n","\n","    if ml_stage == 'training':\n","\n","      train_feat_df = feat_df.copy()\n","      del feat_df\n","\n","      # import feature_transformation as feat_transformer\n","\n","      ft_train = Train_Test_FeatureTransformation(train_feat_df)\n","\n","      feat_normalization_mu_std_df = pd.DataFrame(index=[\"mean\", \"std\", \"transform\"])\n","\n","      # val, new_col_name, mean, std = ft_train.log(\"wap1_log_price_ret_vol\")\n","      # train_feat_df.rename(columns={'wap1_log_price_ret_vol':new_col_name}, inplace=True)\n","      # train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","      train_feat_df.drop(columns=['wap1_log_price_ret_vol'], inplace=True) ## we drop this because we have log_wap1_log_price_ret_vol already\n","\n","\n","      train_feat_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol\"], mean, std  = ft_train.standard_scaling(\"log_liq2_ret_*_wap_eqi_price1_ret_vol\")\n","      feat_normalization_mu_std_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\"], mean, std  = ft_train.standard_scaling(\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\")\n","      feat_normalization_mu_std_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_per_liq2_vol\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_per_liq2_vol\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_per_liq2_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'], mean, std = ft_train.standard_scaling('log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio')\n","      feat_normalization_mu_std_df['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_per_liq2_vol_15_ratio\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_per_liq2_vol_15_ratio\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_per_liq2_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\"], mean, std = ft_train.standard_scaling(\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\")\n","      feat_normalization_mu_std_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      # train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\"], mean, std = ft_train.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\")\n","      # feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      # train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\"], mean, std = ft_train.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\")\n","      # feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\"], mean, std = ft_train.standard_scaling(\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\")\n","      feat_normalization_mu_std_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\"],mean, std, = ft_train.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\")\n","      feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_3p(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0\")\n","      train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_3p_test\"]\n","      feat_normalization_mu_std_df['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0'] = [mean, std, \"log_3p_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0\")\n","      train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df[\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0\"] = [mean, std, \"log_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\"],mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10\")\n","      train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df[\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10\"] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10\")\n","      train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10'] = [mean, std, \"log_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\"],mean, std, = ft_train.standard_scaling(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20\")\n","      train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20\")\n","      train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20\")\n","      train_feat_df.rename(columns={'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20'] = [mean, std, \"log_test\"]\n","\n","      train_feat_df[\"soft_stock_mean_tvpl2_:0\"], mean, std = ft_train.standard_scaling(\"soft_stock_mean_tvpl2_:0\")\n","      feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"soft_stock_mean_tvpl2_:10\"], mean, std = ft_train.standard_scaling(\"soft_stock_mean_tvpl2_:10\")\n","      feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"soft_stock_mean_tvpl2_:20\"], mean, std = ft_train.standard_scaling(\"soft_stock_mean_tvpl2_:20\")\n","      feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"soft_stock_mean_tvpl2_liqf\"], mean, std = ft_train.standard_scaling(\"soft_stock_mean_tvpl2_liqf\")\n","      feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"soft_stock_mean_tvpl2_liqf_volf10\"], mean, std = ft_train.standard_scaling(\"soft_stock_mean_tvpl2_liqf_volf10\")\n","      feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf_volf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"soft_stock_mean_tvpl2_liqf_volf20\"], mean, std = ft_train.standard_scaling(\"soft_stock_mean_tvpl2_liqf_volf20\")\n","      feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf_volf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_15\"], mean, std = ft_train.standard_scaling(\"v1proj_25_15\")\n","      feat_normalization_mu_std_df[\"v1proj_25_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      # val, new_col_name, mean, std = ft_train.log(\"lsvol\")  ## This is causing lsvol to have nan values\n","      # train_feat_df.rename(columns={'lsvol':new_col_name}, inplace=True)\n","      # train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","      train_feat_df[\"lsvol\"], mean, std = ft_train.standard_scaling(\"lsvol\")\n","      feat_normalization_mu_std_df[\"lsvol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"liqvol1\"], mean, std = ft_train.standard_scaling(\"liqvol1\")\n","      feat_normalization_mu_std_df[\"liqvol1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"liqvol1_smean\"], mean, std = ft_train.standard_scaling(\"liqvol1_smean\")\n","      feat_normalization_mu_std_df[\"liqvol1_smean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"liqvol2\"], mean, std = ft_train.standard_scaling(\"liqvol2\")\n","      feat_normalization_mu_std_df[\"liqvol2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"liqvol1_15_15\"], mean, std = ft_train.standard_scaling(\"liqvol1_15_15\")\n","      feat_normalization_mu_std_df[\"liqvol1_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"trade_count\"], mean, std = ft_train.standard_scaling(\"trade_count\")\n","      feat_normalization_mu_std_df[\"trade_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"root_trade_count\"], mean, std = ft_train.standard_scaling(\"root_trade_count\")\n","      feat_normalization_mu_std_df[\"root_trade_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"root_trade_count_smean\"], mean, std = ft_train.standard_scaling(\"root_trade_count_smean\")\n","      feat_normalization_mu_std_df[\"root_trade_count_smean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"root_book_delta_count\"], mean, std = ft_train.standard_scaling(\"root_book_delta_count\")\n","      feat_normalization_mu_std_df[\"root_book_delta_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"root_trade_count_var\"], mean, std = ft_train.standard_scaling(\"root_trade_count_var\")\n","      feat_normalization_mu_std_df[\"root_trade_count_var\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"trade_count_15_15\"], mean, std = ft_train.standard_scaling(\"trade_count_15_15\")\n","      feat_normalization_mu_std_df[\"trade_count_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"root_trade_count_15_15\"], mean, std = ft_train.standard_scaling(\"root_trade_count_15_15\")\n","      feat_normalization_mu_std_df[\"root_trade_count_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29_15\"], mean, std = ft_train.standard_scaling(\"v1proj_29_15\")\n","      feat_normalization_mu_std_df[\"v1proj_29_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_20\"], mean, std = ft_train.standard_scaling(\"v1proj_20\")\n","      feat_normalization_mu_std_df[\"v1proj_20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25\"], mean, std = ft_train.standard_scaling(\"v1proj_25\")\n","      feat_normalization_mu_std_df[\"v1proj_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29\"], mean, std = ft_train.standard_scaling(\"v1proj_29\")\n","      feat_normalization_mu_std_df[\"v1proj_29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29_q1\"], mean, std = ft_train.standard_scaling(\"v1proj_29_q1\")\n","      feat_normalization_mu_std_df[\"v1proj_29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29_q3\"], mean, std = ft_train.standard_scaling(\"v1proj_29_q3\")\n","      feat_normalization_mu_std_df[\"v1proj_29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_q1\"], mean, std = ft_train.standard_scaling(\"v1proj_25_q1\")\n","      feat_normalization_mu_std_df[\"v1proj_25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_q3\"], mean, std = ft_train.standard_scaling(\"v1proj_25_q3\")\n","      feat_normalization_mu_std_df[\"v1proj_25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29_15_q1\"], mean, std = ft_train.standard_scaling(\"v1proj_29_15_q1\")\n","      feat_normalization_mu_std_df[\"v1proj_29_15_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29_15_q3\"], mean, std = ft_train.standard_scaling(\"v1proj_29_15_q3\")\n","      feat_normalization_mu_std_df[\"v1proj_29_15_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_15_q1\"], mean, std = ft_train.standard_scaling(\"v1proj_25_15_q1\")\n","      feat_normalization_mu_std_df[\"v1proj_25_15_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_15_q3\"], mean, std = ft_train.standard_scaling(\"v1proj_25_15_q3\")\n","      feat_normalization_mu_std_df[\"v1proj_25_15_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_15_std\"], mean, std = ft_train.standard_scaling(\"v1proj_25_15_std\")\n","      feat_normalization_mu_std_df[\"v1proj_25_15_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      # train_feat_df.drop(columns=[\"v1proj_29_15_std\"], inplace=True) ## drop this column as it has very low variance\n","\n","      train_feat_df[\"v1proj_20_std\"], mean, std = ft_train.standard_scaling(\"v1proj_20_std\")\n","      feat_normalization_mu_std_df[\"v1proj_20_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_25_std\"], mean, std = ft_train.standard_scaling(\"v1proj_25_std\")\n","      feat_normalization_mu_std_df[\"v1proj_25_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      # train_feat_df[\"v1proj_29_std\"], mean, std = ft_train.standard_scaling(\"v1proj_29_std\")\n","      # feat_normalization_mu_std_df[\"v1proj_29_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1proj_29_q3q1\"], mean, std = ft_train.standard_scaling(\"v1proj_29_q3q1\")\n","      feat_normalization_mu_std_df[\"v1proj_29_q3q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_rmed2v1\"], mean, std = ft_train.standard_scaling(\"tvpl2_rmed2v1\")\n","      feat_normalization_mu_std_df[\"tvpl2_rmed2v1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_rmed2v1lf25\"], mean, std = ft_train.standard_scaling(\"tvpl2_rmed2v1lf25\")\n","      feat_normalization_mu_std_df[\"tvpl2_rmed2v1lf25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_rmed2v1lf29\"], mean, std = ft_train.standard_scaling(\"tvpl2_rmed2v1lf29\")\n","      feat_normalization_mu_std_df[\"tvpl2_rmed2v1lf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2\"], mean, std = ft_train.standard_scaling(\"tvpl2\")\n","      feat_normalization_mu_std_df[\"tvpl2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_liqf10\"], mean, std = ft_train.standard_scaling(\"tvpl2_liqf10\")\n","      feat_normalization_mu_std_df[\"tvpl2_liqf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_liqf20\"], mean, std = ft_train.standard_scaling(\"tvpl2_liqf20\")\n","      feat_normalization_mu_std_df[\"tvpl2_liqf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_liqf29\"], mean, std = ft_train.standard_scaling(\"tvpl2_liqf29\")\n","      feat_normalization_mu_std_df[\"tvpl2_liqf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_smean_vol\"], mean, std = ft_train.standard_scaling(\"tvpl2_smean_vol\")\n","      feat_normalization_mu_std_df[\"tvpl2_smean_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_smean_vol_liqf10\"], mean, std = ft_train.standard_scaling(\"tvpl2_smean_vol_liqf10\")\n","      feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_smean_vol_liqf20\"], mean, std = ft_train.standard_scaling(\"tvpl2_smean_vol_liqf20\")\n","      feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"tvpl2_smean_vol_liqf29\"], mean, std = ft_train.standard_scaling(\"tvpl2_smean_vol_liqf29\")\n","      feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1liq2projt5\"], mean, std = ft_train.standard_scaling(\"v1liq2projt5\")\n","      feat_normalization_mu_std_df[\"v1liq2projt5\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1liq2projt10\"], mean, std = ft_train.standard_scaling(\"v1liq2projt10\")\n","      feat_normalization_mu_std_df[\"v1liq2projt10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1liq2projt20\"], mean, std = ft_train.standard_scaling(\"v1liq2projt20\")\n","      feat_normalization_mu_std_df[\"v1liq2projt20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"liqt10rf29\"], mean, std = ft_train.standard_scaling(\"liqt10rf29\")\n","      feat_normalization_mu_std_df[\"liqt10rf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"liqt20rf29\"], mean, std = ft_train.standard_scaling(\"liqt20rf29\")\n","      feat_normalization_mu_std_df[\"liqt20rf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1liq2sprojt10f25\"], mean, std = ft_train.standard_scaling(\"v1liq2sprojt10f25\")\n","      feat_normalization_mu_std_df[\"v1liq2sprojt10f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1liq2sprojt5f25\"], mean, std = ft_train.standard_scaling(\"v1liq2sprojt5f25\")\n","      feat_normalization_mu_std_df[\"v1liq2sprojt5f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt10f29\"], mean, std = ft_train.standard_scaling(\"v1spprojt10f29\")\n","      feat_normalization_mu_std_df[\"v1spprojt10f29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt15f25\"], mean, std = ft_train.standard_scaling(\"v1spprojt15f25\")\n","      feat_normalization_mu_std_df[\"v1spprojt15f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt15f29\"], mean, std = ft_train.standard_scaling(\"v1spprojt15f29\")\n","      feat_normalization_mu_std_df[\"v1spprojt15f29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt15f29_q1\"], mean, std = ft_train.standard_scaling(\"v1spprojt15f29_q1\")\n","      feat_normalization_mu_std_df[\"v1spprojt15f29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt15f29_q3\"], mean, std = ft_train.standard_scaling(\"v1spprojt15f29_q3\")\n","      feat_normalization_mu_std_df[\"v1spprojt15f29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt15f25_q1\"], mean, std = ft_train.standard_scaling(\"v1spprojt15f25_q1\")\n","      feat_normalization_mu_std_df[\"v1spprojt15f25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojt15f25_q3\"], mean, std = ft_train.standard_scaling(\"v1spprojt15f25_q3\")\n","      feat_normalization_mu_std_df[\"v1spprojt15f25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojtf29_q1\"], mean, std = ft_train.standard_scaling(\"v1spprojtf29_q1\")\n","      feat_normalization_mu_std_df[\"v1spprojtf29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojtf29_q3\"], mean, std = ft_train.standard_scaling(\"v1spprojtf29_q3\")\n","      feat_normalization_mu_std_df[\"v1spprojtf29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojtf25_q1\"], mean, std = ft_train.standard_scaling(\"v1spprojtf25_q1\")\n","      feat_normalization_mu_std_df[\"v1spprojtf25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"v1spprojtf25_q3\"], mean, std = ft_train.standard_scaling(\"v1spprojtf25_q3\")\n","      feat_normalization_mu_std_df[\"v1spprojtf25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df.drop(columns=[\"wap1_log_price_ret_vol_from_0\"], inplace=True) ## drop this column as it has all 0 values\n","\n","      train_feat_df[\"wap1_log_price_ret_volstock_mean_from_0\"],mean, std, = ft_train.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_0\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.exp(\"wap1_log_price_ret_vol_from_10\")\n","      train_feat_df.rename(columns={'wap1_log_price_ret_vol_from_10':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","      feat_normalization_mu_std_df['wap1_log_price_ret_vol_from_10'] = [mean, std, \"exp_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_volstock_mean_from_10\"],mean, std, = ft_train.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_10\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.exp(\"wap1_log_price_ret_vol_from_20\")\n","      train_feat_df.rename(columns={'wap1_log_price_ret_vol_from_20':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","      feat_normalization_mu_std_df['wap1_log_price_ret_vol_from_20'] = [mean, std, \"exp_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_volstock_mean_from_20\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_20\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_vol_from_25\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_vol_from_25\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_vol_from_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"wap1_log_price_ret_volstock_mean_from_25\"], mean, std = ft_train.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_25\")\n","      feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"vol1_mean\"], mean, std = ft_train.standard_scaling(\"vol1_mean\")\n","      feat_normalization_mu_std_df[\"vol1_mean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df.drop(columns=[\"mean_half_delta\"], inplace=True)# drop this column as it has very few unique values\n","\n","      train_feat_df.drop(columns=[\"mean_half_delta_lsprd\"], inplace=True)# drop this column as it has very few unique values\n","\n","      train_feat_df[\"log_wap1_log_price_ret_vol\"], mean, std = ft_train.standard_scaling(\"log_wap1_log_price_ret_vol\")\n","      feat_normalization_mu_std_df[\"log_wap1_log_price_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","\n","      # train_feat_df.drop(columns=[\"log_target\"], inplace=True)# drop log_target as we create it here as tlog_target which is sames as log_target_standardize\n","      # train_feat_df.drop(columns=[\"log_target_standardized\"], inplace=True)#\n","      temp_target = train_feat_df[\"target\"]\n","      val , new_col_name, mean, std = ft_train.log(\"target\")\n","      train_feat_df[new_col_name] = val\n","      feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      train_feat_df[\"target\"] = temp_target\n","      del temp_target\n","\n","      train_feat_df[\"bid_lvl2_min_lvl1_size_feat\"], mean, std = ft_train.standard_scaling(\"bid_lvl2_min_lvl1_size_feat\")\n","      feat_normalization_mu_std_df[\"bid_lvl2_min_lvl1_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"ask_lvl2_min_lvl1_size_feat\"], mean, std = ft_train.standard_scaling(\"ask_lvl2_min_lvl1_size_feat\")\n","      feat_normalization_mu_std_df[\"ask_lvl2_min_lvl1_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"], mean, std = ft_train.standard_scaling(\"lvl2_minus_lvl1_bid_n_ask_size_feat\")\n","      feat_normalization_mu_std_df[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"sum_size\"], mean, std = ft_train.standard_scaling(\"sum_size\")\n","      feat_normalization_mu_std_df[\"sum_size\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"sum_order_count\"], mean, std = ft_train.standard_scaling(\"sum_order_count\")\n","      feat_normalization_mu_std_df[\"sum_order_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"sum_size_per_order_count\"], mean, std = ft_train.standard_scaling(\"sum_size_per_order_count\")\n","      feat_normalization_mu_std_df[\"sum_size_per_order_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_eps5e3(\"trade_price_n_wap1_dev\")\n","      train_feat_df.rename(columns={'trade_price_n_wap1_dev':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps5e3_test\"]\n","      feat_normalization_mu_std_df['trade_price_n_wap1_dev'] = [mean, std, \"log_eps5e3_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_eps5e3(\"trade_price_n_wap_eqi_price0_dev\")\n","      train_feat_df.rename(columns={'trade_price_n_wap_eqi_price0_dev':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps5e3_test\"]\n","      feat_normalization_mu_std_df['trade_price_n_wap_eqi_price0_dev'] = [mean, std, \"log_eps5e3_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"first_10_min_vol\")\n","      train_feat_df.rename(columns={'first_10_min_vol':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['first_10_min_vol'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_eps1e4(\"trade_price_std\")\n","      train_feat_df.rename(columns={'trade_price_std':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps1e4_test\"]\n","      feat_normalization_mu_std_df['trade_price_std'] = [mean, std, \"log_eps1e4_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_eps1e4(\"trade_price_real_vol\")\n","      train_feat_df.rename(columns={'trade_price_real_vol':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps1e4_test\"]\n","      feat_normalization_mu_std_df['trade_price_real_vol'] = [mean, std, \"log_eps1e4_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"trade_size_std\")\n","      train_feat_df.rename(columns={'trade_size_std':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['trade_size_std'] = [mean, std, \"log_1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"trade_size_mean\")\n","      train_feat_df.rename(columns={'trade_size_mean':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"\n","      feat_normalization_mu_std_df['trade_size_mean'] = [mean, std, \"log_1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"trade_order_count_std\")\n","      train_feat_df.rename(columns={'trade_order_count_std':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['trade_order_count_std'] = [mean, std, \"log_1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"trade_order_count_mean\")\n","      train_feat_df.rename(columns={'trade_order_count_mean':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['trade_order_count_mean'] = [mean, std, \"log_1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_sum_stats_4_clusters\")\n","      train_feat_df.rename(columns={'target_vol_sum_stats_4_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_sum_stats_4_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_sum_stats_10_clusters\")\n","      train_feat_df.rename(columns={'target_vol_sum_stats_10_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_sum_stats_10_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_sum_stats_16_clusters\")\n","      train_feat_df.rename(columns={'target_vol_sum_stats_16_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      #feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_sum_stats_16_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_sum_stats_30_clusters\")\n","      train_feat_df.rename(columns={'target_vol_sum_stats_30_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_sum_stats_30_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_robust_sum_stats_2_clusters\")\n","      train_feat_df.rename(columns={'target_vol_robust_sum_stats_2_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_robust_sum_stats_2_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_robust_sum_stats_4_clusters\")\n","      train_feat_df.rename(columns={'target_vol_robust_sum_stats_4_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_robust_sum_stats_4_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_robust_sum_stats_14_clusters\")\n","      train_feat_df.rename(columns={'target_vol_robust_sum_stats_14_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_robust_sum_stats_14_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_robust_sum_stats_20_clusters\")\n","      train_feat_df.rename(columns={'target_vol_robust_sum_stats_20_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_robust_sum_stats_20_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_robust_sum_stats_32_clusters\")\n","      train_feat_df.rename(columns={'target_vol_robust_sum_stats_32_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_robust_sum_stats_32_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p(\"target_vol_robust_sum_stats_60_clusters\")\n","      train_feat_df.rename(columns={'target_vol_robust_sum_stats_60_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","      feat_normalization_mu_std_df['target_vol_robust_sum_stats_60_clusters'] = [mean, std, \"log_log1p_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"target_vol_pcorr_3_clusters\")\n","      train_feat_df.rename(columns={'target_vol_pcorr_3_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['target_vol_pcorr_3_clusters'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"target_vol_pcorr_49_clusters\")\n","      train_feat_df.rename(columns={'target_vol_pcorr_49_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['target_vol_pcorr_49_clusters'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"target_vol_pcorr_90_clusters\")\n","      train_feat_df.rename(columns={'target_vol_pcorr_90_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['target_vol_pcorr_90_clusters'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"target_vol_pcorr_10_clusters\")\n","      train_feat_df.rename(columns={'target_vol_pcorr_10_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['target_vol_pcorr_10_clusters'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"target_vol_pcorr_26_clusters\")\n","      train_feat_df.rename(columns={'target_vol_pcorr_26_clusters':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['target_vol_pcorr_26_clusters'] = [mean, std, \"log_test\"]\n","\n","      train_feat_df[\"min_bid_price1\"], mean, std = ft_train.standard_scaling(\"min_bid_price1\")\n","      feat_normalization_mu_std_df[\"min_bid_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"max_bid_price1\"], mean, std = ft_train.standard_scaling(\"max_bid_price1\")\n","      feat_normalization_mu_std_df[\"max_bid_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"min_ask_price1\"], mean, std = ft_train.standard_scaling(\"min_ask_price1\")\n","      feat_normalization_mu_std_df[\"min_ask_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"max_ask_price1\"], mean, std = ft_train.standard_scaling(\"max_ask_price1\")\n","      feat_normalization_mu_std_df[\"max_ask_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","\n","      train_feat_df.drop(columns=[\"min_bid_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      val, new_col_name, mean, std = ft_train.log(\"max_bid_size1\")\n","      train_feat_df.rename(columns={'max_bid_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['max_bid_size1'] = [mean, std, \"log_test\"]\n","\n","      train_feat_df.drop(columns=[\"min_ask_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      val, new_col_name, mean, std = ft_train.log(\"max_ask_size1\")\n","      train_feat_df.rename(columns={'max_ask_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['max_ask_size1'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p_eps1e4(\"range_ask_price1\")\n","      train_feat_df.rename(columns={'range_ask_price1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","      feat_normalization_mu_std_df['range_ask_price1'] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p_eps1e4(\"range_bid_price1\")\n","      train_feat_df.rename(columns={'range_bid_price1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","      feat_normalization_mu_std_df['range_bid_price1'] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"range_ask_size1\")\n","      train_feat_df.rename(columns={'range_ask_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['range_ask_size1'] = [mean, std, \"log_1p_test\"]\n","\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"range_bid_size1\")\n","      train_feat_df.rename(columns={'range_bid_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['range_bid_size1'] = [mean, std, \"log_1p_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_log1p_eps1e4(\"sad_ask_price1\")\n","      train_feat_df.rename(columns={'sad_ask_price1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","      feat_normalization_mu_std_df['sad_ask_price1'] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"sad_ask_size1\")\n","      train_feat_df.rename(columns={'sad_ask_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['sad_ask_size1'] = [mean, std, \"log_1p_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_bid_price1\")\n","      train_feat_df.rename(columns={'sad_bid_price1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_bid_price1'] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_bid_size1\")\n","      train_feat_df.rename(columns={'sad_bid_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_bid_size1'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      train_feat_df[\"bs_bp_corr1\"], mean, std = ft_train.standard_scaling(\"bs_bp_corr1\")\n","      feat_normalization_mu_std_df[\"bs_bp_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bs_as_corr1\"], mean, std = ft_train.standard_scaling(\"bs_as_corr1\")\n","      feat_normalization_mu_std_df[\"bs_as_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bs_ap_corr1\"], mean, std = ft_train.standard_scaling(\"bs_ap_corr1\")\n","      feat_normalization_mu_std_df[\"bs_ap_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bp_as_corr1\"], mean, std = ft_train.standard_scaling(\"bp_as_corr1\")\n","      feat_normalization_mu_std_df[\"bp_as_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.exp_exp(\"bp_ap_corr1\")\n","      train_feat_df.rename(columns={'bp_ap_corr1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_exp_test\"]\n","      feat_normalization_mu_std_df['bp_ap_corr1'] = [mean, std, \"exp_exp_test\"]\n","\n","      train_feat_df[\"as_ap_corr1\"], mean, std = ft_train.standard_scaling(\"as_ap_corr1\")\n","      feat_normalization_mu_std_df[\"as_ap_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"min_price1\"], mean, std = ft_train.standard_scaling(\"min_price1\")\n","      feat_normalization_mu_std_df[\"min_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"max_price1\"], mean, std = ft_train.standard_scaling(\"max_price1\")\n","      feat_normalization_mu_std_df[\"max_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df.drop(columns=[\"min_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      val, new_col_name, mean, std = ft_train.log(\"max_size1\")\n","      train_feat_df.rename(columns={'max_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['max_size1'] = [mean, std, \"log_test\"]\n","\n","\n","      train_feat_df.drop(columns=[\"min_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      train_feat_df[\"max_order_count1\"], mean, std = ft_train.standard_scaling(\"max_order_count1\")\n","      feat_normalization_mu_std_df[\"max_order_count1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"range_price1\")\n","      train_feat_df.rename(columns={'range_price1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['range_price1'] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"range_size1\")\n","      train_feat_df.rename(columns={'range_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['range_size1'] = [mean, std, \"log_1p_test\"]\n","\n","\n","      train_feat_df.drop(columns=[\"range_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_price1\")\n","      train_feat_df.rename(columns={'sad_price1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_price1'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_size1\")\n","      train_feat_df.rename(columns={'sad_size1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_size1'] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.log_1p(\"sad_order_count1\")\n","      train_feat_df.rename(columns={'sad_order_count1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","      feat_normalization_mu_std_df['sad_order_count1'] = [mean, std, \"log_1p_test\"]\n","\n","\n","      val, new_col_name, mean, std = ft_train.exp(\"size_order_count_corr1\")\n","      train_feat_df.rename(columns={'size_order_count_corr1':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","      feat_normalization_mu_std_df['size_order_count_corr1'] = [mean, std, \"exp_test\"]\n","\n","      # val, new_col_name, mean, std = ft_train.log(\"book_ewma_vol\")\n","      # train_feat_df.rename(columns={'book_ewma_vol':new_col_name}, inplace=True)\n","      # train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","      # val, new_col_name, mean, std = ft_train.log_1p(\"trade_ewma_vol\")\n","      # train_feat_df.rename(columns={'trade_ewma_vol':new_col_name}, inplace=True)\n","      # train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","      train_feat_df[\"min_bid_price2\"], mean, std = ft_train.standard_scaling(\"min_bid_price2\")\n","      feat_normalization_mu_std_df[\"min_bid_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"max_bid_price2\"], mean, std = ft_train.standard_scaling(\"max_bid_price2\")\n","      feat_normalization_mu_std_df[\"max_bid_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"min_ask_price2\"], mean, std = ft_train.standard_scaling(\"min_ask_price2\")\n","      feat_normalization_mu_std_df[\"min_ask_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"max_ask_price2\"], mean, std = ft_train.standard_scaling(\"max_ask_price2\")\n","      feat_normalization_mu_std_df[\"max_ask_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df.drop(columns=[\"min_bid_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      val, new_col_name, mean, std = ft_train.log(\"max_bid_size2\")\n","      train_feat_df.rename(columns={'max_bid_size2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['max_bid_size2'] = [mean, std, \"log_test\"]\n","\n","      train_feat_df.drop(columns=[\"min_ask_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","      val, new_col_name, mean, std = ft_train.log(\"max_ask_size2\")\n","      train_feat_df.rename(columns={'max_ask_size2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['max_ask_size2'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"range_ask_price2\")\n","      train_feat_df.rename(columns={'range_ask_price2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['range_ask_price2'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"range_bid_price2\")\n","      train_feat_df.rename(columns={'range_bid_price2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['range_bid_price2'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log(\"range_ask_size2\")\n","      train_feat_df.rename(columns={'range_ask_size2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","      feat_normalization_mu_std_df['range_ask_size2'] = [mean, std, \"log_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_ask_price2\")\n","      train_feat_df.rename(columns={'sad_ask_price2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_ask_price2'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_ask_size2\")\n","      train_feat_df.rename(columns={'sad_ask_size2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_ask_size2'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      val, new_col_name, mean, std = ft_train.log_lin100_1(\"sad_bid_price2\")\n","      train_feat_df.rename(columns={'sad_bid_price2':new_col_name}, inplace=True)\n","      train_feat_df[new_col_name] = val\n","      # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","      feat_normalization_mu_std_df['sad_bid_price2'] = [mean, std, \"log_lin100_1_test\"]\n","\n","      train_feat_df[\"bs_bp_corr2\"], mean, std = ft_train.standard_scaling(\"bs_bp_corr2\")\n","      feat_normalization_mu_std_df[\"bs_bp_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bs_as_corr2\"], mean, std = ft_train.standard_scaling(\"bs_as_corr2\")\n","      feat_normalization_mu_std_df[\"bs_as_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bs_ap_corr2\"], mean, std = ft_train.standard_scaling(\"bs_ap_corr2\")\n","      feat_normalization_mu_std_df[\"bs_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bp_as_corr2\"], mean, std = ft_train.standard_scaling(\"bp_as_corr2\")\n","      feat_normalization_mu_std_df[\"bp_as_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"bp_ap_corr2\"], mean, std = ft_train.standard_scaling(\"bp_ap_corr2\")\n","      feat_normalization_mu_std_df[\"bp_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"as_ap_corr2\"], mean, std = ft_train.standard_scaling(\"as_ap_corr2\")\n","      feat_normalization_mu_std_df[\"as_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","      train_feat_df[\"sum_stats_4_clusters_labels\"] = train_feat_df[\"sum_stats_4_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"sum_stats_10_clusters_labels\"] = train_feat_df[\"sum_stats_10_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"sum_stats_16_clusters_labels\"] = train_feat_df[\"sum_stats_16_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"sum_stats_30_clusters_labels\"] = train_feat_df[\"sum_stats_30_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"pear_corr_3_clusters_labels\"] = train_feat_df[\"pear_corr_3_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"pear_corr_49_clusters_labels\"] = train_feat_df[\"pear_corr_49_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"pear_corr_90_clusters_labels\"] = train_feat_df[\"pear_corr_90_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"pear_corr_10_clusters_labels\"] = train_feat_df[\"pear_corr_10_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"pear_corr_26_clusters_labels\"] = train_feat_df[\"pear_corr_26_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"robust_sum_stats_2_clusters_labels\"] = train_feat_df[\"robust_sum_stats_2_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"robust_sum_stats_4_clusters_labels\"] = train_feat_df[\"robust_sum_stats_4_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"robust_sum_stats_14_clusters_labels\"] = train_feat_df[\"robust_sum_stats_14_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"robust_sum_stats_20_clusters_labels\"] = train_feat_df[\"robust_sum_stats_20_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"robust_sum_stats_32_clusters_labels\"] = train_feat_df[\"robust_sum_stats_32_clusters_labels\"].astype(\"category\")\n","\n","      train_feat_df[\"robust_sum_stats_60_clusters_labels\"] = train_feat_df[\"robust_sum_stats_60_clusters_labels\"].astype(\"category\")\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    \"\"\"\n","    ######## Feature TRANSFORMATION of the INFERENCE/TEST data ########\n","    \"\"\"\n","\n","\n","    # test_feat_transformation_map = {'tlog_3p_' : 'log_3p_test',\n","    # 'tlog_1p_' : 'log_1p_test',\n","    # 'tlog_eps523_' : 'log_eps5e3_test',\n","    # 'tlog_eps1e4_' : 'log_eps1e4_test',\n","    # 'tlog_' : 'log_test',\n","    # 'tlog_10p_' : 'log_10p_test',\n","    # 'tlog_tlog1p_' : 'log_log1p_test',\n","    # 'tlog_tlog1p_eps1e4_' : 'log_log1p_eps1e4_test',\n","    # 'tlog_tlinear_' : 'log_lin100_1_test',\n","    # 'texp_' : 'exp_test',\n","    # 'texp_texp_' : 'exp_exp_test'}\n","\n","\n","    if ml_stage == 'inference':\n","\n","      test_feat_df = feat_df.copy()\n","      del feat_df\n","      test_feat_df.drop(columns=['wap1_log_price_ret_vol'], inplace=True) ## we drop this because we have log_wap1_log_price_ret_vol already\n","      test_feat_df.drop(columns=[\"wap1_log_price_ret_vol_from_0\"], inplace=True) ## drop this column as it has all 0 values\n","      test_feat_df.drop(columns=[\"mean_half_delta\"], inplace=True)# drop this column as it has very few unique values\n","      test_feat_df.drop(columns=[\"mean_half_delta_lsprd\"], inplace=True)# drop this column as it has very few unique values\n","      test_feat_df.drop(columns=[\"min_bid_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","      test_feat_df.drop(columns=[\"min_ask_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","      test_feat_df.drop(columns=[\"min_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","      test_feat_df.drop(columns=[\"min_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","      test_feat_df.drop(columns=[\"min_bid_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","      test_feat_df.drop(columns=[\"min_ask_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","      test_feat_df.drop(columns=[\"range_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","\n","\n","\n","      ft_test = Train_Test_FeatureTransformation(test_feat_df)\n","\n","      transformable_cols = [col for col in test_feat_df.columns if '_labels' not in col]\n","      transformable_cols.remove('stock_id')\n","      transformable_cols.remove('time_id')\n","\n","      for test_trans_col in transformable_cols:\n","\n","        func = getattr(ft_test, feat_normalization_mu_std_df.loc['transform',test_trans_col])\n","        val, new_col_name = func(test_feat_df[test_trans_col],feat_normalization_mu_std_df.loc['mean',test_trans_col], feat_normalization_mu_std_df.loc['std',test_trans_col],test_trans_col )\n","        test_feat_df.rename(columns={test_trans_col:new_col_name}, inplace=True)\n","        test_feat_df[new_col_name] = val\n","\n","      categorical_cols = [col for col in test_feat_df.columns if '_labels' in col]\n","      categorical_cols.append('stock_id')\n","      categorical_cols.append('time_id')\n","\n","      for test_cat_col in categorical_cols:\n","        test_feat_df[test_cat_col]  = test_feat_df[test_cat_col].astype('category')\n","\n","\n","\n","\n","\n","\n","\n","\n","    if ml_stage == 'training':\n","        # ########## code to reorder time ids to correct sequence/order for training\n","\n","        #feat_dtrain_feat_dff[\"seq_id\"] = -1 ## we exclude seq id because it cannot be generated for the submission data\n","\n","        time_ids_reordered = correct_time_id_order['time_id'].values\n","\n","        def my_reorder_stock_in_df(st_df, time_ids_reordered):\n","            common_values = [value for value in time_ids_reordered if value in st_df['time_id'].values]\n","            st_df = st_df.set_index('time_id')\n","            st_df = st_df.reindex(common_values)\n","            st_df = st_df.reset_index()\n","            #st_df[\"seq_id\"] = range(st_df.shape[0]) ## we exclude seq id because it cannot be generated for the submission data\n","            return st_df\n","\n","\n","        # Assuming you have a dataframe called 'feat_df' and an array of reordered time_ids called 'time_ids_reordered'\n","        train_feat_df_reordered = train_feat_df.groupby('stock_id').apply(my_reorder_stock_in_df, time_ids_reordered=time_ids_reordered).reset_index(drop=True)\n","        del train_feat_df\n","\n","\n","    if ml_stage == 'training':\n","        return train_all_unique_stock_ids,train_feat_df_reordered,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters,feat_normalization_mu_std_df\n","    elif ml_stage == 'inference':\n","        return test_feat_df\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f05751fe","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wi61bbqYwUZNn2H7ramYLe47aQxAxYqd"},"executionInfo":{"elapsed":931651,"status":"ok","timestamp":1726892500245,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"f05751fe","outputId":"982b5a02-56e0-4bd0-bb4b-c0cc61e7ee35"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["final_sum_stats_target_vol_clusters = pd.DataFrame()\n","final_robust_sum_stats_target_vol_clusters = pd.DataFrame()\n","final_pear_corr_target_vol_clusters = pd.DataFrame()\n","feat_normalization_mu_std_df = pd.DataFrame()\n","train_all_unique_stock_ids = pd.DataFrame()\n","ml_stage='training'\n","train_all_unique_stock_ids,train_feat_df_reordered,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters,feat_normalization_mu_std_df = perform_ml_stage(ml_stage, final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters,feat_normalization_mu_std_df,train_all_unique_stock_ids)"]},{"cell_type":"code","execution_count":null,"id":"c0c8986c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441166,"status":"ok","timestamp":1726892941394,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"c0c8986c","outputId":"d32f7268-9855-4754-98e1-f8fd49832ab8"},"outputs":[{"name":"stdout","output_type":"stream","text":["st_ids [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 122, 123, 124, 125, 126]\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-117-73c044ceb40c>:526: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap1_log_price_ret_per_liq2_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:529: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:538: RuntimeWarning: divide by zero encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:538: RuntimeWarning: invalid value encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:538: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:543: RuntimeWarning: divide by zero encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:543: RuntimeWarning: invalid value encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-117-73c044ceb40c>:557: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol'] = np.log(np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","<ipython-input-117-73c044ceb40c>:121: RuntimeWarning: divide by zero encountered in log\n","  final_features['lsvol'] = np.log( np.nanmean(train_buckets['log_spread_ret_sqr_vol_buks'], 2, keepdims=True))\n","<ipython-input-117-73c044ceb40c>:141: RuntimeWarning: divide by zero encountered in log\n","  final_features['trade_count']      = np.log( np.nanmean(train_buckets['trade_count_buks']    , 2, keepdims=True))\n","<ipython-input-117-73c044ceb40c>:143: RuntimeWarning: divide by zero encountered in log\n","  final_features['root_trade_count'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","<ipython-input-117-73c044ceb40c>:167: RuntimeWarning: divide by zero encountered in log\n","  final_features['root_trade_count_var'] = np.log( np.nanvar(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","<ipython-input-117-73c044ceb40c>:170: RuntimeWarning: divide by zero encountered in divide\n","  final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","<ipython-input-117-73c044ceb40c>:170: RuntimeWarning: invalid value encountered in divide\n","  final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","<ipython-input-117-73c044ceb40c>:170: RuntimeWarning: divide by zero encountered in log\n","  final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","<ipython-input-117-73c044ceb40c>:174: RuntimeWarning: divide by zero encountered in divide\n","  final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","<ipython-input-117-73c044ceb40c>:174: RuntimeWarning: invalid value encountered in divide\n","  final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","<ipython-input-117-73c044ceb40c>:174: RuntimeWarning: divide by zero encountered in log\n","  final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","<ipython-input-117-73c044ceb40c>:227: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_25_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1741: RuntimeWarning: invalid value encountered in subtract\n","  np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n","<ipython-input-117-73c044ceb40c>:231: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_29_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,29:]**2,2,keepdims=True)\n","<ipython-input-117-73c044ceb40c>:239: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_25_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","<ipython-input-117-73c044ceb40c>:243: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_29_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","<ipython-input-117-73c044ceb40c>:247: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_29_q3q1'] = np.log(np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","<ipython-input-117-73c044ceb40c>:251: RuntimeWarning: divide by zero encountered in log\n","  np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","<ipython-input-117-73c044ceb40c>:401: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2']        = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg)**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-117-73c044ceb40c>:403: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2_liqf10'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-117-73c044ceb40c>:405: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2_liqf20'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-117-73c044ceb40c>:407: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2_liqf29'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-117-73c044ceb40c>:492: RuntimeWarning: divide by zero encountered in log\n","  final_features[name + suffix] =            np.log(             np.mean(wap1_log_price_ret_vol[:,:,ffrom:]**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n"]},{"name":"stdout","output_type":"stream","text":["wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock shape[1]==1\n","log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock shape[1]==1\n","wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock shape[1]==1\n","v1proj_25_15 shape[1]==1\n","liqvol1_smean shape[1]==1\n","root_trade_count_smean shape[1]==1\n","v1proj_29_15 shape[1]==1\n","v1proj_20 shape[1]==1\n","v1proj_25 shape[1]==1\n","v1proj_29 shape[1]==1\n","v1proj_29_q1 shape[1]==1\n","v1proj_29_q3 shape[1]==1\n","v1proj_25_q1 shape[1]==1\n","v1proj_25_q3 shape[1]==1\n","v1proj_29_15_q1 shape[1]==1\n","v1proj_29_15_q3 shape[1]==1\n","v1proj_25_15_q1 shape[1]==1\n","v1proj_25_15_q3 shape[1]==1\n","v1proj_25_15_std shape[1]==1\n","v1proj_29_15_std shape[1]==1\n","v1proj_20_std shape[1]==1\n","v1proj_25_std shape[1]==1\n","v1proj_29_std shape[1]==1\n","v1proj_29_q3q1 shape[1]==1\n","tvpl2_rmed2v1 shape[1]==1\n","tvpl2_rmed2v1lf25 shape[1]==1\n","tvpl2_rmed2v1lf29 shape[1]==1\n","v1liq2sprojt10f25 shape[1]==1\n","v1liq2sprojt5f25 shape[1]==1\n","v1spprojt10f29 shape[1]==1\n","v1spprojt15f25 shape[1]==1\n","v1spprojt15f29 shape[1]==1\n","v1spprojt15f29_q1 shape[1]==1\n","v1spprojt15f29_q3 shape[1]==1\n","v1spprojt15f25_q1 shape[1]==1\n","v1spprojt15f25_q3 shape[1]==1\n","v1spprojtf29_q1 shape[1]==1\n","v1spprojtf29_q3 shape[1]==1\n","v1spprojtf25_q1 shape[1]==1\n","v1spprojtf25_q3 shape[1]==1\n","mean_half_delta shape[0]==1\n","mean_half_delta_lsprd shape[0]==1\n","final_robust_sum_stats_target_vol_clusters[c]: 0      1\n","1      1\n","2      2\n","3      1\n","4      1\n","      ..\n","107    2\n","108    2\n","109    1\n","110    2\n","111    1\n","Name: 2_clusters, Length: 112, dtype: int32\n","final_robust_sum_stats_target_vol_clusters[c]: 0      2\n","1      2\n","2      3\n","3      1\n","4      2\n","      ..\n","107    3\n","108    3\n","109    2\n","110    3\n","111    2\n","Name: 4_clusters, Length: 112, dtype: int32\n","final_robust_sum_stats_target_vol_clusters[c]: 0       4\n","1       3\n","2      12\n","3       1\n","4       4\n","       ..\n","107    10\n","108     9\n","109     3\n","110    12\n","111     6\n","Name: 14_clusters, Length: 112, dtype: int32\n","final_robust_sum_stats_target_vol_clusters[c]: 0      14\n","1      13\n","2       5\n","3      20\n","4      14\n","       ..\n","107    12\n","108     3\n","109    12\n","110     5\n","111    11\n","Name: 20_clusters, Length: 112, dtype: int32\n","final_robust_sum_stats_target_vol_clusters[c]: 0      24\n","1      23\n","2       9\n","3      34\n","4      24\n","       ..\n","107    21\n","108     4\n","109    20\n","110    11\n","111    18\n","Name: 32_clusters, Length: 112, dtype: int32\n","final_robust_sum_stats_target_vol_clusters[c]: 0      43\n","1      42\n","2      20\n","3      63\n","4      43\n","       ..\n","107    40\n","108     8\n","109    38\n","110    22\n","111    36\n","Name: 60_clusters, Length: 112, dtype: int32\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-119-35b06e0f7912>:50: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  final_sum_stats_target_vol_df.rename(columns={'4_clusters':'target_vol_sum_stats_4_clusters','10_clusters':'target_vol_sum_stats_10_clusters',\n","<ipython-input-119-35b06e0f7912>:54: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  final_robust_sum_stats_target_vol_df.rename(columns={'2_clusters':'target_vol_robust_sum_stats_2_clusters','4_clusters':'target_vol_robust_sum_stats_4_clusters',\n","<ipython-input-119-35b06e0f7912>:59: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  final_pear_corr_target_vol_df.rename(columns={'3_clusters':'target_vol_pcorr_3_clusters','49_clusters': 'target_vol_pcorr_49_clusters',\n"]},{"name":"stdout","output_type":"stream","text":["\n"," feat_df columns\n","stock_id\n","time_id\n","wap1_log_price_ret_vol\n","log_liq2_ret_*_wap_eqi_price1_ret_vol\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\n","wap1_log_price_ret_per_liq2_vol\n","wap1_log_price_ret_per_spread_sqr_vol\n","log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio\n","wap1_log_price_ret_per_liq2_vol_15_ratio\n","wap1_log_price_ret_per_spread_sqr_vol_15_ratio\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\n","log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\n","wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\n","wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\n","wap1_log_price_ret_neg_log_liq_ret_sqr_vol\n","wap1_log_price_ret_pos_log_liq_ret_sqr_vol\n","wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\n","wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0\n","wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0\n","wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\n","wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10\n","wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10\n","wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\n","wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20\n","wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20\n","wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20\n","soft_stock_mean_tvpl2_:0\n","soft_stock_mean_tvpl2_:10\n","soft_stock_mean_tvpl2_:20\n","soft_stock_mean_tvpl2_liqf\n","soft_stock_mean_tvpl2_liqf_volf10\n","soft_stock_mean_tvpl2_liqf_volf20\n","v1proj_25_15\n","lsvol\n","liqvol1\n","liqvol1_smean\n","liqvol2\n","liqvol1_15_15\n","trade_count\n","root_trade_count\n","root_trade_count_smean\n","root_book_delta_count\n","root_trade_count_var\n","trade_count_15_15\n","root_trade_count_15_15\n","v1proj_29_15\n","v1proj_20\n","v1proj_25\n","v1proj_29\n","v1proj_29_q1\n","v1proj_29_q3\n","v1proj_25_q1\n","v1proj_25_q3\n","v1proj_29_15_q1\n","v1proj_29_15_q3\n","v1proj_25_15_q1\n","v1proj_25_15_q3\n","v1proj_25_15_std\n","v1proj_20_std\n","v1proj_25_std\n","v1proj_29_q3q1\n","tvpl2_rmed2v1\n","tvpl2_rmed2v1lf25\n","tvpl2_rmed2v1lf29\n","tvpl2\n","tvpl2_liqf10\n","tvpl2_liqf20\n","tvpl2_liqf29\n","tvpl2_smean_vol\n","tvpl2_smean_vol_liqf10\n","tvpl2_smean_vol_liqf20\n","tvpl2_smean_vol_liqf29\n","v1liq2projt5\n","v1liq2projt10\n","v1liq2projt20\n","liqt10rf29\n","liqt20rf29\n","v1liq2sprojt10f25\n","v1liq2sprojt5f25\n","v1spprojt10f29\n","v1spprojt15f25\n","v1spprojt15f29\n","v1spprojt15f29_q1\n","v1spprojt15f29_q3\n","v1spprojt15f25_q1\n","v1spprojt15f25_q3\n","v1spprojtf29_q1\n","v1spprojtf29_q3\n","v1spprojtf25_q1\n","v1spprojtf25_q3\n","wap1_log_price_ret_vol_from_0\n","wap1_log_price_ret_volstock_mean_from_0\n","wap1_log_price_ret_vol_from_10\n","wap1_log_price_ret_volstock_mean_from_10\n","wap1_log_price_ret_vol_from_20\n","wap1_log_price_ret_volstock_mean_from_20\n","wap1_log_price_ret_vol_from_25\n","wap1_log_price_ret_volstock_mean_from_25\n","vol1_mean\n","mean_half_delta\n","mean_half_delta_lsprd\n","log_wap1_log_price_ret_vol\n","bid_lvl2_min_lvl1_size_feat\n","ask_lvl2_min_lvl1_size_feat\n","lvl2_minus_lvl1_bid_n_ask_size_feat\n","sum_size\n","sum_order_count\n","sum_size_per_order_count\n","trade_price_n_wap1_dev\n","trade_price_n_wap_eqi_price0_dev\n","trade_price_std\n","trade_price_real_vol\n","trade_size_std\n","trade_size_mean\n","trade_order_count_std\n","trade_order_count_mean\n","first_10_min_vol\n","target_vol_sum_stats_4_clusters\n","target_vol_sum_stats_10_clusters\n","target_vol_sum_stats_16_clusters\n","target_vol_sum_stats_30_clusters\n","target_vol_robust_sum_stats_2_clusters\n","target_vol_robust_sum_stats_4_clusters\n","target_vol_robust_sum_stats_14_clusters\n","target_vol_robust_sum_stats_20_clusters\n","target_vol_robust_sum_stats_32_clusters\n","target_vol_robust_sum_stats_60_clusters\n","target_vol_pcorr_3_clusters\n","target_vol_pcorr_49_clusters\n","target_vol_pcorr_90_clusters\n","target_vol_pcorr_10_clusters\n","target_vol_pcorr_26_clusters\n","min_bid_price1\n","max_bid_price1\n","min_ask_price1\n","max_ask_price1\n","min_bid_size1\n","max_bid_size1\n","min_ask_size1\n","max_ask_size1\n","range_ask_price1\n","range_bid_price1\n","range_ask_size1\n","range_bid_size1\n","sad_ask_price1\n","sad_ask_size1\n","sad_bid_price1\n","sad_bid_size1\n","bs_bp_corr1\n","bs_as_corr1\n","bs_ap_corr1\n","bp_as_corr1\n","bp_ap_corr1\n","as_ap_corr1\n","min_price1\n","max_price1\n","min_size1\n","max_size1\n","min_order_count1\n","max_order_count1\n","range_price1\n","range_size1\n","range_order_count1\n","sad_price1\n","sad_size1\n","sad_order_count1\n","size_order_count_corr1\n","sum_stats_4_clusters_labels\n","sum_stats_10_clusters_labels\n","sum_stats_16_clusters_labels\n","sum_stats_30_clusters_labels\n","robust_sum_stats_2_clusters_labels\n","robust_sum_stats_4_clusters_labels\n","robust_sum_stats_14_clusters_labels\n","robust_sum_stats_20_clusters_labels\n","robust_sum_stats_32_clusters_labels\n","robust_sum_stats_60_clusters_labels\n","pear_corr_3_clusters_labels\n","pear_corr_49_clusters_labels\n","pear_corr_90_clusters_labels\n","pear_corr_10_clusters_labels\n","pear_corr_26_clusters_labels\n","min_bid_price2\n","max_bid_price2\n","min_ask_price2\n","max_ask_price2\n","min_bid_size2\n","max_bid_size2\n","min_ask_size2\n","max_ask_size2\n","range_ask_price2\n","range_bid_price2\n","range_ask_size2\n","sad_ask_price2\n","sad_ask_size2\n","sad_bid_price2\n","bs_bp_corr2\n","bs_as_corr2\n","bs_ap_corr2\n","bp_as_corr2\n","bp_ap_corr2\n","as_ap_corr2\n","\n","\n","pos INF:  wap1_log_price_ret_per_liq2_vol_15_ratio :  1\n","neg INF:  wap1_log_price_ret_per_liq2_vol_15_ratio :  1\n","pos INF:  wap1_log_price_ret_per_spread_sqr_vol_15_ratio :  1\n","neg INF:  wap1_log_price_ret_per_spread_sqr_vol_15_ratio :  1\n","pos INF:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol :  218\n","neg INF:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol :  218\n","pos INF:  lsvol :  17\n","neg INF:  lsvol :  17\n","pos INF:  trade_count :  5\n","neg INF:  trade_count :  5\n","pos INF:  root_trade_count :  5\n","neg INF:  root_trade_count :  5\n","pos INF:  root_trade_count_var :  5\n","neg INF:  root_trade_count_var :  5\n","pos INF:  trade_count_15_15 :  122\n","neg INF:  trade_count_15_15 :  63\n","pos INF:  root_trade_count_15_15 :  122\n","neg INF:  root_trade_count_15_15 :  63\n","pos INF:  tvpl2 :  5\n","neg INF:  tvpl2 :  5\n","pos INF:  tvpl2_liqf10 :  5\n","neg INF:  tvpl2_liqf10 :  5\n","pos INF:  tvpl2_liqf20 :  5\n","neg INF:  tvpl2_liqf20 :  5\n","pos INF:  tvpl2_liqf29 :  5\n","neg INF:  tvpl2_liqf29 :  5\n","pos INF:  wap1_log_price_ret_vol_from_25 :  2\n","neg INF:  wap1_log_price_ret_vol_from_25 :  2\n"]}],"source":["\n","ml_stage = 'inference'\n","test_feat_df = perform_ml_stage(ml_stage,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters,feat_normalization_mu_std_df,train_all_unique_stock_ids)\n"]},{"cell_type":"markdown","id":"4ab61745","metadata":{"id":"4ab61745"},"source":["## **Model Training**"]},{"cell_type":"code","execution_count":null,"id":"6186f9bf","metadata":{"id":"6186f9bf"},"outputs":[],"source":["##### remove test from training data #####\n","\n","df_train_reordered = train_feat_df_reordered.copy()\n","del train_feat_df_reordered\n","\n","df_test = test_feat_df.copy()\n","del test_feat_df"]},{"cell_type":"code","execution_count":null,"id":"6dd0461f","metadata":{"id":"6dd0461f"},"outputs":[],"source":["# xgboost_important_feat =['v1spprojt15f25', 'v1spprojt15f25_q1','v1spprojt15f29', 'v1spprojt10f29','wap1_log_price_ret_volstock_mean_from_20', 'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock',\\\n","# 'wap1_log_price_ret_volstock_mean_from_25', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20', 'v1proj_29_15_q3', \"v1spprojtf25_q1\" , 'v1proj_25',\\\n","# 'soft_stock_mean_tvpl2_liqf_volf20', 'tlog_eps523_trade_price_n_wap_eqi_price0_dev', 'bs_bp_corr2', 'tlog_max_bid_size2'] #, 'v1spprojt15f29_q1', 'wap1_log_price_ret_volstock_mean_from_10' ]\n","\n","\n","\n","# target_high_Corr_feat = ['log_wap1_log_price_ret_vol','tlog_first_10_min_vol','tlog_eps1e4_trade_price_real_vol', 'wap1_log_price_ret_per_liq2_vol', 'tlog_eps523_trade_price_n_wap1_dev',\\\n","# 'tlog_eps523_trade_price_n_wap_eqi_price0_dev','vol1_mean','tlog_tlinear_range_ask_price2','tlog_tlinear_range_bid_price2', 'tlog_tlog1p_eps1e4_range_ask_price1', 'tlog_tlog1p_eps1e4_range_bid_price1',\\\n","# 'tlog_tlinear_range_price1','tlog_tlinear_sad_ask_price2']\n","\n","\n","# col_list = list(set(xgboost_important_feat + target_high_Corr_feat))\n","# col_list.append('time_id')\n","# col_list.append('stock_id')\n","\n","# test_col_list = col_list.copy()\n","# train_col_list = col_list.copy()\n","# train_col_list.append('tlog_target')\n","# train_col_list.append('target')\n","# del col_list\n","\n","\n","\n","# df_train_reordered = df_train_reordered[train_col_list]\n","# df_test = df_test[test_col_list]\n"]},{"cell_type":"code","execution_count":null,"id":"b29976a7","metadata":{"id":"b29976a7"},"outputs":[],"source":["class train_validate_n_test(object):\n","\n","    def __init__(self,df_train_reordered, df_test) -> None:\n","\n","        #self.time_id_order = df.loc[:3829,'time_id'].values # select ordered unique time_ids\n","        #self.train_time_id_ind = int(len(self.time_id_order)*0.7)\n","\n","        largest_num_time_id_stock = df_train_reordered.groupby('stock_id')['time_id'].apply(lambda x: x.nunique()).argmax()\n","        self.time_id_order = df_train_reordered[df_train_reordered['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n","        self.n_folds = 10\n","        folds = TimeSeriesSplit(n_splits=self.n_folds,)# max_train_size=None, gap=10)\n","        #self.splits = folds.split( range( self.train_time_id_ind ) ) # split 70% train time_ids into n_fold splits\n","        nunique_train_time_ids = df_train_reordered['time_id'].nunique()\n","        self.splits = folds.split( range( nunique_train_time_ids ) )\n","\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        #self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        #self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","        self.train_stock_id = df_train_reordered['stock_id']\n","        self.train_time_id = df_train_reordered['time_id']\n","\n","        # test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        # self.test_df = df[df['time_id'].isin(test_time_ids)]\n","        self.test_time_id = df_test[df_test['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n","        self.test_df = df_test\n","        self.test_stock_id = self.test_df['stock_id']\n","        self.test_time_id = self.test_df['time_id']\n","\n","        #self.df = df\n","        self.df_train_reordered = df_train_reordered\n","\n","        # feature_importances = pd.DataFrame()\n","        cols = list(df_train_reordered.columns)\n","        cols.remove('tlog_target')\n","        cols.remove('target')\n","        cols.remove('time_id')\n","        self.feat_cols_list =  cols #cat_feat_labels+float32_feat_labels+float64_feat_labels # int32_feat_labels+int64_feat_labels+float32_feat_labels+float64_feat_labels\n","        # feature_importances['feature'] = self.feat_cols_list\n","\n","        self.target_name = 'target' # _standardized' log target is easier to transform back than log_target_standardized\n","\n","        #del df\n","        del df_train_reordered\n","        gc.collect()\n","\n","    # def onehotencode_cat_var(self,full_set):\n","    #     full_set = cat_feat_labels #full_set.astype({\"stn_id\":str,\"block_id\":str,\"ts_of_day\":str,\"hr_of_day\":str,\"day_of_wk\":str,\"day_of_mn\":str,\"wk_of_mon\":str })\n","    #     full_set = pd.get_dummies(full_set, prefix_sep=\"_\",columns =cat_feat_labels,drop_first=True)\n","    #     #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n","    #     return full_set\n","\n","    #### RMSPE cost function\n","    def rmspe(self,y_true, y_pred):\n","        return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","\n","\n","    # Custom RMSPE objective function\n","    def rmspe_objective(self,preds, dtrain):\n","        labels = dtrain.get_label()\n","        errors = (preds - labels) / labels\n","        gradient = 2 * errors / (1 + errors**2)\n","        hessian = 2 * (1 - errors**2) / (1 + errors**2)**2\n","        return gradient, hessian\n","\n","\n","    def xgb_RMSPE(self,preds, train_data):\n","        labels = train_data.get_label()\n","        return 'RMSPE', round(self.rmspe(y_true = labels, y_pred = preds),5)\n","\n","\n","    def nancorr(self,a, b):\n","        v = np.isfinite(a)*np.isfinite(b) > 0\n","        return np.corrcoef(a[v], b[v])[0,1]\n","\n","\n","    def xgb_train_validate(self,params_xgb,n_rounds,esr,trial):\n","        rmspe_val_score = []\n","        models= []\n","        test_y_preds = np.zeros(len(self.test_df))\n","        best_iterations = []\n","        learning_train_rmspe = []\n","        learning_val_rmspe = []\n","\n","        for fold_n, (train_index, valid_index) in enumerate(self.splits):\n","            print('Fold:',fold_n+1)\n","            # print('train_index',train_index)\n","            # print('valid_index',valid_index)\n","            train_time_ids = self.time_id_order[train_index]\n","            val_time_ids = self.time_id_order[valid_index]\n","            train_df = self.df_train_reordered[self.df_train_reordered['time_id'].isin(train_time_ids)]\n","            val_df = self.df_train_reordered[self.df_train_reordered['time_id'].isin(val_time_ids)]\n","\n","            X_train = train_df[self.feat_cols_list]\n","            y_train = train_df[self.target_name] # target\n","            X_valid = val_df[self.feat_cols_list]\n","            y_val = val_df[self.target_name] # target\n","\n","            v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n","            v1v = np.exp(  X_valid['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n","\n","            # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n","            # v1v = np.exp(np.exp(  X_valid['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n","            #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","\n","            w_train = y_train **-2 * v1tr**2\n","            w_val = y_val **-2 * v1v**2\n","\n","            print('Training....')\n","            dtrain = xgb.DMatrix(X_train, label=y_train/v1tr,weight=w_train,enable_categorical=True)\n","            dvalid = xgb.DMatrix(X_valid,   label=  y_val/v1v,weight=w_val,enable_categorical=True)\n","            watchlist  = [(dtrain,'train_loss_fold_'+str(fold_n+1)), (dvalid, 'val_loss_fold_'+str(fold_n+1))]\n","            evals_result = {}\n","            reg = xgb.train(params=params_xgb, dtrain=dtrain, num_boost_round=n_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE,  evals_result=evals_result,maximize=False,  early_stopping_rounds=esr,verbose_eval=False)\n","            learning_train_rmspe.append(evals_result['train_loss_fold_'+str(fold_n+1)])\n","            learning_val_rmspe.append(evals_result['val_loss_fold_'+str(fold_n+1)])\n","\n","            models.append(reg)\n","            best_iterations.append(reg.best_iteration)\n","\n","            p = reg.predict(dvalid)*v1v\n","            val_score =  np.mean( ((p-y_val)/y_val)**2 )**0.5\n","\n","            # full_score += y_val.shape[0]*score**2\n","\n","            print(f'fold: {fold_n+1}, val rmspe score is {val_score}')\n","            print('corr(p/v1v, y_val/v1v)',self.nancorr(       p/v1v ,        y_val/v1v ))\n","            print('log(corr( ))',self.nancorr(np.log(p/v1v), np.log(y_val/v1v)))\n","            print('corr(p, y_val)',self.nancorr(p, y_val))\n","            print('log(corr( ))',self.nancorr(np.log(p), np.log(y_val)))\n","\n","            #test_pred = reg.predict(self.test_df[self.feat_cols_list] )*v1ts ## this method is not suitable for Timeseries cross validation because initial splits are too far from test set.\n","            #test_y_preds += test_pred/self.n_folds\n","\n","            rmspe_val_score.append(val_score)\n","\n","        mean_rmspe_val_score = np.mean(rmspe_val_score)\n","        print(f'mean rmspe val score over {self.n_folds} splits is',mean_rmspe_val_score)\n","        #print(f'mean rmspe test score: ',  np.mean( ((test_y_preds-self.test_df[self.target_name])/self.test_df[self.target_name])**2 )**0.5  ) # target\n","\n","        # Plot learning curves\n","        fig,ax = plt.subplots(2,1,figsize=(10,6))\n","        for fold_n in range(len(rmspe_val_score)):\n","            ax[0].plot(learning_train_rmspe[fold_n]['RMSPE'], label=f'Fold {fold_n+1} Train RMSPE')\n","            ax[0].plot(learning_val_rmspe[fold_n]['RMSPE'],linestyle='dashed', label=f'Fold {fold_n+1} Validation RMSPE')\n","        last_fold = len(rmspe_val_score) - 1\n","        ax[1].plot(learning_val_rmspe[last_fold]['RMSPE'],linestyle='dashed', label=f'Fold {last_fold+1} Validation RMSPE')\n","        ax[1].set_xlabel('Boosting Round')\n","        ax[0].set_ylabel('RMSPE')\n","        ax[1].set_ylabel('RMSPE')\n","        ax[0].legend()\n","        ax[1].legend()\n","        ax[0].grid(True)\n","        ax[1].grid(True)\n","        fig.suptitle(f'Learning Curves, Trial: {trial.number}')\n","        fig.show()\n","\n","        del self.df_train_reordered, X_train, X_valid, y_train, y_val,train_df,val_df,dtrain,dvalid, v1tr, v1v\n","        gc.collect()\n","        return mean_rmspe_val_score,best_iterations[-1]\n","\n","\n","    def manual_shapley_addivity_check(self,model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name):\n","\n","        y_train_true = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","                #### ONLY for Explainer\n","        shap_pred = ( shap_values.base_values + shap_values.values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","                #### ONLY for TreeExplainer\n","        #shap_pred = ( model_base_value + shap_values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","        #print('shap_values.sum(axis=1)',shap_values.sum(axis=1))\n","        # print('shap_values.base_values',shap_values.base_values[0])\n","        # print('shap_values.values',shap_values.values[0].sum())\n","        #print('len(shap_values.values.sum(axis=1))',len(shap_values.values.sum(axis=1)))\n","\n","        model_shap_rmspe = self.rmspe(model_pred, shap_pred)\n","\n","        fig, ax = plt.subplots(2,1,figsize=(30,10))\n","        ax[0].plot(np.arange(0,len(y_train_true)),y_train_true,label='true rvol.',linestyle='dashed',c='g',marker='*',alpha=0.2)\n","        ax[0].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.6)\n","        ax[0].set_title(f'True Rvol. Vs. model predicted Rvol.' )\n","        ax[0].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[0].set_ylabel('rvol.')\n","        ax[0].legend()\n","        ax[0].grid(True)\n","\n","        ax[1].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.4)\n","        ax[1].plot(np.arange(0,len(shap_pred)),shap_pred,label='summed shap values prediction',linestyle='dashed',c='r',marker='*',alpha=0.4)\n","        ax[1].set_title(f'Check additivity of shap values, RMSPE:{model_shap_rmspe} between model and shap values prediction' )\n","        ax[1].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[1].set_ylabel('rvol.')\n","        ax[1].legend()\n","        ax[1].grid(True)\n","        fig.tight_layout()\n","        fig.show()\n","\n","\n","        del all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_PDP_n_Scatter(self,feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df):\n","        ####### compute partial dependence plot of most important features\n","\n","        ###### Partial dependence plot\n","        #fig,ax = plt.subplots()\n","        #shap.plots.partial_dependence(feature_name, model.predict, xgb.DMatrix(X_train,enable_categorical=True), model_expected_value=True, feature_expected_value=True)\n","        #fig.show()\n","\n","        ##### scatter plot\n","        print(f'\\n scatter plot of {feature_name} vs. shap values')\n","        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #fig,ax = plt.subplots()\n","        shap.plots.scatter(shap_values[:,feature_name])\n","        #fig.show()\n","\n","\n","        ##### scatter plot of feature vs. True target rvol. on trianing set\n","        fig,ax = plt.subplots()\n","        yval = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        xval = X[feature_name]\n","        ax.scatter(xval,yval)\n","        ax.plot([min(xval), max(xval)], [min(yval),max(yval)], color = 'red', linewidth = 1)\n","        ax.set_xlabel(feature_name)\n","        ax.set_ylabel('True target rvol.')\n","        ax.grid\n","        ax.set_title(f'scatter plot of {feature_name} Vs. True Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        ##### scatter plot of feature vs. predicted target rvol.on trianing set\n","        fig,ax = plt.subplots()\n","        yval1 = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        ax.scatter(xval,yval1)\n","        ax.plot([min(xval), max(xval)], [min(yval1),max(yval1)], color = 'red', linewidth = 1)\n","        ax.set_xlabel(feature_name)\n","        ax.set_ylabel('Predicted target rvol.')\n","        ax.grid\n","        ax.set_title(f'scatter plot of {feature_name} Vs. Predicted Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","    def compute_shapley_beeswarm(self,X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # shap.summary_plot(shap_values, X)\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.beeswarm(shap_values)\n","        #ax.set_title(f' stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","    def compute_shapley_barplot(self,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # plt.bar(shap_values.abs().sum(axis=1))\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values,)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_individual_stock_SHAP_values(self,final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        stock_id = stock_id\n","        view_time_ids_start = view_time_ids_start\n","        view_time_ids_end = view_time_ids_end\n","        X = X_train[X_train['stock_id'].isin([stock_id])].iloc[view_time_ids_start:view_time_ids_end]\n","\n","        ###### Explainer #######\n","        explainer = shap.Explainer(final_reg,X)\n","        shap_values = explainer(np.array(X),check_additivity=False)\n","        shap_values.feature_names = final_reg.feature_names\n","\n","        ###### TreeExplainer #######\n","        # explainer = shap.TreeExplainer(final_reg,feature_perturbation='interventional')\n","        # shap_values = explainer.shap_values(np.array(X),check_additivity=False)\n","        # shap_values.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","        # print(f'Model base value: {model_base_value} before scaling by v1tr')\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.manual_shapley_addivity_check(model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name )\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values.base_values = shap_values.base_values * all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values.values = np.multiply(shap_values.values.T ,stock_v1tr_df).T\n","        ###### check correctness of shap_values\n","        # sp = shap_values.base_values + shap_values.values.sum(axis=1)\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(len(sp)),sp)\n","        # model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # plt.plot(range(len(sp)), model_pred )\n","        # plt.show()\n","\n","        self.compute_shapley_beeswarm(X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        #shap_values = final_reg.predict(dtrain, pred_contribs=True)\n","        ### Calculate SHAP values for a specific instance (e.g., the first test instance)\n","        ### shap_values = explainer.shap_values(X_test.iloc[0])\n","\n","        self.compute_shapley_barplot(shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        self.compute_shapley_heatmap(shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df)\n","\n","        ####### INDIVIDUAL feature contributions ##############################\n","        ####### compute partial dependence plot of most important features\n","        self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n","\n","        #self.compute_shapley_decision(model_base_value,shap_values.data,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        ##### force plot has some error\n","        #self.compute_shapley_force(model_base_value,shap_values.data,X,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","\n","        del final_reg,X_train\n","        gc.collect()\n","        return\n","\n","\n","    def compute_overall_SHAP_values(self,final_reg,X_train,y_train,train_pred,v1tr):\n","\n","        print(f'\\nGround-Truth Rvol. grand average on train set: {y_train.values.mean()}')\n","        print(f'\\nModel Prediction Rvol. grand average on train set: {train_pred.values.mean()}')\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        X = X_train\n","\n","        ###### Explainer #######\n","        explainer = shap.Explainer(final_reg,X)\n","        shap_values_all = explainer(np.array(X),check_additivity=False)\n","        shap_values_all.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.overall_manual_shapley_addivity_check(train_pred,v1tr,shap_values_all)\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values_all.base_values = shap_values_all.base_values * v1tr.values  # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values_all.values = np.multiply(shap_values_all.values.T ,v1tr.values).T\n","\n","        ###### Beeswarm plot\n","        #### ONLY for Explainer\n","        print(f'\\n Overall Beeswarm plot for all stock ids and time ids')\n","        shap.plots.beeswarm(shap_values_all)\n","        #ax.set_title(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","\n","        ###### Bar plot MEAN Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        ###### Bar plot MAXIMUM Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        del final_reg,X_train,y_train,train_pred,v1tr\n","        gc.collect()\n","        return\n","\n","\n","    def overall_manual_shapley_addivity_check(self,train_pred,v1tr,shap_values_all):\n","\n","        #### ONLY for Explainer\n","        shap_pred_all = ( shap_values_all.base_values + shap_values_all.values.sum(axis=1) ) * v1tr #pd.DataFrame(all_stock_v1tr_df.values.ravel() , columns=['v1tr_all'])['v1tr_all'].values\n","\n","        model_shap_rmspe_all = self.rmspe(train_pred, shap_pred_all)\n","        print(f'\\n Check Additivity of shap values in all stock and time ids, model_shap_rmspe_all: {model_shap_rmspe_all}')\n","\n","        del train_pred,v1tr,shap_values_all\n","        gc.collect()\n","        return\n","\n","\n","\n","    def compute_shapley_heatmap(self,shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #print(' NOTE: Heatmap is sorted with f(X) from smallest values to biggest value !! (picture is wrong)')\n","        # fig,ax = plt.subplots(figsize=(13.5,2))\n","        # y_asc = np.sort( all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values )\n","        # ax.plot( range(len(y_asc)), y_asc, color='g')\n","        # ax.axhline(y_asc.mean(),color='r', linestyle='dashed')\n","        # ax.set_ylabel('Correct f(x) in Asc. order')\n","        # ax.set_yticks(np.arange(0,max(y_asc),0.002))\n","        # fig.show()\n","\n","        fig,ax = plt.subplots()\n","        # order = np.argsort(all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values)\n","        shap.plots.heatmap(shap_values,instance_order=shap_values.sum(1))\n","        #ax.title(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values,all_stock_train_pred_df\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_decision(self,model_base_value,shap_values,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nDECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.decision(model_base_value, shap_values,feature_names=feature_names)\n","        #ax.title(f'\\n DECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_force(self,model_base_value,shap_values,X,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # ### ONLY for Explainer\n","        # print(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig,ax = plt.subplots()\n","        # shap.plots.force(model_base_value,shap_values=shap_values[0],features=X[0],feature_names=feature_names, show=True) #matplotlib=True,\n","        # ax.title(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig.show()\n","\n","        # example_index = 0  # You can change this index to any other example\n","        # example = X[example_index]\n","        # # Explain the prediction of the example\n","        # shap.force_plot(explainer.expected_value, shap_values[example_index], example, feature_names=data.feature_names)\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","\n","\n","    def make_predictions(self,best_params,num_rounds ):\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        #full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","        full_train_df = self.df_train_reordered\n","\n","        X_train = full_train_df[self.feat_cols_list]\n","        y_train = full_train_df[self.target_name] # target\n","        X_test = self.test_df[self.feat_cols_list]\n","        #y_test = self.test_df[self.target_name] # target\n","\n","        # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        # v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train **-2 * v1tr**2\n","        #w_test = y_test **-2 * v1ts**2\n","\n","        print('Final model')\n","        dtrain = xgb.DMatrix(X_train, label=y_train/v1tr,weight=w_train,enable_categorical=True )\n","        #dtest = xgb.DMatrix(X_test, label=y_test/v1ts,weight=w_test,enable_categorical=True )\n","        dtest = xgb.DMatrix(X_test,enable_categorical=True )\n","        watchlist  = [(dtrain,'train_loss')]\n","        evals_result = {}\n","        final_reg = xgb.train(params=best_params, dtrain=dtrain, num_boost_round=num_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE, evals_result=evals_result,maximize=False, verbose_eval=False)\n","        #test_error = evals_result['test_loss']\n","        train_pred = final_reg.predict(dtrain)*v1tr\n","        test_pred = final_reg.predict( dtest )*v1ts\n","\n","\n","\n","\n","        del full_train_df#,X_train,X_test #,feat_names\n","        gc.collect()\n","\n","        return final_reg,test_pred,train_pred,y_train,X_train,X_test,v1tr,w_train\n","\n","\n","    def compute_train_avg_target_rvol(self, unique_stock_ids, y_train):\n","        # unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        train_target_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            train_target_df.loc[t_index, s] = y_train[st_index].values\n","        train_avg_target_rvol = train_target_df.ffill().bfill().mean(axis=1)\n","        return train_avg_target_rvol\n","\n","    def compute_test_avg_target_rvol(self, unique_stock_ids, y_test):\n","        #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        test_target_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            test_target_df.loc[t_index, s] = y_test[st_index].values\n","        test_avg_target_rvol = test_target_df.ffill().bfill().mean(axis=1)\n","        return test_avg_target_rvol\n","\n","\n","    def fraction_above_average(self,signal1, avg):\n","        # Count the fraction of times when signal1 is above signal2\n","        fraction_above_avg = (signal1 > avg).mean()\n","        return fraction_above_avg\n","\n","\n","    def compute_all_stock_v1tr_df(self, unique_stock_ids, v1tr):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_v1tr_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_v1tr_df.loc[t_index, s] = v1tr[st_index].values\n","        all_stock_v1tr_df = all_stock_v1tr_df.ffill().bfill()\n","        return all_stock_v1tr_df\n","\n","    def compute_all_stock_train_pred_df(self, unique_stock_ids, train_pred):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_train_pred_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_train_pred_df.loc[t_index, s] = train_pred[st_index].values\n","        all_stock_train_pred_df = all_stock_train_pred_df.ffill().bfill()\n","        return all_stock_train_pred_df\n","\n","    def compute_all_stock_test_pred_df(self, unique_stock_ids, test_pred):\n","        # unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        all_stock_test_pred_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_test_pred_df.loc[t_index, s] = test_pred[st_index].values\n","        all_stock_test_pred_df = all_stock_test_pred_df.ffill().bfill()\n","        return all_stock_test_pred_df\n","\n","\n","    def compute_all_stock_y_train_df(self, unique_stock_ids, y_train):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_y_train_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_y_train_df.loc[t_index, s] = y_train[st_index].values\n","        all_stock_y_train_df = all_stock_y_train_df.ffill().bfill()\n","        return all_stock_y_train_df\n","\n","    def compute_all_stock_y_test_df(self, unique_stock_ids, y_test):\n","        #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        all_stock_y_test_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_y_test_df.loc[t_index, s] = y_test[st_index].values\n","        all_stock_y_test_df = all_stock_y_test_df.ffill().bfill()\n","        return all_stock_y_test_df\n","\n","\n","\n","    ######## Identify stocks belonging to clusters based on clusterings in dataset\n","    ######## find stock ids of clusters having same feature values\n","    ######## This is reverse-engineering cluster labels of already clustered stocks\n","    def calculate_cluster_fraction(self, column, n_clusters, stock_list):\n","        \"\"\" This function computes the fraction of stock ids in stock_list inside a cluster in the clustering feature.\n","        The fraction is between 0 - 1. 1 indicates all the stock ids in stock_list are in a particular cluster.\n","        \"\"\"\n","\n","        # self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        # self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","\n","        # unique_stock_ids = self.train_stock_id.unique()\n","        # time_id_order = df2.loc[:3829,'time_id'].values\n","        # train_time_id_ind = int(len(time_id_order)*0.7)\n","\n","        # train_time_ids = time_id_order[:train_time_id_ind]\n","        # train_stock_id = df2[df2['time_id'].isin(train_time_ids)]['stock_id']\n","        # train_time_id = df2[df2['time_id'].isin(train_time_ids)]['time_id']\n","\n","        unique_stock_ids = self.train_stock_id.unique()\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","\n","        train_col_df = self.df[self.df['time_id'].isin(train_time_ids)][column]\n","\n","        ## reshape the dataframe\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_column_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_column_df.loc[t_index, s] = train_col_df[st_index].values\n","        all_stock_column_df = all_stock_column_df.ffill().bfill()\n","\n","        features = all_stock_column_df.T.to_numpy()\n","\n","        ## kmeans\n","        kmeans = KMeans(n_clusters=n_clusters,n_init=10)\n","        kmeans.fit(features)\n","        cluster_labels = kmeans.labels_\n","        cluster_labels\n","\n","        clusters_dict = {}\n","        unique_labels = np.unique(cluster_labels)\n","        for label in unique_labels:\n","            indices = np.where(cluster_labels == label)[0]\n","            stocks_in_cluster = unique_stock_ids[indices]\n","            clusters_dict[label] = stocks_in_cluster.tolist()\n","\n","        for c in clusters_dict.keys():\n","            cnt=0\n","            for s in stock_list:\n","                if s in clusters_dict[c]:\n","                    cnt+=1\n","            print(f'cluster: {c}, # stock ids in cluster: {cnt}, clustering fraction: {cnt/len(clusters_dict[c])}')\n","\n","        return\n","\n","\n","\n","    def check_stock_list_in_all_clustering_features(self, stock_list):\n","\n","        clustering_features_list = [    \"log_target_vol_corr_32_clusters_stnd\",\n","                                        \"log_target_vol_sum_stats_16_clusters_stnd\",\n","                                        \"sum_stats_4_clusters_labels\",\n","                                        \"sum_stats_10_clusters_labels\",\n","                                        \"sum_stats_16_clusters_labels\",\n","                                        \"sum_stats_30_clusters_labels\",\n","                                        \"pear_corr_32_clusters_labels\",\n","                                        \"pear_corr_4_clusters_labels\",\n","                                        \"pear_corr_49_clusters_labels\",\n","                                        \"pear_corr_90_clusters_labels\",]\n","\n","        print('stock_list: ' , stock_list)\n","        for feature in clustering_features_list:\n","            n_clusters = int(re.findall(r'\\d+', feature)[0])\n","            print('Feature: ', feature)\n","            print('Cluster Fractions: ')\n","            print(self.calculate_cluster_fraction( feature, n_clusters, stock_list))\n","\n","        return\n","\n","\n","    def compute_acf_pacf(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        plt.close('all')\n","        for s in unique_stock_ids[0:1]:#[0:40]:\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            plot_acf(stock_residual, lags=200,ax=ax[0])\n","            plot_pacf(stock_residual, lags=200,ax=ax[1])\n","            ax[0].set_title(f'Autocorrelation of stock {s} Residuals on train set')\n","            ax[1].set_title(f'Partial Autocorrelation of stock {s} Residuals on train set')\n","            ax[0].set_xticks(range(0,200,5))\n","            ax[1].set_xticks(range(0,200,5))\n","            ax[0].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_xlabel('lags')\n","            ax[0].set_ylabel('ACF')\n","            ax[1].set_ylabel('PACF')\n","            ax[0].grid(True)\n","            ax[1].grid(True)\n","            fig.show()\n","        return\n","\n","\n","\n","    def compute_IFFT(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","\n","        ##### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        ##### IFFT plot of reconstructed time series ######\n","        plt.close('all')\n","        for s in unique_stock_ids[100:]:#[40:112]:\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            x = stock_residual.values\n","            limit = 0.00001\n","\n","            n=len(x)\n","            fhat = np.fft.fft(x,n)\n","            PSD = fhat*np.conj(fhat) / n\n","            freq = (1/n)*np.arange(n)\n","            start=1 #ignore dc component\n","            L = np.arange(start,np.floor(n/2),dtype='int')\n","            # fig,ax = plt.subplots(figsize=(30,6))\n","            # #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            # ax.axhline(limit,  color='k', linestyle='-')\n","            # ax.plot(freq[L],PSD[L])\n","            # ax.set_xlabel('freq')\n","            # ax.set_ylabel('mag')\n","            # ax.set_title(f'mag plot of stock: {s} residual')\n","            # fig.show()\n","\n","            indices = PSD > limit\n","            num_freqs = len(np.where(indices>0)[0])\n","            print('# of frequencies in residual = ',num_freqs)\n","\n","            fhat = fhat*indices\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            ffilt = np.fft.ifft(fhat)\n","            ax[0].plot(np.arange(0,len(x)),ffilt.real,label='top '+str(num_freqs)+' frequencies in residual (train set)',c='g',alpha=1)\n","            ax[0].plot(np.arange(0,len(x)),x,label='original residual',c='r',alpha=0.2)\n","            ax[0].legend()\n","            ax[0].grid()\n","            ax[0].set_xlabel('time id')\n","            ax[0].set_ylabel('residual')\n","            ax[0].set_title(f'IFFT of stock: {s} residual')\n","\n","\n","            x1 = all_stock_y_train_df[s].values\n","            limit1 = 0.00001\n","            n1=len(x1)\n","            fhat1 = np.fft.fft(x1,n1)\n","            PSD1 = fhat1*np.conj(fhat1) / n1\n","            freq1 = (1/n1)*np.arange(n1)\n","            start1=1 #ignore dc component\n","            L1 = np.arange(start1,np.floor(n1/2),dtype='int')\n","            fig1,ax1 = plt.subplots(figsize=(30,6))\n","            #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            ax1.axhline(limit1,  color='k', linestyle='-')\n","            ax1.plot(freq1[L],PSD1[L])\n","            ax1.set_xlabel('freq')\n","            ax1.set_ylabel('mag')\n","            ax1.set_title(f'mag plot of stock: {s} rvol.')\n","            fig1.show()\n","\n","            indices1 = PSD1 > limit1\n","            num_freqs1 = len(np.where(indices1>0)[0])\n","            print('# of frequencies in rvol. = ',num_freqs1)\n","            fhat1 = fhat1*indices1\n","            ffilt1 = np.fft.ifft(fhat1)\n","            ax[1].plot(np.arange(0,len(x1)),ffilt1.real,label='top '+str(num_freqs1)+' frequencies in true rvol. (train set)',c='g',alpha=1)\n","            ax[1].plot(np.arange(0,len(x1)),x1,label='original true rvol.',c='r',alpha=0.2)\n","            ax[1].legend()\n","            ax[1].grid()\n","            ax[1].set_xlabel('time id')\n","            ax[1].set_ylabel('rvol.')\n","            ax[1].set_title(f'IFFT of stock: {s} rvol.')\n","            fig.show()\n","        return\n","\n","\n","\n","    def overall_stock_id_analysis(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals):\n","\n","        ###### Bar plot of RMSPE for all stocks in the training set\n","        fig, ax = plt.subplots(figsize=(40,10))\n","        rmspe_per_stock_train = []\n","        for s in unique_stock_ids:\n","            rmspe_per_stock_train.append( np.mean( ((all_stock_train_pred_df[s]-all_stock_y_train_df[s])/all_stock_y_train_df[s])**2 )**0.5  )\n","        all_stock_rmspe = pd.Series(rmspe_per_stock_train,index=unique_stock_ids)\n","        smallest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index.values[:10]\n","        largest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n","        ax.text(0,0.52,f'largest RMSPE stocks: {largest_10_rmspe_stocks}')\n","        ax.text(0,0.62,f'smallest RMSPE stocks: {smallest_10_rmspe_stocks}')\n","        ax.bar(unique_stock_ids, rmspe_per_stock_train)\n","        ax.set_xticks(unique_stock_ids)\n","        ax.set_yticks(np.arange(0, 1.1, 0.04))\n","        ax.grid()\n","        ax.set_title('RMSPE of Real. Vol. per stock on train set')\n","        ax.set_xlabel('Stock ID')\n","        ax.set_ylabel('RMSPE')\n","        fig.show()\n","        plt.close()\n","        ## check if the largest and smallest fall into a cluster of a clustering feature\n","        print('\\n 10_largest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = largest_10_rmspe_stocks)\n","        print('\\n 10_smallest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = smallest_10_rmspe_stocks)\n","\n","        ####### scatter plot of True Real. Vol. vs. Pred Real. Vol.\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(y_train, train_pred, c='b', )\n","        ax.plot(y_train, y_train, c='r',linestyle='solid' )\n","        ax.set_title('Scatter Plot of True vs Predicted Values on train set')\n","        ax.set_xlabel('True train rvol. Values')\n","        ax.set_ylabel('Predicted train rvol. Values')\n","        fig.show()\n","        plt.close()\n","\n","        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(y_train, train_residuals, c='c', )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        ax.axhline(y=np.mean(y_train), color='r', linestyle='-')\n","        ax.set_title(' True R.V. Vs. Residuals Values Plot on train set')\n","        ax.set_xlabel('True train Values')\n","        ax.set_ylabel('Train residuals')\n","        fig.show()\n","        plt.close()\n","\n","\n","        ####### scatter plot of Fitted rvol. Values Vs. train residuals Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(train_pred, train_residuals, c='m', )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        ax.axhline(y=np.mean(y_train), color='r', linestyle='-')\n","        ax.set_title(' fitted R.V. Vs. Residuals Values Plot on train set')\n","        ax.set_xlabel('fitted train Values')\n","        ax.set_ylabel('Train residuals')\n","        fig.show()\n","        plt.close()\n","\n","        ## Normal Q-Q Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        sm.qqplot(train_residuals, line='q', ax=ax)\n","        ax.set_title('QQ Plot of Residuals on train set')\n","        ax.set_xlabel('Theoretical Quantiles')\n","        ax.set_ylabel('Sample Quantiles')\n","        fig.show()\n","        plt.close()\n","\n","        ## y_train and train_pred Distributions Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( y_train,bins=1000, color='green', alpha=0.9, histtype='bar', rwidth=0.8)\n","        plt.hist( train_pred,bins=1000, color='red', alpha=0.3, ec='r')\n","        ax.set_title(f'Distribution of y_train (skew: {stats.skew(y_train)} , kurt:{stats.skew(y_train)}) and train_pred (skew: {stats.skew(train_pred)} , kurt:{stats.skew(train_pred)}) on train set')\n","        ax.set_xlabel(' y_train and train_pred')\n","        ax.set_ylabel('frequency')\n","        fig.legend(loc=\"upper left\")\n","        fig.show()\n","        plt.close()\n","\n","        ## Residuals Distribution Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( train_residuals,bins=1000)\n","        ax.set_title('Distribution of Residuals on train set')\n","        ax.set_xlabel('train Residuals')\n","        ax.set_ylabel('frequency')\n","        fig.show()\n","        plt.close()\n","\n","        del unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals\n","        gc.collect()\n","        return\n","\n","\n","    def individual_stock_id_analysis(self,picked_stock_id,unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,residuals,train_avg_target_rvol,set_name):\n","\n","        ## 1. scatter plot\n","        plt.figure(figsize=(10,10))\n","        plt.scatter(all_stock_y_train_df[picked_stock_id],all_stock_train_pred_df[picked_stock_id], c='blue',label=picked_stock_id, alpha=0.4)\n","        plt.plot(all_stock_y_train_df[picked_stock_id],all_stock_y_train_df[picked_stock_id],linestyle='solid', c='red',label=picked_stock_id, alpha=1 )\n","        plt.grid()\n","        plt.xlabel('y_train')\n","        plt.ylabel('train_pred')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s scatter plot of y_train vs. train_pred on train set\")\n","        #plt.show()\n","        plt.close()\n","        ## 2. Line plot of true vs average real. vol.\n","        fraction_above_avg = self.fraction_above_average(all_stock_y_train_df[picked_stock_id], train_avg_target_rvol)\n","        plt.figure(figsize=(30,5))\n","        plt.text(1,0.0275,f'fraction of times above mean = {fraction_above_avg}')\n","        plt.plot(range(len(all_stock_y_train_df[picked_stock_id])),all_stock_y_train_df[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.4 )\n","        plt.plot(range(len(train_avg_target_rvol)),train_avg_target_rvol,linestyle='solid', c='blue',label='train_avg_target_rvol', alpha=0.4 )\n","        plt.grid()\n","        plt.xlabel('time id')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s line plot of True y_train vs. train_avg_target_rvol on train set\")\n","        #plt.show()\n","        plt.close()\n","        ## 3. Line plot of pred vs true real. vol.\n","        plt.figure(figsize=(30,5))\n","        plt.plot(range(len(all_stock_y_train_df[picked_stock_id])),all_stock_y_train_df[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.7 )\n","        plt.plot(range(len(all_stock_train_pred_df[picked_stock_id])),all_stock_train_pred_df[picked_stock_id],linestyle='solid', c='red',label='Pred stock id: '+str(picked_stock_id), alpha=0.4 )\n","        plt.grid()\n","        plt.xlabel('time id')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s line plot of True y_train vs train_pred on train set\")\n","        #plt.show()\n","        plt.close()\n","\n","        ###### Autocorrelation Plot\n","        fig, ax = plt.subplots(figsize=(10,3))\n","        plot_acf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        ax.set_xlabel('Lag')\n","        ax.set_ylabel('Autocorrelation')\n","        ax.set_yticks(np.arange(-1, 1, 0.1))\n","        ax.grid()\n","        ax.set_title(f'Autocorrelation of {set_name} Residuals')\n","        fig.show()\n","\n","        ###### Partial Autocorrelation Plot\n","        fig, ax = plt.subplots(figsize=(10,3))\n","        plot_pacf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        ax.set_xlabel('Lag')\n","        ax.set_ylabel('Partial Autocorrelation')\n","        ax.set_yticks(np.arange(-1, 1, 0.1))\n","        ax.grid()\n","        plt.title(f'Partial Autocorrelation of {set_name} Residuals')\n","        plt.show()\n","\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        self.compute_acf_pacf(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n","\n","\n","        # #### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        # #### IFFT plot of reconstructed time series ######\n","        # self.compute_IFFT(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n","\n","        del picked_stock_id,unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,residuals\n","        gc.collect()\n","        return\n","\n","\n","    def overall_time_id_analysis(self, all_stock_train_pred_df,all_stock_y_train_df,train_avg_target_rvol):\n","\n","        ###### Bar plot of RMSPE for all time ids in the training set\n","        fig, ax = plt.subplots(2,1,figsize=(40,10))\n","        rmspe_per_time_id_train = []\n","        unique_time_ids = all_stock_train_pred_df.index\n","        for t in unique_time_ids:\n","            rmspe_per_time_id_train.append( np.mean( ((all_stock_train_pred_df.loc[t]-all_stock_y_train_df.loc[t])/all_stock_y_train_df.loc[t])**2 )**0.5  )\n","\n","        all_time_id_rmspe = pd.Series(rmspe_per_time_id_train,index=unique_time_ids)\n","        smallest_10_rmspe_time_ids = all_time_id_rmspe.sort_values(ascending=True).index.values[:10]\n","        largest_10_rmspe_time_ids = all_time_id_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n","        ax[0].text(0,0.2,f'10 smallest RMSPE time ids: {smallest_10_rmspe_time_ids}')\n","        ax[0].bar(smallest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[smallest_10_rmspe_time_ids])\n","        ax[0].set_yticks(np.arange(0, 0.4, 0.04))\n","        ax[0].set_ylabel('RMSPE')\n","        ax[0].set_title('10 smallest RMSPE time ids on train set')\n","        ax[0].grid()\n","        fig.show()\n","        ax[1].text(0,1.9,f'10 largest RMSPE time ids: {largest_10_rmspe_time_ids}')\n","        ax[1].bar(largest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[largest_10_rmspe_time_ids])\n","        ax[1].set_yticks(np.arange(0, 2.0, 0.08))\n","        ax[1].set_ylabel('RMSPE')\n","        ax[1].set_title('10 largest RMSPE time ids on train set')\n","        ax[1].grid()\n","        fig.show()\n","\n","\n","        ###### visualize the time ids with largest and smallest RMSPE on the average rvol. plot on training set\n","        plt.figure(figsize=(30,5))\n","        x_time_id_idx = range(len(train_avg_target_rvol))\n","        plt.plot(x_time_id_idx,train_avg_target_rvol,linestyle='solid', c='blue',label='train_avg_target_rvol', alpha=0.4 )\n","        large_idx = np.where(np.isin(unique_time_ids,largest_10_rmspe_time_ids))[0]\n","        red_colors = ['black','darkred','crimson','lightcoral','indianred','orchid','hotpink','palevioletred','violet','plum']\n","        for i,s in enumerate(large_idx):\n","            plt.axvline(x=s, ymin=0, ymax=1,color=red_colors[i],linestyle='-',label=str(i))\n","        small_idx = np.where(np.isin(unique_time_ids,smallest_10_rmspe_time_ids))[0]\n","        green_colors = ['gold','yellow','blue','darkgreen','lime','seagreen','mediumseagreen','springgreen','aquamarine','turquoise','lightgreen']\n","        for j,l in enumerate(small_idx):\n","            plt.axvline(x=l, ymin=0, ymax=1,color=green_colors[j],linestyle='-',label=str(j))\n","        plt.grid()\n","        plt.yticks(np.arange(0, 0.04, 0.01))\n","        plt.xlabel('sequential time id index')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.show()\n","\n","\n","        del all_stock_train_pred_df,all_stock_y_train_df,train_avg_target_rvol\n","        gc.collect()\n","        return\n","\n","\n","    def compute_model_bias_variance(self,y_test,y_train,X_train,best_mlxtend_xgb_params):\n","\n","        ## model bias and variance measurement\n","        # estimate bias and variance\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","\n","        full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","\n","        X_train = full_train_df[self.feat_cols_list]\n","        y_train = full_train_df[self.target_name] #target\n","        X_test = self.test_df[self.feat_cols_list]\n","        #y_test = self.test_df[self.target_name] #target\n","\n","        # Assuming best_mlxtend_xgb_params contains the hyperparameters\n","        max_depth, eta, subsample, colsample_bytree, gamma, reg_alpha, reg_lambda, min_child_weight, num_rounds = best_mlxtend_xgb_params\n","\n","        # Create XGBRegressor model\n","        xgb_model = XGBRegressor(\n","            max_depth=max_depth,\n","            learning_rate=eta,\n","            subsample=subsample,\n","            colsample_bytree=colsample_bytree,\n","            gamma=gamma,\n","            reg_alpha=reg_alpha,\n","            reg_lambda=reg_lambda,\n","            min_child_weight=min_child_weight,\n","            n_estimators=num_rounds,\n","            objective='reg:squarederror',\n","            tree_method = \"hist\",\n","            device = \"cuda\"\n","        )\n","\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train **-2 * v1tr**2\n","        #w_test = y_test **-2 * v1ts**2\n","\n","        # Train XGBRegressor model\n","        xgb_model.fit(X_train.values, y_train.values/v1tr.values, sample_weight=w_train)\n","\n","        # Now you can use bias_variance_decomp\n","        mse, bias, var = bias_variance_decomp(xgb_model, X_train.values, y_train.values/v1tr.values, X_test.values, y_test.values/v1ts.values, loss='mse', num_rounds=30, random_seed=1)\n","        print('\\nMSE: %.3f' % mse)\n","        print('Bias: %.3f' % bias)\n","        print('Variance: %.3f' % var)\n","\n","        return\n","\n","\n","    def evaluate_predictions(self,final_reg,test_pred, y_test,train_pred,y_train,X_train,v1tr,w_train,best_mlxtend_xgb_params):\n","\n","        y_true = y_test\n","        y_pred = test_pred\n","        test_residuals = y_true - y_pred\n","        train_residuals = y_train - train_pred\n","        unique_stock_ids = self.train_stock_id.unique()\n","\n","        all_stock_train_pred_df = self.compute_all_stock_train_pred_df(unique_stock_ids, train_pred)\n","        all_stock_v1tr_df = self.compute_all_stock_v1tr_df(unique_stock_ids, v1tr)\n","        all_stock_test_pred_df = self.compute_all_stock_test_pred_df( unique_stock_ids, test_pred)\n","        all_stock_y_train_df = self.compute_all_stock_y_train_df(unique_stock_ids, y_train)\n","        all_stock_y_test_df = self.compute_all_stock_y_test_df( unique_stock_ids, y_test)\n","\n","        train_avg_target_rvol = self.compute_train_avg_target_rvol(unique_stock_ids, y_train)\n","\n","        print('\\n####################################### PREDICTION #################################################')\n","\n","        v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        print('corr(y_pred/v1ts, y_true/v1ts)',self.nancorr(       y_pred/v1ts ,        y_true/v1ts ))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred/v1ts), np.log(y_true/v1ts)))\n","        print('corr(y_pred, y_true)',self.nancorr(y_pred, y_true))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred), np.log(y_true)))\n","        print(f'RMSPE train score: ',  np.mean( ((train_pred-y_train)/y_train)**2 )**0.5  )\n","        print(f'RMSPE test score: ',  np.mean( ((y_pred-y_true)/y_true)**2 )**0.5  )\n","\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET #########################################################\n","        print('\\n####################################### TRAINING SET predictions #################################################')\n","        ###################################################################################################################\n","\n","        ################################################################################################\n","        ############################## OVERALL STOCK ANALYSIS START ######################################\n","\n","        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n","        #self.overall_stock_id_analysis(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals)\n","\n","        print('\\n############################## OVERALL STOCK ANALYSIS END #################################')\n","        ############################## OVERALL STOCK ANALYSIS END ######################################\n","        ################################################################################################\n","\n","\n","        ################################################################################################\n","        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n","\n","\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n","        ##### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n","        picked_stock_id = 0 #\n","        set_name = 'train'\n","        self.individual_stock_id_analysis(picked_stock_id,unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,train_residuals,train_avg_target_rvol,set_name)\n","\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n","        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS START ##################################\n","        ################################################################################################\n","        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n","\n","\n","        #self.overall_time_id_analysis(all_stock_train_pred_df,all_stock_y_train_df,train_avg_target_rvol)\n","\n","        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY START #########################################\n","        ###################################################################################################################\n","        print('\\n###################################### Feature importance & SHAPLEY START #########################################')\n","\n","        #self.compute_overall_SHAP_values(final_reg,X_train,y_train,train_pred,v1tr)\n","\n","        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n","        stock_id = 0\n","        view_time_ids_start = 0\n","        view_time_ids_end = 500\n","        #self.compute_individual_stock_SHAP_values(final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        print('\\n###################################### Feature importance & SHAPLEY END #########################################')\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY END #########################################\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE START ################################################\n","        ###################################################################################################################\n","\n","         #### Plot top 30 feature importances\n","        fig, ax = plt.subplots(figsize=(10, 10))\n","        xgb.plot_importance(final_reg, importance_type='gain', max_num_features=30, height=0.8, show_values=False)\n","        self.compute_model_bias_variance(y_test,y_train,X_train,best_mlxtend_xgb_params)\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE END #################################################\n","        ##################################################################################################################\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS START ########################################\n","        print('\\n####################################### TESTING SET predictions #################################################')\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","\n","\n","\n","        print('##################################################################################################')\n","\n","\n","        del X_train,y_train, all_stock_train_pred_df, all_stock_v1tr_df ,  all_stock_test_pred_df, all_stock_y_train_df,  all_stock_y_test_df\n","        del y_true, y_pred, test_residuals, train_residuals, unique_stock_ids\n","        gc.collect()\n","        return\n","\n","\n","\n","    def visualize_tree(self,):\n","        # feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n","        # feature_importances.to_csv('feature_importances.csv')\n","        # plt.figure(figsize=(16, 12))\n","        # sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(20), x='average', y='feature')\n","        # plt.title('20 TOP feature importance over {} folds average'.format(folds.n_splits));\n","\n","        # importances = pd.DataFrame({'Feature': model.feature_name(),\n","        #                             'Importance': sum( [model.feature_importance(importance_type='gain') for model in models] )})\n","        # importances2 = importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n","        # importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n","\n","        #TODO: #plot decision tree for interpretability\n","\n","        return\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"8e78ed9d","metadata":{"id":"8e78ed9d"},"outputs":[],"source":["num_trees = 3000\n","\n","def objective(trial):\n","\n","\n","    t_v_t = train_validate_n_test(df_train_reordered, df_test)\n","\n","    ######  SET Hyperparameter's range for tuning ######\n","    early_stopping_rounds = 25\n","    num_round= num_trees\n","    seed1=11\n","    missing_value = -np.inf   # Replace with a suitable value\n","\n","    # Hyperparameters and algorithm parameters are described here\n","    params = {'disable_default_eval_metric': 1,\n","              \"max_depth\": trial.suggest_int('max_depth', 2, 20),\n","            \"eta\": trial.suggest_float(name='eta', low=0.0001, high=1,log=True),\n","            \"subsample\" : round(trial.suggest_float(name='subsample', low=0.3, high=1.0,step=0.1),1),\n","            \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.05, high=0.8,step=0.05),1),\n","            'gamma': trial.suggest_int('gamma', 2, 10),\n","            'reg_alpha': trial.suggest_int('reg_alpha', 3, 10),\n","            'reg_lambda': trial.suggest_int('reg_lambda', 3, 10),\n","            'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n","            \"tree_method\": 'hist',\n","            \"device\": \"cuda\",\n","            \"seed\":seed1,\n","            #'missing': missing_value\n","            }\n","\n","\n","    ######  SET Hyperparameter's range for tuning ######\n","\n","    val_avg_error,best_iteration = t_v_t.xgb_train_validate(params,num_round,early_stopping_rounds,trial)\n","    print(f\"val_avg_error: {val_avg_error}, best_iteration: {best_iteration}\")\n","    trial.set_user_attr(\"best_iteration\", best_iteration)\n","\n","    del t_v_t\n","    gc.collect()\n","    return val_avg_error\n","\n"]},{"cell_type":"code","execution_count":null,"id":"fda90815","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1HytZv_DSccOJnm2Gl-ZsWUc0v-GOVweR"},"executionInfo":{"elapsed":12317300,"status":"ok","timestamp":1726905259021,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"fda90815","outputId":"d7f03447-cae1-489c-d1c2-438c67bd3b4d"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["\n","\n","#if __name__ == \"__main__\":\n","\n","#optuna.logging.set_verbosity(optuna.logging.WARNING)\n","# study_name= 'Correct_residual_autocorrrelation_HAR_n_target_lag_feat_n_target_pred'\n","\n","study = optuna.create_study(study_name ='Correct_residual_autocorrrelation_HAR_feat' ,direction=\"minimize\")\n","study.optimize(objective, timeout=12000, n_trials=75) # 50\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"Best number of iteration/boosting rounds: \",study.trials[trial.number].user_attrs['best_iteration'])\n","\n","print(\"Trial no.: \",trial.number)\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))\n","\n","#print(\"Best hyperparameters:\", study.best_params)\n","\n","fig = optuna.visualization.plot_parallel_coordinate(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_optimization_history(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_slice(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_param_importances(study)\n","fig.show()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6d620dba","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93017,"status":"ok","timestamp":1726905351998,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"6d620dba","outputId":"15bfd44b-0074-43b3-dfcd-1e71c4f080ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["final best iteration:  2394\n","Final model\n"]}],"source":["\n","seed1 = 11\n","missing_value = -np.inf  # Replace with a suitable value\n","\n","\n","# ############ Best parameters Manual Start ############\n","# num_rounds = 638 #838 #study.trials[trial.number].user_attrs['best_iteration']\n","# max_depth = 10\n","# eta =  0.02635275365033109\n","# subsample =  1\n","# colsample_bytree =  0.25\n","# gamma =  2\n","# reg_alpha =  4\n","# reg_lambda =  3\n","# min_child_weight =  4\n","# ############ Best parameters Manual End ############\n","\n","\n","\n","############ Best parameters Automatic Start ############\n","best_trial = study.best_trial\n","num_rounds = study.best_trial.user_attrs['best_iteration']\n","print('final best iteration: ',num_rounds )\n","seed1 = 11\n","missing_value = -np.inf  # Replace with a suitable value\n","max_depth = best_trial.params['max_depth']\n","eta =  best_trial.params['eta']\n","subsample =  best_trial.params['subsample']\n","colsample_bytree =  best_trial.params['colsample_bytree']\n","gamma =  best_trial.params['gamma']\n","reg_alpha =  best_trial.params['reg_alpha']\n","reg_lambda = best_trial.params['reg_lambda']\n","min_child_weight = best_trial.params['min_child_weight']\n","############ Best parameters Automatic End ############\n","\n","\n","\n","best_mlxtend_xgb_params = [max_depth,eta,subsample,colsample_bytree,gamma,reg_alpha,reg_lambda,min_child_weight,num_rounds]\n","\n","best_params = { 'disable_default_eval_metric': 1,\n","              \"max_depth\": max_depth,\n","            \"eta\": eta,\n","            \"subsample\" : subsample,\n","            \"colsample_bytree\":  colsample_bytree,\n","            'gamma':gamma,\n","            'reg_alpha': reg_alpha,\n","            'reg_lambda': reg_lambda,\n","            'min_child_weight': min_child_weight,\n","            \"tree_method\": 'hist',\n","            \"device\": \"cuda\",\n","            \"seed\":seed1,\n","            #'missing': missing_value\n","               }\n","\n","t_v_t = train_validate_n_test(df_train_reordered, df_test)\n","final_reg,test_pred,train_pred,y_train,X_train,X_test,v1tr,w_train = t_v_t.make_predictions(best_params,num_rounds)\n","\n","\n","\n","# Merge the DataFrames on 'time_id' and 'stock_id' columns\n","# y_test_df = pd.merge(X_test[['time_id', 'stock_id']], df_train_reordered[['time_id', 'stock_id', 'target']], on=['time_id', 'stock_id'], how='left')\n","# y_test = y_test_df['target'].values\n","#t_v_t.evaluate_predictions(final_reg,test_pred, y_test,train_pred,y_train, X_train,v1tr,w_train,best_mlxtend_xgb_params)\n","\n","\n","\n","# del final_reg,test_pred, y_test,train_pred,y_train,X_train,X_test,v1tr,w_train\n","# gc.collect()\n","\n","# ## save the best model with timestamp for future use.\n","# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","# filename = f'xgb_gpu_{timestamp}.pkl'\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Xgboost_gpu_models/xgb_gpu_model_registry')\n","# with open(filename, 'wb') as file:\n","#     pickle.dump(final_reg, file)\n","# print(f'Model saved to: {filename}')\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/liquidity_features')\n","\n","\n","# del study, trial,t_v_t,final_reg,test_pred, y_test,train_pred,y_train\n","# gc.collect()\n"]},{"cell_type":"markdown","id":"b572ae8d","metadata":{"id":"b572ae8d"},"source":["# **Final submission**\n"]},{"cell_type":"code","execution_count":null,"id":"f670caa9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":48,"status":"error","timestamp":1726905351999,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"f670caa9","outputId":"3d30095b-54b6-4b98-a58b-be53667e2cea"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-129-afcfc33fb90f>:2: SettingWithCopyWarning:\n","\n","\n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-129-afcfc33fb90f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_submission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stock_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/working'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working'"]}],"source":["y_pred_submission = X_test[['time_id', 'stock_id']]\n","y_pred_submission['target'] = test_pred\n","\n","test_csv = pd.read_csv('test.csv')\n","\n","submission = pd.merge(test_csv, y_pred_submission, on=['time_id', 'stock_id'], how='left')\n","submission = submission[['row_id', 'target']]\n","os.chdir('/kaggle/working')\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":2344753,"sourceId":27233,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":15331.990222,"end_time":"2024-09-16T06:04:13.265071","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-16T01:48:41.274849","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
