{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22834,"status":"ok","timestamp":1725345774375,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"CeK4mAk3Ac84","outputId":"07f8e137-9e68-415f-82ef-ea14bd60b5d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Tue Sep  3 06:42:53 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n","| N/A   51C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","Your runtime has 56.9 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/optiver_real_vol')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13063,"status":"ok","timestamp":1725345787436,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"ZUFbQBuN_mgL","outputId":"237bf59f-d311-4da6-cd66-fb5cfeb08f80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting plotly_express\n","  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (2.1.4)\n","Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (5.15.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (0.14.2)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.13.1)\n","Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (0.5.6)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from plotly_express) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.0->plotly_express) (2024.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5->plotly_express) (1.16.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly_express) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.1.0->plotly_express) (24.1)\n","Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n","Installing collected packages: plotly_express\n","Successfully installed plotly_express-0.4.1\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n","Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n","Collecting optuna\n","  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n","Collecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.13.2-py3-none-any.whl.metadata (7.4 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n","Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alembic-1.13.2-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.0/233.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n","Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.5 alembic-1.13.2 colorlog-6.8.2 optuna-4.0.0\n","Collecting shap\n","  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.3.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.1.4)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n","Collecting slicer==0.0.8 (from shap)\n","  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n","Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.46.0 slicer-0.0.8\n"]}],"source":["!pip install plotly_express\n","!pip install numba\n","!pip install optuna\n","!pip install shap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7f7MewqS03xl"},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"msCIPxAI36L9"},"outputs":[],"source":["# import os\n","# import glob\n","# import pandas as pd\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/')\n","\n","# train_trade_paths = sorted(glob.glob('trade_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","# train_book_paths = sorted(glob.glob('book_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","\n","# unique_stock_ids = []\n","# for path in train_book_paths:\n","#     unique_stock_ids.append(int(path.split('=')[1]))\n","\n","# train = pd.read_csv('train.csv')\n","# train_target = train\n","\n","# all_uniq_time_ids = pd.DataFrame({'time_id':train['time_id'].unique()})\n"]},{"cell_type":"markdown","metadata":{"id":"HMTiN9W8KSF7"},"source":["## File Name: features_eda_within_stocks_linux.ipynb\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CgdNKAL2ddFJ"},"outputs":[],"source":["\n","\n","# import pandas as pd\n","# import numpy as np\n","# import glob\n","# import os\n","# import matplotlib.pyplot as plt\n","# from numba import jit\n","# import plotly_express as px\n","# from itertools import combinations,permutations,product,combinations_with_replacement\n","# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","# #import findspark\n","# from statsmodels.tsa.stattools import acf,pacf\n","# from scipy.signal import find_peaks\n","# import statsmodels.api as sm\n","# import pickle\n","\n","# import numpy as np\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# import plotly.express as px\n","# from itertools import combinations\n","# from numba import njit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTSbYNpeiLi9"},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","import statsmodels.api as sm\n","import plotly.subplots as sub_plots\n","import plotly.graph_objects as go\n","import statsmodels.api as sm\n","import scipy.stats as stats\n","\n","from sklearn.cluster import KMeans\n","import re\n","\n","import warnings\n","#warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import confusion_matrix\n","#from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","\n","import glob\n","import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","from numba import jit, njit\n","import numba as nb\n","import plotly_express as px\n","from itertools import combinations, permutations, product, combinations_with_replacement\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from scipy.signal import find_peaks\n","import pickle\n","from joblib import Parallel, delayed\n","import seaborn as sns\n","from sklearn import model_selection\n","from sklearn.metrics import r2_score\n","import gc\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","import scipy as sp\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from sklearn.cluster import SpectralClustering, MiniBatchKMeans, MeanShift, AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from scipy.spatial.distance import squareform\n","from scipy.stats import skew, kurtosis\n","import shap\n","from datetime import datetime\n","import ipywidgets as widgets\n","from matplotlib.patches import Rectangle\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from xgboost import plot_tree, plot_importance\n","from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.genmod.generalized_linear_model import GLM\n","import warnings\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","import re\n","\n","from matplotlib.pyplot import cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WzijOAAOodO4"},"outputs":[],"source":["class create_training_n_inference_general_features( object  ):\n","    def __init__(self,ml_stage='training' ,):\n","        self.ml_stage = ml_stage\n","        if self.ml_stage == 'training':\n","            os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","            self.trade_paths = sorted(glob.glob('trade_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.book_paths = sorted(glob.glob('book_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.unique_stock_ids = []\n","            for path in self.book_paths:\n","                self.unique_stock_ids.append(int(path.split('=')[1]))\n","            self.train = pd.read_csv('train.csv')\n","            self.train_target = pd.read_csv('train.csv')\n","            self.all_uniq_time_ids = pd.DataFrame({'time_id':self.train['time_id'].unique()})\n","        elif self.ml_stage == 'inference':\n","            os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data')\n","            self.trade_paths = sorted(glob.glob('trade_test_sub.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.book_paths = sorted(glob.glob('book_test_sub.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","            self.test = pd.read_csv('test_sub.csv')\n","            self.all_uniq_time_ids = pd.DataFrame({'time_id':self.test['time_id'].unique()})\n","            self.unique_stock_ids = []\n","            for path in self.book_paths:\n","                self.unique_stock_ids.append(int(path.split('=')[1]))\n","            self.train = pd.read_csv('train.csv')\n","            self.train_target = pd.read_csv('train.csv')\n","        else:\n","            print('invalid ml_stage param')\n","        return\n","\n","\n","\n","    def create_bk_level1_2_size_imbalance_feat(self,):\n","\n","        \"\"\"\n","        New features start\n","        \"\"\"\n","\n","        subset_paths = self.book_paths\n","        level = 1  # set level 1 or 2 for book_train data\n","        corr_method = 'spearman'  # set 'pearson' or 'spearman'\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","        bk_level1_2_size_imbalance_feat = {\n","            'bid_lvl2_min_lvl1_size_feat': pd.DataFrame(),\n","            'ask_lvl2_min_lvl1_size_feat': pd.DataFrame(),\n","            'lvl2_minus_lvl1_bid_n_ask_size_feat': pd.DataFrame()\n","        }\n","\n","        @njit\n","        def calculate_features_numba(bid_size1, bid_size2, ask_size1, ask_size2):\n","            bid_lvl2_min_lvl1_size_feat = np.minimum(bid_size2 - bid_size1, 0)\n","            ask_lvl2_min_lvl1_size_feat = np.minimum(ask_size2 - ask_size1, 0)\n","            lvl2_minus_lvl1_bid_n_ask_size_feat = np.minimum((bid_size2 + ask_size2) - (bid_size1 + ask_size1), 0)\n","            return bid_lvl2_min_lvl1_size_feat, ask_lvl2_min_lvl1_size_feat, lvl2_minus_lvl1_bid_n_ask_size_feat\n","\n","        def calculate_features(book_train_st):\n","            # Extract the necessary numpy arrays\n","            bid_size1 = book_train_st[\"bid_size1\"].values\n","            bid_size2 = book_train_st[\"bid_size2\"].values\n","            ask_size1 = book_train_st[\"ask_size1\"].values\n","            ask_size2 = book_train_st[\"ask_size2\"].values\n","\n","            # Perform the calculations using numba\n","            bid_feat, ask_feat, level_feat = calculate_features_numba(bid_size1, bid_size2, ask_size1, ask_size2)\n","\n","            # Add the results back to the DataFrame\n","            book_train_st[\"bid_lvl2_min_lvl1_size_feat\"] = bid_feat\n","            book_train_st[\"ask_lvl2_min_lvl1_size_feat\"] = ask_feat\n","            book_train_st[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = level_feat\n","\n","            # Aggregate the features by time_id\n","            bid_sum = book_train_st.groupby('time_id')[\"bid_lvl2_min_lvl1_size_feat\"].sum()\n","            ask_sum = book_train_st.groupby('time_id')[\"ask_lvl2_min_lvl1_size_feat\"].sum()\n","            level_sum = book_train_st.groupby('time_id')[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"].sum()\n","\n","            # Transformation to make the data more normal\n","            bid_sum = np.log1p((-bid_sum) ** 0.5)\n","            ask_sum = np.log1p((-ask_sum) ** 0.5)\n","            level_sum = np.log1p((-level_sum) ** 0.5)\n","\n","            return bid_sum, ask_sum, level_sum\n","\n","        def process_path_train(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            target_st = self.train_target.loc[self.train_target['stock_id'] == st_id].set_index(\"time_id\")\n","            book_train_st = pd.read_parquet(path)\n","            bid_sum, ask_sum, level_sum = calculate_features(book_train_st)\n","            result = {\n","                'bid_lvl2_min_lvl1_size_feat': bid_sum.reindex(target_st.index).ffill().bfill(),\n","                'ask_lvl2_min_lvl1_size_feat': ask_sum.reindex(target_st.index).ffill().bfill(),\n","                'lvl2_minus_lvl1_bid_n_ask_size_feat': level_sum.reindex(target_st.index).ffill().bfill(),\n","            }\n","            return result\n","\n","        def process_path_inference(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            target_st = self.test.loc[self.test['stock_id'] == st_id].set_index(\"time_id\")\n","            book_train_st = pd.read_parquet(path)\n","            bid_sum, ask_sum, level_sum = calculate_features(book_train_st)\n","            result = {\n","                'bid_lvl2_min_lvl1_size_feat': bid_sum.reindex(target_st.index).ffill().bfill(),\n","                'ask_lvl2_min_lvl1_size_feat': ask_sum.reindex(target_st.index).ffill().bfill(),\n","                'lvl2_minus_lvl1_bid_n_ask_size_feat': level_sum.reindex(target_st.index).ffill().bfill(),\n","            }\n","            return result\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_path_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_path_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_level1_2_size_imbalance_feat[key] = pd.concat([bk_level1_2_size_imbalance_feat[key], value], axis=0)\n","\n","        return bk_level1_2_size_imbalance_feat\n","\n","\n","\n","    def create_trade_sum_size_sum_order_count_sum_size_per_order_count(self,):\n","\n","\n","        subset_paths = self.trade_paths\n","        # Set parameters\n","        corr_method = 'spearman'  # set 'pearson' or 'spearman'\n","        file = 'trade_train'  # set 'book_train' or 'trade_train'\n","\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","\n","        trade_sum_size_sum_order_count_sum_size_per_order_count = {\n","            'sum_size': pd.DataFrame(),\n","            'sum_order_count': pd.DataFrame(),\n","            'sum_size_per_order_count': pd.DataFrame()\n","        }\n","\n","        @njit\n","        def calculate_sum_and_normalize(values):\n","            sum_values = np.sum(values)\n","            normalized_sum = np.log(sum_values)\n","            return normalized_sum\n","\n","        def process_stock_data_train(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Compute sum and normalize\n","            st_sum_size = trade_train_st.groupby(by='time_id')[size].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            st_sum_order_count = trade_train_st.groupby(by='time_id')[order_count].apply(lambda x: calculate_sum_and_normalize(x.values))\n","\n","            trade_train_st[\"size_per_order_count\"] = trade_train_st[\"size\"] / trade_train_st[\"order_count\"]\n","            st_size_per_order_count = trade_train_st.groupby(by='time_id')[\"size_per_order_count\"].apply(lambda x: calculate_sum_and_normalize(x.values))\n","\n","            result = {\n","                'sum_size': st_sum_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'sum_order_count': st_sum_order_count.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'sum_size_per_order_count': st_size_per_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","        def process_stock_data_inference(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            print(path)\n","            trade_train_st = pd.read_parquet(path)\n","            # Compute sum and normalize\n","            st_sum_size = trade_train_st.groupby(by='time_id')[size].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            st_sum_order_count = trade_train_st.groupby(by='time_id')[order_count].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            trade_train_st[\"size_per_order_count\"] = trade_train_st[\"size\"] / trade_train_st[\"order_count\"]\n","            st_size_per_order_count = trade_train_st.groupby(by='time_id')[\"size_per_order_count\"].apply(lambda x: calculate_sum_and_normalize(x.values))\n","            result = {\n","                'sum_size': st_sum_size,\n","                'sum_order_count': st_sum_order_count,\n","                'sum_size_per_order_count': st_size_per_order_count\n","            }\n","            return result\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_sum_size_sum_order_count_sum_size_per_order_count[key] = pd.concat([trade_sum_size_sum_order_count_sum_size_per_order_count[key], value], axis=0)\n","\n","        ####### Saving File #######\n","        ## The trade_sum_size_sum_order_count_sum_size_per_order_count is saved later below\n","        return trade_sum_size_sum_order_count_sum_size_per_order_count\n","        \"\"\"\n","        New features end\n","        \"\"\"\n","\n","\n","\n","\n","    def create_bk_price_size_min_max_range(self,):\n","\n","        # \"\"\"\n","        # old features start\n","        # \"\"\"\n","\n","        # ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","\n","        level = 1\n","        subset_paths = self.book_paths\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","        # Define your custom range function for price and size\n","        @njit\n","        def my_range_price(values):\n","            return np.max(values) - np.min(values)\n","\n","        # Initialize dictionaries\n","        bk_price_size_min_max_range = {\n","            'st_min_max_bid_price'+str(level): pd.DataFrame(),\n","            'st_min_max_ask_price'+str(level): pd.DataFrame(),\n","            'st_min_max_bid_size'+str(level): pd.DataFrame(),\n","            'st_min_max_ask_size'+str(level): pd.DataFrame(),\n","            'st_range_ask_price'+str(level): pd.DataFrame(),\n","            'st_range_bid_price'+str(level): pd.DataFrame(),\n","            'st_range_ask_size'+str(level): pd.DataFrame(),\n","            'st_range_bid_size'+str(level): pd.DataFrame()\n","        }\n","\n","        @njit\n","        def calculate_min_max_range(values):\n","            min_val = np.min(values)\n","            max_val = np.max(values)\n","            return min_val, max_val\n","\n","        def process_book_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate min and max\n","            bid_price_min_max = book_train_st.groupby(by='time_id')[bid_price].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_price_min_max = book_train_st.groupby(by='time_id')[ask_price].apply(lambda x: calculate_min_max_range(x.values))\n","            bid_size_min_max = book_train_st.groupby(by='time_id')[bid_size].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_size_min_max = book_train_st.groupby(by='time_id')[ask_size].apply(lambda x: calculate_min_max_range(x.values))\n","\n","            min_max_bid_price = pd.DataFrame(bid_price_min_max.tolist(), index=bid_price_min_max.index, columns=['min_bid_price', 'max_bid_price'])\n","            min_max_ask_price = pd.DataFrame(ask_price_min_max.tolist(), index=ask_price_min_max.index, columns=['min_ask_price', 'max_ask_price'])\n","            min_max_bid_size = pd.DataFrame(bid_size_min_max.tolist(), index=bid_size_min_max.index, columns=['min_bid_size', 'max_bid_size'])\n","            min_max_ask_size = pd.DataFrame(ask_size_min_max.tolist(), index=ask_size_min_max.index, columns=['min_ask_size', 'max_ask_size'])\n","\n","            # Calculate ranges\n","            range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: lambda x: my_range_price(x.values)}).rename(columns={ask_price: 'range_ask_price'})\n","            range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: lambda x: my_range_price(x.values)}).rename(columns={bid_price: 'range_bid_price'})\n","            range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: lambda x: my_range_price(x.values)}).rename(columns={ask_size: 'range_ask_size'})\n","            range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: lambda x: my_range_price(x.values)}).rename(columns={bid_size: 'range_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_max_bid_price'+str(level): min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_max_ask_price'+str(level): min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_max_bid_size'+str(level): min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_max_ask_size'+str(level): min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_ask_price'+str(level): range_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_bid_price'+str(level): range_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_ask_size'+str(level): range_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_bid_size'+str(level): range_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","        def process_book_data_inference(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate min and max\n","            bid_price_min_max = book_train_st.groupby(by='time_id')[bid_price].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_price_min_max = book_train_st.groupby(by='time_id')[ask_price].apply(lambda x: calculate_min_max_range(x.values))\n","            bid_size_min_max = book_train_st.groupby(by='time_id')[bid_size].apply(lambda x: calculate_min_max_range(x.values))\n","            ask_size_min_max = book_train_st.groupby(by='time_id')[ask_size].apply(lambda x: calculate_min_max_range(x.values))\n","\n","            min_max_bid_price = pd.DataFrame(bid_price_min_max.tolist(), index=bid_price_min_max.index, columns=['min_bid_price', 'max_bid_price'])\n","            min_max_ask_price = pd.DataFrame(ask_price_min_max.tolist(), index=ask_price_min_max.index, columns=['min_ask_price', 'max_ask_price'])\n","            min_max_bid_size = pd.DataFrame(bid_size_min_max.tolist(), index=bid_size_min_max.index, columns=['min_bid_size', 'max_bid_size'])\n","            min_max_ask_size = pd.DataFrame(ask_size_min_max.tolist(), index=ask_size_min_max.index, columns=['min_ask_size', 'max_ask_size'])\n","\n","            # Calculate ranges\n","            range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: lambda x: my_range_price(x.values)}).rename(columns={ask_price: 'range_ask_price'})\n","            range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: lambda x: my_range_price(x.values)}).rename(columns={bid_price: 'range_bid_price'})\n","            range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: lambda x: my_range_price(x.values)}).rename(columns={ask_size: 'range_ask_size'})\n","            range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: lambda x: my_range_price(x.values)}).rename(columns={bid_size: 'range_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_max_bid_price'+str(level): min_max_bid_price,\n","                'st_min_max_ask_price'+str(level): min_max_ask_price,\n","                'st_min_max_bid_size'+str(level): min_max_bid_size,\n","                'st_min_max_ask_size'+str(level): min_max_ask_size,\n","                'st_range_ask_price'+str(level): range_ask_price,\n","                'st_range_bid_price'+str(level): range_bid_price,\n","                'st_range_ask_size'+str(level): range_ask_size,\n","                'st_range_bid_size'+str(level): range_bid_size\n","            }\n","\n","            return result\n","\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_price_size_min_max_range[key] = pd.concat([bk_price_size_min_max_range[key], value], axis=0)\n","\n","        \"\"\"\n","        Old features end\n","        \"\"\"\n","        return bk_price_size_min_max_range\n","\n","\n","\n","\n","\n","\n","    def create_bk_price_size_sad(self,):\n","\n","        ## 7f) - 7i) Check if  the sum of absolute differences is correlated with target\n","\n","\n","        subset_paths = self.book_paths\n","        level = 1\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","        # Define your custom SAD function\n","        @njit\n","        def my_sum_abs_diff(values):\n","            return np.sum(np.abs(np.diff(values)))\n","\n","        # Initialize dictionaries\n","        bk_price_size_sad = {\n","            'st_sad_ask_price'+str(level): pd.DataFrame(),\n","            'st_sad_ask_size'+str(level): pd.DataFrame(),\n","            'st_sad_bid_price'+str(level): pd.DataFrame(),\n","            'st_sad_bid_size'+str(level): pd.DataFrame()\n","        }\n","\n","        def process_book_data_sad_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate SAD\n","            def calculate_sad(series):\n","                return my_sum_abs_diff(series.values)\n","\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: calculate_sad}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: calculate_sad}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: calculate_sad}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: calculate_sad}).rename(columns={bid_size: 'sad_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_ask_price'+str(level): st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_ask_size'+str(level): st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_bid_price'+str(level): st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_bid_size'+str(level): st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","        def process_book_data_sad_inference(path):\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate SAD\n","            def calculate_sad(series):\n","                return my_sum_abs_diff(series.values)\n","\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: calculate_sad}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: calculate_sad}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: calculate_sad}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: calculate_sad}).rename(columns={bid_size: 'sad_bid_size'})\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_ask_price'+str(level): st_sad_ask_price,\n","                'st_sad_ask_size'+str(level): st_sad_ask_size,\n","                'st_sad_bid_price'+str(level): st_sad_bid_price,\n","                'st_sad_bid_size'+str(level): st_sad_bid_size\n","            }\n","\n","            return result\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_sad_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_sad_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_price_size_sad[key] = pd.concat([bk_price_size_sad[key], value], axis=0)\n","\n","        ######## SAVING FILE\n","        # This bk_price_size_sad is saved later below\n","\n","        return bk_price_size_sad\n","\n","\n","\n","\n","\n","    def create_bk_size_price_corr(self,):\n","\n","        ## 7g) - 7j) Check if the correlation of any pair of bidsize1,bid_price1,asksize1,ask_price1 is correlated with target realized volatitlity for all the time_ids?\n","\n","        level=1\n","        subset_paths = self.book_paths\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","\n","        def process_book_data_corr_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            # Reindex with target_st time_ids and apply forward and backward fill\n","            for key in corr_dict.keys():\n","                corr_dict[key] = corr_dict[key].reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return corr_dict\n","\n","\n","        def process_book_data_corr_inference(path):\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            return corr_dict\n","\n","\n","        # Initialize dictionaries\n","        bk_size_price_corr = {\n","            'st_bs_bp_corr'+str(level): pd.DataFrame(),\n","            'st_bs_as_corr'+str(level): pd.DataFrame(),\n","            'st_bs_ap_corr'+str(level): pd.DataFrame(),\n","            'st_bp_as_corr'+str(level): pd.DataFrame(),\n","            'st_bp_ap_corr'+str(level): pd.DataFrame(),\n","            'st_as_ap_corr'+str(level): pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_corr_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_book_data_corr_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    bk_size_price_corr[key+str(level)] = pd.concat([bk_size_price_corr[key+str(level)], value], axis=0)\n","\n","        ######## saving FILE\n","        # This bk_size_price_corr is saved later below\n","\n","        return bk_size_price_corr\n","\n","\n","\n","\n","\n","    def create_trade_price_size_order_count_min_max_range(self,):\n","\n","        ## SET PARAMETERS HERE for trade_train.parquet files ONLY!!\n","        ## set the Correlation method\n","\n","        corr_method = 'pearson' # set 'pearson' or 'spearman'\n","        file = 'trade_train' # set 'book_train' or 'trade_train'\n","\n","        price = \"price\"\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","        ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","        subset_paths = self.trade_paths\n","\n","        # Define custom range function\n","\n","        def my_range_price(values):\n","            return np.max(values) - np.min(values)\n","\n","        # Define a function to calculate min, max, and range\n","        def calculate_min_max_range(df, column, range_func):\n","            min_max = df.groupby(by='time_id')[column].agg(['min', 'max']).rename(columns={'min': f'min_{column}', 'max': f'max_{column}'})\n","            range_df = df.groupby(by='time_id').agg({column: [range_func]}).rename(columns={column: f'range_{column}'})\n","            range_df.columns = range_df.columns.droplevel(1)\n","            return min_max, range_df\n","\n","        def process_trade_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate min, max, and range for each column\n","            min_max_price, range_price = calculate_min_max_range(trade_train_st, price, my_range_price)\n","            min_max_size, range_size = calculate_min_max_range(trade_train_st, size, my_range_price)\n","            min_max_order_count, range_order_count = calculate_min_max_range(trade_train_st, order_count, my_range_price)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_price': min_max_price['min_price'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_max_price': min_max_price['max_price'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_size': min_max_size['min_size'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_max_size': min_max_size['max_size'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_min_order_count': min_max_order_count['min_order_count'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_max_order_count': min_max_order_count['max_order_count'].reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_price': range_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_size': range_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_range_order_count': range_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","\n","        def process_trade_data_inference(path):\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate min, max, and range for each column\n","            min_max_price, range_price = calculate_min_max_range(trade_train_st, price, my_range_price)\n","            min_max_size, range_size = calculate_min_max_range(trade_train_st, size, my_range_price)\n","            min_max_order_count, range_order_count = calculate_min_max_range(trade_train_st, order_count, my_range_price)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_min_price': min_max_price['min_price'],\n","                'st_max_price': min_max_price['max_price'],\n","                'st_min_size': min_max_size['min_size'],\n","                'st_max_size': min_max_size['max_size'],\n","                'st_min_order_count': min_max_order_count['min_order_count'],\n","                'st_max_order_count': min_max_order_count['max_order_count'],\n","                'st_range_price': range_price,\n","                'st_range_size': range_size,\n","                'st_range_order_count': range_order_count\n","            }\n","\n","            return result\n","\n","        # Initialize dictionaries\n","        trade_price_size_order_count_min_max_range = {\n","            'st_min_price': pd.DataFrame(),\n","            'st_max_price': pd.DataFrame(),\n","            'st_min_size': pd.DataFrame(),\n","            'st_max_size': pd.DataFrame(),\n","            'st_min_order_count': pd.DataFrame(),\n","            'st_max_order_count': pd.DataFrame(),\n","            'st_range_price': pd.DataFrame(),\n","            'st_range_size': pd.DataFrame(),\n","            'st_range_order_count': pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_price_size_order_count_min_max_range[key] = pd.concat([trade_price_size_order_count_min_max_range[key], value], axis=0)\n","\n","        ######## Saving File\n","        # This trade_price_size_order_count_min_max_range is saved below\n","\n","        return trade_price_size_order_count_min_max_range\n","\n","\n","\n","\n","\n","    def create_trade_price_size_order_count_sad(self,):\n","\n","        subset_paths = self.trade_paths\n","\n","        price = \"price\"\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","        # Define custom sum of absolute differences function\n","\n","        def my_sum_abs_diff(values):\n","            return np.sum(np.abs(np.diff(values)))\n","\n","        # Define a function to calculate sum of absolute differences\n","        def calculate_sad(df, column, sad_func):\n","            sad_df = df.groupby(by='time_id').agg({column: [sad_func]}).rename(columns={column: f'sad_{column}'})\n","            sad_df.columns = sad_df.columns.droplevel(1)\n","            return sad_df\n","\n","        def process_trade_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate sum of absolute differences for each column\n","            sad_price = calculate_sad(trade_train_st, price, my_sum_abs_diff)\n","            sad_size = calculate_sad(trade_train_st, size, my_sum_abs_diff)\n","            sad_order_count = calculate_sad(trade_train_st, order_count, my_sum_abs_diff)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_price': sad_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_size': sad_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","                'st_sad_order_count': sad_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","\n","            return result\n","\n","\n","\n","        def process_trade_data_inference(path):\n","\n","            trade_train_st = pd.read_parquet(path)\n","            # Calculate sum of absolute differences for each column\n","            sad_price = calculate_sad(trade_train_st, price, my_sum_abs_diff)\n","            sad_size = calculate_sad(trade_train_st, size, my_sum_abs_diff)\n","            sad_order_count = calculate_sad(trade_train_st, order_count, my_sum_abs_diff)\n","            # Reindex and concatenate\n","            result = {\n","                'st_sad_price': sad_price,\n","                'st_sad_size': sad_size,\n","                'st_sad_order_count': sad_order_count\n","            }\n","            return result\n","\n","        # Initialize dictionaries\n","        trade_price_size_order_count_sad = {\n","            'st_sad_price': pd.DataFrame(),\n","            'st_sad_size': pd.DataFrame(),\n","            'st_sad_order_count': pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_price_size_order_count_sad[key] = pd.concat([trade_price_size_order_count_sad[key], value], axis=0)\n","\n","        ######## Saving File\n","        # This trade_price_size_order_count_sad is saved below\n","\n","        return trade_price_size_order_count_sad\n","\n","\n","\n","\n","    def create_trade_price_size_order_count_corr(self,):\n","\n","        subset_paths = self.trade_paths\n","\n","        price = \"price\"\n","        size = \"size\"\n","        order_count = \"order_count\"\n","\n","        # Define a function to calculate correlation for given columns\n","        def calculate_corr(df, col1, col2):\n","            corr_df = df.groupby('time_id')[[col1, col2]].corr().iloc[0::2, -1]\n","            corr_df.index = corr_df.index.droplevel(1)\n","            return corr_df\n","\n","        def process_trade_data_train(path):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = self.train_target[self.train_target['stock_id'] == st_id]\n","            target_st.index = [target_st[\"time_id\"]]\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations\n","            size_order_count_corr = calculate_corr(trade_train_st, size, order_count)\n","\n","            # Reindex and concatenate\n","            result = {\n","                'st_size_order_count_corr': size_order_count_corr.reindex(target_st['time_id'].values).ffill().bfill()\n","            }\n","            return result\n","\n","        def process_trade_data_inference(path):\n","            trade_train_st = pd.read_parquet(path)\n","            # Calculate correlations\n","            size_order_count_corr = calculate_corr(trade_train_st, size, order_count)\n","            # Reindex and concatenate\n","            result = {\n","                'st_size_order_count_corr': size_order_count_corr\n","            }\n","            return result\n","\n","        # Initialize dictionary\n","        trade_price_size_order_count_corr = {\n","            'st_size_order_count_corr': pd.DataFrame()\n","        }\n","\n","        # Parallel processing using joblib\n","        if self.ml_stage == 'training':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_train)(path) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          results = Parallel(n_jobs=-1)(delayed(process_trade_data_inference)(path) for path in subset_paths)\n","\n","        # Combine results\n","        for result in results:\n","            if result:\n","                for key, value in result.items():\n","                    trade_price_size_order_count_corr[key] = pd.concat([trade_price_size_order_count_corr[key], value], axis=0)\n","\n","        return trade_price_size_order_count_corr\n","    \"\"\"old features end\"\"\"\n","\n","\n","\n","\n","    def create_trade_price_n_wap1_deviation_df_AND_trade_price_n_wap_eqi_price0_deviation_df(self,):\n","\n","        # Define the Numba-accelerated function for WAP1 price calculation\n","        @njit\n","        def compute_wap1(bid_price1, ask_price1, bid_size1, ask_size1):\n","            return (bid_price1 * ask_size1 + ask_price1 * bid_size1) / (bid_size1 + ask_size1)\n","\n","        # Function to process trade parquet files\n","        def process_trade_file(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            trade_train_st = pd.read_parquet(path)\n","            trade_train_st['stock_id'] = st_id\n","            return trade_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'price']].rename(columns={'price': 'trade_price'})\n","\n","        # Function to process book parquet files\n","        def process_book_file(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            book_train_st['stock_id'] = st_id\n","            # Apply Numba-accelerated WAP1 calculation\n","            book_train_st['wap1_price'] = compute_wap1(\n","                book_train_st['bid_price1'].values,\n","                book_train_st['ask_price1'].values,\n","                book_train_st['bid_size1'].values,\n","                book_train_st['ask_size1'].values\n","            )\n","            return book_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'wap1_price']]\n","\n","\n","        # Use joblib's Parallel and delayed to process trade files in parallel\n","        trade_dfs = Parallel(n_jobs=-1)(delayed(process_trade_file)(path) for path in self.trade_paths)\n","\n","        # Use joblib's Parallel and delayed to process book files in parallel\n","        book_dfs = Parallel(n_jobs=-1)(delayed(process_book_file)(path) for path in self.book_paths)\n","\n","        # Concatenate the DataFrames into the final DataFrames\n","        trade_price_df = pd.concat(trade_dfs, axis=0)\n","        book_price_df = pd.concat(book_dfs, axis=0)\n","\n","        ##### merge trade_price and book_price data and calculate trade_price_n_wap1_deviation #####\n","        trade_price_n_wap1_deviation = pd.DataFrame()\n","\n","        # Group by 'stock_id' and perform operations in one go\n","        for st, trade_price_st in trade_price_df.groupby('stock_id'):\n","            book_price_st = book_price_df[book_price_df['stock_id'] == st]\n","\n","            # Ensure consistent types for merging using .loc to avoid SettingWithCopyWarning\n","            trade_price_st.loc[:, 'time_id'] = trade_price_st['time_id'].astype(int)\n","            book_price_st.loc[:, 'time_id'] = book_price_st['time_id'].astype(int)\n","            trade_price_st.loc[:, 'seconds_in_bucket'] = trade_price_st['seconds_in_bucket'].astype(int)\n","            book_price_st.loc[:, 'seconds_in_bucket'] = book_price_st['seconds_in_bucket'].astype(int)\n","\n","            # Merge trade and book data on 'stock_id', 'time_id', 'seconds_in_bucket'\n","            merged = trade_price_st.merge(book_price_st, on=['stock_id', 'time_id', 'seconds_in_bucket'], how='inner')\n","\n","            # Calculate the ratio with vectorized operations\n","            merged['ratio'] = (merged['wap1_price'] / merged['trade_price'])**0.5\n","\n","            # Group by 'stock_id' and 'time_id', and calculate standard deviation of 'ratio'\n","            temp_df1 = merged.groupby(['stock_id', 'time_id'])['ratio'].std().reset_index()\n","\n","            # Append to the final DataFrame\n","            trade_price_n_wap1_deviation = pd.concat([trade_price_n_wap1_deviation, temp_df1], axis=0)\n","\n","        ####### Calculate correlation between trade_price_n_wap1_deviation and target ########\n","        #merged_df = train.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","        if self.ml_stage == 'training':\n","          trade_price_n_wap1_deviation_df = self.train.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","        elif self.ml_stage == 'inference':\n","          trade_price_n_wap1_deviation_df = trade_price_n_wap1_deviation\n","        del book_price_df, trade_dfs, book_dfs, trade_price_st, book_price_st, merged\n","\n","\n","        # Assuming find_equilibrium_price is already defined elsewhere\n","        # Function to process book parquet files and calculate WAP equilibrium price\n","        def process_book_eqi_file(path):\n","            st_id = int(path.split('/stock_id=')[1])\n","            book_train_st = pd.read_parquet(path)\n","            book_train_st['stock_id'] = st_id\n","\n","            # Avoid log(0) errors with a small adjustment to prices if necessary\n","            book_train_st['log_ask_price2'] = book_train_st['ask_price2'] #.replace(0, np.nan)\n","            book_train_st['log_bid_price2'] = book_train_st['bid_price2'] #.replace(0, np.nan)\n","            book_train_st['log_ask_price1'] = book_train_st['ask_price1'] #.replace(0, np.nan)\n","            book_train_st['log_bid_price1'] = book_train_st['bid_price1'] #.replace(0, np.nan)\n","\n","            # Calculate the equilibrium price\n","            book_train_st['wap_eqi_price0'] = find_equilibrium_price(book_train_st, 0)\n","\n","            return book_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'wap_eqi_price0']]\n","\n","\n","        # Use joblib's Parallel and delayed to process book files in parallel\n","        book_eqi_dfs = Parallel(n_jobs=-1)(delayed(process_book_eqi_file)(path) for path in self.book_paths)\n","\n","        # Concatenate the DataFrames into the final DataFrame\n","        book_eqi_price_df = pd.concat(book_eqi_dfs, axis=0)\n","\n","        ##### Merge trade_price and book_price data and calculate trade_price_n_wap_eqi_price0_deviation #####\n","        trade_price_n_wap_eqi_price0_deviation = pd.DataFrame()\n","\n","        # Group by 'stock_id' and perform operations in one go\n","        for st, trade_price_st in trade_price_df.groupby('stock_id'):\n","            book_price_st = book_eqi_price_df[book_eqi_price_df['stock_id'] == st]\n","\n","            # Ensure consistent types for merging using .loc to avoid SettingWithCopyWarning\n","            trade_price_st.loc[:, 'time_id'] = trade_price_st['time_id'].astype(int)\n","            book_price_st.loc[:, 'time_id'] = book_price_st['time_id'].astype(int)\n","            trade_price_st.loc[:, 'seconds_in_bucket'] = trade_price_st['seconds_in_bucket'].astype(int)\n","            book_price_st.loc[:, 'seconds_in_bucket'] = book_price_st['seconds_in_bucket'].astype(int)\n","\n","            # Merge trade and book data on 'stock_id', 'time_id', 'seconds_in_bucket'\n","            merged1 = trade_price_st.merge(book_price_st, on=['stock_id', 'time_id', 'seconds_in_bucket'], how='inner')\n","\n","            # Calculate the ratio with vectorized operations\n","            merged1['ratio'] = (merged1['wap_eqi_price0'] / (merged1['trade_price'] ) )**0.5\n","\n","            # Group by 'stock_id' and 'time_id', and calculate standard deviation of 'ratio'\n","            temp_df1 = merged1.groupby(['stock_id', 'time_id'])['ratio'].std().reset_index()\n","\n","            # Append to the final DataFrame\n","            trade_price_n_wap_eqi_price0_deviation = pd.concat([trade_price_n_wap_eqi_price0_deviation, temp_df1], axis=0)\n","\n","        ####### Calculate correlation between trade_price_n_wap_eqi_price0_deviation and target ########\n","        if self.ml_stage == 'training':\n","          trade_price_n_wap_eqi_price0_deviation_df = train.merge(trade_price_n_wap_eqi_price0_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","        elif self.ml_stage == 'inference':\n","          trade_price_n_wap_eqi_price0_deviation_df = trade_price_n_wap_eqi_price0_deviation\n","        del trade_price_df, trade_price_st, book_price_st, merged1, temp_df1, book_eqi_price_df, book_eqi_dfs\n","\n","        return trade_price_n_wap1_deviation_df,  trade_price_n_wap_eqi_price0_deviation_df\n","\n","\n","\n","\n","    def merge_trade_price_n_wap_eqi_price0_deviation_df(self,bk_level1_2_size_imbalance_feat,trade_sum_size_sum_order_count_sum_size_per_order_count,trade_price_n_wap1_deviation_df,trade_price_n_wap_eqi_price0_deviation_df, feat_df):\n","        for key in bk_level1_2_size_imbalance_feat.keys():\n","            bk_level1_2_size_imbalance_feat[key].index = range(bk_level1_2_size_imbalance_feat[key].shape[0])\n","            bk_level1_2_size_imbalance_feat[key].columns = [key]\n","            feat_df = feat_df.merge(bk_level1_2_size_imbalance_feat[key], left_index=True, right_index=True)\n","        del bk_level1_2_size_imbalance_feat\n","\n","        for key in trade_sum_size_sum_order_count_sum_size_per_order_count.keys():\n","            trade_sum_size_sum_order_count_sum_size_per_order_count[key].index = range(trade_sum_size_sum_order_count_sum_size_per_order_count[key].shape[0])\n","            trade_sum_size_sum_order_count_sum_size_per_order_count[key].columns = [key]\n","            feat_df = feat_df.merge(trade_sum_size_sum_order_count_sum_size_per_order_count[key], left_index=True, right_index=True)\n","        del trade_sum_size_sum_order_count_sum_size_per_order_count\n","\n","        trade_price_n_wap1_deviation_df[\"trade_price_n_wap1_dev\"] = trade_price_n_wap1_deviation_df[\"ratio\"]**0.5\n","        trade_price_n_wap1_deviation_df.drop(columns=[\"ratio\"], inplace=True)\n","        feat_df = feat_df.merge(trade_price_n_wap1_deviation_df, on=['stock_id', 'time_id'], how='left')\n","        del trade_price_n_wap1_deviation_df\n","\n","        trade_price_n_wap_eqi_price0_deviation_df[\"trade_price_n_wap_eqi_price0_dev\"] = trade_price_n_wap_eqi_price0_deviation_df[\"ratio\"]**0.5\n","        trade_price_n_wap_eqi_price0_deviation_df.drop(columns=[\"ratio\"], inplace=True)\n","        feat_df = feat_df.merge(trade_price_n_wap_eqi_price0_deviation_df, on=['stock_id', 'time_id'], how='left')\n","        del trade_price_n_wap_eqi_price0_deviation_df\n","\n","        return feat_df\n","\n","\n","    def log_return(self,series):\n","        return np.log(series).diff()\n","\n","    def realized_volatility(self,series_log_return):\n","        return np.sqrt(np.sum(series_log_return**2))\n","\n","\n","    def create_df_20_min_volatility(self,):\n","\n","        # def log_return(series):\n","        #     return np.log(series).diff()\n","\n","        # def realized_volatility(series_log_return):\n","        #     return np.sqrt(np.sum(series_log_return**2))\n","\n","        def realized_volatility_per_time_id(file_path, prediction_column_name):\n","            df_book_data = pd.read_parquet(file_path)\n","\n","            # Calculate WAP\n","            df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1'] +\n","                                  df_book_data['ask_price1'] * df_book_data['bid_size1']) / (\n","                                  df_book_data['bid_size1'] + df_book_data['ask_size1'])\n","\n","            # Calculate log return\n","            df_book_data['log_return'] = df_book_data.groupby('time_id')['wap'].transform(self.log_return)\n","\n","            # Remove rows with NaN log returns\n","            df_book_data = df_book_data.dropna(subset=['log_return'])\n","\n","            # Calculate realized volatility per time_id\n","            df_realized_vol_per_stock = (df_book_data.groupby('time_id')['log_return']\n","                                        .agg(self.realized_volatility)\n","                                        .reset_index(name=prediction_column_name))\n","\n","            stock_id = file_path.split('=')[1]\n","            df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x: f'{stock_id}-{x}')\n","\n","            return df_realized_vol_per_stock[['row_id', prediction_column_name]]\n","\n","        def past_realized_volatility_per_stock(list_file, prediction_column_name):\n","            # Parallel processing of files\n","            results = Parallel(n_jobs=-1)(delayed(realized_volatility_per_time_id)(file, prediction_column_name) for file in list_file)\n","            df_past_realized = pd.concat(results, ignore_index=True)\n","            return df_past_realized\n","\n","        list_order_book_file_train = self.book_paths\n","\n","        df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n","                                                                  prediction_column_name='first_10_min_vol')\n","\n","        if self.ml_stage == 'training':\n","\n","          # Prepare self.train DataFrame\n","          self.train['row_id'] = self.train['stock_id'].astype(str) + '-' + self.train['time_id'].astype(str)\n","          # Merge with past realized volatility\n","          df_20_min_volatility = self.train[['row_id', 'target']].merge(df_past_realized_train[['row_id', 'first_10_min_vol']],\n","                                            on='row_id', how='left')\n","\n","        elif self.ml_stage == 'inference':\n","          # Merge with past realized volatility\n","          df_20_min_volatility = df_past_realized_train\n","\n","        # Extract stock_id and time_id\n","        df_20_min_volatility[['stock_id', 'time_id']] = df_20_min_volatility['row_id'].str.split('-', expand=True)\n","\n","        return df_20_min_volatility\n","\n","\n","\n","\n","\n","\n","    def create_n_merge_trade_price_std_df(self,feat_df):\n","\n","        subset_paths = self.trade_paths\n","\n","\n","        def process_trade_data(path, idx):\n","            trade_train_st = pd.read_parquet(path)\n","\n","            # Calculate statistics for price\n","            std_price = trade_train_st.groupby('time_id')['price'].std()\n","            std_price = pd.merge(idx, std_price, left_index=True, right_index=True, how='left')\n","            std_price.fillna(std_price['price'].mean(), inplace=True)\n","\n","            log_ret_price = trade_train_st.groupby('time_id')['price'].apply(self.log_return)\n","            trade_train_st['log_ret_price'] = trade_train_st.groupby('time_id')['price'].transform(self.log_return)\n","            trade_train_st = trade_train_st[~trade_train_st['log_ret_price'].isnull()]\n","            real_vol_price = trade_train_st.groupby('time_id')['log_ret_price'].agg(self.realized_volatility)\n","            real_vol_price = pd.merge(idx, real_vol_price, left_index=True, right_index=True, how='left')\n","            real_vol_price.fillna(real_vol_price['log_ret_price'].mean(), inplace=True)\n","\n","            # Calculate statistics for size\n","            std_size = trade_train_st.groupby('time_id')['size'].std()\n","            std_size = pd.merge(idx, std_size, left_index=True, right_index=True, how='left')\n","            std_size.fillna(std_size['size'].mean(), inplace=True)\n","\n","            mean_size = trade_train_st.groupby('time_id')['size'].mean()\n","            mean_size = pd.merge(idx, mean_size, left_index=True, right_index=True, how='left')\n","            mean_size.fillna(mean_size['size'].mean(), inplace=True)\n","\n","            # Calculate statistics for order_count\n","            std_order_count = trade_train_st.groupby('time_id')['order_count'].std()\n","            std_order_count = pd.merge(idx, std_order_count, left_index=True, right_index=True, how='left')\n","            std_order_count.fillna(std_order_count['order_count'].mean(), inplace=True)\n","\n","            mean_order_count = trade_train_st.groupby('time_id')['order_count'].mean()\n","            mean_order_count = pd.merge(idx, mean_order_count, left_index=True, right_index=True, how='left')\n","            mean_order_count.fillna(mean_order_count['order_count'].mean(), inplace=True)\n","\n","            # Return results\n","            return {\n","                'trade_price_std': std_price['price'].values,\n","                'trade_price_real_vol': real_vol_price['log_ret_price'].values,\n","                'trade_size_std': std_size['size'].values,\n","                'trade_size_mean': mean_size['size'].values,\n","                'trade_order_count_std': std_order_count['order_count'].values,\n","                'trade_order_count_mean': mean_order_count['order_count'].values\n","            }\n","\n","        if self.ml_stage == 'training':\n","          subset_paths = self.trade_paths\n","          train_idx = self.train.set_index('time_id')[['stock_id']]\n","          results = []\n","          for path in subset_paths:\n","              st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","              if st_id in self.train['stock_id'].values:\n","                  result = process_trade_data(path, train_idx[train_idx['stock_id'] == st_id])\n","                  results.append(result)\n","\n","        elif self.ml_stage == 'inference':\n","          subset_paths = self.trade_paths\n","          test_idx = self.test.set_index('time_id')[['stock_id']]\n","          results = []\n","          for path in subset_paths:\n","              st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","              if st_id in self.test['stock_id'].values:\n","                  result = process_trade_data(path, test_idx[test_idx['stock_id'] == st_id])\n","                  results.append(result)\n","\n","        # Concatenate results\n","        trade_price_std_arr = np.concatenate([result['trade_price_std'] for result in results])\n","        trade_price_real_vol_arr = np.concatenate([result['trade_price_real_vol'] for result in results])\n","        trade_size_std_arr = np.concatenate([result['trade_size_std'] for result in results])\n","        trade_size_mean_arr = np.concatenate([result['trade_size_mean'] for result in results])\n","        trade_order_count_std_arr = np.concatenate([result['trade_order_count_std'] for result in results])\n","        trade_order_count_mean_arr = np.concatenate([result['trade_order_count_mean'] for result in results])\n","\n","        # Create DataFrame and merge\n","        trade_price_std_df = pd.DataFrame({\n","            'trade_price_std': trade_price_std_arr,\n","            'trade_price_real_vol': trade_price_real_vol_arr,\n","            'trade_size_std': trade_size_std_arr,\n","            'trade_size_mean': trade_size_mean_arr,\n","            'trade_order_count_std': trade_order_count_std_arr,\n","            'trade_order_count_mean': trade_order_count_mean_arr\n","        })\n","\n","        feat_df = feat_df.merge(trade_price_std_df, left_index=True, right_index=True)\n","        del trade_price_std_df\n","\n","        return feat_df, self.train\n","\n","\n","\n","\n","\n","\n","    def create_all_stocks_first_10_min_vol_df_AND_merge_first_10_min_vol_df(self,feat_df,df_20_min_volatility):\n","\n","        first_10_min_vol_df = df_20_min_volatility\n","        first_10_min_vol_df['time_id'] = first_10_min_vol_df['time_id'].astype(int)\n","        first_10_min_vol_df['stock_id'] = first_10_min_vol_df['stock_id'].astype(int)\n","\n","        # # Initialize an array with NaN values\n","        # all_stocks_first_10_min_vol_array = np.full((len(self.all_uniq_time_ids), len(self.unique_stock_ids)), np.nan)\n","\n","        # # Create a dictionary for quick access to stock index\n","        # stock_id_to_index = {st_id: idx for idx, st_id in enumerate(self.unique_stock_ids)}\n","\n","        # # Fill the array with data\n","        # for st_id, subset in first_10_min_vol_df.groupby('stock_id'):\n","        #     time_id_indices = self.all_uniq_time_ids['time_id'].searchsorted(subset['time_id'].astype(int))\n","        #     all_stocks_first_10_min_vol_array[range(len(self.all_uniq_time_ids)), stock_id_to_index[int(st_id)]] = subset['first_10_min_vol'].values\n","\n","        # # Initialize an array with NaN values\n","        # all_stocks_first_10_min_vol_array = np.full((len(self.all_uniq_time_ids), len(self.unique_stock_ids)), np.nan)\n","\n","        # # Create a dictionary for quick access to stock index\n","        # stock_id_to_index = {st_id: idx for idx, st_id in enumerate(self.unique_stock_ids)}\n","\n","        # time_id_to_index = {time_id: idx for idx, time_id in enumerate(self.all_uniq_time_ids['time_id'])}\n","\n","\n","        # # Iterate over each stock_id\n","        # for st_id, subset in first_10_min_vol_df.groupby('stock_id'):\n","        #     # Map the stock_id to the correct index in the array\n","        #     stock_index = stock_id_to_index[int(st_id)]\n","\n","        #     subset = pd.merge(self.all_uniq_time_ids, subset, left_on='time_id', right_on='time_id', how='left')\n","\n","        #     time_indices = subset['time_id'].map(time_id_to_index)\n","\n","        #     # Fill the array with the first_10_min_vol values\n","        #     all_stocks_first_10_min_vol_array[time_indices, stock_index] = subset['first_10_min_vol'].values\n","\n","\n","\n","        # # Forward fill and backward fill missing values\n","        # all_stocks_first_10_min_vol_array = pd.DataFrame(all_stocks_first_10_min_vol_array).ffill().bfill().to_numpy()\n","\n","        pivot_table = df_20_min_volatility.pivot(columns= 'stock_id', values='first_10_min_vol', index='time_id')\n","        all_stocks_first_10_min_vol_array = pivot_table.values\n","        all_unique_time_ids = pivot_table.index.values\n","        all_unique_stock_ids = pivot_table.columns.values\n","        # Reshape to the desired shape\n","        all_stocks_first_10_min_vol_array = all_stocks_first_10_min_vol_array[:, :, np.newaxis]\n","\n","        all_stocks_first_10_min_vol_df = all_stocks_first_10_min_vol_array\n","\n","        feat_df['first_10_min_vol'] = first_10_min_vol_df['first_10_min_vol']\n","        del first_10_min_vol_df\n","\n","        return feat_df, all_stocks_first_10_min_vol_df, all_unique_time_ids, all_unique_stock_ids\n","\n","\n","\n","\n","\n","\n","\n","    def merge_bk_price_size_min_max_range(self,feat_df,bk_price_size_min_max_range):\n","        bk_price_size_min_max_range['st_min_max_bid_price1'].rename(columns={'min_bid_price':'min_bid_price1', 'max_bid_price':'max_bid_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_bid_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_bid_price1']], axis=1)\n","        feat_df.columns\n","\n","        bk_price_size_min_max_range['st_min_max_ask_price1'].rename(columns={'min_ask_price':'min_ask_price1', 'max_ask_price':'max_ask_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_ask_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_ask_price1']], axis=1)\n","        feat_df.columns\n","\n","        bk_price_size_min_max_range['st_min_max_bid_size1'].rename(columns={'min_bid_size':'min_bid_size1', 'max_bid_size':'max_bid_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_bid_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_bid_size1']], axis=1)\n","        feat_df.columns\n","\n","        bk_price_size_min_max_range['st_min_max_ask_size1'].rename(columns={'min_ask_size':'min_ask_size1', 'max_ask_size':'max_ask_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_min_max_ask_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_min_max_ask_size1']], axis=1)\n","\n","        bk_price_size_min_max_range['st_range_ask_price1'].rename(columns={'range_ask_price':'range_ask_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_ask_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_ask_price1']], axis=1)\n","\n","        bk_price_size_min_max_range['st_range_bid_price1'].rename(columns={'range_bid_price':'range_bid_price1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_bid_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_bid_price1']], axis=1)\n","\n","\n","        bk_price_size_min_max_range['st_range_ask_size1'].rename(columns={'range_ask_size':'range_ask_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_ask_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_ask_size1']], axis=1)\n","\n","        bk_price_size_min_max_range['st_range_bid_size1'].rename(columns={'range_bid_size':'range_bid_size1'}, inplace=True)\n","        bk_price_size_min_max_range['st_range_bid_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range['st_range_bid_size1']], axis=1)\n","        feat_df.columns\n","        del bk_price_size_min_max_range\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","    def merge_bk_price_size_sad(self,feat_df,bk_price_size_sad):\n","\n","        ####### bk_price_size_sad\n","\n","        bk_price_size_sad['st_sad_ask_price1'].rename(columns={'sad_ask_price':'sad_ask_price1'}, inplace=True)\n","        bk_price_size_sad['st_sad_ask_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_ask_price1']], axis=1)\n","\n","        bk_price_size_sad['st_sad_ask_size1'].rename(columns={'sad_ask_size':'sad_ask_size1'}, inplace=True)\n","        bk_price_size_sad['st_sad_ask_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_ask_size1']], axis=1)\n","\n","        bk_price_size_sad['st_sad_bid_price1'].rename(columns={'sad_bid_price':'sad_bid_price1'}, inplace=True)\n","        bk_price_size_sad['st_sad_bid_price1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_bid_price1']], axis=1)\n","\n","        bk_price_size_sad['st_sad_bid_size1'].rename(columns={'sad_bid_size':'sad_bid_size1'}, inplace=True)\n","        bk_price_size_sad['st_sad_bid_size1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad['st_sad_bid_size1']], axis=1)\n","        del bk_price_size_sad\n","\n","        return feat_df\n","\n","\n","    def merge_bk_size_price_corr(self,feat_df,bk_size_price_corr):\n","\n","        ## bk_size_price_corr\n","\n","        bk_size_price_corr['st_bs_bp_corr1'].rename(columns={0:'bs_bp_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_bp_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bs_bp_corr1']], axis=1)\n","\n","        bk_size_price_corr['st_bs_as_corr1'].rename(columns={0:'bs_as_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_as_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bs_as_corr1']], axis=1)\n","\n","        bk_size_price_corr['st_bs_ap_corr1'].rename(columns={0:'bs_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bs_ap_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bs_ap_corr1']], axis=1)\n","\n","        bk_size_price_corr['st_bp_as_corr1'].rename(columns={0:'bp_as_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bp_as_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bp_as_corr1']], axis=1)\n","\n","        bk_size_price_corr['st_bp_ap_corr1'].rename(columns={0:'bp_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_bp_ap_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_bp_ap_corr1']], axis=1)\n","\n","        bk_size_price_corr['st_as_ap_corr1'].rename(columns={0:'as_ap_corr1'}, inplace=True)\n","        bk_size_price_corr['st_as_ap_corr1'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr['st_as_ap_corr1']], axis=1)\n","        del bk_size_price_corr\n","        return feat_df\n","\n","\n","    def merge_trade_price_size_order_count_min_max_range(self,feat_df,trade_price_size_order_count_min_max_range):\n","        ## trade_price_size_order_count_min_max_range\n","\n","        trade_price_size_order_count_min_max_range['st_min_price'].rename(columns={0:'min_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_min_price']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_max_price'].rename(columns={0:'max_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_max_price']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_min_size'].rename(columns={0:'min_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_min_size']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_max_size'].rename(columns={0:'max_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_max_size']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_min_order_count'].rename(columns={0:'min_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_min_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_min_order_count']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_max_order_count'].rename(columns={0:'max_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_max_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_max_order_count']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_range_price'].rename(columns={'range_price':'range_price1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_range_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_range_price']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_range_size'].rename(columns={'range_size':'range_size1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_range_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_range_size']], axis=1)\n","\n","        trade_price_size_order_count_min_max_range['st_range_order_count'].rename(columns={'range_order_count':'range_order_count1'}, inplace=True)\n","        trade_price_size_order_count_min_max_range['st_range_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_min_max_range['st_range_order_count']], axis=1)\n","        del trade_price_size_order_count_min_max_range\n","\n","        return feat_df\n","\n","\n","    def merge_trade_price_size_order_count_sad(self,feat_df,trade_price_size_order_count_sad):\n","        ## trade_price_size_order_count_sad\n","\n","        trade_price_size_order_count_sad['st_sad_price'].rename(columns={'sad_price':'sad_price1'}, inplace=True)\n","        trade_price_size_order_count_sad['st_sad_price'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_sad['st_sad_price']], axis=1)\n","\n","        trade_price_size_order_count_sad['st_sad_size'].rename(columns={'sad_size':'sad_size1'}, inplace=True)\n","        trade_price_size_order_count_sad['st_sad_size'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_sad['st_sad_size']], axis=1)\n","\n","        trade_price_size_order_count_sad['st_sad_order_count'].rename(columns={'sad_order_count':'sad_order_count1'}, inplace=True)\n","        trade_price_size_order_count_sad['st_sad_order_count'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_sad['st_sad_order_count']], axis=1)\n","        del trade_price_size_order_count_sad\n","\n","        return feat_df\n","\n","\n","    def merge_trade_price_size_order_count_corr(self,feat_df,trade_price_size_order_count_corr):\n","\n","        ## trade_price_size_order_count_corr\n","\n","        trade_price_size_order_count_corr['st_size_order_count_corr'].rename(columns={0:'size_order_count_corr1'}, inplace=True)\n","        trade_price_size_order_count_corr['st_size_order_count_corr'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, trade_price_size_order_count_corr['st_size_order_count_corr']], axis=1)\n","        del trade_price_size_order_count_corr\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","    ## apply summary statistics and pearson correlated cluster labels to stock ids, similar labels represent similarity in stocks\n","    ## These will be considered as categorical features\n","    ## we can also input our own feature clustering this way as well instead of taking median of the first_10_min_target_volatility for each cluster\n","\n","    def merge_final_sum_stats_target_vol_clusters(self,feat_df,final_sum_stats_target_vol_clusters,unique_stock_ids):\n","\n","        for c in final_sum_stats_target_vol_clusters.columns:\n","            labels = final_sum_stats_target_vol_clusters[c]\n","            feat_df['sum_stats_'+c+'_labels'] = feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","        del final_sum_stats_target_vol_clusters\n","\n","        return feat_df\n","\n","\n","    def merge_final_robust_sum_stats_target_vol_clusters(self,feat_df,final_robust_sum_stats_target_vol_clusters,unique_stock_ids):\n","        for c in final_robust_sum_stats_target_vol_clusters.columns:\n","            labels = final_robust_sum_stats_target_vol_clusters[c]\n","            feat_df['robust_sum_stats_'+c+'_labels'] = feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","        del final_robust_sum_stats_target_vol_clusters\n","\n","        return feat_df\n","\n","\n","    def merge_final_pear_corr_target_vol_clusters(self,feat_df,final_pear_corr_target_vol_clusters,unique_stock_ids):\n","\n","        for c in final_pear_corr_target_vol_clusters.columns:\n","            labels = final_pear_corr_target_vol_clusters[c]\n","            feat_df['pear_corr_'+c+'_labels'] = feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","        del final_pear_corr_target_vol_clusters\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","\n","    def create_bk_price_size_min_max_range_2(self,):\n","\n","        level = 2\n","        subset_paths = self.book_paths\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","\n","        train_target = self.train\n","\n","        def my_range_price(series):\n","            return series.max()-series.min()\n","\n","        def process_stock_data_train(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = train_target[train_target['stock_id'] == st_id]\n","            target_st.index = target_st['time_id']\n","\n","            book_train_st = pd.read_parquet(path)\n","\n","            result = {}\n","\n","            # Min-Max Bid Price\n","            st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min', 'max']).rename(columns={'min': 'min_bid_price', 'max': 'max_bid_price'})\n","            result['st_min_max_bid_price'] = st_min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Min-Max Ask Price\n","            st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min', 'max']).rename(columns={'min': 'min_ask_price', 'max': 'max_ask_price'})\n","            result['st_min_max_ask_price'] = st_min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Min-Max Bid Size\n","            st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min', 'max']).rename(columns={'min': 'min_bid_size', 'max': 'max_bid_size'})\n","            result['st_min_max_bid_size'] = st_min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Min-Max Ask Size\n","            st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min', 'max']).rename(columns={'min': 'min_ask_size', 'max': 'max_ask_size'})\n","            result['st_min_max_ask_size'] = st_min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Ask Price\n","            st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_range_price]}).rename(columns={ask_price: 'range_ask_price'})\n","            st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","            result['st_range_ask_price'] = st_range_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Bid Price\n","            st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_range_price]}).rename(columns={bid_price: 'range_bid_price'})\n","            st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","            result['st_range_bid_price'] = st_range_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Ask Size\n","            st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_range_price]}).rename(columns={ask_size: 'range_ask_size'})\n","            st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","            result['st_range_ask_size'] = st_range_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            # Range Bid Size\n","            st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_range_price]}).rename(columns={bid_size: 'range_bid_size'})\n","            st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","            result['st_range_bid_size'] = st_range_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return result\n","\n","\n","        def process_stock_data_inference(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price):\n","\n","            book_train_st = pd.read_parquet(path)\n","\n","            result = {}\n","\n","            # Min-Max Bid Price\n","            st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min', 'max']).rename(columns={'min': 'min_bid_price', 'max': 'max_bid_price'})\n","            result['st_min_max_bid_price'] = st_min_max_bid_price\n","\n","            # Min-Max Ask Price\n","            st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min', 'max']).rename(columns={'min': 'min_ask_price', 'max': 'max_ask_price'})\n","            result['st_min_max_ask_price'] = st_min_max_ask_price\n","\n","            # Min-Max Bid Size\n","            st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min', 'max']).rename(columns={'min': 'min_bid_size', 'max': 'max_bid_size'})\n","            result['st_min_max_bid_size'] = st_min_max_bid_size\n","\n","            # Min-Max Ask Size\n","            st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min', 'max']).rename(columns={'min': 'min_ask_size', 'max': 'max_ask_size'})\n","            result['st_min_max_ask_size'] = st_min_max_ask_size\n","\n","            # Range Ask Price\n","            st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_range_price]}).rename(columns={ask_price: 'range_ask_price'})\n","            st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","            result['st_range_ask_price'] = st_range_ask_price\n","\n","            # Range Bid Price\n","            st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_range_price]}).rename(columns={bid_price: 'range_bid_price'})\n","            st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","            result['st_range_bid_price'] = st_range_bid_price\n","\n","            # Range Ask Size\n","            st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_range_price]}).rename(columns={ask_size: 'range_ask_size'})\n","            st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","            result['st_range_ask_size'] = st_range_ask_size\n","\n","            # Range Bid Size\n","            st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_range_price]}).rename(columns={bid_size: 'range_bid_size'})\n","            st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","            result['st_range_bid_size'] = st_range_bid_size\n","\n","            return result\n","\n","\n","        if self.ml_stage == 'training':\n","          # Parallel processing\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_train)(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          # Sequential processing\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_data_inference)(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price) for path in subset_paths)\n","\n","        # Combine the results\n","        bk_price_size_min_max_range_2 = {\n","            'st_min_max_bid_price'+str(level): pd.concat([res['st_min_max_bid_price'] for res in results if res is not None], axis=0),\n","            'st_min_max_ask_price'+str(level): pd.concat([res['st_min_max_ask_price'] for res in results if res is not None], axis=0),\n","            'st_min_max_bid_size'+str(level): pd.concat([res['st_min_max_bid_size'] for res in results if res is not None], axis=0),\n","            'st_min_max_ask_size'+str(level): pd.concat([res['st_min_max_ask_size'] for res in results if res is not None], axis=0),\n","            'st_range_ask_price'+str(level): pd.concat([res['st_range_ask_price'] for res in results if res is not None], axis=0),\n","            'st_range_bid_price'+str(level): pd.concat([res['st_range_bid_price'] for res in results if res is not None], axis=0),\n","            'st_range_ask_size'+str(level): pd.concat([res['st_range_ask_size'] for res in results if res is not None], axis=0),\n","            'st_range_bid_size'+str(level): pd.concat([res['st_range_bid_size'] for res in results if res is not None], axis=0),\n","        }\n","        return bk_price_size_min_max_range_2\n","\n","\n","    def merge_bk_price_size_min_max_range_2(self,bk_price_size_min_max_range_2,feat_df):\n","\n","        ####### bk_price_size_min_max_range_2\n","\n","        bk_price_size_min_max_range_2['st_min_max_bid_price2'].rename(columns={'min_bid_price':'min_bid_price2', 'max_bid_price':'max_bid_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_bid_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_bid_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_min_max_ask_price2'].rename(columns={'min_ask_price':'min_ask_price2', 'max_ask_price':'max_ask_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_ask_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_ask_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_min_max_bid_size2'].rename(columns={'min_bid_size':'min_bid_size2', 'max_bid_size':'max_bid_size2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_bid_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_bid_size2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_min_max_ask_size2'].rename(columns={'min_ask_size':'min_ask_size2', 'max_ask_size':'max_ask_size2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_min_max_ask_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_min_max_ask_size2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_range_ask_price2'].rename(columns={'range_ask_price':'range_ask_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_range_ask_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_range_ask_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_range_bid_price2'].rename(columns={'range_bid_price':'range_bid_price2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_range_bid_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_range_bid_price2']], axis=1)\n","\n","        bk_price_size_min_max_range_2['st_range_ask_size2'].rename(columns={'range_ask_size':'range_ask_size2'}, inplace=True)\n","        bk_price_size_min_max_range_2['st_range_ask_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_min_max_range_2['st_range_ask_size2']], axis=1)\n","        del bk_price_size_min_max_range_2\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    def create_bk_price_size_sad_2(self,):\n","\n","        level = 2\n","        subset_paths = self.book_paths\n","        train_target = self.train\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","        def my_sum_abs_diff(values):\n","            return np.sum(np.abs(np.diff(values)))\n","\n","        # Function to process each stock ID\n","        def process_stock_train(path, level):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = train_target[train_target['stock_id'] == st_id]\n","            target_st.index = target_st[\"time_id\"]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Aggregate features\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_sum_abs_diff]}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","            st_sad_ask_price = st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_sum_abs_diff]}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","            st_sad_ask_size = st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_sum_abs_diff]}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","            st_sad_bid_price = st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_sum_abs_diff]}).rename(columns={bid_size: 'sad_bid_size'})\n","            st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","            st_sad_bid_size = st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return (st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size)\n","\n","\n","        # Function to process each stock ID\n","        def process_stock_inference(path, level):\n","\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Aggregate features\n","            st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_sum_abs_diff]}).rename(columns={ask_price: 'sad_ask_price'})\n","            st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","            st_sad_ask_price = st_sad_ask_price\n","\n","            st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_sum_abs_diff]}).rename(columns={ask_size: 'sad_ask_size'})\n","            st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","            st_sad_ask_size = st_sad_ask_size\n","\n","            st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_sum_abs_diff]}).rename(columns={bid_price: 'sad_bid_price'})\n","            st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","            st_sad_bid_price = st_sad_bid_price\n","\n","            st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_sum_abs_diff]}).rename(columns={bid_size: 'sad_bid_size'})\n","            st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","            st_sad_bid_size = st_sad_bid_size\n","\n","            return (st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size)\n","\n","\n","\n","        if self.ml_stage == 'training':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_train)(path, level) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_inference)(path, level) for path in subset_paths)\n","\n","        # Initialize empty DataFrames to store the results\n","        bk_price_size_sad_2 = {\n","            'st_sad_ask_price' + str(level): pd.DataFrame(),\n","            'st_sad_ask_size' + str(level): pd.DataFrame(),\n","            'st_sad_bid_price' + str(level): pd.DataFrame(),\n","            'st_sad_bid_size' + str(level): pd.DataFrame()\n","        }\n","\n","        # Collect results into the final DataFrames\n","        for result in results:\n","            if result is not None:\n","                st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size = result\n","                bk_price_size_sad_2['st_sad_ask_price' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_ask_price' + str(level)], st_sad_ask_price], axis=0)\n","                bk_price_size_sad_2['st_sad_ask_size' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_ask_size' + str(level)], st_sad_ask_size], axis=0)\n","                bk_price_size_sad_2['st_sad_bid_price' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_bid_price' + str(level)], st_sad_bid_price], axis=0)\n","                bk_price_size_sad_2['st_sad_bid_size' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_bid_size' + str(level)], st_sad_bid_size], axis=0)\n","\n","        # The result is stored in bk_price_size_sad\n","        return bk_price_size_sad_2\n","\n","\n","\n","    def merge_bk_price_size_sad_2(self,feat_df,bk_price_size_sad_2):\n","\n","        ####### bk_price_size_sad_2\n","\n","        bk_price_size_sad_2['st_sad_ask_price2'].rename(columns={'sad_ask_price':'sad_ask_price2'}, inplace=True)\n","        bk_price_size_sad_2['st_sad_ask_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad_2['st_sad_ask_price2']], axis=1)\n","\n","        bk_price_size_sad_2['st_sad_ask_size2'].rename(columns={'sad_ask_size':'sad_ask_size2'}, inplace=True)\n","        bk_price_size_sad_2['st_sad_ask_size2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad_2['st_sad_ask_size2']], axis=1)\n","\n","        bk_price_size_sad_2['st_sad_bid_price2'].rename(columns={'sad_bid_price':'sad_bid_price2'}, inplace=True)\n","        bk_price_size_sad_2['st_sad_bid_price2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_price_size_sad_2['st_sad_bid_price2']], axis=1)\n","        del bk_price_size_sad_2\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","    def create_bk_size_price_corr_2(self,):\n","\n","        level=2\n","        subset_paths = self.book_paths\n","\n","        bid_price = f\"bid_price{level}\"\n","        ask_price = f\"ask_price{level}\"\n","        bid_size = f\"bid_size{level}\"\n","        ask_size = f\"ask_size{level}\"\n","\n","\n","        train_target = self.train\n","\n","        from joblib import Parallel, delayed\n","        import pandas as pd\n","\n","        # Function to process each stock ID\n","        def process_stock_train(path, level):\n","            st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","            target_st = train_target[train_target['stock_id'] == st_id]\n","            target_st.index = target_st[\"time_id\"]\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            # Reindex with target_st time_ids and apply forward and backward fill\n","            for key in corr_dict.keys():\n","                corr_dict[key] = corr_dict[key].reindex(target_st['time_id'].values).ffill().bfill()\n","\n","            return corr_dict\n","\n","\n","\n","        # Function to process each stock ID\n","        def process_stock_inference(path, level):\n","\n","            book_train_st = pd.read_parquet(path)\n","\n","            # Calculate correlations and drop the multi-level index\n","            corr_dict = {}\n","            corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","            corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","            return corr_dict\n","\n","\n","        if self.ml_stage == 'training':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_train)(path, level) for path in subset_paths)\n","        elif self.ml_stage == 'inference':\n","          # Run the function in parallel\n","          results = Parallel(n_jobs=-1)(delayed(process_stock_inference)(path, level) for path in subset_paths)\n","\n","        # Initialize empty DataFrames to store the results\n","        bk_size_price_corr_2 = {\n","            'st_bs_bp_corr' + str(level): pd.DataFrame(),\n","            'st_bs_as_corr' + str(level): pd.DataFrame(),\n","            'st_bs_ap_corr' + str(level): pd.DataFrame(),\n","            'st_bp_as_corr' + str(level): pd.DataFrame(),\n","            'st_bp_ap_corr' + str(level): pd.DataFrame(),\n","            'st_as_ap_corr' + str(level): pd.DataFrame()\n","        }\n","\n","        # Collect results into the final DataFrames\n","        for result in results:\n","            if result is not None:\n","                for key in result.keys():\n","                    bk_size_price_corr_2[key + str(level)] = pd.concat([bk_size_price_corr_2[key + str(level)], result[key]], axis=0)\n","\n","        return bk_size_price_corr_2\n","\n","\n","\n","\n","    def merge_bk_size_price_corr_2(self,feat_df,bk_size_price_corr_2):\n","\n","        ###### bk_size_price_corr_2\n","\n","        bk_size_price_corr_2['st_bs_bp_corr2'].rename(columns={0:'bs_bp_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_bp_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bs_bp_corr2']], axis=1)\n","\n","        bk_size_price_corr_2['st_bs_as_corr2'].rename(columns={0:'bs_as_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_as_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bs_as_corr2']], axis=1)\n","\n","        bk_size_price_corr_2['st_bs_ap_corr2'].rename(columns={0:'bs_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bs_ap_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bs_ap_corr2']], axis=1)\n","\n","        bk_size_price_corr_2['st_bp_as_corr2'].rename(columns={0:'bp_as_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bp_as_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bp_as_corr2']], axis=1)\n","\n","        bk_size_price_corr_2['st_bp_ap_corr2'].rename(columns={0:'bp_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_bp_ap_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_bp_ap_corr2']], axis=1)\n","\n","        bk_size_price_corr_2['st_as_ap_corr2'].rename(columns={0:'as_ap_corr2'}, inplace=True)\n","        bk_size_price_corr_2['st_as_ap_corr2'].reset_index(drop=True, inplace=True)\n","        feat_df = pd.concat([feat_df, bk_size_price_corr_2['st_as_ap_corr2']], axis=1)\n","        del bk_size_price_corr_2\n","\n","        return feat_df\n","\n","\n","\n","\n","\n","\n","\n","    def merge_clustering_features_to_training_data(self,feat_df,final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df ):\n","        # Merge clustering features with training data\n","\n","        feat_df = pd.concat([feat_df, final_sum_stats_target_vol_df], axis=1)\n","        feat_df = pd.concat([feat_df, final_robust_sum_stats_target_vol_df], axis=1)\n","        feat_df = pd.concat([feat_df, final_pear_corr_target_vol_df], axis=1)\n","\n","        return feat_df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNubpbc_43nD"},"outputs":[],"source":["\n","# class create_training_n_inference_liquidity_features(object):\n","#   def __init__(self,ml_stage='training' ,):\n","#       self.ml_stage = ml_stage\n","#       if self.ml_stage == 'training':\n","#           os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/')\n","#           self.trade_paths = sorted(glob.glob('trade_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","#           self.book_paths = sorted(glob.glob('book_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","#           self.unique_stock_ids = []\n","#           for path in self.book_paths:\n","#               self.unique_stock_ids.append(int(path.split('=')[1]))\n","#           self.train = pd.read_csv('train.csv')\n","#           self.train_target = pd.read_csv('train.csv')\n","#           self.all_uniq_time_ids = pd.DataFrame({'time_id':self.train['time_id'].unique()})\n","#       elif self.ml_stage == 'inference':\n","#           os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data/')\n","#           self.trade_paths = sorted(glob.glob('trade_test_sub.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","#           self.book_paths = sorted(glob.glob('book_test_sub.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","#           self.test_sub = train = pd.read_csv('test_sub.csv')\n","#           self.train = pd.read_csv('train.csv')\n","#           self.train_target = pd.read_csv('train.csv')\n","#           self.all_uniq_time_ids = pd.DataFrame({'time_id':self.train['time_id'].unique()})\n","#       else:\n","#           print('invalid ml_stage param')\n","#       return\n","\n","def find_equilibrium_price(book_data, lvl, iterations=22):\n","    loga2 = np.array(book_data['log_ask_price2'])\n","    loga1 = np.array(book_data['log_ask_price1'])\n","    logb1 = np.array(book_data['log_bid_price1'])\n","    logb2 = np.array(book_data['log_bid_price2'])\n","\n","    asize2 = np.array(book_data['ask_size2'])\n","    asize1 = np.array(book_data['ask_size1'])\n","    bsize1 = np.array(book_data['bid_size1'])\n","    bsize2 = np.array(book_data['bid_size2'])\n","\n","    ub = loga1\n","    lb = logb1\n","\n","    s = (-1)**lvl\n","    for iter in range(iterations):\n","        mid_price = (ub + lb)/2.0\n","        inv_diff_a2 = 1.0/( 1000*( mid_price - loga2 ) )\n","        inv_diff_a1 = 1.0/( 1000*( mid_price - loga1 ) )\n","        inv_diff_b1 = 1.0/( 1000*( mid_price - logb1 ) ) # negative\n","        inv_diff_b2 = 1.0/( 1000*( mid_price - logb2 ) ) # negative\n","\n","        f  = -(   ( bsize2*inv_diff_b2**(lvl+1) + bsize1*inv_diff_b1**(lvl+1) )\n","              + s*( asize1*inv_diff_a1**(lvl+1) + asize2*inv_diff_a2**(lvl+1) ) )\n","\n","        # when lvl = even, f is positive when buy side missing volume is larger than sell side missing volume\n","        # when lvl = even, f is negative when sell side missing volume is larger than buy side missing volume\n","        # when lvl = odd, f is positive when sell side missing volume is larger than buy side missing volume and vice versa\n","\n","        dub = - (ub-lb)/2.0*(f>=0)\n","        dlb =   (ub-lb)/2.0*(f< 0)\n","\n","        # when f is positive, mid price moves towards buy side (bid_price) by reducing the upper bound\n","        # when f is negative, mid price moves towards sell side (ask_price) by increasing the lower bound\n","        ub = ub + dub\n","        lb = lb + dlb\n","\n","    equilibrium_price = (ub + lb)/2.0\n","\n","    return equilibrium_price\n","\n","\n","\n","def diff(list_stock_prices):\n","    return list_stock_prices.diff()\n","\n","\n","\n","@jit()\n","def bucketized_summed_data(seconds_arr, time_id, data, buk_width, n_buks, time_ids_size):\n","    z = np.zeros( (time_ids_size,n_buks) ) # 30 buckets for 600 seconds (10 minutes)\n","\n","    t_id  = 0\n","    for s in range(seconds_arr.shape[0]): # seconds.shape[0] is total size of the seconds column i.e. total rows in seconds column\n","\n","        if time_id[s] != time_id[max(s-1,0)]:\n","            t_id = t_id + 1\n","\n","        z[t_id, int(seconds_arr[s]//buk_width)] += data[s]\n","\n","    return z\n","\n","\n","@jit(nopython=True)\n","def end_bucket(buk_width, buk_sum:float, buk_weight:float, last_val, last_weight, last_time)->float:\n","    dt = buk_width - last_time%buk_width\n","\n","    buk_weight += 1.0*last_weight*dt\n","    buk_sum    += 1.0*last_weight*dt*last_val\n","\n","    return float(buk_sum/(buk_weight + 1e-8))\n","\n","\n","\n","@jit()\n","def bucketized_time_weighted_avg_data(seconds_arr, time_id_arr, data,weights, buk_width, n_buks, time_ids_size):\n","\n","    z = np.zeros( (time_ids_size,n_buks) )\n","\n","    prev_time   = 0\n","    prev_weight = 0.0\n","    prev_val    = 0.0\n","\n","    buk_sum = 0.0\n","    buk_weight = 0.0\n","\n","    t_id  = 0  # time id\n","    buk = 0  # bucket id\n","    for idx in range(seconds_arr.shape[0]): # seconds.shape[0] is total size of the seconds column i.e. total rows in seconds column\n","\n","        if time_id_arr[idx] != time_id_arr[max(idx-1,0)]: # transition to new time id\n","            z[t_id, buk] = float(end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time))\n","            buk += 1\n","\n","            while buk < z.shape[1]:\n","                z[t_id, buk] = prev_val\n","                buk += 1\n","            t_id += 1\n","            buk = 0\n","\n","            prev_time  = 0\n","            buk_sum    = 0.0\n","            buk_weight = 0.0\n","\n","        if int(seconds_arr[idx]//buk_width) != int(prev_time//buk_width): # transition to new bucket\n","\n","            z[t_id, buk] = float(end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time)) # end the previous bucket\n","            buk += 1 # move to next bucket\n","\n","            while buk < seconds_arr[idx]//buk_width:\n","                z[t_id, buk] = prev_val\n","                buk += 1\n","\n","            prev_time  = buk_width*(seconds_arr[idx]//buk_width)\n","            buk_sum    = 0.0\n","            buk_weight = 0.0\n","\n","        buk_sum    += prev_val*prev_weight*(seconds_arr[idx] - prev_time)  # in the same bucket\n","        buk_weight +=          prev_weight*(seconds_arr[idx] - prev_time)  # in the same bucket\n","\n","        prev_time   = seconds_arr[idx] # in the same bucket\n","        prev_val    = data[idx] # in the same bucket\n","        prev_weight = weights[idx] # in the same bucket\n","\n","    z[t_id, buk] = end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time)\n","\n","    for buk in range(buk+1, z.shape[1]): # all buckets of the last time id\n","        z[t_id, buk] = prev_val\n","\n","    return z\n","\n","\n","\n","def create_stock_data(st_id, dset):\n","\n","    cols = ['st_id', 'time_id', 'seconds_in_bucket']\n","\n","    ############################## BOOK DATA ##########################################\n","\n","    book_data = pd.read_parquet(os.path.join(data_dir, 'book_{}.parquet/stock_id={}/'.format(dset, st_id)))\n","\n","    book_data['st_id'] = st_id\n","\n","    columns = cols + [col for col in book_data.columns if col not in cols]\n","    book_data = book_data[columns]\n","    # columns = 'st_id', 'time_id', 'seconds_in_bucket', 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2'\n","\n","    # volume\n","    book_data['ask_volume1'] = book_data['ask_price1']*book_data['ask_size1']\n","    book_data['ask_volume2'] = book_data['ask_price2']*book_data['ask_size2']\n","    book_data['bid_volume1'] = book_data['bid_price1']*book_data['bid_size1']\n","    book_data['bid_volume2'] = book_data['bid_price2']*book_data['bid_size2']\n","\n","    # becomes zero centered, reduces skew and kurtosis slightly bringing it slightly closer to normal for right skewed data, It is increases/worsens\n","    # skew and kurtosis for left-skewed data\n","    # correct way is to use box-cox transformation, variance stabilization\n","    # Most stocks are right skewed only a few are left skewed?\n","    book_data['log_ask_price1'] = np.log( book_data['ask_price1'] )\n","    book_data['log_ask_price2'] = np.log( book_data['ask_price2'] )\n","    book_data['log_bid_price1'] = np.log( book_data['bid_price1'] )\n","    book_data['log_bid_price2'] = np.log( book_data['bid_price2'] )\n","\n","    # redefining WAP using log prices\n","    book_data['wap1_log_price'] = ( book_data['log_bid_price1'] * book_data['ask_size1'] + book_data['log_ask_price1'] * book_data['bid_size1'] ) / (book_data['bid_size1'] + book_data['ask_size1'])\n","    book_data['wap2_log_price'] = ( book_data['log_bid_price2'] * book_data['ask_size2'] + book_data['log_ask_price2'] * book_data['bid_size2'] ) / (book_data['bid_size2'] + book_data['ask_size2'])\n","\n","    # Find equilibrium price at which trades are likely to happen\n","    # This price minimizes the missing total volume from buy and sell side\n","    book_data['wap_eqi_price0'] = find_equilibrium_price( book_data, lvl=0)\n","    book_data['wap_eqi_price1'] = find_equilibrium_price( book_data, lvl=1)\n","    book_data['wap_eqi_price2'] = find_equilibrium_price( book_data, lvl=2)\n","    # book_data['wap_eqi_price3'] = find_equilibrium_price( book_data, lvl=3)\n","    # book_data['wap_eqi_price4'] = find_equilibrium_price( book_data, lvl=4)\n","\n","\n","    # equilibrium price has converged closer to\n","    book_data['liquidity0'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_bid_price1']) )\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_bid_price2']) )\n","                - book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_ask_price1']) )\n","                - book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_ask_price2']) )\n","    )\n","\n","    # liquidity 0 and liquidity 1 are negatively correlated with each other, if one has prices moving towards buy side, the other has price moving towards sell side\n","    book_data['liquidity1'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price1']) )\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price2']) )\n","                - book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price1']) )\n","                - book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price2']) )\n","    )\n","\n","    book_data['liquidity2'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_bid_price1']) )**2\n","                + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_bid_price2']) )**2\n","                + book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_ask_price1']) )**2\n","                + book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_ask_price2']) )**2\n","    )\n","\n","    book_data['spread']     = book_data['log_ask_price1'] - book_data['log_bid_price1']\n","    book_data['inv_spread'] = (book_data['log_ask_price1'] - book_data['log_bid_price1'])**-2 # inverse of spread has the effect of amplifying low values and diminishing high values\n","    book_data['log_spread'] = book_data['spread'].apply(np.log) # log of spread has the effect of amplifying low values and diminishing high values. It can normalize right skewed data\n","    book_data['log_spread2'] = np.log(book_data['log_ask_price2'] - book_data['log_bid_price2'])\n","\n","    book_data['book_size1'] = book_data['ask_volume1'] + book_data['bid_volume1']\n","    book_data['book_size'] = book_data['ask_volume1'] + book_data['bid_volume1'] + book_data['ask_volume2'] + book_data['bid_volume2']\n","\n","    # difference betweeen ask's level 1 and level 2 liquidity\n","    # posiitive means level 2 ask liquidity is higher than level 1 ask liquidity\n","    book_data['ask_liq1_diff'] = (\n","                  book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price1']) )**1\n","              -  book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price2']) )**1\n","    )\n","\n","    # difference betweeen bid's level 1 and level 2 liquidity\n","    # posiitive means level 1 bid liquidity is higher than level 2 bid liquidity\n","    book_data['bid_liq1_diff'] = (\n","                  book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price1']) )**1\n","              -  book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price2']) )**1\n","    )\n","\n","    # simple returns on prices wap1_log_price,wap2_log_price,wap_eqi_price0,wap_eqi_price1\n","    book_data['wap1_log_price_ret' ] = book_data.groupby(by = ['time_id'])['wap1_log_price'].apply(diff).fillna(0).values\n","    book_data['wap2_log_price_ret' ] = book_data.groupby(by = ['time_id'])['wap2_log_price'].apply(diff).fillna(0).values\n","    book_data['wap_eqi_price0_ret'] = book_data.groupby(by = ['time_id'])['wap_eqi_price0'].apply(diff).fillna(0).values\n","    book_data['wap_eqi_price1_ret'] = book_data.groupby(by = ['time_id'])['wap_eqi_price1'].apply(diff).fillna(0).values\n","\n","    # this indicates the changes in level 2 wap when level 1 wap does NOT change\n","    # This happens when all orders in level 1 are filled and new orders are placed in level 2\n","    # indication of liquidity as prices in level 2 are moving towards level 1\n","    # Aggressive Market Orders, Imbalance in Market Depth, Execution of Large Orders, Liquidity Changes:\n","    book_data['wap2_log_price_ret_changes_n_wap1_log_price_ret_constant'] = book_data['wap2_log_price_ret' ]*(book_data['wap1_log_price_ret' ]==0)\n","\n","    # variance stabilization of right skewed data.\n","    book_data['log_liquidity1'] = np.log(book_data['liquidity1'])\n","    book_data['log_liquidity2'] = np.log(book_data['liquidity2'])\n","\n","    # simple returns on liquidity / first order changes in liquidity\n","    book_data['log_liquidity1_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity1'].apply(diff).fillna(0).values\n","    book_data['log_liquidity2_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity2'].apply(diff).fillna(0).values\n","    # simple returns on log_spread / first order changes in log_spread\n","    book_data['log_spread_ret'] = book_data.groupby(by = ['time_id'])['log_spread'].apply(diff).fillna(0).values\n","\n","    # wap1 price returns when liquidity1 is positive/increases and negative/decreases\n","    book_data['wap1_log_price_ret_pos_log_liq_ret'] = (book_data['log_liquidity1_ret']>0)*book_data['wap1_log_price_ret']\n","    book_data['wap1_log_price_ret_neg_log_liq_ret'] = (book_data['log_liquidity1_ret']<0)*book_data['wap1_log_price_ret']\n","\n","\n","    ids = np.array(book_data[['st_id', 'time_id']]) # single stock and all time_ids and seconda_in_bucket\n","    ids = np.unique(ids, axis=0)\n","    book_n_trade_data = {}\n","    book_n_trade_data['time_id'] = ids[:,1:2]\n","\n","    # bucketized data for book data\n","    # Amount of wap1 price movements in a time bucket of 30 seconds, i.e. wap1 returns volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap1_log_price_ret']),\n","                                    20, 30, ids.shape[0])\n","\n","    # Amount of absolute wap1 price movements in a time bucket of 30 seconds, i.e. ahsolute wap1 returns volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.abs(np.array(book_data['wap1_log_price_ret'])),\n","                                    20, 30, ids.shape[0])\n","\n","    # Amount of absolute wap2 price movements in a time bucket of 30 seconds, i.e.  ahsolute wap2 returns volatitlity in bucket\n","    book_n_trade_data['wap2_log_price_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.abs(np.array(book_data['wap2_log_price_ret'])),\n","                                    20, 30, ids.shape[0])\n","\n","      # wap1 returns variance/ squared volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap1_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])\n","    # wap2 returns variance/ squared volatitlity in bucket\n","    book_n_trade_data['wap2_log_price_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap2_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])\n","\n","    # squared wap1 returns volatitlity in bucket\n","    book_n_trade_data['wap1_log_price_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap1_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])**0.5\n","    # squared wap2 returns volatitlity in bucket\n","    book_n_trade_data['wap2_log_price_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap2_log_price_ret'])**2,\n","                                    20, 30, ids.shape[0])**0.5\n","\n","    # squared wap2_log_price_ret_changes_n_wap1_log_price_ret_constant volatitlity in bucket\n","    book_n_trade_data['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                    np.array(book_data['time_id']),\n","                                    np.array(book_data['wap2_log_price_ret_changes_n_wap1_log_price_ret_constant'])**2,\n","                                    20, 30, ids.shape[0])**0.5\n","\n","    # equilibrium price returns absolute volatitlity in bucket\n","    book_n_trade_data['wap_eqi_price0_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.abs(np.array(book_data['wap_eqi_price0_ret'])),\n","                                      20, 30, ids.shape[0])\n","\n","    # squared equilibrium price returns volatitlity in bucket\n","    book_n_trade_data['wap_eqi_price0_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap_eqi_price0_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","\n","    # volatitlity in wap1_log_price_ret when liquidity1 return is positive. i.e. increases\n","    book_n_trade_data['wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap1_log_price_ret_pos_log_liq_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","    # volatitlity in wap1_log_price_ret when liquidity1 is negative. i.e. decreases\n","    book_n_trade_data['wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap1_log_price_ret_neg_log_liq_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","\n","    # squared wap equilibrium price 1 returns volatitlity in bucket\n","    book_n_trade_data['wap_eqi_price1_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['wap_eqi_price1_ret'])**2,\n","                                      20, 30, ids.shape[0])**0.5\n","\n","    book_n_trade_data['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array((book_data['log_liquidity2_ret']*book_data['wap_eqi_price1_ret'])**2 ),\n","                                      20, 30, ids.shape[0])\n","\n","    # squared wap equilibrium price 1 returns volatitlity in bucket amplified (> 1) by positive/increasing liquidity returns (through exponent)\n","    # and diminished ( < 1) by negative/decreasing liquidity returns (through exponent)\n","    book_n_trade_data['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(( np.exp(book_data['log_liquidity1_ret'])*book_data['wap_eqi_price1_ret'])**2 ),\n","                                      20, 30, ids.shape[0])\n","    # copy of above\n","    book_n_trade_data['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(( np.exp(book_data['log_liquidity1_ret'])*book_data['wap_eqi_price1_ret'])**2 ),\n","                                      20, 30, ids.shape[0])\n","    # variance/ squared volatitliy of wap1 price returns per unit of spread\n","    # large value indicates volatilty\n","    book_n_trade_data['wap1_log_price_ret_per_spread_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(( book_data['wap1_log_price_ret']/book_data['spread'])**2 ),\n","                                      20, 30, ids.shape[0])\n","    # variance/ squared volatitliy of wap1 price returns per unit of liquidity\n","    # small value indicates volatilty?\n","    book_n_trade_data['wap1_log_price_ret_per_liq2_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array( ( book_data['wap1_log_price_ret'])**2/book_data['liquidity2'] ),\n","                                      20, 30, ids.shape[0])\n","\n","    # measure of variance/ squared volatility of liquidity1 returns\n","    book_n_trade_data['log_liquidity1_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['log_liquidity1_ret'])**2,\n","                                      20, 30, ids.shape[0])\n","\n","    # measure of variance/ squared volatility of liquidity2 returns\n","    book_n_trade_data['log_liquidity2_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['log_liquidity2_ret'])**2,\n","                                      20, 30, ids.shape[0])\n","\n","    # measure of variance/ squared volatility of log spread returns\n","    book_n_trade_data['log_spread_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                      np.array(book_data['log_spread_ret'])**2,\n","                                      20, 30, ids.shape[0])\n","\n","    # counting number of data points available in each time bucket\n","    book_n_trade_data['book_delta_count_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array(book_data['wap1_log_price_ret']*0+1.0),\n","                                        20, 30, ids.shape[0])\n","\n","    # time weighted average of wap1_log_price in each time bucket\n","    book_n_trade_data['wap1_log_price_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap1_log_price']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","    # time weighted average of wap2_log_price in each time bucket\n","    book_n_trade_data['wap2_log_price_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap2_log_price']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","    # time weighted average of wap_eqi_price0 equilibrium price in each time bucket\n","    book_n_trade_data['wap_eqi_price0_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap_eqi_price0']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","    # time weighted average of wap_eqi_price1 equilibrium price in each time bucket\n","    book_n_trade_data['wap_eqi_price1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.array(book_data['wap_eqi_price1']),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0])\n","\n","\n","    # filter out the extremely high and low prices of wap1_log_price by amplifying with postiive and negative exponential of wap1_log_price\n","    # apply time weighted average to the amplified wap1_log_price\n","    # what may be the physical meaning?\n","    book_n_trade_data['wap1_log_price_amp_max_wavg'] = np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp( 4000*np.array(book_data['wap1_log_price'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","    book_n_trade_data['wap1_log_price_amp_min_wavg'] = -np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp(-4000*np.array(book_data['wap1_log_price'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","    # amplification of the difference between max and min\n","    book_n_trade_data['wavg_wap1_log_price_amp_diff']  = np.exp(book_n_trade_data['wap1_log_price_amp_max_wavg'] - book_n_trade_data['wap1_log_price_amp_min_wavg'])\n","\n","    # filter out the extremely high and low prices of wap_eqi_price0 by amplifying with postiive and negative exponential of wap_eqi_price0\n","    # apply time weighted average to the amplified wap_eqi_price0\n","    book_n_trade_data['wap_eqi_price0_amp_max_wavg'] = np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp( 4000*np.array(book_data['wap_eqi_price0'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","\n","    book_n_trade_data['wap_eqi_price0_amp_min_wavg'] = -np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                                  np.exp(-4000*np.array(book_data['wap_eqi_price0'])),\n","                                                  np.ones((book_data.shape[0])),\n","                                                  20, 30, ids.shape[0]) )/4000\n","    # amplification of the difference between max and min\n","    book_n_trade_data['wavg_wap_eqi_price0_amp_diff']  = np.exp(book_n_trade_data['wap_eqi_price0_amp_max_wavg'] - book_n_trade_data['wap_eqi_price0_amp_min_wavg'])\n","\n","    del book_n_trade_data['wap1_log_price_amp_max_wavg'], book_n_trade_data['wap1_log_price_amp_min_wavg']\n","    del book_n_trade_data['wap_eqi_price0_amp_max_wavg'], book_n_trade_data['wap_eqi_price0_amp_min_wavg']\n","\n","    book_n_trade_data['liquidity1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity1'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    book_n_trade_data['liquidity2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity2'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    book_n_trade_data['root_liquidity2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['liquidity2']))**0.5,\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    # time weighted average of spread in each time bucket\n","    book_n_trade_data['spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['spread'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of inverse spread in each time bucket\n","    book_n_trade_data['inv_spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['spread']))**-1,\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of log spread in each time bucket\n","    book_n_trade_data['log_spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.log(np.array((book_data['spread']))),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of log spread 2 in each time bucket\n","    book_n_trade_data['log_spread2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['log_spread2'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of book size1 in each time bucket\n","    book_n_trade_data['book_size1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['book_size1'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of book size in each time bucket\n","    book_n_trade_data['book_size_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                      np.array(book_data['time_id']),\n","                                        np.array((book_data['book_size'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","\n","    ############################## TRADE DATA ##########################################\n","\n","    trade_data =  pd.read_parquet(os.path.join(data_dir,'trade_{}.parquet/stock_id={}'.format( dset, st_id)))\n","    trade_data['trade_volume'] = trade_data['size']*trade_data['price']\n","\n","    # bucketized trade volume\n","    book_n_trade_data['trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']),\n","                                        20, 30, ids.shape[0])\n","    # bucketized root of trade volume\n","    book_n_trade_data['sqrt_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**.5),\n","                                        20, 30, ids.shape[0])\n","    # bucketized cube root of trade volume\n","    book_n_trade_data['cube_root_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**(1/3)),\n","                                        20, 30, ids.shape[0])\n","\n","    # bucketized square of cube root of trade volume\n","    book_n_trade_data['trade_volume_p2/3_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**(2/3)),\n","                                        20, 30, ids.shape[0])\n","\n","    # bucketized quart root of trade volume\n","    book_n_trade_data['quart_root_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**.25),\n","                                        20, 30, ids.shape[0])\n","\n","    # count the number of trades in each time bucket\n","    book_n_trade_data['trade_count_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","                                      np.array(trade_data['time_id']),\n","                                        np.array(trade_data['trade_volume']**0),\n","                                        20, 30, ids.shape[0])\n","\n","\n","    # trade volume per unit of liquidity1\n","    book_n_trade_data['trade_volume_per_liquidity1_wavg_buks'] = book_n_trade_data['trade_volume_buks']/book_n_trade_data['liquidity1_wavg']\n","    book_n_trade_data['trade_volume_per_liquidity2_wavg_buks'] = book_n_trade_data['trade_volume_buks']/book_n_trade_data['liquidity2_wavg']\n","\n","    # time weighted average of difference betweeen ask's level 1 and level 2 liquidity\n","    book_n_trade_data['ask_liq1_diff_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                        np.array(book_data['time_id']),\n","                                        np.array((book_data['ask_liq1_diff'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","    # time weighted average of difference betweeen bid's level 1 and level 2 liquidity\n","    book_n_trade_data['bid_liq1_diff_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","                                        np.array(book_data['time_id']),\n","                                        np.array((book_data['bid_liq1_diff'])),\n","                                        np.ones((book_data.shape[0])),\n","                                        20, 30, ids.shape[0])\n","\n","    return book_n_trade_data\n","\n","\n","\n","\n","@jit()\n","def identify_missing_time_ids(all_time_ids, st_time_ids): # all_time_ids is all unique time_ids from all stocks, st_time_ids is time_ids for particular single stock\n","    j = 0\n","    z = 1 == np.zeros(  all_time_ids.shape[0]) # set all unique time_ids to False\n","    for i in range(st_time_ids.shape[0]):\n","        while all_time_ids[j] != st_time_ids[i]: # missing time id in the stock\n","            z[j] = False # set the missing time id index in all unique time ids array z to False\n","            j = j+1\n","            if j >= all_time_ids.shape[0]:\n","                return z\n","        z[j] = True\n","        j = j+1\n","    return z\n","\n","\n","\n","\n","def create_dataSet(st_ids,dset):\n","\n","    st_ids = sorted(st_ids)\n","\n","    print('st_ids',st_ids)\n","\n","    # a list contains all stock data each element of list is a dictionary of features for a particular stock\n","    all_stock_data = Parallel(n_jobs = os.cpu_count() - 5)( delayed(create_stock_data)(st_id, dset) for st_id in st_ids)\n","\n","    final_data = {}\n","\n","    # get all unique time ids from all stocks. This is helpful to fill missing time ids.\n","    t_ids = sum([list(ss['time_id']) for ss in all_stock_data], [] )\n","    t_ids = list(np.unique(t_ids))\n","\n","    num_buks = 30\n","    t_ids_size = len(t_ids)\n","    st_ids_size = len(st_ids)\n","\n","    final_data['time_ids' ] = np.array(t_ids)\n","    final_data['stock_ids'] = np.array(st_ids)\n","\n","\n","    for key in all_stock_data[0].keys(): # common columns (features) to all stocks\n","        if key == 'time_id':\n","            continue\n","\n","        Z = np.zeros(( t_ids_size, st_ids_size, num_buks))\n","\n","        for st in range(st_ids_size):\n","            ss = all_stock_data[st]\n","\n","            #ts = index_into_set(np.array(time_ids), ss['time_id']).astype(int)\n","\n","            b = identify_missing_time_ids(np.array(t_ids), ss['time_id']) # all unique time ids from all stocks and time ids of a particular stock are input\n","\n","            #print(b)\n","            #print(b.shape)\n","\n","            Z[ b, st, :] = ss[key] # fill with features for avaialble time ids\n","\n","            Z[~b, st, :] = np.nanmean(ss[key]) # fill with mean of features for missing time ids\n","\n","            Z[:,st,:][np.isnan(Z[:,st,:])] = np.nanmean(Z[:,st,:]) # fill with mean of features for missing time ids and any missing bins\n","\n","            #del ss[key]\n","\n","        final_data[key] = Z\n","\n","        gc.collect()\n","\n","\n","    del all_stock_data\n","    gc.collect()\n","\n","\n","    # arbitrarily weighted average of wap1_log_price_ret_abs_vol_buks and wap2_log_price_ret_abs_vol_buks\n","    final_data['wap1_log_price_ret_vol_buks'] = ( final_data['wap1_log_price_ret_vol_buks']**2 + .25*final_data['wap2_log_price_ret_vol_buks']**2)**0.5\n","\n","    return final_data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGwPQBLYMHV1"},"outputs":[],"source":["def get_cohesion_features(train_buckets, final_features, ffrom=0):\n","    wap1_log_price_ret_buks = train_buckets['wap1_log_price_ret_buks'] # shape of (3830,112,30)\n","\n","    if ffrom > 0:\n","        buk = f'_:{ffrom}'\n","    else:\n","        buk = '_:0'\n","\n","    # variance along time_id axis, mean along bucket axis and then square root\n","    # basically standard deviation of wap1_log_price_ret_buks in each stock. This is like (overall) volatility over entire time period for each stock.\n","    stocks_overall_wap1_log_price_ret_vol = np.mean( np.var(wap1_log_price_ret_buks, 0,keepdims=True), 2, keepdims=True)**0.5 # shape of (1,112,1)\n","\n","    # normalize the variance of wap1_log_price_ret_buks by overall volatility, assume that mean of wap1_log_price_ret_buks is zero\n","    wap1_log_price_ret_normalized = wap1_log_price_ret_buks[:,:,ffrom:]/stocks_overall_wap1_log_price_ret_vol # shape of (3830,112,30)\n","\n","    # variance of wap1_log_price_ret_normalized along stock id axis, mean along bucket axis and then square root, shape of (3830,1,1) and then normalized by wap1_log_price_ret_vol (shape 3830 x 112, 1). dim 1 is broadcasted to dim 112\n","    # multiply by stocks_overall_wap1_log_price_ret_vol (shape = (1,112,1) ) to get the original variance of wap1_log_price_ret_buks to get final shape of (3830,112,1)\n","    # multiplication by stocks_overall_wap1_log_price_ret_vol (overall volatility) is the reverse of normalization\n","    # Volatility across stocks at each time id divided by volatility at each stocks and time id = factor of overall volatility across stocks contributed by each stock in each time id.\n","    # This is multiplied by overall volatility across time for each stock.\n","    # It is just a scalar giving importance to amount of variance across all time.\n","    # ati = across time, ast = across stock,     # shape of (3830,112,1),\n","    final_features['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol'+ buk] = stocks_overall_wap1_log_price_ret_vol*np.mean(  np.var( wap1_log_price_ret_normalized, 1, keepdims=True), 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","    # shape of (3830,112,1), market is np.mean(wap1_log_price_ret_normalized, 1, keepdims=True). IT is just mean over all stock ids.\n","    final_features['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol'+ buk] = stocks_overall_wap1_log_price_ret_vol*np.mean( (np.mean(wap1_log_price_ret_normalized, 1, keepdims=True) )**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","\n","    # deviation from market is np.mean(wap1_log_price_ret_normalized, 1, keepdims=True)  minus wap1_log_price_ret_normalized\n","    final_features['wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol' + buk] = stocks_overall_wap1_log_price_ret_vol*np.mean( (np.mean(wap1_log_price_ret_normalized, 1, keepdims=True) - wap1_log_price_ret_normalized)**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","\n","\n","\n","\n","\n","\n","def cluster_agg(x, clusters, agg_fun):\n","    r = 0*x\n","\n","    for k in range(np.max(clusters)+1):\n","        z = agg_fun(x[:,clusters==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","        r[:,clusters==k,:] = np.repeat(z, repeats=int(np.sum(clusters==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","    return r\n","\n","\n","\n","\n","def get_misc_features(train_buckets, final_features):\n","\n","    trade_volume_buks    = train_buckets['trade_volume_buks']\n","    wap1_log_price_ret_vol_buks    = train_buckets['wap1_log_price_ret_vol_buks']\n","    sqrt_trade_volume_buks   = train_buckets['sqrt_trade_volume_buks']\n","    liquidity2_wavg   = train_buckets['liquidity2_wavg']\n","    log_spread2_wavg  = train_buckets['log_spread2_wavg']\n","\n","    # average out along the time ids and buckets axis\n","    stocks_overall_trade_volume  = np.nanmean( trade_volume_buks, (0,2), keepdims=True) # shape of (1,112,1)\n","    stocks_overall_sqrt_trade_volume = np.nanmean(sqrt_trade_volume_buks, (0,2), keepdims=True) # shape of (1,112,1)\n","    stocks_overall_liquidity2 = np.nanmean(liquidity2_wavg, (0,2), keepdims=True) # shape of (1,112,1)\n","    # average out along the buckets axis\n","    stocks_overall_wap1_log_price_ret_vol = np.nanmean(wap1_log_price_ret_vol_buks**2, 2, keepdims=True)**.5 # shape of (3830,112,1)\n","\n","    # (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)  = (vol[:,:, 0:] / s ) / (liq2[:,:, 0:]) / l2), standardized volume divided by standardized liquidity2\n","    # (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8) , diminish the effect of outliers, or reduce large values\n","    # np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True) , average over stock_id and buckets axis\n","    # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8) , destandardize and then take the power of 8 to undo the effect of 1/8\n","    # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5 , take the square root as it is liquidity of level 2\n","    # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1 , divide by wap1_log_price_ret_vol to check > 1\n","    # for greater than wap1_log_price_ret_vol or wap1_log_price_ret_vol < 1 for less than wap1_log_price_ret_vol\n","    # log detects > 1 or < 1\n","    final_features['soft_stock_mean_tvpl2_:0'    ] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    # using only the last 20 buckets\n","    final_features['soft_stock_mean_tvpl2_:10'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    # using only the last 10 buckets\n","    final_features['soft_stock_mean_tvpl2_:20'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","    # same as above but only using the last bucket of liquidity2\n","    final_features['soft_stock_mean_tvpl2_liqf'       ] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['soft_stock_mean_tvpl2_liqf_volf10'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,10:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","    final_features['soft_stock_mean_tvpl2_liqf_volf20'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,20:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","    # np.mean(vol1[:,:,25:]**2,2,keepdims=True) ,squared wap1_log_price_ret_vol_buks average for last 5 buckets\n","    # np.mean(vol1[:,:,:15]**2,2,keepdims=True) , squared wap1_log_price_ret_vol_buks average for first 15 buckets\n","    # np.nanmedian( np.mean(vol1[:,:,25:]**2,2,keepdims=True) / np.mean(vol1[:,:,:15]**2,2,keepdims=True),1,keepdims=True) , median of ratio  along stock_id axis , shape of (3830,1,1)\n","    # square root to get back standard deviation / volatility\n","    # log to detect > 1 or < 1\n","    final_features['v1proj_25_15'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True) / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # same as above but only for wap1_log_price_ret high correlation stocks\n","    # final_features['v1proj_25_15_lr1_high_corr_stocks'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                             / np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","    # same as above but only for wap1_log_price_ret_vol high correlation stocks\n","    #final_features['v1proj_25_15_vol1_high_corr_stocks'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                             / np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","    # same as above but ratio of average of last 5 buckets to average of all buckets\n","    #final_features['v1proj_25_lr1_high_corr_stocks']     = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                         / np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","    # same as above but ratio of average of last 5 buckets to average of all buckets\n","    #final_features['v1proj_25_vol1_high_corr_stocks']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,25:]**2,2,keepdims=True)\n","    #                                                          / np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # average of log_spread_ret_sqr_vol_buks over all buckets\n","    final_features['lsvol'] = np.log( np.nanmean(train_buckets['log_spread_ret_sqr_vol_buks'], 2, keepdims=True))\n","\n","\n","    # average of log_liquidity1_ret_sqr_vol_buks over all buckets\n","    final_features['liqvol1'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], 2, keepdims=True))\n","    # average of log_liquidity2_ret_sqr_vol_buks over all stock ids and buckets\n","    final_features['liqvol1_smean'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], (1,2), keepdims=True)) # shape of (3830,1,1)\n","\n","    # average of log_liquidity1_ret_sqr_vol_buks over all buckets for each cluster\n","    # is grouped by cluster and then median is taken. repeat the median value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['liqvol1_smean_c3'] = np.log( cluster_agg(np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], 2, keepdims=True), wap1_log_price_ret_cluster3 ,np.nanmedian))\n","\n","    # average of log_liquidity2_ret_sqr_vol_buks over all buckets\n","    final_features['liqvol2'] = np.log( np.nanmean(train_buckets['log_liquidity2_ret_sqr_vol_buks'], 2, keepdims=True))\n","\n","    # ratio of average log_liquidity1_ret_sqr_vol_buks in last 15 buckets to first 15 buckets\n","    final_features['liqvol1_15_15'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'][:,:,15:  ], 2, keepdims=True)\n","                                            /np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'][:,:,  :15], 2, keepdims=True))\n","\n","    # average of trade_count over all buckets\n","    final_features['trade_count']      = np.log( np.nanmean(train_buckets['trade_count_buks']    , 2, keepdims=True))\n","    # average of squre root of trade_count over all buckets\n","    final_features['root_trade_count'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","\n","    # average of squre root of trade_count over all stock ids and all buckets\n","    final_features['root_trade_count_smean'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, (1,2), keepdims=True))\n","\n","    # average of squre root of number of data points in a bucket over all buckets\n","    final_features['root_book_delta_count'] = np.log( np.nanmean(train_buckets['book_delta_count_buks']**.5, 2, keepdims=True))\n","\n","    # average of square root of trade_count over all buckets for each cluster\n","    # is grouped by cluster 1 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['root_trade_count_smean_c1'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","    ####                                                                 wap1_log_price_ret_cluster1,np.nanmean))\n","\n","    # average of square root of trade_count over all buckets for each cluster\n","    # is grouped by cluster 2 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['root_trade_count_smean_c2'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","    ####                                                                 wap1_log_price_ret_cluster2,np.nanmean))\n","\n","    # average of square root of trade_count over all buckets for each cluster\n","    # is grouped by cluster 3 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","    #### final_features['root_trade_count_smean_c3'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","    ####                                                                 wap1_log_price_ret_cluster3,np.nanmean))\n","\n","    # variance of square root of trade_count over all buckets for each cluster\n","    final_features['root_trade_count_var'] = np.log( np.nanvar(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","\n","    # ratio of average trade_count in last 15 buckets to first 15 buckets\n","    final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","                                                      np.nanmean(train_buckets['trade_count_buks'][:,:,  :15], 2, keepdims=True))\n","\n","    # ratio of average square root trade_count in last 15 buckets to first 15 buckets\n","    final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","                                                       np.nanmean(train_buckets['trade_count_buks'][:,:,  :15]**.5, 2, keepdims=True))\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last bucket to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,29:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True),1,keepdims=True)**.5)\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last 10 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_20']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,20:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # median of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","    # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_15_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","    # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_15_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","                                                           / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                               / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last bucket to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,29:]**2,2,keepdims=True)\n","                                                               / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 10 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_20_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,20:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_25_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","    # std of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","    # difference between 75% and 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","    final_features['v1proj_29_q3q1'] = np.log(np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True))\n","                                                    ,0.75, axis=1,keepdims=True)\n","                                                -\n","                                                np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","                                                            /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True))\n","                                                    ,0.25, axis=1,keepdims=True))\n","\n","\n","    # CHECK IF there are more than 100 stocks\n","    # Basically, all features calcualted above are repeated and aggregated for each cluster using mean, median and std etc..\n","#     if wap1_log_price_ret_vol_buks.shape[1]>100:\n","\n","\n","#         final_features['v1proj_25_c1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                              / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), wap1_log_price_ret_cluster1, np.median)**0.5 )\n","\n","\n","#         final_features['v1proj_25_c2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         wap1_log_price_ret_cluster2, np.median)**0.5 )\n","#         final_features['v1proj_25_c3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         wap1_log_price_ret_cluster3, np.median)**0.5 )\n","#         final_features['v1proj_25_c4'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                        wap1_log_price_ret_cluster4, np.median)**0.5 )\n","#         final_features['v1proj_25_c5'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         wap1_log_price_ret_cluster5, np.median)**0.5 )\n","\n","\n","\n","#         final_features['soft_stock_mean_tvpl2_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","#         final_features['soft_stock_mean_tvpl2_10_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_10_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_10_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","#         final_features['soft_stock_mean_tvpl2_20_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_20_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","#         final_features['soft_stock_mean_tvpl2_20_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","#             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","#                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","\n","#         final_features['v1proj_25_c1_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster1, np.nanstd)**0.5  )\n","#         final_features['v1proj_25_c2_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster2, np.nanstd)**0.5 )\n","#         final_features['v1proj_25_c3_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster3, np.nanstd)**0.5 )\n","#         final_features['v1proj_25_c4_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster4, np.nanstd)**0.5 )\n","#         final_features['v1proj_25_c5_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","#                                                         wap1_log_price_ret_cluster5, np.nanstd)**0.5 )\n","\n","\n","#         final_features['v1proj_25_vc1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters1, np.median)**0.5  )\n","#         final_features['v1proj_25_vc2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters2, np.median)**0.5 )\n","#         final_features['v1proj_25_vc3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters3, np.median)**0.5 )\n","\n","#         final_features['v1proj_25_vc4'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_wap1_log_price_ret_vol_clusters4, np.median)**0.5 )\n","\n","\n","\n","\n","#         final_features['v1proj_25_vvc1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_quart_trade_volume_clusters1, np.median)**0.5  )\n","#         final_features['v1proj_25_vvc2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_quart_trade_volume_clusters2, np.median)**0.5 )\n","#         final_features['v1proj_25_vvc3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","#                                                         log_quart_trade_volume_clusters3, np.median)**0.5 )\n","\n","\n","\n","#         final_features['v1spprojt15f25_c1'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster1, np.median) )\n","#         final_features['v1spprojt15f25_c2'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster2, np.median) )\n","#         final_features['v1spprojt15f25_c3'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster3, np.median) )\n","#         final_features['v1spprojt15f25_c4'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            wap1_log_price_ret_cluster4, np.median) )\n","\n","#         final_features['v1spprojt15f25_vc1'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            log_wap1_log_price_ret_vol_clusters1, np.median) )\n","#         final_features['v1spprojt15f25_vc2'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            log_wap1_log_price_ret_vol_clusters2, np.median) )\n","#         final_features['v1spprojt15f25_vc3'] = np.log( cluster_agg(\n","#                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","#                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","#                            log_wap1_log_price_ret_vol_clusters3, np.median) )\n","\n","\n","    # ratio of average of all buckets trade_volume to average of all buckets liquidity2, shape of (3830,1,1)\n","    final_features['tvpl2_rmed2v1']     = np.log( np.median( ( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","                                                    / np.mean( liquidity2_wavg[:,:, 0:]**.5, 2, keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","    # ratio of average of all buckets trade_volume to average of last 5 buckets liquidity2, shape of (3830,1,1)\n","    final_features['tvpl2_rmed2v1lf25'] = np.log( np.median(( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","                                                    / np.mean( (liquidity2_wavg[:,:, 25:])**.5, (2), keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","    # ratio of average of all buckets trade_volume to average of last 1 buckets liquidity2, shape of (3830,1,1)\n","    final_features['tvpl2_rmed2v1lf29'] = np.log( np.median(( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","                                                    / np.mean( (liquidity2_wavg[:,:, 29:])**.5, (2), keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","\n","\n","\n","    # ratio of average of all buckets trade_volume to average of    all buckets liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2']        = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg)**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all buckets trade_volume to average of last 20 buckets liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2_liqf10'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all buckets trade_volume to average of last 10 buckets liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2_liqf20'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all buckets trade_volume to average of last 1 bucket liquidity2, shape of (3830,112,1)\n","    final_features['tvpl2_liqf29'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","\n","    # ratio of average of all time ids and buckets trade_volume to average of all bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol'       ] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:, 0:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all time ids and buckets trade_volume to average of last 20 bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol_liqf10'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all time ids and buckets trade_volume to average of last 10 bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol_liqf20'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","    # ratio of average of all time ids and buckets trade_volume to average of last 1 bucket liquidity2, shape of (3830, 112, 1)\n","    final_features['tvpl2_smean_vol_liqf29'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","\n","\n","    #\n","    final_features['v1liq2projt5'] = np.log( ( np.mean( liquidity2_wavg[:,:,  : 5]**(1/8), 2, keepdims=True)**8\n","                                            / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","    #\n","    final_features['v1liq2projt10'] = np.log( ( np.mean( liquidity2_wavg[:,:,  :10]**(1/8), 2, keepdims=True)**8\n","                                             / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","\n","    final_features['v1liq2projt20'] = np.log( ( np.mean( liquidity2_wavg[:,:,  :20]**(1/8), 2, keepdims=True)**8\n","                                             / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","    # ratio of average of first 10 buckets liquidity2_wavg to average of last 1 buckets liquidity2, shape of (3830,112,1)\n","    final_features['liqt10rf29'] = np.log( np.mean( liquidity2_wavg[:,:,:10]**.5, (2), keepdims=True)**2 / liquidity2_wavg[:,:,29:] )\n","    # ratio of average of first 20 buckets liquidity2_wavg to average of last 1 buckets liquidity2, shape of (3830,112,1)\n","    final_features['liqt20rf29'] = np.log( np.mean( liquidity2_wavg[:,:,:20]**.5, (2), keepdims=True)**2 / liquidity2_wavg[:,:,29:] )\n","\n","\n","\n","    # median along stock id axis, how liquidity2_wavg changes/ratio in first 10 bins to last 5 bins. shape of (3830,1,1)\n","    final_features['v1liq2sprojt10f25'] = np.log( np.median(\n","                          np.mean(liquidity2_wavg[:,:,:10]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity2_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/2) )\n","\n","    final_features['v1liq2sprojt5f25'] = np.log( np.median(\n","                          np.mean(liquidity2_wavg[:,:,  : 5]**.125, (2),keepdims=True)**8/\n","                          np.mean(liquidity2_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","                        , 1, keepdims=True)**(1/2) )\n","\n","    # median along stock id axis, of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt10f29'] = np.median( - np.mean(log_spread2_wavg[:,:,  :  ], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:  ], (2),keepdims=True) , 1, keepdims=True)\n","    # median along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f25'] = np.median( - np.mean(log_spread2_wavg[:,:,  :15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:  ], (2),keepdims=True) , 1, keepdims=True)\n","    # median along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f29'] = np.median( - np.mean(log_spread2_wavg[:,:,  :15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:  ], (2),keepdims=True), 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f29_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f29_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f25_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis,of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojt15f25_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf29_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf29_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","    # 25% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf25_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","    # 75% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","    final_features['v1spprojtf25_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","\n","    return\n","\n","\n","\n","\n","\n","def get_simple_features(binned_features, final_features, name, ffrom=0):\n","    wap1_log_price_ret_vol = binned_features[name+'_buks']\n","\n","    suffix = f'_from_{ffrom}'\n","\n","    # average of wap1_log_price_ret_buks in all buckets of a time id. Then average over all time ids. Then take square root, shape of (1,112,1)\n","    stocks_overall_wap1_log_price_ret_mean  = np.mean(np.mean(wap1_log_price_ret_vol**2, (2), keepdims=True)**.5, 0, keepdims=True)\n","\n","    # wap1_log_price_ret_vol and then normalized by wap1_log_price_ret_vol (shape 3830 x 112, 1), is it 1 for ffrom=0?\n","    final_features[name + suffix] =            np.log(             np.mean(wap1_log_price_ret_vol[:,:,ffrom:]**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","    #\n","    final_features[name+'stock_mean'+suffix] = np.log(stocks_overall_wap1_log_price_ret_mean*np.median( np.mean( wap1_log_price_ret_vol[:,:,ffrom:]/stocks_overall_wap1_log_price_ret_mean, 2, keepdims=True), 1, keepdims=True)/final_features['wap1_log_price_ret_vol'])\n","\n","\n","\n","\n","\n","\n","\n","def generate_liquidity_features(train_buckets):\n","\n","    final_features = {}\n","\n","    final_features['time_ids'] = (1*train_buckets['time_ids'][:,np.newaxis] + 0*train_buckets['stock_ids'][np.newaxis,:])[:,:,np.newaxis] # (3830, 112, 1) containing only repeated time ids along stock axis\n","    final_features['stock_ids'] = (0*train_buckets['time_ids'][:,np.newaxis] + 1*train_buckets['stock_ids'][np.newaxis,:])[:,:,np.newaxis] # (3830, 112, 1) containing only repeated stock ids along time axis\n","\n","    # average out along the buckets axis\n","    final_features['wap1_log_price_ret_vol'] = np.mean(train_buckets['wap1_log_price_ret_vol_buks']**2, 2, keepdims=True)**0.5\n","\n","    # (log liquidity ret x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","    # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","    # check if greater than or less than wap1 price volatility\n","    final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol'] = np.log(np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol'] = np.log(np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2'] = np.log(np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","\n","    # average out along the buckets axis\n","    final_features['wap1_log_price_ret_per_liq2_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks']**2, 2, keepdims=True)**0.5)\n","    final_features['wap1_log_price_ret_per_spread_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks']**2, 2, keepdims=True)**0.5)\n","\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = np.log( np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    final_features['wap1_log_price_ret_per_liq2_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = np.log( np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio'] = np.log( np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                          /np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    # median across all stocks (dimension 1) for that time id.\n","    final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                                    / np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    # median across all stocks (dimension 1) for that time id.\n","    final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                                    / np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","    # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","    # median across all stocks (dimension 1) for that time id.\n","    final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","                                                    / np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","\n","    # (liquidity x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","    # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","    # check if greater than or less than wap1 price volatility\n","    final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol'] = np.log(np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","\n","    # (liquidity x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","    # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","    # check if greater than or less than wap1 price volatility\n","    # volatitlity in wap1_log_price_ret when liquidity1 is negative. i.e. decreases divided by wap1_log_price_ret_vol\n","    final_features['wap1_log_price_ret_neg_log_liq_ret_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'] )\n","    # volatitlity in wap1_log_price_ret when liquidity1 is positive. i.e. increases divided by wap1_log_price_ret_vol\n","    final_features['wap1_log_price_ret_pos_log_liq_ret_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'] )\n","    # difference between volatitlity in wap1_log_price_ret when liquidity1 is positive and negative. i.e. increases minus decreases\n","    final_features['wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol'] = final_features['wap1_log_price_ret_pos_log_liq_ret_sqr_vol'] - final_features['wap1_log_price_ret_neg_log_liq_ret_sqr_vol']\n","\n","\n","    get_cohesion_features(train_buckets, final_features, ffrom=0) # uses wap1_log_price_ret_buks from bucket 0 to 30 of a time id, all\n","    get_cohesion_features(train_buckets, final_features, ffrom=10) # uses wap1_log_price_ret_buks from bucket 10 to 30 of a time id, last two thirds\n","    get_cohesion_features(train_buckets, final_features, ffrom=20) # uses wap1_log_price_ret_buks from bucket 20 to 30 of a time id, last one third\n","\n","    get_misc_features(train_buckets, final_features)\n","\n","\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=0)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=10)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=20)\n","    get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=25)\n","\n","    # standardize wap1_log_price_ret_vol by dividing by mean of wap1_log_price_ret_vol\n","    final_features['vol1_mean'] = np.log(final_features['wap1_log_price_ret_vol']/np.nanmean(final_features['wap1_log_price_ret_vol'], 0, keepdims=True))\n","\n","    # std of ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend, shape of (1,112,1)\n","    final_features['mean_half_delta'] = np.nanstd( np.log( np.mean( train_buckets['wap1_log_price_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)\n","                                                         / np.mean( train_buckets['wap1_log_price_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True) ) , 0, keepdims=True)\n","\n","    # std of difference between last bin and first bin of log_spread_wavg,  shape of (1,112,1)\n","    final_features['mean_half_delta_lsprd'] = np.log(np.nanstd( (  train_buckets['log_spread_wavg'][:,:,-1: ]\n","                                                         -  train_buckets['log_spread_wavg'][:,:,  :1] ), 0, keepdims=True) )\n","\n","    # take log\n","    final_features['log_wap1_log_price_ret_vol'] = np.log(final_features['wap1_log_price_ret_vol'])\n","\n","    return final_features\n","\n","\n","\n","\n","\n","\n","\n","def read_targets_from_df(df, s, t):\n","    # returns a matrix of shape (len(t), len(s)) with target values as entries\n","    t = list(t)\n","    s = list(s)\n","\n","\n","    Z = np.zeros((len(t), len(s)))\n","\n","    dft = np.array(df['time_id'])\n","    dfs = np.array(df['stock_id'])\n","    dfr = np.array(df['target'])\n","\n","\n","    for k in range(df.shape[0]):\n","        Z[t.index(dft[k]), s.index(dfs[k])] = dfr[k]\n","    return Z\n","\n","\n","\n","\n","\n","\n","\n","def merge_features_to_df(fdict, df, features):\n","\n","    T = fdict['time_ids'][:,:,0]\n","    S = fdict['stock_ids'][:,:,0]\n","\n","    T = np.reshape(T, [T.shape[0]*T.shape[1]])\n","    S = np.reshape(S, [S.shape[0]*S.shape[1]])\n","\n","    sh = np.max( [fdict[f].shape[1] for f in features] )\n","    sq = np.max( [fdict[f].shape[0] for f in features] )\n","\n","    for f in features:\n","        if fdict[f].shape[1]==1:\n","            print(f, 'shape[1]==1')\n","            fdict[f] = np.repeat(fdict[f], repeats=sh, axis=1)\n","        if fdict[f].shape[0]==1:\n","            print(f, 'shape[0]==1')\n","            fdict[f] = np.repeat(fdict[f], repeats=sq, axis=0)\n","\n","    reshaped_features = [np.reshape( fdict[feature], [fdict[feature].shape[0]*fdict[feature].shape[1]] )\n","                         for feature in features]\n","\n","    dfz = pd.DataFrame(data=np.vstack([S,T] + reshaped_features  ).T, columns=['stock_id', 'time_id']+features)\n","    dfz['time_id' ] = dfz['time_id' ].astype(int)\n","    dfz['stock_id'] = dfz['stock_id'].astype(int)\n","\n","    dfz = df.merge(dfz, on=['stock_id', 'time_id'], how='left')\n","    # some time ids are missing in the df, so df is smaller than dfz\n","    # 428960 to 428932, 151 to 152\n","    # suffix _x and _y are added for clashing columns during merge\n","    return dfz\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6uownZyogEg"},"outputs":[],"source":["\"\"\"\n","Clustering on training data to get categorical features. This is NOT done for test data\n","\"\"\"\n","\n","\n","class FeatureTransformation:\n","    def __init__(self):\n","        self.scaler = None\n","\n","    def feature_normalization(self, X_train, transform_type='minmax'):\n","        \"\"\"\n","        Normalize features using MinMaxScaler or StandardScaler.\n","        \"\"\"\n","        self.scaler = StandardScaler() if transform_type == 'standard' else MinMaxScaler()\n","        X_train_normalized = self.scaler.fit_transform(X_train)\n","        return X_train_normalized\n","\n","    def inv_feature_normalization(self, X_train_normalized):\n","        \"\"\"\n","        Inverse transform normalized features to their original scale.\n","        \"\"\"\n","        return self.scaler.inverse_transform(X_train_normalized)\n","\n","\n","def clustering_on_training_data(all_stocks_first_10_min_vol_df):\n","\n","    os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/')\n","\n","    train_trade_paths = sorted(glob.glob('trade_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","    train_book_paths = sorted(glob.glob('book_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","\n","    unique_stock_ids = []\n","    for path in train_book_paths:\n","        unique_stock_ids.append(int(path.split('=')[1]))\n","\n","    train = pd.read_csv('train.csv')\n","    train_target = train\n","\n","    all_uniq_time_ids = pd.DataFrame({'time_id':train['time_id'].unique()})\n","\n","\n","\n","\n","    def get_alternating_cluster_coloring(labels):\n","\n","        sorted_labels = np.sort(labels)\n","        colors = ['yellow', 'magenta']\n","        coloring = []\n","\n","        sorted_labels = np.insert(sorted_labels, 0, sorted_labels[0], axis=0)\n","        for i in range(len(sorted_labels)-1):\n","            if sorted_labels[i+1] == sorted_labels[i]:\n","                coloring.append(colors[0])\n","            else:\n","                colors = colors[::-1]\n","                coloring.append(colors[0])\n","        return coloring\n","\n","\n","\n","\n","\n","\n","\n","        ## verify the clustering using silhouette score for particular clustring parameters\n","    def plot_silhouette_scores(X_train,labels,metric,linkage,clustering_type):\n","        unique_clusters = np.unique(labels)\n","        num_clusters = len(unique_clusters)\n","        sample_silhouette_values = silhouette_samples(X=X_train, labels=labels,metric=metric)\n","        silhouette_avg = silhouette_score(X=X_train, labels=labels,metric=metric)\n","        fig, ax1 = plt.subplots()\n","        y_lower = 10\n","        mean_num_zones_dev_in_clusters = []\n","\n","        mean_num_zones_in_clusters = X_train.shape[0] /num_clusters\n","\n","        for i in range(num_clusters): # exclude the\n","            # Aggregate the silhouette scores for samples belonging to\n","            # cluster i, and sort them\n","            ith_cluster_silhouette_values = sample_silhouette_values[labels == i+1]\n","            ith_cluster_silhouette_values.sort()\n","            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","            y_upper = y_lower + size_cluster_i\n","            color = cm.nipy_spectral(float(i+1) / num_clusters)\n","            ax1.fill_betweenx(\n","                np.arange(y_lower, y_upper),\n","                0,\n","                ith_cluster_silhouette_values,\n","                facecolor=color,\n","                edgecolor=color,\n","                alpha=0.7,\n","            )\n","            # Label the silhouette plots with their cluster numbers at the middle\n","            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n","            # Compute the new y_lower for next plot\n","            y_lower = y_upper + 10  # 10 for the 0 samples\n","            mean_num_zones_dev_in_clusters.append(abs(mean_num_zones_in_clusters - np.sum(labels == i+1) ))\n","\n","        num_neg_silhouette_scores = np.sum(sample_silhouette_values < 0 )\n","        min_silhouette_score = min(sample_silhouette_values)\n","        dev_from_mean_num_zone = np.sum(np.array(mean_num_zones_dev_in_clusters))\n","\n","        print(\"[\",num_clusters,',',silhouette_avg,',',num_neg_silhouette_scores,',',min_silhouette_score,\"]\")\n","        ax1.set_title(\"met: \"+metric+\", link: \"+linkage+\", sil_avg: \"+str(silhouette_avg)+\",\\n num_neg_sil_score: \"+str(num_neg_silhouette_scores)+\", min_sil_score: \"+str(min_silhouette_score)+\",\\n num_clust: \"+str(num_clusters)+\", Clust_type : \"+clustering_type )\n","        ax1.set_xlabel(\"The silhouette coefficient values\")\n","        ax1.set_ylabel(\"Cluster label\")\n","        # The vertical line for average silhouette score of all the values\n","        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","        return silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone\n","\n","\n","\n","\n","\n","\n","\n","\n","    def find_unique_time_ids_in_all_stocks(train):\n","        all_time_ids = np.array([])\n","        for st_id in unique_stock_ids:\n","            all_time_ids = np.append(all_time_ids,train[train['stock_id'] == st_id]['time_id'])\n","        return np.unique(all_time_ids)\n","\n","    unique_time_ids = find_unique_time_ids_in_all_stocks(train)\n","\n","\n","    train_common_time_ids_df = pd.DataFrame()\n","    for st_id in unique_stock_ids:\n","        temp_df = pd.DataFrame()\n","        temp_df[str(st_id)] = train[train['stock_id'] == st_id]['target']\n","        temp_df.index = train[train['stock_id'] == st_id]['time_id']\n","        temp_df = temp_df.reindex(unique_time_ids).ffill().bfill() ## forward and backward fill the missing values so that data is available at all time_id\n","        train_common_time_ids_df = pd.concat([train_common_time_ids_df,temp_df],axis=1)\n","\n","    train_common_time_ids_df\n","\n","\n","    target_corr_mat = train_common_time_ids_df.corr()\n","    target_corr_mat\n","\n","\n","    if ~(np.any(np.where(target_corr_mat < 0))):\n","        dissimilarity = 1 - abs(target_corr_mat)\n","\n","    # create map of labels_order (0,1,..., 111, 112) to stock_id_dict (0,1,..., 125, 126)\n","    map_labels_order_to_stock_id_dict = {x: target_corr_mat.columns[x] for x in range(112)}\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using complete for best small number of clusters\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","    metrics = ['null']\n","\n","    final_pear_corr_target_vol_clusters = pd.DataFrame(columns=['stock_id'])\n","    final_pear_corr_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","\n","\n","    c = [3] # number of clusters\n","    i=0\n","    for metric in metrics:\n","        for method in ['complete']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.5]: #[0.2,0.45]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order,ax= ax)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix with weighted linkage\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","    metrics = ['null']\n","\n","    c = [49] # number of clusters\n","    i=0\n","    for metric in metrics:\n","        for method in ['weighted']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.15]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using ward\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","\n","    c=[90] # number of clusters\n","    i=0\n","    metrics = ['nil']\n","    for metric in metrics:\n","        for method in ['ward']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.1]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using median\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","\n","    c=[10] # number of clusters\n","    i=0\n","    metrics = ['nil']\n","    for metric in metrics:\n","        for method in ['median']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.25]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    ## Best clusterings on pearson correlation matrix using average\n","\n","    clustering_type = 'Agg. Hier. Clustering'\n","    Best_silhouette_parameters = {}\n","\n","    c=[26] # number of clusters\n","    i=0\n","    metrics = ['nil']\n","    for metric in metrics:\n","        for method in ['average']:\n","            Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","            for t in [0.2]:\n","                # Calculate the cluster\n","                labels = fcluster(Z, t, criterion='distance')\n","                # Keep the indices to sort labels\n","                labels_order = np.argsort(labels)\n","                stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","                final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","                i+=1\n","\n","                plt.figure(figsize=(20,5))\n","                plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","                dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","                plt.show()\n","\n","                silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","                Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","                # Build a new dataframe with the sorted columns\n","                #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","                clustered = train_common_time_ids_df[stock_id_order]\n","\n","                # Plot the correlation heatmap\n","                correlations = clustered.corr()\n","                fig, ax = plt.subplots(figsize=(20,20))\n","                sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","                            xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","                ## show clusters as alternating colors on the correlation heatmap\n","                cluster_colorings = get_alternating_cluster_coloring(labels)\n","                for i in range(len(cluster_colorings)):\n","                    ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","                plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","                plt.show()\n","\n","\n","    max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","    print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    def create_feature_matrix(feature_dicts):\n","        \"\"\"\n","        Create a feature matrix from a list of dictionaries containing feature values.\n","        \"\"\"\n","        n_samples = len(feature_dicts[0])\n","        n_features = len(feature_dicts)\n","        X_feat = np.zeros((n_samples, n_features))\n","\n","        for i, feature_dict in enumerate(feature_dicts):\n","            X_feat[:, i] = np.array(list(feature_dict.values()))\n","\n","        return X_feat\n","\n","\n","\n","\n","\n","\n","\n","\n","    def calculate_summary_statistics(train_df):\n","        \"\"\"\n","        Calculate summary statistics for each stock's target column.\n","        \"\"\"\n","        summary_stats = {\n","            'total_time_id': {},\n","            'mean_vol': {},\n","            'std_vol': {},\n","            'min_vol': {},\n","            'p25_vol': {},\n","            'median_vol': {},\n","            'p75_vol': {},\n","            'max_vol': {},\n","            'skew_vol': {},\n","            'kurt_vol': {}\n","        }\n","\n","        for st_id, group in train_df.groupby('stock_id'):\n","            target = group['target']\n","            summary_stats['total_time_id'][st_id] = len(target)\n","            summary_stats['mean_vol'][st_id] = target.mean()\n","            summary_stats['std_vol'][st_id] = target.std()\n","            summary_stats['min_vol'][st_id] = target.min()\n","            summary_stats['p25_vol'][st_id] = target.quantile(0.25)\n","            summary_stats['median_vol'][st_id] = target.median()\n","            summary_stats['p75_vol'][st_id] = target.quantile(0.75)\n","            summary_stats['max_vol'][st_id] = target.max()\n","            summary_stats['skew_vol'][st_id] = skew(target)\n","            summary_stats['kurt_vol'][st_id] = kurtosis(target)\n","\n","        return summary_stats\n","\n","\n","    # Usage\n","    # train = pd.read_csv('your_train_data.csv')  # Assuming 'train' is your DataFrame\n","    summary_stats = calculate_summary_statistics(train)\n","\n","\n","    std_vol = summary_stats['std_vol']\n","    mean_vol = summary_stats['mean_vol']\n","    min_vol = summary_stats['min_vol']\n","    median_vol = summary_stats['median_vol']\n","    max_vol = summary_stats['max_vol']\n","    kurt_vol = summary_stats['kurt_vol']\n","    p25_vol = summary_stats['p25_vol']\n","    p75_vol = summary_stats['p75_vol']\n","    skew_vol = summary_stats['skew_vol']\n","\n","\n","\n","\n","\n","\n","\n","\n","    ####### NON-ROBUST features #######\n","\n","\n","    new_X_feat = [std_vol,min_vol,median_vol,max_vol,kurt_vol]\n","    X_feat = create_feature_matrix(new_X_feat)\n","\n","    feat_transform = FeatureTransformation()\n","    X_feat_normalized = feat_transform.feature_normalization(X_feat)\n","\n","\n","    ## Select the BEST agglomerative hierarchical clustering for features: std_vol,min_vol,median_vol,max_vol,kurt_vol\n","\n","    #['single','complete','average','weighted','centroid','median','ward']\n","\n","    final_sum_stats_target_vol_clusters = pd.DataFrame(columns=['stock_id','4_clusters', '10_clusters', '16_clusters', '30_clusters'])\n","    final_sum_stats_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","    c = [4,10,16,30]\n","    clustering_type = 'Agg. Hier. Clustering'\n","    metric = 'euclidean'\n","    for method in ['complete']:\n","        Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","        i=0\n","\n","        for t in [1,0.5,0.4,0.25]:#np.arange(0.2,0.3,0.1):\n","            # Calculate the cluster\n","            labels = fcluster(Z, t, criterion='distance')\n","            # Keep the indices to sort labels\n","            final_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","            i+=1\n","            labels_order = np.argsort(labels)\n","\n","\n","            plt.figure(figsize=(20,5))\n","            plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","            dendrogram(Z,color_threshold=t)\n","            plt.show()\n","\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","    print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","\n","\n","\n","    # # others that can be tried are\n","    # Num_clusters = 10, linkage = ward,threshold = 0.7\n","    # Num_clusters = 15, linkage = ward,threshold = 0.5\n","    # Num_clusters = 5, linkage = average,threshold = 0.5\n","    # Num_clusters = 20, linkage = average,threshold = 0.25\n","    # Num_clusters = 4, linkage =weighted,threshold = 0.6\n","    # Num_clusters = 8, linkage =weighted,threshold = 0.4\n","    # Num_clusters = 16, linkage =weighted,threshold = 0.3\n","\n","\n","\n","\n","\n","\n","\n","\n","    ####### ROBUST features #######\n","\n","    new_X_feat = [std_vol,p25_vol,median_vol,p75_vol,skew_vol,kurt_vol]\n","    X_feat = create_feature_matrix(new_X_feat)\n","\n","    feat_transform = FeatureTransformation()\n","    X_feat_normalized = feat_transform.feature_normalization(X_feat, transform_type=\"standard\")\n","\n","\n","    ## Select the BEST agglomerative hierarchical clustering for features : std_vol,p25_vol,median_vol,p75_vol,skew_vol,kurt_vol\n","\n","    #['single','complete','average','weighted','centroid','median','ward']\n","\n","    final_robust_sum_stats_target_vol_clusters = pd.DataFrame(columns=['stock_id','2_clusters', '4_clusters', '14_clusters','20_clusters','32_clusters', '60_clusters' ])\n","    final_robust_sum_stats_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","    c = [20,32,60]\n","    clustering_type = 'Agg. Hier. Clustering'\n","    metric = 'euclidean'\n","    for method in ['complete']:\n","        Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","        i=0\n","\n","        for t in [1.5,1,0.5]:#np.arange(0.2,0.3,0.1):\n","            # Calculate the cluster\n","            labels = fcluster(Z, t, criterion='distance')\n","            # Keep the indices to sort labels\n","            final_robust_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","            i+=1\n","            labels_order = np.argsort(labels)\n","\n","\n","            plt.figure(figsize=(20,5))\n","            plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","            dendrogram(Z,color_threshold=t)\n","            plt.show()\n","\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","    print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","    c = [2,4,14]\n","    metric = 'euclidean'\n","    for method in ['ward']:\n","        Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","        i=0\n","\n","        for t in [20,10,3]:#np.arange(0.2,0.3,0.1):\n","            # Calculate the cluster\n","            labels = fcluster(Z, t, criterion='distance')\n","            # Keep the indices to sort labels\n","            final_robust_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","            i+=1\n","            labels_order = np.argsort(labels)\n","\n","\n","            plt.figure(figsize=(20,5))\n","            plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","            dendrogram(Z,color_threshold=t)\n","            plt.show()\n","\n","            silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","    print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","\n","    # # others that can be tried are\n","    # Num_clusters = 10, linkage = ward,threshold = 0.7\n","    # Num_clusters = 15, linkage = ward,threshold = 0.5\n","    # Num_clusters = 5, linkage = average,threshold = 0.5\n","    # Num_clusters = 20, linkage = average,threshold = 0.25\n","    # Num_clusters = 4, linkage =weighted,threshold = 0.6\n","    # Num_clusters = 8, linkage =weighted,threshold = 0.4\n","    # Num_clusters = 16, linkage =weighted,threshold = 0.3\n","\n","\n","\n","\n","\n","\n","\n","    def cluster_agg(x, clusters, agg_fun):\n","        r = 0*x\n","\n","        for k in range(np.max(clusters)+1):\n","            z = agg_fun(x[:,clusters==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","            r[:,clusters==k,:] = np.repeat(z, repeats=int(np.sum(clusters==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","        return r\n","\n","\n","\n","\n","\n","    final_sum_stats_target_vol_clusters = final_sum_stats_target_vol_clusters.drop(columns=['stock_id'])\n","    final_robust_sum_stats_target_vol_clusters = final_robust_sum_stats_target_vol_clusters.drop(columns=['stock_id'])\n","\n","    final_pear_corr_target_vol_clusters = final_pear_corr_target_vol_clusters.drop(columns=['stock_id'])\n","\n","\n","\n","    final_sum_stats_target_vol_dict = {}\n","    final_robust_sum_stats_target_vol_clusters_dict = {}\n","    final_pear_corr_target_vol_dict = {}\n","\n","    for c in final_sum_stats_target_vol_clusters.columns:\n","        labels = final_sum_stats_target_vol_clusters[c]\n","        labels = labels + 1 # shift to remove 0 labels\n","        final_sum_stats_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","    for c in final_robust_sum_stats_target_vol_clusters.columns:\n","        labels = final_robust_sum_stats_target_vol_clusters[c]\n","        labels = labels + 1 # shift to remove 0 labels\n","        final_robust_sum_stats_target_vol_clusters_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","    for c in final_pear_corr_target_vol_clusters.columns:\n","        labels = final_pear_corr_target_vol_clusters[c]\n","        labels = labels + 1 # shift to remove 0 labels\n","        final_pear_corr_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","\n","\n","\n","    final_sum_stats_target_vol_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","    final_sum_stats_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","\n","    sum_stats_df = merge_features_to_df(final_sum_stats_target_vol_dict, train,   [f for f in list(final_sum_stats_target_vol_dict.keys()) ]   )\n","    sum_stats_df.shape\n","\n","    final_robust_sum_stats_target_vol_clusters_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","    final_robust_sum_stats_target_vol_clusters_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","\n","    robust_sum_stats_df = merge_features_to_df(final_robust_sum_stats_target_vol_clusters_dict, train,   [f for f in list(final_robust_sum_stats_target_vol_clusters_dict.keys()) ]   )\n","    robust_sum_stats_df.shape\n","\n","\n","\n","    final_pear_corr_target_vol_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","    final_pear_corr_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","\n","    pear_corr_df = merge_features_to_df(final_pear_corr_target_vol_dict, train,   [f for f in list(final_pear_corr_target_vol_dict.keys()) ]   )\n","    pear_corr_df.shape\n","\n","\n","\n","    final_sum_stats_target_vol_df = sum_stats_df[['4_clusters','10_clusters','16_clusters','30_clusters']]\n","    final_sum_stats_target_vol_df.rename(columns={'4_clusters':'target_vol_sum_stats_4_clusters','10_clusters':'target_vol_sum_stats_10_clusters',\n","                                                  '16_clusters':'target_vol_sum_stats_16_clusters','30_clusters':'target_vol_sum_stats_30_clusters'}, inplace=True)\n","\n","    final_robust_sum_stats_target_vol_df = robust_sum_stats_df[[\"2_clusters\",\"4_clusters\",\"14_clusters\",\"20_clusters\",\"32_clusters\",\"60_clusters\"]]\n","    final_robust_sum_stats_target_vol_df.rename(columns={'2_clusters':'target_vol_robust_sum_stats_2_clusters','4_clusters':'target_vol_robust_sum_stats_4_clusters',\n","                                                          '14_clusters':'target_vol_robust_sum_stats_14_clusters','20_clusters':'target_vol_robust_sum_stats_20_clusters',\n","                                                          '32_clusters':'target_vol_robust_sum_stats_32_clusters','60_clusters':'target_vol_robust_sum_stats_60_clusters'}, inplace=True)\n","\n","    final_pear_corr_target_vol_df = pear_corr_df[['3_clusters','49_clusters','90_clusters','10_clusters','26_clusters']]\n","    final_pear_corr_target_vol_df.rename(columns={'3_clusters':'target_vol_pcorr_3_clusters','49_clusters': 'target_vol_pcorr_49_clusters',\n","                                                    '90_clusters':'target_vol_pcorr_90_clusters','10_clusters':'target_vol_pcorr_10_clusters',\n","                                                    '26_clusters':'target_vol_pcorr_26_clusters'}, inplace=True)\n","\n","\n","\n","    return final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df , \\\n","         final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OZz1KqwbenRN"},"outputs":[],"source":["def apply_trained_clusters_to_inference(test,all_unique_time_ids, all_unique_stock_ids,feat_df,all_stocks_first_10_min_vol_df,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters):\n","\n","    final_sum_stats_target_vol_dict = {}\n","    final_robust_sum_stats_target_vol_clusters_dict = {}\n","    final_pear_corr_target_vol_dict = {}\n","\n","    for c in final_sum_stats_target_vol_clusters.columns:\n","        labels = final_sum_stats_target_vol_clusters[c]\n","        labels = labels + 1 # shift to remove 0 labels\n","        final_sum_stats_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","    for c in final_robust_sum_stats_target_vol_clusters.columns:\n","        labels = final_robust_sum_stats_target_vol_clusters[c]\n","        labels = labels + 1 # shift to remove 0 labels\n","        final_robust_sum_stats_target_vol_clusters_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","    for c in final_pear_corr_target_vol_clusters.columns:\n","        labels = final_pear_corr_target_vol_clusters[c]\n","        labels = labels + 1 # shift to remove 0 labels\n","        final_pear_corr_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","\n","    all_unique_time_ids = pd.DataFrame({'time_id':test['time_id'].unique()})\n","    all_unique_stock_ids = pd.DataFrame({'stock_id':test['stock_id'].unique()})\n","\n","    final_sum_stats_target_vol_dict['time_ids'] = np.repeat(all_unique_time_ids, repeats=len(all_unique_stock_ids), axis=1)[:,:,np.newaxis]\n","    final_sum_stats_target_vol_dict['stock_ids'] = np.repeat([all_unique_stock_ids], repeats=len(all_unique_time_ids), axis=0)[:,:,np.newaxis]\n","\n","    sum_stats_df = merge_features_to_df(final_sum_stats_target_vol_dict, test,   [f for f in list(final_sum_stats_target_vol_dict.keys()) ]   )\n","    sum_stats_df.shape\n","\n","    final_robust_sum_stats_target_vol_clusters_dict['time_ids'] = np.repeat(all_unique_time_ids, repeats=len(all_unique_stock_ids), axis=1)[:,:,np.newaxis]\n","    final_robust_sum_stats_target_vol_clusters_dict['stock_ids'] = np.repeat([all_unique_stock_ids], repeats=len(all_unique_time_ids), axis=0)[:,:,np.newaxis]\n","\n","    robust_sum_stats_df = merge_features_to_df(final_robust_sum_stats_target_vol_clusters_dict, test,   [f for f in list(final_robust_sum_stats_target_vol_clusters_dict.keys()) ]   )\n","    robust_sum_stats_df.shape\n","\n","\n","\n","    final_pear_corr_target_vol_dict['time_ids'] = np.repeat(all_unique_time_ids, repeats=len(all_unique_stock_ids), axis=1)[:,:,np.newaxis]\n","    final_pear_corr_target_vol_dict['stock_ids'] = np.repeat([all_unique_stock_ids], repeats=len(all_unique_time_ids), axis=0)[:,:,np.newaxis]\n","\n","    pear_corr_df = merge_features_to_df(final_pear_corr_target_vol_dict, test,   [f for f in list(final_pear_corr_target_vol_dict.keys()) ]   )\n","    pear_corr_df.shape\n","\n","\n","\n","    final_sum_stats_target_vol_df = sum_stats_df[['4_clusters','10_clusters','16_clusters','30_clusters']]\n","    final_sum_stats_target_vol_df.rename(columns={'4_clusters':'target_vol_sum_stats_4_clusters','10_clusters':'target_vol_sum_stats_10_clusters',\n","                                                  '16_clusters':'target_vol_sum_stats_16_clusters','30_clusters':'target_vol_sum_stats_30_clusters'}, inplace=True)\n","\n","    final_robust_sum_stats_target_vol_df = robust_sum_stats_df[[\"2_clusters\",\"4_clusters\",\"14_clusters\",\"20_clusters\",\"32_clusters\",\"60_clusters\"]]\n","    final_robust_sum_stats_target_vol_df.rename(columns={'2_clusters':'target_vol_robust_sum_stats_2_clusters','4_clusters':'target_vol_robust_sum_stats_4_clusters',\n","                                                          '14_clusters':'target_vol_robust_sum_stats_14_clusters','20_clusters':'target_vol_robust_sum_stats_20_clusters',\n","                                                          '32_clusters':'target_vol_robust_sum_stats_32_clusters','60_clusters':'target_vol_robust_sum_stats_60_clusters'}, inplace=True)\n","\n","    final_pear_corr_target_vol_df = pear_corr_df[['3_clusters','49_clusters','90_clusters','10_clusters','26_clusters']]\n","    final_pear_corr_target_vol_df.rename(columns={'3_clusters':'target_vol_pcorr_3_clusters','49_clusters': 'target_vol_pcorr_49_clusters',\n","                                                    '90_clusters':'target_vol_pcorr_90_clusters','10_clusters':'target_vol_pcorr_10_clusters',\n","                                                    '26_clusters':'target_vol_pcorr_26_clusters'}, inplace=True)\n","\n","\n","\n","    return final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnUhrNtf52ja"},"outputs":[],"source":["\n","train_or_inference = 'training' # 'training' or 'inference'\n","\n","ml_stage = train_or_inference\n","\n","if ml_stage == 'training':\n","  os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","  data_dir = '/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/'\n","  train = pd.read_csv('train.csv')\n","  train_buckets = create_dataSet(st_ids = list(np.unique(train['stock_id'])), dset = 'train')\n","elif ml_stage == 'inference':\n","  os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data')\n","  data_dir = '/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data'\n","  test = pd.read_csv('test_sub.csv')\n","  test_buckets = create_dataSet(st_ids = list(np.unique(test['stock_id'])), dset = 'test_sub')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aldjOnQztFp4"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","\n","# with open('train_buckets.pkl','wb') as fb:\n","#     pickle.dump(train_buckets ,fb)\n","\n","with open('train_buckets.pkl', 'rb') as fb: # train_buckets.pkl, test_buckets.pkl\n","  train_buckets = pickle.load(fb)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2532,"status":"ok","timestamp":1725280672773,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"UMAx_DvYJXsX","outputId":"858a83e7-550d-4ff9-9109-79404fde93a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Number of missing values in all buckets_type columns\n","\n","Total number of missing values:  0\n","\n","Min and Max values\n","\n","time_ids  Min:  5\n","time_ids  Max:  32767\n","stock_ids  Min:  0\n","stock_ids  Max:  126\n","wap1_log_price_ret_buks  Min:  -0.0504569613840431\n","wap1_log_price_ret_buks  Max:  0.038755740970373154\n","wap1_log_price_ret_abs_vol_buks  Min:  0.0\n","wap1_log_price_ret_abs_vol_buks  Max:  0.11738480143418448\n","wap2_log_price_ret_abs_vol_buks  Min:  0.0\n","wap2_log_price_ret_abs_vol_buks  Max:  0.1668617313863554\n","wap1_log_price_ret_sqr_vol_buks  Min:  0.0\n","wap1_log_price_ret_sqr_vol_buks  Max:  0.002042929015366308\n","wap2_log_price_ret_sqr_vol_buks  Min:  0.0\n","wap2_log_price_ret_sqr_vol_buks  Max:  0.0041502761372511614\n","wap1_log_price_ret_vol_buks  Min:  0.0\n","wap1_log_price_ret_vol_buks  Max:  0.04737312744137772\n","wap2_log_price_ret_vol_buks  Min:  0.0\n","wap2_log_price_ret_vol_buks  Max:  0.06442263683870104\n","wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks  Min:  0.0\n","wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks  Max:  0.05040043972196377\n","wap_eqi_price0_ret_abs_vol_buks  Min:  0.0\n","wap_eqi_price0_ret_abs_vol_buks  Max:  0.0877096951007843\n","wap_eqi_price0_ret_sqr_vol_buks  Min:  0.0\n","wap_eqi_price0_ret_sqr_vol_buks  Max:  0.033276415922070215\n","wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks  Min:  0.0\n","wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks  Max:  0.03543800926410828\n","wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks  Min:  0.0\n","wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks  Max:  0.038783381964609086\n","wap_eqi_price1_ret_sqr_vol_buks  Min:  0.0\n","wap_eqi_price1_ret_sqr_vol_buks  Max:  0.028223475702996294\n","log_liq2_ret_*_wap_eqi_price1_ret_vol_buks  Min:  0.0\n","log_liq2_ret_*_wap_eqi_price1_ret_vol_buks  Max:  0.011967065676820519\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks  Min:  0.0\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks  Max:  15.4314858288986\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2  Min:  0.0\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2  Max:  15.4314858288986\n","wap1_log_price_ret_per_spread_sqr_vol_buks  Min:  0.0\n","wap1_log_price_ret_per_spread_sqr_vol_buks  Max:  63893.42029636643\n","wap1_log_price_ret_per_liq2_vol_buks  Min:  0.0\n","wap1_log_price_ret_per_liq2_vol_buks  Max:  0.0025897431585209917\n","log_liquidity1_ret_sqr_vol_buks  Min:  0.0\n","log_liquidity1_ret_sqr_vol_buks  Max:  216.44643432321027\n","log_liquidity2_ret_sqr_vol_buks  Min:  0.0\n","log_liquidity2_ret_sqr_vol_buks  Max:  376.0612356118409\n","log_spread_ret_sqr_vol_buks  Min:  0.0\n","log_spread_ret_sqr_vol_buks  Max:  71.81969750951976\n","book_delta_count_buks  Min:  0.0\n","book_delta_count_buks  Max:  20.0\n","wap1_log_price_wavg  Min:  -0.11760727161249153\n","wap1_log_price_wavg  Max:  0.11684247286736096\n","wap2_log_price_wavg  Min:  -0.11822155782629762\n","wap2_log_price_wavg  Max:  0.11654617929591773\n","wap_eqi_price0_wavg  Min:  -0.11805799672065125\n","wap_eqi_price0_wavg  Max:  0.1167889437985601\n","wap_eqi_price1_wavg  Min:  -0.11764709198552993\n","wap_eqi_price1_wavg  Max:  0.11691338116045352\n","wavg_wap1_log_price_amp_diff  Min:  0.9999999999351177\n","wavg_wap1_log_price_amp_diff  Max:  inf\n","wavg_wap_eqi_price0_amp_diff  Min:  0.9999999999406891\n","wavg_wap_eqi_price0_amp_diff  Max:  inf\n","liquidity1_wavg  Min:  0.850008628347739\n","liquidity1_wavg  Max:  2997063.2110014684\n","liquidity2_wavg  Min:  0.1854020731394554\n","liquidity2_wavg  Max:  108551400.92072429\n","root_liquidity2_wavg  Min:  0.429666022039096\n","root_liquidity2_wavg  Max:  9794.487019028536\n","spread_wavg  Min:  2.514241494843901e-05\n","spread_wavg  Max:  0.03775830443923237\n","inv_spread_wavg  Min:  29.723422893921246\n","inv_spread_wavg  Max:  39773.426542613284\n","log_spread_wavg  Min:  -10.590954441497125\n","log_spread_wavg  Max:  -3.3271451575809645\n","log_spread2_wavg  Min:  -9.492342324279097\n","log_spread2_wavg  Max:  -2.7989674792600012\n","book_size1_wavg  Min:  1.9349169731140137\n","book_size1_wavg  Max:  1058850.6744705746\n","book_size_wavg  Min:  3.9555423239957395\n","book_size_wavg  Max:  1427941.6492860292\n","trade_volume_buks  Min:  0.0\n","trade_volume_buks  Max:  740413.5085173845\n","sqrt_trade_volume_buks  Min:  0.0\n","sqrt_trade_volume_buks  Max:  2764.92093142591\n","cube_root_trade_volume_buks  Min:  0.0\n","cube_root_trade_volume_buks  Max:  503.86618878563735\n","trade_volume_p2/3_buks  Min:  0.0\n","trade_volume_p2/3_buks  Max:  15831.805230788712\n","quart_root_trade_volume_buks  Min:  0.0\n","quart_root_trade_volume_buks  Max:  219.1650938964031\n","trade_count_buks  Min:  0.0\n","trade_count_buks  Max:  20.0\n","trade_volume_per_liquidity1_wavg_buks  Min:  0.0\n","trade_volume_per_liquidity1_wavg_buks  Max:  279.1180033415984\n","trade_volume_per_liquidity2_wavg_buks  Min:  0.0\n","trade_volume_per_liquidity2_wavg_buks  Max:  907.2084105345349\n","ask_liq1_diff_wavg  Min:  -2925519.34853724\n","ask_liq1_diff_wavg  Max:  1525556.4992372217\n","bid_liq1_diff_wavg  Min:  -1590983.930454508\n","bid_liq1_diff_wavg  Max:  2545604.6237271978\n"]}],"source":["## check for missing and extereme values\n","\n","if ml_stage == 'training':\n","  buckets_type = train_buckets\n","elif ml_stage == 'inference':\n","  buckets_type = test_buckets\n","# 'train_buckets' or 'test_buckets'\n","\n","print('\\nNumber of missing values in all buckets_type columns\\n')\n","\n","nan_sum = 0\n","for k in buckets_type.keys():\n","    #print(k, np.isnan(buckets_type[k]).sum())\n","    nan_sum += np.isnan(buckets_type[k]).sum()\n","print('Total number of missing values: ', nan_sum)\n","\n","my_mins= []\n","my_maxs = []\n","print('\\nMin and Max values\\n')\n","for k in buckets_type.keys():\n","    print(k,' Min: ', np.min(buckets_type[k]))\n","    my_mins.append(np.min(buckets_type[k]))\n","    print(k,' Max: ', np.max(buckets_type[k]))\n","    my_maxs.append(np.max(buckets_type[k]))\n","\n","\n","######## IDENTIFIED extreme values ########\n","\n","##wap1_log_price_ret_per_spread_sqr_vol_buks  Min:  0.0\n","##wap1_log_price_ret_per_spread_sqr_vol_buks  Max:  63893.42029636643\n","\n","## wavg_wap1_log_price_amp_diff  Min:  0.9999999999351177\n","## wavg_wap1_log_price_amp_diff  Max:  inf\n","## wavg_wap_eqi_price0_amp_diff  Min:  0.9999999999406891\n","## wavg_wap_eqi_price0_amp_diff  Max:  inf\n","## liquidity1_wavg  Min:  0.850008628347739\n","## liquidity1_wavg  Max:  2997063.2110014684\n","## liquidity2_wavg  Min:  0.1854020731394554\n","## liquidity2_wavg  Max:  108551400.92072429\n","## root_liquidity2_wavg  Min:  0.429666022039096\n","## root_liquidity2_wavg  Max:  9794.487019028536\n","## spread_wavg  Min:  2.514241494843901e-05\n","## spread_wavg  Max:  0.03775830443923237\n","## book_size1_wavg  Min:  1.9349169731140137\n","## book_size1_wavg  Max:  1058850.6744705746\n","# book_size_wavg  Min:  3.9555423239957395\n","# book_size_wavg  Max:  1427941.6492860292\n","# trade_volume_buks  Min:  0.0\n","# trade_volume_buks  Max:  740413.5085173845\n","# sqrt_trade_volume_buks  Min:  0.0\n","# sqrt_trade_volume_buks  Max:  2764.92093142591\n","# cube_root_trade_volume_buks  Min:  0.0\n","# cube_root_trade_volume_buks  Max:  503.86618878563735\n","# trade_volume_p2/3_buks  Min:  0.0\n","# trade_volume_p2/3_buks  Max:  15831.80523078871\n","# ask_liq1_diff_wavg  Min:  -2925519.34853724\n","# ask_liq1_diff_wavg  Max:  1525556.4992372217\n","# bid_liq1_diff_wavg  Min:  -1590983.930454508\n","# bid_liq1_diff_wavg  Max:  2545604.6237271978"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26701,"status":"ok","timestamp":1725280699470,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"CO7cc2B9JXqp","outputId":"eff0f8c1-8cd9-43ce-fae4-a4efe81e6cba"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-92-575a2fd88714>:506: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap1_log_price_ret_per_liq2_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:509: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:518: RuntimeWarning: divide by zero encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:518: RuntimeWarning: invalid value encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:518: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:523: RuntimeWarning: divide by zero encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:523: RuntimeWarning: invalid value encountered in divide\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","<ipython-input-92-575a2fd88714>:537: RuntimeWarning: divide by zero encountered in log\n","  final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol'] = np.log(np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","<ipython-input-92-575a2fd88714>:101: RuntimeWarning: divide by zero encountered in log\n","  final_features['lsvol'] = np.log( np.nanmean(train_buckets['log_spread_ret_sqr_vol_buks'], 2, keepdims=True))\n","<ipython-input-92-575a2fd88714>:121: RuntimeWarning: divide by zero encountered in log\n","  final_features['trade_count']      = np.log( np.nanmean(train_buckets['trade_count_buks']    , 2, keepdims=True))\n","<ipython-input-92-575a2fd88714>:123: RuntimeWarning: divide by zero encountered in log\n","  final_features['root_trade_count'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","<ipython-input-92-575a2fd88714>:147: RuntimeWarning: divide by zero encountered in log\n","  final_features['root_trade_count_var'] = np.log( np.nanvar(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","<ipython-input-92-575a2fd88714>:150: RuntimeWarning: divide by zero encountered in divide\n","  final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","<ipython-input-92-575a2fd88714>:150: RuntimeWarning: invalid value encountered in divide\n","  final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","<ipython-input-92-575a2fd88714>:150: RuntimeWarning: divide by zero encountered in log\n","  final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","<ipython-input-92-575a2fd88714>:154: RuntimeWarning: divide by zero encountered in divide\n","  final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","<ipython-input-92-575a2fd88714>:154: RuntimeWarning: invalid value encountered in divide\n","  final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","<ipython-input-92-575a2fd88714>:154: RuntimeWarning: divide by zero encountered in log\n","  final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","<ipython-input-92-575a2fd88714>:207: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_25_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py:1741: RuntimeWarning: invalid value encountered in subtract\n","  np.subtract(arr, avg, out=arr, casting='unsafe', where=where)\n","<ipython-input-92-575a2fd88714>:211: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_29_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,29:]**2,2,keepdims=True)\n","<ipython-input-92-575a2fd88714>:219: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_25_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","<ipython-input-92-575a2fd88714>:223: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_29_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","<ipython-input-92-575a2fd88714>:227: RuntimeWarning: divide by zero encountered in log\n","  final_features['v1proj_29_q3q1'] = np.log(np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","<ipython-input-92-575a2fd88714>:231: RuntimeWarning: divide by zero encountered in log\n","  np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","<ipython-input-92-575a2fd88714>:381: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2']        = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg)**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-92-575a2fd88714>:383: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2_liqf10'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-92-575a2fd88714>:385: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2_liqf20'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-92-575a2fd88714>:387: RuntimeWarning: divide by zero encountered in log\n","  final_features['tvpl2_liqf29'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","<ipython-input-92-575a2fd88714>:472: RuntimeWarning: divide by zero encountered in log\n","  final_features[name + suffix] =            np.log(             np.mean(wap1_log_price_ret_vol[:,:,ffrom:]**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n"]},{"output_type":"stream","name":"stdout","text":["wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock shape[1]==1\n","log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock shape[1]==1\n","wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock shape[1]==1\n","v1proj_25_15 shape[1]==1\n","liqvol1_smean shape[1]==1\n","root_trade_count_smean shape[1]==1\n","v1proj_29_15 shape[1]==1\n","v1proj_20 shape[1]==1\n","v1proj_25 shape[1]==1\n","v1proj_29 shape[1]==1\n","v1proj_29_q1 shape[1]==1\n","v1proj_29_q3 shape[1]==1\n","v1proj_25_q1 shape[1]==1\n","v1proj_25_q3 shape[1]==1\n","v1proj_29_15_q1 shape[1]==1\n","v1proj_29_15_q3 shape[1]==1\n","v1proj_25_15_q1 shape[1]==1\n","v1proj_25_15_q3 shape[1]==1\n","v1proj_25_15_std shape[1]==1\n","v1proj_29_15_std shape[1]==1\n","v1proj_20_std shape[1]==1\n","v1proj_25_std shape[1]==1\n","v1proj_29_std shape[1]==1\n","v1proj_29_q3q1 shape[1]==1\n","tvpl2_rmed2v1 shape[1]==1\n","tvpl2_rmed2v1lf25 shape[1]==1\n","tvpl2_rmed2v1lf29 shape[1]==1\n","v1liq2sprojt10f25 shape[1]==1\n","v1liq2sprojt5f25 shape[1]==1\n","v1spprojt10f29 shape[1]==1\n","v1spprojt15f25 shape[1]==1\n","v1spprojt15f29 shape[1]==1\n","v1spprojt15f29_q1 shape[1]==1\n","v1spprojt15f29_q3 shape[1]==1\n","v1spprojt15f25_q1 shape[1]==1\n","v1spprojt15f25_q3 shape[1]==1\n","v1spprojtf29_q1 shape[1]==1\n","v1spprojtf29_q3 shape[1]==1\n","v1spprojtf25_q1 shape[1]==1\n","v1spprojtf25_q3 shape[1]==1\n","mean_half_delta shape[0]==1\n","mean_half_delta_lsprd shape[0]==1\n","feat_df columns Index(['stock_id', 'time_id', 'stock_ids', 'wap1_log_price_ret_vol',\n","       'log_liq2_ret_*_wap_eqi_price1_ret_vol',\n","       'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol',\n","       'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\n","       'wap1_log_price_ret_per_liq2_vol',\n","       'wap1_log_price_ret_per_spread_sqr_vol',\n","       'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio',\n","       ...\n","       'wap1_log_price_ret_volstock_mean_from_10',\n","       'wap1_log_price_ret_vol_from_20',\n","       'wap1_log_price_ret_volstock_mean_from_20',\n","       'wap1_log_price_ret_vol_from_25',\n","       'wap1_log_price_ret_volstock_mean_from_25', 'vol1_mean',\n","       'mean_half_delta', 'mean_half_delta_lsprd',\n","       'log_wap1_log_price_ret_vol', 'target'],\n","      dtype='object', length=110)\n"]}],"source":["features = generate_liquidity_features(buckets_type)\n","\n","if ml_stage == 'training':\n","  train = pd.read_csv('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/train.csv')\n","  features['target'] = read_targets_from_df(train, buckets_type['stock_ids'], buckets_type['time_ids'])\n","  feat_df = merge_features_to_df(features, train, [f for f in list(features.keys()) if ('time_id' not in f)])\n","  del features\n","  feat_df['target'] = feat_df['target_y']\n","  feat_df.drop(columns=['target_x', 'target_y'], inplace=True)\n","  print('feat_df columns', feat_df.columns)\n","\n","\n","elif ml_stage == 'inference':\n","  os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data')\n","  test = pd.read_csv('test_sub.csv')\n","  feat_df = merge_features_to_df(features, test, [f for f in list(features.keys()) if ('time_id' not in f)])\n","  del features\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mR4orUk2uP5D"},"outputs":[],"source":["\n","# ##### Training and Inference features generation #####\n","general_features = create_training_n_inference_general_features(ml_stage=train_or_inference)\n","\n","bk_level1_2_size_imbalance_feat = general_features.create_bk_level1_2_size_imbalance_feat()\n","trade_sum_size_sum_order_count_sum_size_per_order_count = general_features.create_trade_sum_size_sum_order_count_sum_size_per_order_count()\n","bk_price_size_min_max_range = general_features.create_bk_price_size_min_max_range()\n","bk_price_size_sad = general_features.create_bk_price_size_sad()\n","bk_size_price_corr = general_features.create_bk_size_price_corr()\n","trade_price_size_order_count_min_max_range = general_features.create_trade_price_size_order_count_min_max_range()\n","trade_price_size_order_count_sad = general_features.create_trade_price_size_order_count_sad()\n","trade_price_size_order_count_corr = general_features.create_trade_price_size_order_count_corr()\n","trade_price_n_wap1_deviation_df, trade_price_n_wap_eqi_price0_deviation_df = general_features.create_trade_price_n_wap1_deviation_df_AND_trade_price_n_wap_eqi_price0_deviation_df()\n","\n","feat_df = general_features.merge_trade_price_n_wap_eqi_price0_deviation_df(bk_level1_2_size_imbalance_feat,trade_sum_size_sum_order_count_sum_size_per_order_count,trade_price_n_wap1_deviation_df,trade_price_n_wap_eqi_price0_deviation_df, feat_df)\n","\n","feat_df.drop(columns=['stock_ids'], inplace=True) # drop this redundant column\n","\n","if ml_stage == 'inference':\n","  feat_df.drop(columns=['row_id'], inplace=True) # drop this redundant column\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_partial.pkl','wb') as fb:\n","    pickle.dump( feat_df,fb)\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_partial.pkl','rb') as fb:\n","    feat_df = pickle.load( fb)\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data')\n","df_20_min_volatility = general_features.create_df_20_min_volatility()\n","\n","if ml_stage == 'training':\n","  os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","\n","feat_df, x = general_features.create_n_merge_trade_price_std_df(feat_df)\n","\n","feat_df,all_stocks_first_10_min_vol_df, all_unique_time_ids, all_unique_stock_ids = general_features.create_all_stocks_first_10_min_vol_df_AND_merge_first_10_min_vol_df(feat_df,df_20_min_volatility)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1725281344515,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"hK-k-o2PEFGm","outputId":"25a71506-967d-41b8-8f8b-29df0dfea456"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        stock_id  time_id  wap1_log_price_ret_vol  \\\n","0              0        5                0.001041   \n","1              0       11                0.000315   \n","2              0       16                0.000616   \n","3              0       31                0.000575   \n","4              0       62                0.000456   \n","...          ...      ...                     ...   \n","428927       126    32751                0.000861   \n","428928       126    32753                0.000877   \n","428929       126    32758                0.000791   \n","428930       126    32763                0.000828   \n","428931       126    32767                0.000472   \n","\n","        log_liq2_ret_*_wap_eqi_price1_ret_vol  \\\n","0                                   -7.507561   \n","1                                   -8.516471   \n","2                                   -8.392386   \n","3                                   -7.143974   \n","4                                   -7.694488   \n","...                                       ...   \n","428927                              -8.418616   \n","428928                              -7.147290   \n","428929                              -6.193789   \n","428930                              -6.538962   \n","428931                              -7.472979   \n","\n","        exp_log_liq1_ret_*_wap_eqi_price1_ret_vol  \\\n","0                                       -6.710572   \n","1                                       -9.012040   \n","2                                       -7.488347   \n","3                                       -7.243542   \n","4                                       -6.761988   \n","...                                           ...   \n","428927                                  -6.736662   \n","428928                                  -6.097322   \n","428929                                  -3.761104   \n","428930                                  -7.312112   \n","428931                                  -7.064325   \n","\n","        exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2  \\\n","0                                         -6.710572   \n","1                                         -9.012040   \n","2                                         -7.488347   \n","3                                         -7.243542   \n","4                                         -6.761988   \n","...                                             ...   \n","428927                                    -6.736662   \n","428928                                    -6.097322   \n","428929                                    -3.761104   \n","428930                                    -7.312112   \n","428931                                    -7.064325   \n","\n","        wap1_log_price_ret_per_liq2_vol  \\\n","0                            -20.247208   \n","1                            -23.939039   \n","2                            -21.516460   \n","3                            -21.530967   \n","4                            -22.205537   \n","...                                 ...   \n","428927                       -20.089406   \n","428928                       -19.808887   \n","428929                       -20.237544   \n","428930                       -21.529928   \n","428931                       -23.463953   \n","\n","        wap1_log_price_ret_per_spread_sqr_vol  \\\n","0                                    0.400597   \n","1                                   -0.848347   \n","2                                   -0.484976   \n","3                                   -0.603562   \n","4                                    0.100811   \n","...                                       ...   \n","428927                              -0.257992   \n","428928                               0.416879   \n","428929                               1.634011   \n","428930                               0.793896   \n","428931                               0.541138   \n","\n","        log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio  \\\n","0                                             0.556414   \n","1                                             1.821248   \n","2                                             0.139933   \n","3                                             1.499260   \n","4                                             0.131151   \n","...                                                ...   \n","428927                                       -0.265401   \n","428928                                        1.403245   \n","428929                                        1.009528   \n","428930                                        1.321496   \n","428931                                       -1.215606   \n","\n","        wap1_log_price_ret_per_liq2_vol_15_ratio  ...  trade_price_n_wap1_dev  \\\n","0                                       0.429856  ...                0.012283   \n","1                                       1.033114  ...                0.008724   \n","2                                      -1.716690  ...                0.009948   \n","3                                       0.128663  ...                0.013609   \n","4                                       0.253346  ...                0.009946   \n","...                                          ...  ...                     ...   \n","428927                                  1.613937  ...                0.012744   \n","428928                                  1.311790  ...                0.012897   \n","428929                                  2.112065  ...                0.012144   \n","428930                                  0.201789  ...                0.009728   \n","428931                                 -0.505356  ...                0.009157   \n","\n","          target  trade_price_n_wap_eqi_price0_dev  trade_price_std  \\\n","0       0.004136                          0.011467         0.000578   \n","1       0.001445                          0.008205         0.000304   \n","2       0.002168                          0.009533         0.000932   \n","3       0.002195                          0.013211         0.000729   \n","4       0.001747                          0.010011         0.000182   \n","...          ...                               ...              ...   \n","428927  0.003461                          0.010908         0.000461   \n","428928  0.003113                          0.011912         0.001231   \n","428929  0.004070                          0.012086         0.000409   \n","428930  0.003357                          0.009328         0.000387   \n","428931  0.002090                          0.008504         0.000332   \n","\n","        trade_price_real_vol  trade_size_std  trade_size_mean  \\\n","0                   0.002006      112.876397        73.153846   \n","1                   0.000901       78.800115        44.379310   \n","2                   0.001961      114.630021        89.958333   \n","3                   0.001561      146.441471       139.214286   \n","4                   0.000871      119.416877        85.238095   \n","...                      ...             ...              ...   \n","428927              0.002171      103.529196        71.361111   \n","428928              0.002180       65.750015        49.642857   \n","428929              0.001921      116.913946       106.911765   \n","428930              0.002051      138.269595       113.822785   \n","428931              0.001041      129.027259       143.714286   \n","\n","        trade_order_count_std  trade_order_count_mean  first_10_min_vol  \n","0                    1.985100                2.512821          0.004499  \n","1                    1.472239                1.896552          0.001204  \n","2                    2.321528                2.791667          0.002369  \n","3                    4.111095                4.142857          0.002574  \n","4                    4.142693                4.190476          0.001894  \n","...                       ...                     ...               ...  \n","428927               2.843539                2.833333          0.003691  \n","428928               3.037990                3.119048          0.004104  \n","428929               1.954929                2.764706          0.003117  \n","428930               2.316978                2.873418          0.003661  \n","428931               2.434693                2.885714          0.002092  \n","\n","[428932 rows x 126 columns]"],"text/html":["\n","  <div id=\"df-763ae8a5-8a8b-4382-983e-8fa5ae9ac073\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>stock_id</th>\n","      <th>time_id</th>\n","      <th>wap1_log_price_ret_vol</th>\n","      <th>log_liq2_ret_*_wap_eqi_price1_ret_vol</th>\n","      <th>exp_log_liq1_ret_*_wap_eqi_price1_ret_vol</th>\n","      <th>exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2</th>\n","      <th>wap1_log_price_ret_per_liq2_vol</th>\n","      <th>wap1_log_price_ret_per_spread_sqr_vol</th>\n","      <th>log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio</th>\n","      <th>wap1_log_price_ret_per_liq2_vol_15_ratio</th>\n","      <th>...</th>\n","      <th>trade_price_n_wap1_dev</th>\n","      <th>target</th>\n","      <th>trade_price_n_wap_eqi_price0_dev</th>\n","      <th>trade_price_std</th>\n","      <th>trade_price_real_vol</th>\n","      <th>trade_size_std</th>\n","      <th>trade_size_mean</th>\n","      <th>trade_order_count_std</th>\n","      <th>trade_order_count_mean</th>\n","      <th>first_10_min_vol</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0.001041</td>\n","      <td>-7.507561</td>\n","      <td>-6.710572</td>\n","      <td>-6.710572</td>\n","      <td>-20.247208</td>\n","      <td>0.400597</td>\n","      <td>0.556414</td>\n","      <td>0.429856</td>\n","      <td>...</td>\n","      <td>0.012283</td>\n","      <td>0.004136</td>\n","      <td>0.011467</td>\n","      <td>0.000578</td>\n","      <td>0.002006</td>\n","      <td>112.876397</td>\n","      <td>73.153846</td>\n","      <td>1.985100</td>\n","      <td>2.512821</td>\n","      <td>0.004499</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>0.000315</td>\n","      <td>-8.516471</td>\n","      <td>-9.012040</td>\n","      <td>-9.012040</td>\n","      <td>-23.939039</td>\n","      <td>-0.848347</td>\n","      <td>1.821248</td>\n","      <td>1.033114</td>\n","      <td>...</td>\n","      <td>0.008724</td>\n","      <td>0.001445</td>\n","      <td>0.008205</td>\n","      <td>0.000304</td>\n","      <td>0.000901</td>\n","      <td>78.800115</td>\n","      <td>44.379310</td>\n","      <td>1.472239</td>\n","      <td>1.896552</td>\n","      <td>0.001204</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0.000616</td>\n","      <td>-8.392386</td>\n","      <td>-7.488347</td>\n","      <td>-7.488347</td>\n","      <td>-21.516460</td>\n","      <td>-0.484976</td>\n","      <td>0.139933</td>\n","      <td>-1.716690</td>\n","      <td>...</td>\n","      <td>0.009948</td>\n","      <td>0.002168</td>\n","      <td>0.009533</td>\n","      <td>0.000932</td>\n","      <td>0.001961</td>\n","      <td>114.630021</td>\n","      <td>89.958333</td>\n","      <td>2.321528</td>\n","      <td>2.791667</td>\n","      <td>0.002369</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>31</td>\n","      <td>0.000575</td>\n","      <td>-7.143974</td>\n","      <td>-7.243542</td>\n","      <td>-7.243542</td>\n","      <td>-21.530967</td>\n","      <td>-0.603562</td>\n","      <td>1.499260</td>\n","      <td>0.128663</td>\n","      <td>...</td>\n","      <td>0.013609</td>\n","      <td>0.002195</td>\n","      <td>0.013211</td>\n","      <td>0.000729</td>\n","      <td>0.001561</td>\n","      <td>146.441471</td>\n","      <td>139.214286</td>\n","      <td>4.111095</td>\n","      <td>4.142857</td>\n","      <td>0.002574</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>62</td>\n","      <td>0.000456</td>\n","      <td>-7.694488</td>\n","      <td>-6.761988</td>\n","      <td>-6.761988</td>\n","      <td>-22.205537</td>\n","      <td>0.100811</td>\n","      <td>0.131151</td>\n","      <td>0.253346</td>\n","      <td>...</td>\n","      <td>0.009946</td>\n","      <td>0.001747</td>\n","      <td>0.010011</td>\n","      <td>0.000182</td>\n","      <td>0.000871</td>\n","      <td>119.416877</td>\n","      <td>85.238095</td>\n","      <td>4.142693</td>\n","      <td>4.190476</td>\n","      <td>0.001894</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>428927</th>\n","      <td>126</td>\n","      <td>32751</td>\n","      <td>0.000861</td>\n","      <td>-8.418616</td>\n","      <td>-6.736662</td>\n","      <td>-6.736662</td>\n","      <td>-20.089406</td>\n","      <td>-0.257992</td>\n","      <td>-0.265401</td>\n","      <td>1.613937</td>\n","      <td>...</td>\n","      <td>0.012744</td>\n","      <td>0.003461</td>\n","      <td>0.010908</td>\n","      <td>0.000461</td>\n","      <td>0.002171</td>\n","      <td>103.529196</td>\n","      <td>71.361111</td>\n","      <td>2.843539</td>\n","      <td>2.833333</td>\n","      <td>0.003691</td>\n","    </tr>\n","    <tr>\n","      <th>428928</th>\n","      <td>126</td>\n","      <td>32753</td>\n","      <td>0.000877</td>\n","      <td>-7.147290</td>\n","      <td>-6.097322</td>\n","      <td>-6.097322</td>\n","      <td>-19.808887</td>\n","      <td>0.416879</td>\n","      <td>1.403245</td>\n","      <td>1.311790</td>\n","      <td>...</td>\n","      <td>0.012897</td>\n","      <td>0.003113</td>\n","      <td>0.011912</td>\n","      <td>0.001231</td>\n","      <td>0.002180</td>\n","      <td>65.750015</td>\n","      <td>49.642857</td>\n","      <td>3.037990</td>\n","      <td>3.119048</td>\n","      <td>0.004104</td>\n","    </tr>\n","    <tr>\n","      <th>428929</th>\n","      <td>126</td>\n","      <td>32758</td>\n","      <td>0.000791</td>\n","      <td>-6.193789</td>\n","      <td>-3.761104</td>\n","      <td>-3.761104</td>\n","      <td>-20.237544</td>\n","      <td>1.634011</td>\n","      <td>1.009528</td>\n","      <td>2.112065</td>\n","      <td>...</td>\n","      <td>0.012144</td>\n","      <td>0.004070</td>\n","      <td>0.012086</td>\n","      <td>0.000409</td>\n","      <td>0.001921</td>\n","      <td>116.913946</td>\n","      <td>106.911765</td>\n","      <td>1.954929</td>\n","      <td>2.764706</td>\n","      <td>0.003117</td>\n","    </tr>\n","    <tr>\n","      <th>428930</th>\n","      <td>126</td>\n","      <td>32763</td>\n","      <td>0.000828</td>\n","      <td>-6.538962</td>\n","      <td>-7.312112</td>\n","      <td>-7.312112</td>\n","      <td>-21.529928</td>\n","      <td>0.793896</td>\n","      <td>1.321496</td>\n","      <td>0.201789</td>\n","      <td>...</td>\n","      <td>0.009728</td>\n","      <td>0.003357</td>\n","      <td>0.009328</td>\n","      <td>0.000387</td>\n","      <td>0.002051</td>\n","      <td>138.269595</td>\n","      <td>113.822785</td>\n","      <td>2.316978</td>\n","      <td>2.873418</td>\n","      <td>0.003661</td>\n","    </tr>\n","    <tr>\n","      <th>428931</th>\n","      <td>126</td>\n","      <td>32767</td>\n","      <td>0.000472</td>\n","      <td>-7.472979</td>\n","      <td>-7.064325</td>\n","      <td>-7.064325</td>\n","      <td>-23.463953</td>\n","      <td>0.541138</td>\n","      <td>-1.215606</td>\n","      <td>-0.505356</td>\n","      <td>...</td>\n","      <td>0.009157</td>\n","      <td>0.002090</td>\n","      <td>0.008504</td>\n","      <td>0.000332</td>\n","      <td>0.001041</td>\n","      <td>129.027259</td>\n","      <td>143.714286</td>\n","      <td>2.434693</td>\n","      <td>2.885714</td>\n","      <td>0.002092</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>428932 rows × 126 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-763ae8a5-8a8b-4382-983e-8fa5ae9ac073')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-763ae8a5-8a8b-4382-983e-8fa5ae9ac073 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-763ae8a5-8a8b-4382-983e-8fa5ae9ac073');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-772e1e99-dbb9-4a33-a334-16e199b76d83\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-772e1e99-dbb9-4a33-a334-16e199b76d83')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-772e1e99-dbb9-4a33-a334-16e199b76d83 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_95c8b7f4-3dd1-4705-92a8-df7aba5b471a\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('feat_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_95c8b7f4-3dd1-4705-92a8-df7aba5b471a button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('feat_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"feat_df"}},"metadata":{},"execution_count":100}],"source":["feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":250267,"status":"ok","timestamp":1725281594776,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"7WqwvzqhyPs_","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"17OGUZLoafJAe_3I2dy4NeyHBn1CSeQFo"},"outputId":"fda6cc56-008f-4f3b-cdfc-d867d46a8ece"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","if ml_stage == 'training':\n","  final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df, \\\n","  final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters  = clustering_on_training_data(all_stocks_first_10_min_vol_df)\n","  feat_df = general_features.merge_clustering_features_to_training_data(feat_df,final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df )\n","  os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data')\n","  with open('final_sum_stats_target_vol_clusters.pkl','wb') as fb:\n","    pickle.dump( final_sum_stats_target_vol_clusters,fb)\n","\n","  with open('final_robust_sum_stats_target_vol_clusters.pkl','wb') as fb:\n","    pickle.dump( final_robust_sum_stats_target_vol_clusters,fb)\n","\n","  with open('final_pear_corr_target_vol_clusters.pkl','wb') as fb:\n","    pickle.dump( final_pear_corr_target_vol_clusters,fb)\n","\n","elif ml_stage == 'inference':\n","  ## repeat the clustering of final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters for test time ids as well\n","  os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/Final_submission_data')\n","  with open('final_sum_stats_target_vol_clusters.pkl','rb') as fb:\n","    final_sum_stats_target_vol_clusters = pickle.load(fb)\n","\n","  with open('final_robust_sum_stats_target_vol_clusters.pkl','rb') as fb:\n","    final_robust_sum_stats_target_vol_clusters = pickle.load(fb)\n","\n","  with open('final_pear_corr_target_vol_clusters.pkl','rb') as fb:\n","    final_pear_corr_target_vol_clusters = pickle.load(fb)\n","\n","  final_sum_stats_target_vol_df, final_robust_sum_stats_target_vol_df, final_pear_corr_target_vol_df = apply_trained_clusters_to_inference( test,all_unique_time_ids, all_unique_stock_ids,feat_df,all_stocks_first_10_min_vol_df,final_sum_stats_target_vol_clusters, final_robust_sum_stats_target_vol_clusters, final_pear_corr_target_vol_clusters)\n","\n","feat_df = general_features.merge_bk_price_size_min_max_range(feat_df,bk_price_size_min_max_range)\n","\n","feat_df = general_features.merge_bk_price_size_sad(feat_df,bk_price_size_sad)\n","\n","feat_df = general_features.merge_bk_size_price_corr(feat_df,bk_size_price_corr)\n","\n","\n","feat_df = general_features.merge_trade_price_size_order_count_min_max_range(feat_df,trade_price_size_order_count_min_max_range)\n","\n","feat_df = general_features.merge_trade_price_size_order_count_sad(feat_df,trade_price_size_order_count_sad)\n","\n","\n","feat_df = general_features.merge_trade_price_size_order_count_corr(feat_df,trade_price_size_order_count_corr)\n","\n","\n","if ml_stage == 'training':\n","  unique_stock_ids = np.unique(train['stock_id'])\n","elif ml_stage == 'inference':\n","  unique_stock_ids = np.unique(test['stock_id'])\n","\n","feat_df = general_features.merge_final_sum_stats_target_vol_clusters(feat_df,final_sum_stats_target_vol_clusters,unique_stock_ids)\n","\n","feat_df = general_features.merge_final_robust_sum_stats_target_vol_clusters(feat_df,final_robust_sum_stats_target_vol_clusters,unique_stock_ids)\n","\n","feat_df = general_features.merge_final_pear_corr_target_vol_clusters(feat_df,final_pear_corr_target_vol_clusters,unique_stock_ids)\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_before_transformation.pkl','wb') as fb:\n","#     pickle.dump( feat_df,fb)\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_before_transformation.pkl','rb') as fb:\n","    feat_df = pickle.load( fb)\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","bk_price_size_min_max_range_2 = general_features.create_bk_price_size_min_max_range_2()\n","\n","feat_df = general_features.merge_bk_price_size_min_max_range_2(bk_price_size_min_max_range_2,feat_df)\n","\n","bk_price_size_sad_2 = general_features.create_bk_price_size_sad_2()\n","\n","feat_df = general_features.merge_bk_price_size_sad_2(feat_df,bk_price_size_sad_2)\n","\n","bk_size_price_corr_2 = general_features.create_bk_size_price_corr_2()\n","\n","feat_df = general_features.merge_bk_size_price_corr_2(feat_df,bk_size_price_corr_2)\n","\n","\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_before_transformation.pkl','wb') as fb:\n","#     pickle.dump( feat_df,fb)\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_before_transformation.pkl','rb') as fb:\n","    feat_df = pickle.load( fb)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25169,"status":"ok","timestamp":1725345834161,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"CXHuhwwZ3AWi","outputId":"d3929d06-e75d-4c3d-f5c4-c58ce95b6c10"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['stock_id',\n"," 'time_id',\n"," 'wap1_log_price_ret_vol',\n"," 'log_liq2_ret_*_wap_eqi_price1_ret_vol',\n"," 'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol',\n"," 'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\n"," 'wap1_log_price_ret_per_liq2_vol',\n"," 'wap1_log_price_ret_per_spread_sqr_vol',\n"," 'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio',\n"," 'wap1_log_price_ret_per_liq2_vol_15_ratio',\n"," 'wap1_log_price_ret_per_spread_sqr_vol_15_ratio',\n"," 'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio',\n"," 'exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio',\n"," 'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio',\n"," 'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock',\n"," 'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock',\n"," 'wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock',\n"," 'wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol',\n"," 'wap1_log_price_ret_neg_log_liq_ret_sqr_vol',\n"," 'wap1_log_price_ret_pos_log_liq_ret_sqr_vol',\n"," 'wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol',\n"," 'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0',\n"," 'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0',\n"," 'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0',\n"," 'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10',\n"," 'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10',\n"," 'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10',\n"," 'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n"," 'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20',\n"," 'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20',\n"," 'soft_stock_mean_tvpl2_:0',\n"," 'soft_stock_mean_tvpl2_:10',\n"," 'soft_stock_mean_tvpl2_:20',\n"," 'soft_stock_mean_tvpl2_liqf',\n"," 'soft_stock_mean_tvpl2_liqf_volf10',\n"," 'soft_stock_mean_tvpl2_liqf_volf20',\n"," 'v1proj_25_15',\n"," 'lsvol',\n"," 'liqvol1',\n"," 'liqvol1_smean',\n"," 'liqvol2',\n"," 'liqvol1_15_15',\n"," 'trade_count',\n"," 'root_trade_count',\n"," 'root_trade_count_smean',\n"," 'root_book_delta_count',\n"," 'root_trade_count_var',\n"," 'trade_count_15_15',\n"," 'root_trade_count_15_15',\n"," 'v1proj_29_15',\n"," 'v1proj_20',\n"," 'v1proj_25',\n"," 'v1proj_29',\n"," 'v1proj_29_q1',\n"," 'v1proj_29_q3',\n"," 'v1proj_25_q1',\n"," 'v1proj_25_q3',\n"," 'v1proj_29_15_q1',\n"," 'v1proj_29_15_q3',\n"," 'v1proj_25_15_q1',\n"," 'v1proj_25_15_q3',\n"," 'v1proj_25_15_std',\n"," 'v1proj_29_15_std',\n"," 'v1proj_20_std',\n"," 'v1proj_25_std',\n"," 'v1proj_29_std',\n"," 'v1proj_29_q3q1',\n"," 'tvpl2_rmed2v1',\n"," 'tvpl2_rmed2v1lf25',\n"," 'tvpl2_rmed2v1lf29',\n"," 'tvpl2',\n"," 'tvpl2_liqf10',\n"," 'tvpl2_liqf20',\n"," 'tvpl2_liqf29',\n"," 'tvpl2_smean_vol',\n"," 'tvpl2_smean_vol_liqf10',\n"," 'tvpl2_smean_vol_liqf20',\n"," 'tvpl2_smean_vol_liqf29',\n"," 'v1liq2projt5',\n"," 'v1liq2projt10',\n"," 'v1liq2projt20',\n"," 'liqt10rf29',\n"," 'liqt20rf29',\n"," 'v1liq2sprojt10f25',\n"," 'v1liq2sprojt5f25',\n"," 'v1spprojt10f29',\n"," 'v1spprojt15f25',\n"," 'v1spprojt15f29',\n"," 'v1spprojt15f29_q1',\n"," 'v1spprojt15f29_q3',\n"," 'v1spprojt15f25_q1',\n"," 'v1spprojt15f25_q3',\n"," 'v1spprojtf29_q1',\n"," 'v1spprojtf29_q3',\n"," 'v1spprojtf25_q1',\n"," 'v1spprojtf25_q3',\n"," 'wap1_log_price_ret_vol_from_0',\n"," 'wap1_log_price_ret_volstock_mean_from_0',\n"," 'wap1_log_price_ret_vol_from_10',\n"," 'wap1_log_price_ret_volstock_mean_from_10',\n"," 'wap1_log_price_ret_vol_from_20',\n"," 'wap1_log_price_ret_volstock_mean_from_20',\n"," 'wap1_log_price_ret_vol_from_25',\n"," 'wap1_log_price_ret_volstock_mean_from_25',\n"," 'vol1_mean',\n"," 'mean_half_delta',\n"," 'mean_half_delta_lsprd',\n"," 'log_wap1_log_price_ret_vol',\n"," 'target_x',\n"," 'bid_lvl2_min_lvl1_size_feat',\n"," 'ask_lvl2_min_lvl1_size_feat',\n"," 'lvl2_minus_lvl1_bid_n_ask_size_feat',\n"," 'sum_size',\n"," 'sum_order_count',\n"," 'sum_size_per_order_count',\n"," 'target_y',\n"," 'trade_price_n_wap1_dev',\n"," 'target',\n"," 'trade_price_n_wap_eqi_price0_dev',\n"," 'trade_price_std',\n"," 'trade_price_real_vol',\n"," 'trade_size_std',\n"," 'trade_size_mean',\n"," 'trade_order_count_std',\n"," 'trade_order_count_mean',\n"," 'first_10_min_vol',\n"," 'target_vol_sum_stats_4_clusters',\n"," 'target_vol_sum_stats_10_clusters',\n"," 'target_vol_sum_stats_16_clusters',\n"," 'target_vol_sum_stats_30_clusters',\n"," 'target_vol_robust_sum_stats_2_clusters',\n"," 'target_vol_robust_sum_stats_4_clusters',\n"," 'target_vol_robust_sum_stats_14_clusters',\n"," 'target_vol_robust_sum_stats_20_clusters',\n"," 'target_vol_robust_sum_stats_32_clusters',\n"," 'target_vol_robust_sum_stats_60_clusters',\n"," 'target_vol_pcorr_3_clusters',\n"," 'target_vol_pcorr_49_clusters',\n"," 'target_vol_pcorr_90_clusters',\n"," 'target_vol_pcorr_10_clusters',\n"," 'target_vol_pcorr_26_clusters',\n"," 'min_bid_price1',\n"," 'max_bid_price1',\n"," 'min_ask_price1',\n"," 'max_ask_price1',\n"," 'min_bid_size1',\n"," 'max_bid_size1',\n"," 'min_ask_size1',\n"," 'max_ask_size1',\n"," 'range_ask_price1',\n"," 'range_bid_price1',\n"," 'range_ask_size1',\n"," 'range_bid_size1',\n"," 'sad_ask_price1',\n"," 'sad_ask_size1',\n"," 'sad_bid_price1',\n"," 'sad_bid_size1',\n"," 'bs_bp_corr1',\n"," 'bs_as_corr1',\n"," 'bs_ap_corr1',\n"," 'bp_as_corr1',\n"," 'bp_ap_corr1',\n"," 'as_ap_corr1',\n"," 'min_price1',\n"," 'max_price1',\n"," 'min_size1',\n"," 'max_size1',\n"," 'min_order_count1',\n"," 'max_order_count1',\n"," 'range_price1',\n"," 'range_size1',\n"," 'range_order_count1',\n"," 'sad_price1',\n"," 'sad_size1',\n"," 'sad_order_count1',\n"," 'size_order_count_corr1',\n"," 'sum_stats_4_clusters_labels',\n"," 'sum_stats_10_clusters_labels',\n"," 'sum_stats_16_clusters_labels',\n"," 'sum_stats_30_clusters_labels',\n"," 'robust_sum_stats_2_clusters_labels',\n"," 'robust_sum_stats_4_clusters_labels',\n"," 'robust_sum_stats_14_clusters_labels',\n"," 'robust_sum_stats_20_clusters_labels',\n"," 'robust_sum_stats_32_clusters_labels',\n"," 'robust_sum_stats_60_clusters_labels',\n"," 'pear_corr_3_clusters_labels',\n"," 'pear_corr_49_clusters_labels',\n"," 'pear_corr_90_clusters_labels',\n"," 'pear_corr_10_clusters_labels',\n"," 'pear_corr_26_clusters_labels',\n"," 'min_bid_price2',\n"," 'max_bid_price2',\n"," 'min_ask_price2',\n"," 'max_ask_price2',\n"," 'min_bid_size2',\n"," 'max_bid_size2',\n"," 'min_ask_size2',\n"," 'max_ask_size2',\n"," 'range_ask_price2',\n"," 'range_bid_price2',\n"," 'range_ask_size2',\n"," 'sad_ask_price2',\n"," 'sad_ask_size2',\n"," 'sad_bid_price2',\n"," 'bs_bp_corr2',\n"," 'bs_as_corr2',\n"," 'bs_ap_corr2',\n"," 'bp_as_corr2',\n"," 'bp_ap_corr2',\n"," 'as_ap_corr2']"]},"metadata":{},"execution_count":10}],"source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_before_transformation.pkl','rb') as fb:\n","    feat_df = pickle.load( fb)\n","\n","cols = []\n","for c in feat_df.columns:\n","  cols.append(str(c))\n","\n","cols"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1725345834161,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"AmG7EYnS4J5A","outputId":"a34f1554-8820-4816-d721-5509655e9585"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["        stock_id  time_id  wap1_log_price_ret_vol  \\\n","0              0        5                0.001041   \n","1              0       11                0.000315   \n","2              0       16                0.000616   \n","3              0       31                0.000575   \n","4              0       62                0.000456   \n","...          ...      ...                     ...   \n","428927       126    32751                0.000861   \n","428928       126    32753                0.000877   \n","428929       126    32758                0.000791   \n","428930       126    32763                0.000828   \n","428931       126    32767                0.000472   \n","\n","        log_liq2_ret_*_wap_eqi_price1_ret_vol  \\\n","0                                   -7.507561   \n","1                                   -8.516471   \n","2                                   -8.392386   \n","3                                   -7.143974   \n","4                                   -7.694488   \n","...                                       ...   \n","428927                              -8.418616   \n","428928                              -7.147290   \n","428929                              -6.193789   \n","428930                              -6.538962   \n","428931                              -7.472979   \n","\n","        exp_log_liq1_ret_*_wap_eqi_price1_ret_vol  \\\n","0                                       -6.710572   \n","1                                       -9.012040   \n","2                                       -7.488347   \n","3                                       -7.243542   \n","4                                       -6.761988   \n","...                                           ...   \n","428927                                  -6.736662   \n","428928                                  -6.097322   \n","428929                                  -3.761104   \n","428930                                  -7.312112   \n","428931                                  -7.064325   \n","\n","        exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2  \\\n","0                                         -6.710572   \n","1                                         -9.012040   \n","2                                         -7.488347   \n","3                                         -7.243542   \n","4                                         -6.761988   \n","...                                             ...   \n","428927                                    -6.736662   \n","428928                                    -6.097322   \n","428929                                    -3.761104   \n","428930                                    -7.312112   \n","428931                                    -7.064325   \n","\n","        wap1_log_price_ret_per_liq2_vol  \\\n","0                            -20.247208   \n","1                            -23.939039   \n","2                            -21.516460   \n","3                            -21.530967   \n","4                            -22.205537   \n","...                                 ...   \n","428927                       -20.089406   \n","428928                       -19.808887   \n","428929                       -20.237544   \n","428930                       -21.529928   \n","428931                       -23.463953   \n","\n","        wap1_log_price_ret_per_spread_sqr_vol  \\\n","0                                    0.400597   \n","1                                   -0.848347   \n","2                                   -0.484976   \n","3                                   -0.603562   \n","4                                    0.100811   \n","...                                       ...   \n","428927                              -0.257992   \n","428928                               0.416879   \n","428929                               1.634011   \n","428930                               0.793896   \n","428931                               0.541138   \n","\n","        log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio  \\\n","0                                             0.556414   \n","1                                             1.821248   \n","2                                             0.139933   \n","3                                             1.499260   \n","4                                             0.131151   \n","...                                                ...   \n","428927                                       -0.265401   \n","428928                                        1.403245   \n","428929                                        1.009528   \n","428930                                        1.321496   \n","428931                                       -1.215606   \n","\n","        wap1_log_price_ret_per_liq2_vol_15_ratio  ...  range_ask_size2  \\\n","0                                       0.429856  ...              328   \n","1                                       1.033114  ...              377   \n","2                                      -1.716690  ...              299   \n","3                                       0.128663  ...              366   \n","4                                       0.253346  ...              403   \n","...                                          ...  ...              ...   \n","428927                                  1.613937  ...              348   \n","428928                                  1.311790  ...              172   \n","428929                                  2.112065  ...              227   \n","428930                                  0.201789  ...              425   \n","428931                                 -0.505356  ...              605   \n","\n","        sad_ask_price2  sad_ask_size2  sad_bid_price2  bs_bp_corr2  \\\n","0             0.018721           8251        0.025339     0.245811   \n","1             0.003915           5730        0.006676     0.085185   \n","2             0.018861           7209        0.010675     0.010500   \n","3             0.006292           4908        0.009762    -0.191183   \n","4             0.005361           5700        0.007692    -0.089634   \n","...                ...            ...             ...          ...   \n","428927        0.013878           6581        0.018808     0.093886   \n","428928        0.018198           3590        0.014682    -0.046853   \n","428929        0.016636           3917        0.022709     0.223081   \n","428930        0.030485          15575        0.030483     0.271296   \n","428931        0.014863          10884        0.007869     0.037647   \n","\n","        bs_as_corr2  bs_ap_corr2  bp_as_corr2  bp_ap_corr2  as_ap_corr2  \n","0         -0.061228     0.248983    -0.032249     0.935175     0.041180  \n","1          0.019559     0.159129     0.132679     0.630423     0.306733  \n","2          0.035393     0.028125    -0.219330     0.926692    -0.252432  \n","3         -0.082091    -0.269280     0.085397     0.871938     0.184047  \n","4          0.012950     0.037374    -0.168369     0.630604    -0.127827  \n","...             ...          ...          ...          ...          ...  \n","428927     0.054738     0.170494    -0.017337     0.899943    -0.052305  \n","428928     0.083372    -0.005831     0.172328     0.985156     0.185020  \n","428929    -0.161036     0.047822    -0.209329     0.839888    -0.121916  \n","428930     0.005878     0.261403    -0.068766     0.881712    -0.041544  \n","428931    -0.090892     0.031230    -0.135952     0.904962    -0.161252  \n","\n","[428932 rows x 211 columns]"],"text/html":["\n","  <div id=\"df-a9e0a743-70f6-4421-abd2-d989af9156d3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>stock_id</th>\n","      <th>time_id</th>\n","      <th>wap1_log_price_ret_vol</th>\n","      <th>log_liq2_ret_*_wap_eqi_price1_ret_vol</th>\n","      <th>exp_log_liq1_ret_*_wap_eqi_price1_ret_vol</th>\n","      <th>exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2</th>\n","      <th>wap1_log_price_ret_per_liq2_vol</th>\n","      <th>wap1_log_price_ret_per_spread_sqr_vol</th>\n","      <th>log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio</th>\n","      <th>wap1_log_price_ret_per_liq2_vol_15_ratio</th>\n","      <th>...</th>\n","      <th>range_ask_size2</th>\n","      <th>sad_ask_price2</th>\n","      <th>sad_ask_size2</th>\n","      <th>sad_bid_price2</th>\n","      <th>bs_bp_corr2</th>\n","      <th>bs_as_corr2</th>\n","      <th>bs_ap_corr2</th>\n","      <th>bp_as_corr2</th>\n","      <th>bp_ap_corr2</th>\n","      <th>as_ap_corr2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0.001041</td>\n","      <td>-7.507561</td>\n","      <td>-6.710572</td>\n","      <td>-6.710572</td>\n","      <td>-20.247208</td>\n","      <td>0.400597</td>\n","      <td>0.556414</td>\n","      <td>0.429856</td>\n","      <td>...</td>\n","      <td>328</td>\n","      <td>0.018721</td>\n","      <td>8251</td>\n","      <td>0.025339</td>\n","      <td>0.245811</td>\n","      <td>-0.061228</td>\n","      <td>0.248983</td>\n","      <td>-0.032249</td>\n","      <td>0.935175</td>\n","      <td>0.041180</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>0.000315</td>\n","      <td>-8.516471</td>\n","      <td>-9.012040</td>\n","      <td>-9.012040</td>\n","      <td>-23.939039</td>\n","      <td>-0.848347</td>\n","      <td>1.821248</td>\n","      <td>1.033114</td>\n","      <td>...</td>\n","      <td>377</td>\n","      <td>0.003915</td>\n","      <td>5730</td>\n","      <td>0.006676</td>\n","      <td>0.085185</td>\n","      <td>0.019559</td>\n","      <td>0.159129</td>\n","      <td>0.132679</td>\n","      <td>0.630423</td>\n","      <td>0.306733</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>0.000616</td>\n","      <td>-8.392386</td>\n","      <td>-7.488347</td>\n","      <td>-7.488347</td>\n","      <td>-21.516460</td>\n","      <td>-0.484976</td>\n","      <td>0.139933</td>\n","      <td>-1.716690</td>\n","      <td>...</td>\n","      <td>299</td>\n","      <td>0.018861</td>\n","      <td>7209</td>\n","      <td>0.010675</td>\n","      <td>0.010500</td>\n","      <td>0.035393</td>\n","      <td>0.028125</td>\n","      <td>-0.219330</td>\n","      <td>0.926692</td>\n","      <td>-0.252432</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>31</td>\n","      <td>0.000575</td>\n","      <td>-7.143974</td>\n","      <td>-7.243542</td>\n","      <td>-7.243542</td>\n","      <td>-21.530967</td>\n","      <td>-0.603562</td>\n","      <td>1.499260</td>\n","      <td>0.128663</td>\n","      <td>...</td>\n","      <td>366</td>\n","      <td>0.006292</td>\n","      <td>4908</td>\n","      <td>0.009762</td>\n","      <td>-0.191183</td>\n","      <td>-0.082091</td>\n","      <td>-0.269280</td>\n","      <td>0.085397</td>\n","      <td>0.871938</td>\n","      <td>0.184047</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>62</td>\n","      <td>0.000456</td>\n","      <td>-7.694488</td>\n","      <td>-6.761988</td>\n","      <td>-6.761988</td>\n","      <td>-22.205537</td>\n","      <td>0.100811</td>\n","      <td>0.131151</td>\n","      <td>0.253346</td>\n","      <td>...</td>\n","      <td>403</td>\n","      <td>0.005361</td>\n","      <td>5700</td>\n","      <td>0.007692</td>\n","      <td>-0.089634</td>\n","      <td>0.012950</td>\n","      <td>0.037374</td>\n","      <td>-0.168369</td>\n","      <td>0.630604</td>\n","      <td>-0.127827</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>428927</th>\n","      <td>126</td>\n","      <td>32751</td>\n","      <td>0.000861</td>\n","      <td>-8.418616</td>\n","      <td>-6.736662</td>\n","      <td>-6.736662</td>\n","      <td>-20.089406</td>\n","      <td>-0.257992</td>\n","      <td>-0.265401</td>\n","      <td>1.613937</td>\n","      <td>...</td>\n","      <td>348</td>\n","      <td>0.013878</td>\n","      <td>6581</td>\n","      <td>0.018808</td>\n","      <td>0.093886</td>\n","      <td>0.054738</td>\n","      <td>0.170494</td>\n","      <td>-0.017337</td>\n","      <td>0.899943</td>\n","      <td>-0.052305</td>\n","    </tr>\n","    <tr>\n","      <th>428928</th>\n","      <td>126</td>\n","      <td>32753</td>\n","      <td>0.000877</td>\n","      <td>-7.147290</td>\n","      <td>-6.097322</td>\n","      <td>-6.097322</td>\n","      <td>-19.808887</td>\n","      <td>0.416879</td>\n","      <td>1.403245</td>\n","      <td>1.311790</td>\n","      <td>...</td>\n","      <td>172</td>\n","      <td>0.018198</td>\n","      <td>3590</td>\n","      <td>0.014682</td>\n","      <td>-0.046853</td>\n","      <td>0.083372</td>\n","      <td>-0.005831</td>\n","      <td>0.172328</td>\n","      <td>0.985156</td>\n","      <td>0.185020</td>\n","    </tr>\n","    <tr>\n","      <th>428929</th>\n","      <td>126</td>\n","      <td>32758</td>\n","      <td>0.000791</td>\n","      <td>-6.193789</td>\n","      <td>-3.761104</td>\n","      <td>-3.761104</td>\n","      <td>-20.237544</td>\n","      <td>1.634011</td>\n","      <td>1.009528</td>\n","      <td>2.112065</td>\n","      <td>...</td>\n","      <td>227</td>\n","      <td>0.016636</td>\n","      <td>3917</td>\n","      <td>0.022709</td>\n","      <td>0.223081</td>\n","      <td>-0.161036</td>\n","      <td>0.047822</td>\n","      <td>-0.209329</td>\n","      <td>0.839888</td>\n","      <td>-0.121916</td>\n","    </tr>\n","    <tr>\n","      <th>428930</th>\n","      <td>126</td>\n","      <td>32763</td>\n","      <td>0.000828</td>\n","      <td>-6.538962</td>\n","      <td>-7.312112</td>\n","      <td>-7.312112</td>\n","      <td>-21.529928</td>\n","      <td>0.793896</td>\n","      <td>1.321496</td>\n","      <td>0.201789</td>\n","      <td>...</td>\n","      <td>425</td>\n","      <td>0.030485</td>\n","      <td>15575</td>\n","      <td>0.030483</td>\n","      <td>0.271296</td>\n","      <td>0.005878</td>\n","      <td>0.261403</td>\n","      <td>-0.068766</td>\n","      <td>0.881712</td>\n","      <td>-0.041544</td>\n","    </tr>\n","    <tr>\n","      <th>428931</th>\n","      <td>126</td>\n","      <td>32767</td>\n","      <td>0.000472</td>\n","      <td>-7.472979</td>\n","      <td>-7.064325</td>\n","      <td>-7.064325</td>\n","      <td>-23.463953</td>\n","      <td>0.541138</td>\n","      <td>-1.215606</td>\n","      <td>-0.505356</td>\n","      <td>...</td>\n","      <td>605</td>\n","      <td>0.014863</td>\n","      <td>10884</td>\n","      <td>0.007869</td>\n","      <td>0.037647</td>\n","      <td>-0.090892</td>\n","      <td>0.031230</td>\n","      <td>-0.135952</td>\n","      <td>0.904962</td>\n","      <td>-0.161252</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>428932 rows × 211 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9e0a743-70f6-4421-abd2-d989af9156d3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a9e0a743-70f6-4421-abd2-d989af9156d3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a9e0a743-70f6-4421-abd2-d989af9156d3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-236547e5-425d-48cd-b231-ecaef3ec8157\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-236547e5-425d-48cd-b231-ecaef3ec8157')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-236547e5-425d-48cd-b231-ecaef3ec8157 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_d6149455-b568-4c98-a71d-f3efff2e114c\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('feat_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_d6149455-b568-4c98-a71d-f3efff2e114c button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('feat_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"feat_df"}},"metadata":{},"execution_count":11}],"source":["feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQ3Vp6932j2z"},"outputs":[],"source":["# check for duplicate columns in train_feat_df\n","# max_price1 and min_price1 were added twice so we check for duplicate columns\n","\n","uniq_cols = np.unique(feat_df.columns)\n","\n","## check for missing values\n","(feat_df.columns).shape\n","\n","twice_ctr = {} # twice occurance counter\n","for c in uniq_cols:\n","    twice_ctr[c] = 0\n","\n","for c in feat_df.columns:\n","    twice_ctr[c]+=1\n","\n","idx = np.where(np.array(list(twice_ctr.values())) == 2) ## col 41 and 54 in uniq_cols are repeated\n","\n","np.array(list(twice_ctr.keys()))[41]\n","np.array(list(twice_ctr.keys()))[54]\n","\n","# remove duplicate columns\n","feat_df = feat_df.loc[:,~feat_df.columns.duplicated()].copy()\n","\n","#feat_df.drop(columns=['stock_ids'], inplace=True) # drop this redundant column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYN-5I-hRYFA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725345864714,"user_tz":-480,"elapsed":20527,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"}},"outputId":"8047a5ba-f1bd-4839-89de-c8fea386e4b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["exp_log_liq1_ret_*_wap_eqi_price1_ret_vol exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2 exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio\n","exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\n","target_x target_y\n","target_x target\n","target_y target_x\n","target_y target\n","target target_x\n","target target_y\n"]}],"source":["#### Check that no two columns have the same values\n","\n","drop_cols_for_inference = np.array([])\n","\n","for c in feat_df.columns:\n","    ctr=0\n","    for c1 in feat_df.columns:\n","        if np.array_equal(feat_df[c], feat_df[c1]) and c!=c1:\n","            ctr+=1\n","            print(c, c1)\n","        # if (ctr>1):\n","        #     print(c, c1)\n","\n","## Drop the columns that have the same values\n","feat_df.drop(columns=['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio\",\"target_x\",\"target_y\" ], inplace=True)\n","drop_cols_for_inference = np.append(drop_cols_for_inference,['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2','exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio'])\n"]},{"cell_type":"code","source":["\n","# Check for NULL and INF values in feat_df\n","\n","nan_cols = []\n","pos_inf_cols = []\n","neg_inf_cols = []\n","\n","float_cols = [ c for c in feat_df.columns if feat_df[c].dtype != 'category']\n","\n","for c in float_cols:\n","    if np.isinf(feat_df[c]).any():\n","        print(\"pos INF: \",c,': ', np.isinf(feat_df[c]).sum())\n","        pos_inf_cols.append(c)\n","\n","    if np.isneginf(feat_df[c]).any():\n","        print(\"neg INF: \",c,': ', np.isneginf(feat_df[c]).sum())\n","        neg_inf_cols.append(c)\n","\n","    if np.isnan(feat_df[c]).any():\n","        print(\"NaN: \", c ,': ', np.isnan(feat_df[c]).sum())\n","        nan_cols.append(c)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nJWDwyZ0XhV","executionInfo":{"status":"ok","timestamp":1725345865028,"user_tz":-480,"elapsed":332,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"}},"outputId":"b3c91440-dd9f-49fa-887f-a0d1116c9f5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pos INF:  wap1_log_price_ret_per_liq2_vol_15_ratio :  1\n","neg INF:  wap1_log_price_ret_per_liq2_vol_15_ratio :  1\n","pos INF:  wap1_log_price_ret_per_spread_sqr_vol_15_ratio :  1\n","neg INF:  wap1_log_price_ret_per_spread_sqr_vol_15_ratio :  1\n","pos INF:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio :  2831\n","neg INF:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio :  1323\n","NaN:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio :  589\n","NaN:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock :  54086\n","pos INF:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol :  589\n","neg INF:  wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol :  589\n","pos INF:  lsvol :  139\n","neg INF:  lsvol :  139\n","pos INF:  trade_count :  19\n","neg INF:  trade_count :  19\n","pos INF:  root_trade_count :  19\n","neg INF:  root_trade_count :  19\n","pos INF:  root_trade_count_var :  19\n","neg INF:  root_trade_count_var :  19\n","pos INF:  trade_count_15_15 :  397\n","neg INF:  trade_count_15_15 :  219\n","NaN:  trade_count_15_15 :  19\n","pos INF:  root_trade_count_15_15 :  397\n","neg INF:  root_trade_count_15_15 :  219\n","NaN:  root_trade_count_15_15 :  19\n","NaN:  v1proj_25_15_std :  560\n","NaN:  v1proj_29_15_std :  282002\n","NaN:  v1proj_25_std :  560\n","NaN:  v1proj_29_std :  51517\n","pos INF:  tvpl2 :  19\n","neg INF:  tvpl2 :  19\n","pos INF:  tvpl2_liqf10 :  19\n","neg INF:  tvpl2_liqf10 :  19\n","pos INF:  tvpl2_liqf20 :  19\n","neg INF:  tvpl2_liqf20 :  19\n","pos INF:  tvpl2_liqf29 :  19\n","neg INF:  tvpl2_liqf29 :  19\n","pos INF:  wap1_log_price_ret_vol_from_25 :  5\n","neg INF:  wap1_log_price_ret_vol_from_25 :  5\n"]}]},{"cell_type":"code","source":["####### Drop any feature with large number of nan, positive and negative infinity\n","\n","\n","for c in feat_df.columns:\n","    if  np.isinf(feat_df[c]).sum() > 1000 or np.isneginf(feat_df[c]).sum() > 1000 or np.isnan(feat_df[c]).sum() > 1000:\n","        feat_df.drop(columns=[c], inplace=True)\n","        print(c)\n","        drop_cols_for_inference = np.append(drop_cols_for_inference,c)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-PHfwx9_06yk","executionInfo":{"status":"ok","timestamp":1725345866334,"user_tz":-480,"elapsed":1307,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"}},"outputId":"a22c458f-247b-42d5-b53a-9761adabc09a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\n","wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\n","v1proj_29_15_std\n","v1proj_29_std\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","drop_cols_for_inference = list(np.reshape(drop_cols_for_inference,(-1)))\n","with open('drop_cols_for_inference.pkl','wb') as fb:\n","    pickle.dump( drop_cols_for_inference,fb)"],"metadata":{"id":"GMsJ7ytgD3zq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####### HANDLING NULL and INF VALUES ########\n","\n","### drop columns with very high fraction of NaN values\n","# feat_df.drop(columns=[\"v1proj_29_15_std\"], inplace=True)\n","\n","# ### groupby stock id and fill NaN values with median of the stock id or leave it the same\n","# for c in nan_cols:\n","#     feat_df[c] = feat_df.groupby('stock_id')[c].transform(lambda x: x.fillna(np.nanmedian(x)))\n","\n","pos_inf_cols.remove('wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio')\n","neg_inf_cols.remove('wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio')\n","\n","\n","### groupby stock id and fill positive infinity values with max of the stock id or 1e8\n","for c in pos_inf_cols:\n","    #feat_df[c] = feat_df.groupby('stock_id')[c].transform(lambda x: x.replace([np.inf], x.loc[~np.isinf(x)].max()))\n","    feat_df[c].replace(np.inf, 1e8, inplace=True)\n","\n","### groupby stock id and fill negative infinity values with min of the stock id or -1e8\n","for c in neg_inf_cols:\n","    #feat_df[c] = feat_df.groupby('stock_id')[c].transform(lambda x: x.replace([-np.inf], x.loc[~np.isneginf(x)].min()))\n","    feat_df[c].replace(-np.inf, -1e8, inplace=True)\n","\n"],"metadata":{"id":"1ALLBfP-1Czr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_before_transformation_nan_inf_replaced.pkl','wb') as fb:\n","#     pickle.dump( feat_df,fb)\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_before_transformation_nan_inf_replaced.pkl','rb') as fb:\n","    feat_df = pickle.load( fb)"],"metadata":{"id":"SmkM1SXK0Eu_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/liquidity_features')\n","\n","with open('train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced.pkl','rb') as fb:\n","     train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced = pickle.load( fb)\n","\n"],"metadata":{"id":"fxD2VIsk20Xn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for c in set(train_feat_df.columns):\n","  if np.array_equal(train_feat_df[c], train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c]):\n","    print(c, 'YES')\n","  else:\n","    print(c, 'NO')\n","    diff = abs(train_feat_df[c] - train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c])\n","    if max(diff) > 1e-6:\n","      print(max(diff),'\\n')\n"],"metadata":{"id":"fW43ettH23kU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced.drop(columns=['book_ewma_vol', 'log_target', 'log_target_standardized', 'trade_ewma_vol'], inplace=True)\n","\n","for c in train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced.columns:\n","    if  not (np.array_equal(train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c], train_feat_df[c])):\n","        diff_df = train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c] - train_feat_df[c]\n","        print(f'{c} # diff {sum(diff_df >= 1e-6)}')#, diff {diff_df[diff_df >= 1e-6]} scale {train_feat_df[diff_df >= 1e-6][c]}')\n","\n"],"metadata":{"id":"FSMf7sWX26gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ########## code to reorder time ids to correct sequence/order for training\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","\n","import glob\n","\n","import numpy as np\n","import pandas as pd\n","from joblib import Parallel, delayed\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import minmax_scale\n","\n","\n","def calc_price_from_tick(df):\n","    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n","    return 0.01 / tick\n","\n","\n","def calc_prices(r):\n","    df = pd.read_parquet(r.book_path,\n","                         columns=[\n","                             'time_id',\n","                             'ask_price1',\n","                             'ask_price2',\n","                             'bid_price1',\n","                             'bid_price2'\n","                         ])\n","    df = df.groupby('time_id') \\\n","        .apply(calc_price_from_tick).to_frame('price').reset_index()\n","    df['stock_id'] = r.stock_id\n","    return df\n","\n","\n","paths = sorted(glob.glob('book_train.parquet/stock_id=*'), key=lambda x: int(x.split('=')[1]))\n","\n","df_files = pd.DataFrame(\n","    {'book_path': paths}) \\\n","    .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")',\n","          engine='python')\n","\n","# build price matrix using tick-size\n","df_prices = pd.concat(\n","    Parallel(n_jobs=4)(\n","        delayed(calc_prices)(r) for _, r in df_files.iterrows()\n","    )\n",")\n","\n","\n","df_prices = df_prices.pivot(columns='stock_id', values='price', index='time_id')\n","\n","# t-SNE to recovering time-id order\n","clf = TSNE(\n","    n_components=1,\n","    perplexity=400,\n","    random_state=0,\n","    n_iter=2000\n",")\n","compressed = clf.fit_transform(\n","    pd.DataFrame(minmax_scale(df_prices.fillna(df_prices.mean())))\n",")\n","\n","order = np.argsort(compressed[:, 0])\n","ordered = df_prices.reindex(order).reset_index(drop=True)\n","\n","# correct direction of time-id order using known stock (id61 = AMZN)\n","if ordered[61].iloc[0] > ordered[61].iloc[-1]:\n","    ordered = ordered.reindex(ordered.index[::-1])\\\n","        .reset_index(drop=True)\n","\n","train = pd.read_csv('train.csv')\n","all_uniq_time_ids = pd.DataFrame({'time_id':train['time_id'].unique()})\n","correct_time_id_order = all_uniq_time_ids.reindex(order)[::-1].reset_index(drop=True)\n"],"metadata":{"id":"5dJjsrW6EK6R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/liquidity_features')\n","\n","with open('correct_time_id_order.pkl', 'rb') as f:\n","    correct_time_id_order = pickle.load(f)\n"],"metadata":{"id":"cm31j9GNELv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ########## code to reorder time ids to correct sequence/order for training\n","\n","#feat_df[\"seq_id\"] = -1 ## we exclude seq id because it cannot be generated for the submission data\n","\n","time_ids_reordered = correct_time_id_order['time_id'].values\n","\n","def my_reorder_stock_in_df(st_df, time_ids_reordered):\n","    common_values = [value for value in time_ids_reordered if value in st_df['time_id'].values]\n","    st_df = st_df.set_index('time_id')\n","    st_df = st_df.reindex(common_values)\n","    st_df = st_df.reset_index()\n","    #st_df[\"seq_id\"] = range(st_df.shape[0]) ## we exclude seq id because it cannot be generated for the submission data\n","    return st_df\n","\n","# Assuming you have a dataframe called 'feat_df' and an array of reordered time_ids called 'time_ids_reordered'\n","feat_df_reordered = feat_df.groupby('stock_id').apply(my_reorder_stock_in_df, time_ids_reordered=time_ids_reordered).reset_index(drop=True)\n","del feat_df\n","\n"],"metadata":{"id":"U1HwjFTXENdr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_before_transformation_nan_inf_replaced_reordered.pkl','wb') as fb:\n","    pickle.dump( feat_df_reordered,fb)\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","with open('train_feat_df_before_transformation_nan_inf_replaced_reordered.pkl','rb') as fb:\n","    feat_df_reordered = pickle.load( fb)"],"metadata":{"id":"GLJOgPRHEPmV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feat_df_reordered"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"id":"RuNkAAqkp1lE","executionInfo":{"status":"ok","timestamp":1725346308207,"user_tz":-480,"elapsed":425,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"}},"outputId":"87ac5e2d-48b1-4162-836d-e9c45ceebff0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        time_id  stock_id  wap1_log_price_ret_vol  \\\n","0          4294         0                0.001613   \n","1         31984         0                0.000351   \n","2         31570         0                0.000254   \n","3          5666         0                0.000565   \n","4         29740         0                0.000464   \n","...         ...       ...                     ...   \n","428927    24913       126                0.001561   \n","428928    15365       126                0.001210   \n","428929    29316       126                0.001053   \n","428930    32195       126                0.001233   \n","428931    10890       126                0.001040   \n","\n","        log_liq2_ret_*_wap_eqi_price1_ret_vol  \\\n","0                                   -5.934349   \n","1                                   -7.451430   \n","2                                   -8.310555   \n","3                                   -8.137130   \n","4                                   -8.037395   \n","...                                       ...   \n","428927                              -7.289034   \n","428928                              -6.890628   \n","428929                              -6.986471   \n","428930                              -6.594648   \n","428931                              -7.441125   \n","\n","        exp_log_liq1_ret_*_wap_eqi_price1_ret_vol  \\\n","0                                       -6.810587   \n","1                                       -8.245446   \n","2                                       -8.679023   \n","3                                       -4.350507   \n","4                                       -8.226672   \n","...                                           ...   \n","428927                                  -6.646298   \n","428928                                  -2.398457   \n","428929                                  -5.495227   \n","428930                                  -4.616600   \n","428931                                  -5.449819   \n","\n","        wap1_log_price_ret_per_liq2_vol  \\\n","0                            -17.351526   \n","1                            -23.841381   \n","2                            -24.343382   \n","3                            -22.429991   \n","4                            -23.393270   \n","...                                 ...   \n","428927                       -18.611494   \n","428928                       -18.541900   \n","428929                       -19.725357   \n","428930                       -19.147121   \n","428931                       -20.547594   \n","\n","        wap1_log_price_ret_per_spread_sqr_vol  \\\n","0                                   -0.288068   \n","1                                   -0.091547   \n","2                                   -0.661090   \n","3                                    0.547199   \n","4                                    1.294799   \n","...                                       ...   \n","428927                               0.409686   \n","428928                               0.457009   \n","428929                               0.205459   \n","428930                               0.732570   \n","428931                               0.643669   \n","\n","        log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio  \\\n","0                                            -1.833425   \n","1                                            -1.305534   \n","2                                             0.946295   \n","3                                             0.736498   \n","4                                             0.827131   \n","...                                                ...   \n","428927                                       -1.037227   \n","428928                                        1.055824   \n","428929                                        0.003620   \n","428930                                       -0.129259   \n","428931                                       -0.801394   \n","\n","        wap1_log_price_ret_per_liq2_vol_15_ratio  \\\n","0                                      -1.269886   \n","1                                       0.432976   \n","2                                      -0.798889   \n","3                                       1.278856   \n","4                                       0.271693   \n","...                                          ...   \n","428927                                  0.821153   \n","428928                                 -0.140664   \n","428929                                 -0.096869   \n","428930                                  0.202878   \n","428931                                 -0.825760   \n","\n","        wap1_log_price_ret_per_spread_sqr_vol_15_ratio  ...  range_ask_size2  \\\n","0                                            -1.201815  ...              999   \n","1                                             0.112902  ...              218   \n","2                                            -0.658543  ...              299   \n","3                                             0.063039  ...              299   \n","4                                             2.520512  ...              430   \n","...                                                ...  ...              ...   \n","428927                                        0.495971  ...             2599   \n","428928                                       -0.283738  ...              301   \n","428929                                       -0.435193  ...             2307   \n","428930                                        0.125500  ...              522   \n","428931                                        0.039664  ...              399   \n","\n","        sad_ask_price2  sad_ask_size2  sad_bid_price2  bs_bp_corr2  \\\n","0             0.023504           6268        0.036494     0.080427   \n","1             0.006359           3774        0.013130    -0.042634   \n","2             0.003999           4963        0.010766    -0.006466   \n","3             0.017104           9096        0.014698     0.014301   \n","4             0.011737          11234        0.010149     0.200608   \n","...                ...            ...             ...          ...   \n","428927        0.026530          27485        0.024297     0.323561   \n","428928        0.027022           7761        0.017443    -0.097204   \n","428929        0.016124          15714        0.011446     0.358827   \n","428930        0.025497          14200        0.018408     0.363703   \n","428931        0.030890          17284        0.027961     0.146666   \n","\n","        bs_as_corr2  bs_ap_corr2  bp_as_corr2  bp_ap_corr2  as_ap_corr2  \n","0          0.104507     0.222682     0.374197     0.645008     0.366270  \n","1         -0.028048    -0.035978    -0.191972     0.859622    -0.231770  \n","2         -0.229185    -0.042428    -0.060536     0.787929     0.041905  \n","3         -0.100436    -0.079601    -0.221614     0.853954     0.025725  \n","4         -0.096892     0.091529    -0.424228     0.786944    -0.255468  \n","...             ...          ...          ...          ...          ...  \n","428927     0.210561     0.318678     0.104992     0.973739     0.125456  \n","428928     0.024666    -0.049572    -0.258912     0.982739    -0.234815  \n","428929    -0.111898     0.319494    -0.016156     0.889851    -0.127169  \n","428930    -0.211824     0.398834    -0.261142     0.979920    -0.263461  \n","428931     0.085647     0.177233    -0.016643     0.984503     0.015227  \n","\n","[428932 rows x 203 columns]"],"text/html":["\n","  <div id=\"df-9bd5d88b-2915-4508-be0e-ecba0860f354\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>time_id</th>\n","      <th>stock_id</th>\n","      <th>wap1_log_price_ret_vol</th>\n","      <th>log_liq2_ret_*_wap_eqi_price1_ret_vol</th>\n","      <th>exp_log_liq1_ret_*_wap_eqi_price1_ret_vol</th>\n","      <th>wap1_log_price_ret_per_liq2_vol</th>\n","      <th>wap1_log_price_ret_per_spread_sqr_vol</th>\n","      <th>log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio</th>\n","      <th>wap1_log_price_ret_per_liq2_vol_15_ratio</th>\n","      <th>wap1_log_price_ret_per_spread_sqr_vol_15_ratio</th>\n","      <th>...</th>\n","      <th>range_ask_size2</th>\n","      <th>sad_ask_price2</th>\n","      <th>sad_ask_size2</th>\n","      <th>sad_bid_price2</th>\n","      <th>bs_bp_corr2</th>\n","      <th>bs_as_corr2</th>\n","      <th>bs_ap_corr2</th>\n","      <th>bp_as_corr2</th>\n","      <th>bp_ap_corr2</th>\n","      <th>as_ap_corr2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4294</td>\n","      <td>0</td>\n","      <td>0.001613</td>\n","      <td>-5.934349</td>\n","      <td>-6.810587</td>\n","      <td>-17.351526</td>\n","      <td>-0.288068</td>\n","      <td>-1.833425</td>\n","      <td>-1.269886</td>\n","      <td>-1.201815</td>\n","      <td>...</td>\n","      <td>999</td>\n","      <td>0.023504</td>\n","      <td>6268</td>\n","      <td>0.036494</td>\n","      <td>0.080427</td>\n","      <td>0.104507</td>\n","      <td>0.222682</td>\n","      <td>0.374197</td>\n","      <td>0.645008</td>\n","      <td>0.366270</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31984</td>\n","      <td>0</td>\n","      <td>0.000351</td>\n","      <td>-7.451430</td>\n","      <td>-8.245446</td>\n","      <td>-23.841381</td>\n","      <td>-0.091547</td>\n","      <td>-1.305534</td>\n","      <td>0.432976</td>\n","      <td>0.112902</td>\n","      <td>...</td>\n","      <td>218</td>\n","      <td>0.006359</td>\n","      <td>3774</td>\n","      <td>0.013130</td>\n","      <td>-0.042634</td>\n","      <td>-0.028048</td>\n","      <td>-0.035978</td>\n","      <td>-0.191972</td>\n","      <td>0.859622</td>\n","      <td>-0.231770</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>31570</td>\n","      <td>0</td>\n","      <td>0.000254</td>\n","      <td>-8.310555</td>\n","      <td>-8.679023</td>\n","      <td>-24.343382</td>\n","      <td>-0.661090</td>\n","      <td>0.946295</td>\n","      <td>-0.798889</td>\n","      <td>-0.658543</td>\n","      <td>...</td>\n","      <td>299</td>\n","      <td>0.003999</td>\n","      <td>4963</td>\n","      <td>0.010766</td>\n","      <td>-0.006466</td>\n","      <td>-0.229185</td>\n","      <td>-0.042428</td>\n","      <td>-0.060536</td>\n","      <td>0.787929</td>\n","      <td>0.041905</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5666</td>\n","      <td>0</td>\n","      <td>0.000565</td>\n","      <td>-8.137130</td>\n","      <td>-4.350507</td>\n","      <td>-22.429991</td>\n","      <td>0.547199</td>\n","      <td>0.736498</td>\n","      <td>1.278856</td>\n","      <td>0.063039</td>\n","      <td>...</td>\n","      <td>299</td>\n","      <td>0.017104</td>\n","      <td>9096</td>\n","      <td>0.014698</td>\n","      <td>0.014301</td>\n","      <td>-0.100436</td>\n","      <td>-0.079601</td>\n","      <td>-0.221614</td>\n","      <td>0.853954</td>\n","      <td>0.025725</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>29740</td>\n","      <td>0</td>\n","      <td>0.000464</td>\n","      <td>-8.037395</td>\n","      <td>-8.226672</td>\n","      <td>-23.393270</td>\n","      <td>1.294799</td>\n","      <td>0.827131</td>\n","      <td>0.271693</td>\n","      <td>2.520512</td>\n","      <td>...</td>\n","      <td>430</td>\n","      <td>0.011737</td>\n","      <td>11234</td>\n","      <td>0.010149</td>\n","      <td>0.200608</td>\n","      <td>-0.096892</td>\n","      <td>0.091529</td>\n","      <td>-0.424228</td>\n","      <td>0.786944</td>\n","      <td>-0.255468</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>428927</th>\n","      <td>24913</td>\n","      <td>126</td>\n","      <td>0.001561</td>\n","      <td>-7.289034</td>\n","      <td>-6.646298</td>\n","      <td>-18.611494</td>\n","      <td>0.409686</td>\n","      <td>-1.037227</td>\n","      <td>0.821153</td>\n","      <td>0.495971</td>\n","      <td>...</td>\n","      <td>2599</td>\n","      <td>0.026530</td>\n","      <td>27485</td>\n","      <td>0.024297</td>\n","      <td>0.323561</td>\n","      <td>0.210561</td>\n","      <td>0.318678</td>\n","      <td>0.104992</td>\n","      <td>0.973739</td>\n","      <td>0.125456</td>\n","    </tr>\n","    <tr>\n","      <th>428928</th>\n","      <td>15365</td>\n","      <td>126</td>\n","      <td>0.001210</td>\n","      <td>-6.890628</td>\n","      <td>-2.398457</td>\n","      <td>-18.541900</td>\n","      <td>0.457009</td>\n","      <td>1.055824</td>\n","      <td>-0.140664</td>\n","      <td>-0.283738</td>\n","      <td>...</td>\n","      <td>301</td>\n","      <td>0.027022</td>\n","      <td>7761</td>\n","      <td>0.017443</td>\n","      <td>-0.097204</td>\n","      <td>0.024666</td>\n","      <td>-0.049572</td>\n","      <td>-0.258912</td>\n","      <td>0.982739</td>\n","      <td>-0.234815</td>\n","    </tr>\n","    <tr>\n","      <th>428929</th>\n","      <td>29316</td>\n","      <td>126</td>\n","      <td>0.001053</td>\n","      <td>-6.986471</td>\n","      <td>-5.495227</td>\n","      <td>-19.725357</td>\n","      <td>0.205459</td>\n","      <td>0.003620</td>\n","      <td>-0.096869</td>\n","      <td>-0.435193</td>\n","      <td>...</td>\n","      <td>2307</td>\n","      <td>0.016124</td>\n","      <td>15714</td>\n","      <td>0.011446</td>\n","      <td>0.358827</td>\n","      <td>-0.111898</td>\n","      <td>0.319494</td>\n","      <td>-0.016156</td>\n","      <td>0.889851</td>\n","      <td>-0.127169</td>\n","    </tr>\n","    <tr>\n","      <th>428930</th>\n","      <td>32195</td>\n","      <td>126</td>\n","      <td>0.001233</td>\n","      <td>-6.594648</td>\n","      <td>-4.616600</td>\n","      <td>-19.147121</td>\n","      <td>0.732570</td>\n","      <td>-0.129259</td>\n","      <td>0.202878</td>\n","      <td>0.125500</td>\n","      <td>...</td>\n","      <td>522</td>\n","      <td>0.025497</td>\n","      <td>14200</td>\n","      <td>0.018408</td>\n","      <td>0.363703</td>\n","      <td>-0.211824</td>\n","      <td>0.398834</td>\n","      <td>-0.261142</td>\n","      <td>0.979920</td>\n","      <td>-0.263461</td>\n","    </tr>\n","    <tr>\n","      <th>428931</th>\n","      <td>10890</td>\n","      <td>126</td>\n","      <td>0.001040</td>\n","      <td>-7.441125</td>\n","      <td>-5.449819</td>\n","      <td>-20.547594</td>\n","      <td>0.643669</td>\n","      <td>-0.801394</td>\n","      <td>-0.825760</td>\n","      <td>0.039664</td>\n","      <td>...</td>\n","      <td>399</td>\n","      <td>0.030890</td>\n","      <td>17284</td>\n","      <td>0.027961</td>\n","      <td>0.146666</td>\n","      <td>0.085647</td>\n","      <td>0.177233</td>\n","      <td>-0.016643</td>\n","      <td>0.984503</td>\n","      <td>0.015227</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>428932 rows × 203 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bd5d88b-2915-4508-be0e-ecba0860f354')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9bd5d88b-2915-4508-be0e-ecba0860f354 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9bd5d88b-2915-4508-be0e-ecba0860f354');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b5c8ba97-0898-49ee-b186-e2ebadd4d371\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b5c8ba97-0898-49ee-b186-e2ebadd4d371')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b5c8ba97-0898-49ee-b186-e2ebadd4d371 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8002f4c3-d865-4ad3-93cd-78659b738cd8\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('feat_df_reordered')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8002f4c3-d865-4ad3-93cd-78659b738cd8 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('feat_df_reordered');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"feat_df_reordered"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["\n","import numpy as np\n","\n","\n","## Naming convention has t for transformation, e.g. tlog_1p for log transformation, texp for exp transformation\n","\n","class FeatureTransformation():\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def log_3p(self, col_name):\n","        \"\"\"log epsilon1 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 3)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_3p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_3p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_3p_test(self, x, mean, std):\n","        \"\"\"log epsilon1 transformation for test data\"\"\"\n","        x = np.log(x+ 3)\n","        x = (x - mean) / std\n","        return x\n","\n","    def log_1p(self, col_name):\n","        \"\"\"log epsilon1 transformation\"\"\"\n","        self.data[col_name] = np.log1p(self.data[col_name])\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_1p_test(self, x, mean, std):\n","        \"\"\"log epsilon1 transformation for test data\"\"\"\n","        x = np.log1p(x)\n","        x = (x - mean) / std\n","        return x\n","\n","    def log_eps5e3(self, col_name):\n","        \"\"\"log epsilon3 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 0.005)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_eps523_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_eps523_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_eps5e3_test(self, x, mean, std):\n","        \"\"\"log epsilon3 transformation for test data\"\"\"\n","        x = np.log(x+ 0.005)\n","        x = (x - mean) / std\n","        return x\n","\n","    def log_eps1e4(self, col_name):\n","        \"\"\"log epsilon4 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 0.0001)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def log_eps1e4_test(self, x, mean, std):\n","        \"\"\"log epsilon4 transformation for test data\"\"\"\n","        x = np.log(x+ 0.0001)\n","        x = (x - mean) / std\n","        return x\n","\n","\n","    def log(self, col_name):\n","        \"\"\"log transformation\"\"\"\n","        val = np.log(self.data[col_name])\n","        val1 = (val - val.mean()) / val.std()\n","        #return val1, 'tlog_' + col_name , val.mean(), val.std()\n","        return val, 'tlog_' + col_name , val.mean(), val.std()\n","\n","    def log_test(self, x, mean, std):\n","        \"\"\"log transformation for test data\"\"\"\n","        x = np.log(x)\n","        x = (x - mean) / std\n","        return x\n","\n","    def log_10p(self, col_name):\n","        \"\"\"log epsilon2 transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]+ 10)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_10p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_10p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_10p_test(self, x, mean, std):\n","        \"\"\"log epsilon2 transformation for test data\"\"\"\n","        x = np.log(x+ 10)\n","        x = (x - mean) / std\n","        return x\n","\n","    def log_log1p(self, col_name):\n","        \"\"\"log log epsilon1 transformation\"\"\"\n","        self.data[col_name] = np.log(np.log1p(self.data[col_name]))\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_tlog1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_tlog1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def log_log1p_test(self, x, mean, std):\n","        \"\"\"log log epsilon1 transformation for test data\"\"\"\n","        x = np.log(np.log1p(x))\n","        x = (x - mean) / std\n","        return x\n","\n","    def log_log1p_eps1e4(self, col_name):\n","        \"\"\"log log epsilon4 transformation\"\"\"\n","        self.data[col_name] = np.log(np.log1p(self.data[col_name]+ 0.0001))\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_tlog1p_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name] , 'tlog_tlog1p_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def log_log1p_eps1e4_test(self, x, mean, std):\n","        \"\"\"log log epsilon4 transformation for test data\"\"\"\n","        x = np.log(np.log1p(x+ 0.0001))\n","        x = (x - mean) / std\n","        return x\n","\n","\n","    def log_lin100_1(self, col_name):\n","        \"\"\"log of linear transformation\"\"\"\n","        self.data[col_name] = np.log(self.data[col_name]*100 + 1)\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'tlog_tlinear_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'tlog_tlinear_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def log_lin100_1_test(self, x, mean, std):\n","        \"\"\"log of linear transformation for test data\"\"\"\n","        x = np.log(x*100 + 1)\n","        x = (x - mean) / std\n","        return x\n","\n","    def standard_scaling(self, col_name):\n","        \"\"\"standard scaling transformation\"\"\"\n","        #self.data[col_name] = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return self.data[col_name], self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], self.data[col_name].mean(), self.data[col_name].std()\n","\n","    def standard_scaling_test(self, x, mean, std):\n","        \"\"\"standard scaling transformation for test data\"\"\"\n","        x = (x - mean) / std\n","        return x\n","\n","    def exp(self, col_name):\n","        \"\"\"exp transformation\"\"\"\n","        self.data[col_name] = np.exp(self.data[col_name])\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def exp_test(self, x, mean, std):\n","        \"\"\"exp transformation for test data\"\"\"\n","        x = np.exp(x)\n","        x = (x - mean) / std\n","        return x\n","\n","    def exp_exp(self, col_name):\n","        \"\"\"exp exp transformation\"\"\"\n","        self.data[col_name] = np.exp(np.exp(self.data[col_name]))\n","        val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","        #return val, 'texp_texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","        return self.data[col_name], 'texp_texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","    def exp_exp_test(self, x, mean, std):\n","        \"\"\"exp exp transformation for test data\"\"\"\n","        x = np.exp(np.exp(x))\n","        x = (x - mean) / std\n","        return x\n","\n","\n"],"metadata":{"id":"IqKxoXU7285z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import feature_transformation as feat_transformer\n","\n","ft = FeatureTransformation(train_feat_df)\n","\n","feat_normalization_mu_std_df = pd.DataFrame(index=[\"mean\", \"std\", \"transform\"])\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_log_price_ret_vol\")\n","# train_feat_df.rename(columns={'wap1_log_price_ret_vol':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df.drop(columns=['wap1_log_price_ret_vol'], inplace=True) ## we drop this because we have log_wap1_log_price_ret_vol already\n","\n","\n","train_feat_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol\"], mean, std  = ft.standard_scaling(\"log_liq2_ret_*_wap_eqi_price1_ret_vol\")\n","feat_normalization_mu_std_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\"], mean, std  = ft.standard_scaling(\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\")\n","feat_normalization_mu_std_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_per_liq2_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_liq2_vol\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_per_liq2_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'], mean, std = ft.standard_scaling('log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio')\n","feat_normalization_mu_std_df['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_per_liq2_vol_15_ratio\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_liq2_vol_15_ratio\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_per_liq2_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\"], mean, std = ft.standard_scaling(\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\")\n","feat_normalization_mu_std_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\"], mean, std = ft.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\")\n","# feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\"], mean, std = ft.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\")\n","# feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\"], mean, std = ft.standard_scaling(\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\")\n","feat_normalization_mu_std_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\"],mean, std, = ft.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\")\n","feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.log_3p(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0\")\n","train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_3p_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0\")\n","train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\"],mean, std = ft.standard_scaling(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10\")\n","train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10\")\n","train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\"],mean, std, = ft.standard_scaling(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20\")\n","train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20\")\n","train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20\")\n","train_feat_df.rename(columns={'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df[\"soft_stock_mean_tvpl2_:0\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_:0\")\n","feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"soft_stock_mean_tvpl2_:10\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_:10\")\n","feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"soft_stock_mean_tvpl2_:20\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_:20\")\n","feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"soft_stock_mean_tvpl2_liqf\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_liqf\")\n","feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"soft_stock_mean_tvpl2_liqf_volf10\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_liqf_volf10\")\n","feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf_volf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"soft_stock_mean_tvpl2_liqf_volf20\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_liqf_volf20\")\n","feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf_volf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_15\"], mean, std = ft.standard_scaling(\"v1proj_25_15\")\n","feat_normalization_mu_std_df[\"v1proj_25_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"lsvol\")  ## This is causing lsvol to have nan values\n","# train_feat_df.rename(columns={'lsvol':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df[\"lsvol\"], mean, std = ft.standard_scaling(\"lsvol\")\n","feat_normalization_mu_std_df[\"lsvol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"liqvol1\"], mean, std = ft.standard_scaling(\"liqvol1\")\n","feat_normalization_mu_std_df[\"liqvol1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"liqvol1_smean\"], mean, std = ft.standard_scaling(\"liqvol1_smean\")\n","feat_normalization_mu_std_df[\"liqvol1_smean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"liqvol2\"], mean, std = ft.standard_scaling(\"liqvol2\")\n","feat_normalization_mu_std_df[\"liqvol2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"liqvol1_15_15\"], mean, std = ft.standard_scaling(\"liqvol1_15_15\")\n","feat_normalization_mu_std_df[\"liqvol1_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"trade_count\"], mean, std = ft.standard_scaling(\"trade_count\")\n","feat_normalization_mu_std_df[\"trade_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"root_trade_count\"], mean, std = ft.standard_scaling(\"root_trade_count\")\n","feat_normalization_mu_std_df[\"root_trade_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"root_trade_count_smean\"], mean, std = ft.standard_scaling(\"root_trade_count_smean\")\n","feat_normalization_mu_std_df[\"root_trade_count_smean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"root_book_delta_count\"], mean, std = ft.standard_scaling(\"root_book_delta_count\")\n","feat_normalization_mu_std_df[\"root_book_delta_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"root_trade_count_var\"], mean, std = ft.standard_scaling(\"root_trade_count_var\")\n","feat_normalization_mu_std_df[\"root_trade_count_var\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"trade_count_15_15\"], mean, std = ft.standard_scaling(\"trade_count_15_15\")\n","feat_normalization_mu_std_df[\"trade_count_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"root_trade_count_15_15\"], mean, std = ft.standard_scaling(\"root_trade_count_15_15\")\n","feat_normalization_mu_std_df[\"root_trade_count_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29_15\"], mean, std = ft.standard_scaling(\"v1proj_29_15\")\n","feat_normalization_mu_std_df[\"v1proj_29_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_20\"], mean, std = ft.standard_scaling(\"v1proj_20\")\n","feat_normalization_mu_std_df[\"v1proj_20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25\"], mean, std = ft.standard_scaling(\"v1proj_25\")\n","feat_normalization_mu_std_df[\"v1proj_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29\"], mean, std = ft.standard_scaling(\"v1proj_29\")\n","feat_normalization_mu_std_df[\"v1proj_29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29_q1\"], mean, std = ft.standard_scaling(\"v1proj_29_q1\")\n","feat_normalization_mu_std_df[\"v1proj_29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29_q3\"], mean, std = ft.standard_scaling(\"v1proj_29_q3\")\n","feat_normalization_mu_std_df[\"v1proj_29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_q1\"], mean, std = ft.standard_scaling(\"v1proj_25_q1\")\n","feat_normalization_mu_std_df[\"v1proj_25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_q3\"], mean, std = ft.standard_scaling(\"v1proj_25_q3\")\n","feat_normalization_mu_std_df[\"v1proj_25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29_15_q1\"], mean, std = ft.standard_scaling(\"v1proj_29_15_q1\")\n","feat_normalization_mu_std_df[\"v1proj_29_15_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29_15_q3\"], mean, std = ft.standard_scaling(\"v1proj_29_15_q3\")\n","feat_normalization_mu_std_df[\"v1proj_29_15_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_15_q1\"], mean, std = ft.standard_scaling(\"v1proj_25_15_q1\")\n","feat_normalization_mu_std_df[\"v1proj_25_15_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_15_q3\"], mean, std = ft.standard_scaling(\"v1proj_25_15_q3\")\n","feat_normalization_mu_std_df[\"v1proj_25_15_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_15_std\"], mean, std = ft.standard_scaling(\"v1proj_25_15_std\")\n","feat_normalization_mu_std_df[\"v1proj_25_15_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df.drop(columns=[\"v1proj_29_15_std\"], inplace=True) ## drop this column as it has very low variance\n","\n","train_feat_df[\"v1proj_20_std\"], mean, std = ft.standard_scaling(\"v1proj_20_std\")\n","feat_normalization_mu_std_df[\"v1proj_20_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_25_std\"], mean, std = ft.standard_scaling(\"v1proj_25_std\")\n","feat_normalization_mu_std_df[\"v1proj_25_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_std\"], mean, std = ft.standard_scaling(\"v1proj_29_std\")\n","# feat_normalization_mu_std_df[\"v1proj_29_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1proj_29_q3q1\"], mean, std = ft.standard_scaling(\"v1proj_29_q3q1\")\n","feat_normalization_mu_std_df[\"v1proj_29_q3q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_rmed2v1\"], mean, std = ft.standard_scaling(\"tvpl2_rmed2v1\")\n","feat_normalization_mu_std_df[\"tvpl2_rmed2v1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_rmed2v1lf25\"], mean, std = ft.standard_scaling(\"tvpl2_rmed2v1lf25\")\n","feat_normalization_mu_std_df[\"tvpl2_rmed2v1lf25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_rmed2v1lf29\"], mean, std = ft.standard_scaling(\"tvpl2_rmed2v1lf29\")\n","feat_normalization_mu_std_df[\"tvpl2_rmed2v1lf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2\"], mean, std = ft.standard_scaling(\"tvpl2\")\n","feat_normalization_mu_std_df[\"tvpl2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_liqf10\"], mean, std = ft.standard_scaling(\"tvpl2_liqf10\")\n","feat_normalization_mu_std_df[\"tvpl2_liqf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_liqf20\"], mean, std = ft.standard_scaling(\"tvpl2_liqf20\")\n","feat_normalization_mu_std_df[\"tvpl2_liqf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_liqf29\"], mean, std = ft.standard_scaling(\"tvpl2_liqf29\")\n","feat_normalization_mu_std_df[\"tvpl2_liqf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_smean_vol\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol\")\n","feat_normalization_mu_std_df[\"tvpl2_smean_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_smean_vol_liqf10\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol_liqf10\")\n","feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_smean_vol_liqf20\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol_liqf20\")\n","feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"tvpl2_smean_vol_liqf29\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol_liqf29\")\n","feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1liq2projt5\"], mean, std = ft.standard_scaling(\"v1liq2projt5\")\n","feat_normalization_mu_std_df[\"v1liq2projt5\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1liq2projt10\"], mean, std = ft.standard_scaling(\"v1liq2projt10\")\n","feat_normalization_mu_std_df[\"v1liq2projt10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1liq2projt20\"], mean, std = ft.standard_scaling(\"v1liq2projt20\")\n","feat_normalization_mu_std_df[\"v1liq2projt20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"liqt10rf29\"], mean, std = ft.standard_scaling(\"liqt10rf29\")\n","feat_normalization_mu_std_df[\"liqt10rf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"liqt20rf29\"], mean, std = ft.standard_scaling(\"liqt20rf29\")\n","feat_normalization_mu_std_df[\"liqt20rf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1liq2sprojt10f25\"], mean, std = ft.standard_scaling(\"v1liq2sprojt10f25\")\n","feat_normalization_mu_std_df[\"v1liq2sprojt10f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1liq2sprojt5f25\"], mean, std = ft.standard_scaling(\"v1liq2sprojt5f25\")\n","feat_normalization_mu_std_df[\"v1liq2sprojt5f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt10f29\"], mean, std = ft.standard_scaling(\"v1spprojt10f29\")\n","feat_normalization_mu_std_df[\"v1spprojt10f29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt15f25\"], mean, std = ft.standard_scaling(\"v1spprojt15f25\")\n","feat_normalization_mu_std_df[\"v1spprojt15f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt15f29\"], mean, std = ft.standard_scaling(\"v1spprojt15f29\")\n","feat_normalization_mu_std_df[\"v1spprojt15f29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt15f29_q1\"], mean, std = ft.standard_scaling(\"v1spprojt15f29_q1\")\n","feat_normalization_mu_std_df[\"v1spprojt15f29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt15f29_q3\"], mean, std = ft.standard_scaling(\"v1spprojt15f29_q3\")\n","feat_normalization_mu_std_df[\"v1spprojt15f29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt15f25_q1\"], mean, std = ft.standard_scaling(\"v1spprojt15f25_q1\")\n","feat_normalization_mu_std_df[\"v1spprojt15f25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojt15f25_q3\"], mean, std = ft.standard_scaling(\"v1spprojt15f25_q3\")\n","feat_normalization_mu_std_df[\"v1spprojt15f25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojtf29_q1\"], mean, std = ft.standard_scaling(\"v1spprojtf29_q1\")\n","feat_normalization_mu_std_df[\"v1spprojtf29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojtf29_q3\"], mean, std = ft.standard_scaling(\"v1spprojtf29_q3\")\n","feat_normalization_mu_std_df[\"v1spprojtf29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojtf25_q1\"], mean, std = ft.standard_scaling(\"v1spprojtf25_q1\")\n","feat_normalization_mu_std_df[\"v1spprojtf25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"v1spprojtf25_q3\"], mean, std = ft.standard_scaling(\"v1spprojtf25_q3\")\n","feat_normalization_mu_std_df[\"v1spprojtf25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df.drop(columns=[\"wap1_log_price_ret_vol_from_0\"], inplace=True) ## drop this column as it has all 0 values\n","\n","train_feat_df[\"wap1_log_price_ret_volstock_mean_from_0\"],mean, std, = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_0\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.exp(\"wap1_log_price_ret_vol_from_10\")\n","train_feat_df.rename(columns={'wap1_log_price_ret_vol_from_10':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_volstock_mean_from_10\"],mean, std, = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_10\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.exp(\"wap1_log_price_ret_vol_from_20\")\n","train_feat_df.rename(columns={'wap1_log_price_ret_vol_from_20':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","\n","\n","train_feat_df[\"wap1_log_price_ret_volstock_mean_from_20\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_20\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_vol_from_25\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_vol_from_25\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_vol_from_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"wap1_log_price_ret_volstock_mean_from_25\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_25\")\n","feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"vol1_mean\"], mean, std = ft.standard_scaling(\"vol1_mean\")\n","feat_normalization_mu_std_df[\"vol1_mean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df.drop(columns=[\"mean_half_delta\"], inplace=True)# drop this column as it has very few unique values\n","\n","train_feat_df.drop(columns=[\"mean_half_delta_lsprd\"], inplace=True)# drop this column as it has very few unique values\n","\n","train_feat_df[\"log_wap1_log_price_ret_vol\"], mean, std = ft.standard_scaling(\"log_wap1_log_price_ret_vol\")\n","feat_normalization_mu_std_df[\"log_wap1_log_price_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","\n","# train_feat_df.drop(columns=[\"log_target\"], inplace=True)# drop log_target as we create it here as tlog_target which is sames as log_target_standardize\n","# train_feat_df.drop(columns=[\"log_target_standardized\"], inplace=True)#\n","temp_target = train_feat_df[\"target\"]\n","val , new_col_name, mean, std = ft.log(\"target\")\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","train_feat_df[\"target\"] = temp_target\n","del temp_target\n","\n","train_feat_df[\"bid_lvl2_min_lvl1_size_feat\"], mean, std = ft.standard_scaling(\"bid_lvl2_min_lvl1_size_feat\")\n","feat_normalization_mu_std_df[\"bid_lvl2_min_lvl1_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"ask_lvl2_min_lvl1_size_feat\"], mean, std = ft.standard_scaling(\"ask_lvl2_min_lvl1_size_feat\")\n","feat_normalization_mu_std_df[\"ask_lvl2_min_lvl1_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"], mean, std = ft.standard_scaling(\"lvl2_minus_lvl1_bid_n_ask_size_feat\")\n","feat_normalization_mu_std_df[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"sum_size\"], mean, std = ft.standard_scaling(\"sum_size\")\n","feat_normalization_mu_std_df[\"sum_size\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"sum_order_count\"], mean, std = ft.standard_scaling(\"sum_order_count\")\n","feat_normalization_mu_std_df[\"sum_order_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"sum_size_per_order_count\"], mean, std = ft.standard_scaling(\"sum_size_per_order_count\")\n","feat_normalization_mu_std_df[\"sum_size_per_order_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.log_eps5e3(\"trade_price_n_wap1_dev\")\n","train_feat_df.rename(columns={'trade_price_n_wap1_dev':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps5e3_test\"]\n","\n","val, new_col_name, mean, std = ft.log_eps5e3(\"trade_price_n_wap_eqi_price0_dev\")\n","train_feat_df.rename(columns={'trade_price_n_wap_eqi_price0_dev':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps5e3_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"first_10_min_vol\")\n","train_feat_df.rename(columns={'first_10_min_vol':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_eps1e4(\"trade_price_std\")\n","train_feat_df.rename(columns={'trade_price_std':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps1e4_test\"]\n","\n","val, new_col_name, mean, std = ft.log_eps1e4(\"trade_price_real_vol\")\n","train_feat_df.rename(columns={'trade_price_real_vol':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps1e4_test\"]\n","\n","val, new_col_name, mean, std = ft.log_1p(\"trade_size_std\")\n","train_feat_df.rename(columns={'trade_size_std':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_1p(\"trade_size_mean\")\n","train_feat_df.rename(columns={'trade_size_mean':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_1p(\"trade_order_count_std\")\n","train_feat_df.rename(columns={'trade_order_count_std':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_1p(\"trade_order_count_mean\")\n","train_feat_df.rename(columns={'trade_order_count_mean':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_4_clusters\")\n","train_feat_df.rename(columns={'target_vol_sum_stats_4_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_10_clusters\")\n","train_feat_df.rename(columns={'target_vol_sum_stats_10_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_16_clusters\")\n","train_feat_df.rename(columns={'target_vol_sum_stats_16_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_30_clusters\")\n","train_feat_df.rename(columns={'target_vol_sum_stats_30_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_2_clusters\")\n","train_feat_df.rename(columns={'target_vol_robust_sum_stats_2_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_4_clusters\")\n","train_feat_df.rename(columns={'target_vol_robust_sum_stats_4_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_14_clusters\")\n","train_feat_df.rename(columns={'target_vol_robust_sum_stats_14_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_20_clusters\")\n","train_feat_df.rename(columns={'target_vol_robust_sum_stats_20_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_32_clusters\")\n","train_feat_df.rename(columns={'target_vol_robust_sum_stats_32_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_60_clusters\")\n","train_feat_df.rename(columns={'target_vol_robust_sum_stats_60_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_3_clusters\")\n","train_feat_df.rename(columns={'target_vol_pcorr_3_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_49_clusters\")\n","train_feat_df.rename(columns={'target_vol_pcorr_49_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_90_clusters\")\n","train_feat_df.rename(columns={'target_vol_pcorr_90_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_10_clusters\")\n","train_feat_df.rename(columns={'target_vol_pcorr_10_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_26_clusters\")\n","train_feat_df.rename(columns={'target_vol_pcorr_26_clusters':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df[\"min_bid_price1\"], mean, std = ft.standard_scaling(\"min_bid_price1\")\n","feat_normalization_mu_std_df[\"min_bid_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"max_bid_price1\"], mean, std = ft.standard_scaling(\"max_bid_price1\")\n","feat_normalization_mu_std_df[\"max_bid_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"min_ask_price1\"], mean, std = ft.standard_scaling(\"min_ask_price1\")\n","feat_normalization_mu_std_df[\"min_ask_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"max_ask_price1\"], mean, std = ft.standard_scaling(\"max_ask_price1\")\n","feat_normalization_mu_std_df[\"max_ask_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","\n","train_feat_df.drop(columns=[\"min_bid_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","val, new_col_name, mean, std = ft.log(\"max_bid_size1\")\n","train_feat_df.rename(columns={'max_bid_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df.drop(columns=[\"min_ask_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","val, new_col_name, mean, std = ft.log(\"max_ask_size1\")\n","train_feat_df.rename(columns={'max_ask_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_log1p_eps1e4(\"range_ask_price1\")\n","train_feat_df.rename(columns={'range_ask_price1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","val, new_col_name, mean, std = ft.log_log1p_eps1e4(\"range_bid_price1\")\n","train_feat_df.rename(columns={'range_bid_price1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","val, new_col_name, mean, std = ft.log_1p(\"range_ask_size1\")\n","train_feat_df.rename(columns={'range_ask_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","\n","val, new_col_name, mean, std = ft.log_1p(\"range_bid_size1\")\n","train_feat_df.rename(columns={'range_bid_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_log1p_eps1e4(\"sad_ask_price1\")\n","train_feat_df.rename(columns={'sad_ask_price1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","val, new_col_name, mean, std = ft.log_1p(\"sad_ask_size1\")\n","train_feat_df.rename(columns={'sad_ask_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_bid_price1\")\n","train_feat_df.rename(columns={'sad_bid_price1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_bid_size1\")\n","train_feat_df.rename(columns={'sad_bid_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","train_feat_df[\"bs_bp_corr1\"], mean, std = ft.standard_scaling(\"bs_bp_corr1\")\n","feat_normalization_mu_std_df[\"bs_bp_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bs_as_corr1\"], mean, std = ft.standard_scaling(\"bs_as_corr1\")\n","feat_normalization_mu_std_df[\"bs_as_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bs_ap_corr1\"], mean, std = ft.standard_scaling(\"bs_ap_corr1\")\n","feat_normalization_mu_std_df[\"bs_ap_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bp_as_corr1\"], mean, std = ft.standard_scaling(\"bp_as_corr1\")\n","feat_normalization_mu_std_df[\"bp_as_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.exp_exp(\"bp_ap_corr1\")\n","train_feat_df.rename(columns={'bp_ap_corr1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_exp_test\"]\n","\n","train_feat_df[\"as_ap_corr1\"], mean, std = ft.standard_scaling(\"as_ap_corr1\")\n","feat_normalization_mu_std_df[\"as_ap_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"min_price1\"], mean, std = ft.standard_scaling(\"min_price1\")\n","feat_normalization_mu_std_df[\"min_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"max_price1\"], mean, std = ft.standard_scaling(\"max_price1\")\n","feat_normalization_mu_std_df[\"max_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df.drop(columns=[\"min_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","val, new_col_name, mean, std = ft.log(\"max_size1\")\n","train_feat_df.rename(columns={'max_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","train_feat_df.drop(columns=[\"min_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","train_feat_df[\"max_order_count1\"], mean, std = ft.standard_scaling(\"max_order_count1\")\n","feat_normalization_mu_std_df[\"max_order_count1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"range_price1\")\n","train_feat_df.rename(columns={'range_price1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_1p(\"range_size1\")\n","train_feat_df.rename(columns={'range_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","train_feat_df.drop(columns=[\"range_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_price1\")\n","train_feat_df.rename(columns={'sad_price1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_size1\")\n","train_feat_df.rename(columns={'sad_size1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","val, new_col_name, mean, std = ft.log_1p(\"sad_order_count1\")\n","train_feat_df.rename(columns={'sad_order_count1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","val, new_col_name, mean, std = ft.exp(\"size_order_count_corr1\")\n","train_feat_df.rename(columns={'size_order_count_corr1':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"book_ewma_vol\")\n","# train_feat_df.rename(columns={'book_ewma_vol':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"trade_ewma_vol\")\n","# train_feat_df.rename(columns={'trade_ewma_vol':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","train_feat_df[\"min_bid_price2\"], mean, std = ft.standard_scaling(\"min_bid_price2\")\n","feat_normalization_mu_std_df[\"min_bid_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"max_bid_price2\"], mean, std = ft.standard_scaling(\"max_bid_price2\")\n","feat_normalization_mu_std_df[\"max_bid_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"min_ask_price2\"], mean, std = ft.standard_scaling(\"min_ask_price2\")\n","feat_normalization_mu_std_df[\"min_ask_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"max_ask_price2\"], mean, std = ft.standard_scaling(\"max_ask_price2\")\n","feat_normalization_mu_std_df[\"max_ask_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df.drop(columns=[\"min_bid_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","val, new_col_name, mean, std = ft.log(\"max_bid_size2\")\n","train_feat_df.rename(columns={'max_bid_size2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","train_feat_df.drop(columns=[\"min_ask_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","val, new_col_name, mean, std = ft.log(\"max_ask_size2\")\n","train_feat_df.rename(columns={'max_ask_size2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"range_ask_price2\")\n","train_feat_df.rename(columns={'range_ask_price2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"range_bid_price2\")\n","train_feat_df.rename(columns={'range_bid_price2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","val, new_col_name, mean, std = ft.log(\"range_ask_size2\")\n","train_feat_df.rename(columns={'range_ask_size2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_ask_price2\")\n","train_feat_df.rename(columns={'sad_ask_price2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_ask_size2\")\n","train_feat_df.rename(columns={'sad_ask_size2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","val, new_col_name, mean, std = ft.log_lin100_1(\"sad_bid_price2\")\n","train_feat_df.rename(columns={'sad_bid_price2':new_col_name}, inplace=True)\n","train_feat_df[new_col_name] = val\n","feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","train_feat_df[\"bs_bp_corr2\"], mean, std = ft.standard_scaling(\"bs_bp_corr2\")\n","feat_normalization_mu_std_df[\"bs_bp_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bs_as_corr2\"], mean, std = ft.standard_scaling(\"bs_as_corr2\")\n","feat_normalization_mu_std_df[\"bs_as_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bs_ap_corr2\"], mean, std = ft.standard_scaling(\"bs_ap_corr2\")\n","feat_normalization_mu_std_df[\"bs_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bp_as_corr2\"], mean, std = ft.standard_scaling(\"bp_as_corr2\")\n","feat_normalization_mu_std_df[\"bp_as_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"bp_ap_corr2\"], mean, std = ft.standard_scaling(\"bp_ap_corr2\")\n","feat_normalization_mu_std_df[\"bp_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"as_ap_corr2\"], mean, std = ft.standard_scaling(\"as_ap_corr2\")\n","feat_normalization_mu_std_df[\"as_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","train_feat_df[\"sum_stats_4_clusters_labels\"] = train_feat_df[\"sum_stats_4_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"sum_stats_10_clusters_labels\"] = train_feat_df[\"sum_stats_10_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"sum_stats_16_clusters_labels\"] = train_feat_df[\"sum_stats_16_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"sum_stats_30_clusters_labels\"] = train_feat_df[\"sum_stats_30_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"pear_corr_3_clusters_labels\"] = train_feat_df[\"pear_corr_3_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"pear_corr_49_clusters_labels\"] = train_feat_df[\"pear_corr_49_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"pear_corr_90_clusters_labels\"] = train_feat_df[\"pear_corr_90_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"pear_corr_10_clusters_labels\"] = train_feat_df[\"pear_corr_10_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"pear_corr_26_clusters_labels\"] = train_feat_df[\"pear_corr_26_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"robust_sum_stats_2_clusters_labels\"] = train_feat_df[\"robust_sum_stats_2_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"robust_sum_stats_4_clusters_labels\"] = train_feat_df[\"robust_sum_stats_4_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"robust_sum_stats_14_clusters_labels\"] = train_feat_df[\"robust_sum_stats_14_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"robust_sum_stats_20_clusters_labels\"] = train_feat_df[\"robust_sum_stats_20_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"robust_sum_stats_32_clusters_labels\"] = train_feat_df[\"robust_sum_stats_32_clusters_labels\"].astype(\"category\")\n","\n","train_feat_df[\"robust_sum_stats_60_clusters_labels\"] = train_feat_df[\"robust_sum_stats_60_clusters_labels\"].astype(\"category\")\n","\n","\n","\n"],"metadata":{"id":"olKXg_E_5teB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for NULL and INF values in train_feat_df\n","\n","nan_cols = []\n","pos_inf_cols = []\n","neg_inf_cols = []\n","\n","float_cols = [c for c in train_feat_df.columns if train_feat_df[c].dtype != \"category\"]\n","\n","for c in float_cols: ## exclude the caetgorical columns\n","    if np.isinf(train_feat_df[c]).any():\n","        print(\"pos INF: \",c,': ', np.isinf(train_feat_df[c]).sum())\n","        pos_inf_cols.append(c)\n","\n","    if np.isneginf(train_feat_df[c]).any():\n","        print(\"neg INF: \",c,': ', np.isneginf(train_feat_df[c]).sum())\n","        neg_inf_cols.append(c)\n","\n","    if np.isnan(train_feat_df[c]).any():\n","        print(\"NaN: \", c ,': ', np.isnan(train_feat_df[c]).sum())\n","        nan_cols.append(c)\n","\n"],"metadata":{"id":"0vKL-rWc6E_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","#### Check that no two columns have the same values\n","for c in train_feat_df.columns:\n","    ctr=0\n","    for c1 in train_feat_df.columns:\n","        if np.array_equal(train_feat_df[c], train_feat_df[c1]) and c!=c1:\n","            ctr+=1\n","            print(c, c1)\n","        # if (ctr>1):\n","        #     print(c, c1)\n","\n","# ### Drop the columns that have the same values\n","# train_feat_df.drop(columns=['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio\", \"min_price\", \"max_price\" ], inplace=True)\n","# train_feat_df\n","\n","\n"],"metadata":{"id":"BU8P3_iT6IgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_full_after_transformation.pkl','wb') as fb:\n","#       pickle.dump(train_feat_df, fb)\n","\n","with open('train_feat_df_full_after_transformation.pkl','rb') as fb:\n","      train_feat_df = pickle.load(fb)\n","\n","\n"],"metadata":{"id":"cn8O9K9u6L8Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1F3zBPYQJsy"},"source":[]},{"cell_type":"markdown","metadata":{"id":"mBF5ylKtQJqD"},"source":[]},{"cell_type":"markdown","metadata":{"id":"62SksiI6QJoH"},"source":[]},{"cell_type":"markdown","metadata":{"id":"JL1Y8enOQJmL"},"source":[]},{"cell_type":"markdown","metadata":{"id":"sJsYx_kjQJkP"},"source":[]},{"cell_type":"markdown","metadata":{"id":"W-FaiF-_QJiH"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TfeRp7ZUvRp9"},"outputs":[],"source":["print("]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTd-blS2ZNEi"},"outputs":[],"source":["# level = 1 # set level 1 or 2 for book_train data\n","# corr_method = 'spearman' # set 'pearson' or 'spearman'\n","# file = 'book_train'\n","\n","# bid_price = \"bid_price\"+str(level)\n","# ask_price = \"ask_price\"+str(level)\n","# bid_size = \"bid_size\"+str(level)\n","# ask_size = \"ask_size\"+str(level)\n","\n","\n","# ## give a dataframe with two columns to plot scatter and return correlation coefficient, x axis is column 1 and y axis is column 2\n","# def plot_scatter_n_correlation(df,st_id,method='pearson'):\n","#     corr = df.corr(method=method)\n","#     corr_coef = corr[corr.columns[0]][1] # convert from matrix to single number\n","#     fig = px.scatter(df,x=df.columns[0],y=df.columns[1])\n","#     fig.update_traces(textposition='top center')\n","\n","#     fig.update_layout(\n","#         height=400,width=700,\n","#         title_text='scatter plot, stock id: '+str(st_id)+ '. Correlation coef: '+ str(corr_coef) + ' method: '+method,\n","#     )\n","#     #fig.show()  # enable/disable plot\n","\n","#     return corr_coef\n","\n","\n","# def my_func(bid_size, bid_price):\n","#     return bid_size.corr(bid_price)\n","\n","\n","# def my_range_price(series):\n","#     return series.max()-series.min()\n","\n","# def my_sum_abs_diff(series):\n","#     return np.sum(np.abs(series.diff()))\n","\n","\n","# columns = [bid_size,bid_price,ask_size,ask_price]\n","\n","# def all_pairs_of_columns_list(columns):\n","#     return list(combinations(columns, 2))\n","\n","\n","# \"\"\"\n","# New features start\n","# \"\"\"\n","\n","# ### check the feature: \"if level 2 bid/ask size exceeds level 1 bid/ask size then volatitliy is higher\"\n","\n","# train_target = train\n","# #subset_paths = glob.glob('c:/Finance_projects/optiver_backup2/data/book_train'+'.parquet/stock_id=*')\n","# #subset_paths = glob.glob('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/book_train'+'.parquet/stock_id=*')\n","# subset_paths = train_book_paths\n","# corr_method = \"spearman\" #\"pearson\"\n","# bid_lvl2_min_lvl1_size_feat_n_target_vol_corr_coef = {}\n","# ask_lvl2_min_lvl1_size_feat_n_target_vol_corr_coef= {}\n","# lvl2_minus_lvl1_bid_n_ask_size_feat_n_target_vol_corr_coef = {}\n","\n","# bk_level1_2_size_imbalance_feat = {}\n","\n","# bk_level1_2_size_imbalance_feat['bid_lvl2_min_lvl1_size_feat'] = pd.DataFrame()\n","# bk_level1_2_size_imbalance_feat['ask_lvl2_min_lvl1_size_feat'] = pd.DataFrame()\n","# bk_level1_2_size_imbalance_feat['lvl2_minus_lvl1_bid_n_ask_size_feat'] = pd.DataFrame()\n","\n","# def my_sum(values,index):\n","#     return np.sum(values)\n","\n","# for path in subset_paths:\n","#     #print(path.split('/stock_id=')[1] )\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     #st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     #print(st_id)\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","#         book_train_st_copy = book_train_st.copy()\n","\n","#         ###  sum the min(bid_size2 - bid_size1, 0) within a time id over all time ids, ALSO DO seperately for ask_size\n","#         book_train_st_copy[\"bid_lvl2_min_lvl1_size_feat\"] = book_train_st_copy[\"bid_size2\"] - book_train_st_copy[\"bid_size1\"]\n","#         book_train_st_copy[\"bid_lvl2_min_lvl1_size_feat\"] = book_train_st_copy[\"bid_lvl2_min_lvl1_size_feat\"].apply(lambda x: min(x,0))\n","#         book_train_st_copy[\"ask_lvl2_min_lvl1_size_feat\"] = book_train_st_copy[\"ask_size2\"] - book_train_st_copy[\"ask_size1\"]\n","#         book_train_st_copy[\"ask_lvl2_min_lvl1_size_feat\"] = book_train_st_copy[\"ask_lvl2_min_lvl1_size_feat\"].apply(lambda x: min(x,0))\n","#         st_lvl2_min_lvl1_bid_size_feat = book_train_st_copy.groupby(by='time_id')[\"bid_lvl2_min_lvl1_size_feat\"].agg(my_sum,engine=\"numba\") #mean, sum\n","#         st_lvl2_min_lvl1_ask_size_feat = book_train_st_copy.groupby(by='time_id')[\"ask_lvl2_min_lvl1_size_feat\"].agg(my_sum,engine=\"numba\") #mean, sum\n","#         ## transformation to make the data more normal\n","#         st_lvl2_min_lvl1_bid_size_feat = np.log1p((st_lvl2_min_lvl1_bid_size_feat*-1 )**0.5)\n","#         ## transformation to make the data more normal\n","#         st_lvl2_min_lvl1_ask_size_feat = np.log1p((st_lvl2_min_lvl1_ask_size_feat*-1 )**0.5)\n","#         common_time_id = np.intersect1d(st_lvl2_min_lvl1_bid_size_feat.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         bid_lvl2_min_lvl1_size_feat_n_target_vol_df = pd.DataFrame({'bid_lvl2_min_lvl1_size_feat':st_lvl2_min_lvl1_bid_size_feat.loc[common_time_id].values , 'target_vol':target_st.loc[common_time_id,\"target\"].values})\n","#         corr_coef_bid_lvl2_min_lvl1_size_feat = plot_scatter_n_correlation(bid_lvl2_min_lvl1_size_feat_n_target_vol_df,st_id,method=corr_method)\n","#         bid_lvl2_min_lvl1_size_feat_n_target_vol_corr_coef[st_id] = corr_coef_bid_lvl2_min_lvl1_size_feat\n","#         bk_level1_2_size_imbalance_feat['bid_lvl2_min_lvl1_size_feat'] = pd.concat([ bk_level1_2_size_imbalance_feat['bid_lvl2_min_lvl1_size_feat'], st_lvl2_min_lvl1_bid_size_feat.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","#         common_time_id = np.intersect1d(st_lvl2_min_lvl1_ask_size_feat.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         ask_lvl2_min_lvl1_size_feat_n_target_vol_df = pd.DataFrame({'ask_lvl2_min_lvl1_size_feat':st_lvl2_min_lvl1_ask_size_feat.loc[common_time_id].values , 'target_vol':target_st.loc[common_time_id,\"target\"].values})\n","#         corr_coef_ask_lvl2_min_lvl1_size_feat = plot_scatter_n_correlation(ask_lvl2_min_lvl1_size_feat_n_target_vol_df,st_id,method=corr_method)\n","#         ask_lvl2_min_lvl1_size_feat_n_target_vol_corr_coef[st_id] = corr_coef_ask_lvl2_min_lvl1_size_feat\n","#         bk_level1_2_size_imbalance_feat['ask_lvl2_min_lvl1_size_feat'] = pd.concat([ bk_level1_2_size_imbalance_feat['ask_lvl2_min_lvl1_size_feat'], st_lvl2_min_lvl1_ask_size_feat.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","#         # ###  sum the min(bid_size2 + ask_size2 - bid_size1 - ask_size1, 0) within a time id over all time ids\n","#         book_train_st_copy[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = book_train_st_copy[\"bid_size2\"] + book_train_st_copy[\"ask_size2\"] - book_train_st_copy[\"bid_size1\"] - book_train_st_copy[\"ask_size1\"]\n","#         book_train_st_copy[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = book_train_st_copy[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"].apply(lambda x: min(x,0))\n","#         st_lvl2_minus_lvl1_bid_n_ask_size_feat = book_train_st_copy.groupby(by='time_id')[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"].agg(my_sum,engine=\"numba\") #mean, sum\n","#         ## transformation to make the data more normal\n","#         st_lvl2_minus_lvl1_bid_n_ask_size_feat = np.log1p((st_lvl2_minus_lvl1_bid_n_ask_size_feat*-1 )**0.5)\n","#         common_time_id = np.intersect1d(st_lvl2_minus_lvl1_bid_n_ask_size_feat.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         lvl2_minus_lvl1_bid_n_ask_size_feat_n_target_vol_df = pd.DataFrame({'lvl2_minus_lvl1_bid_n_ask_size_feat':st_lvl2_minus_lvl1_bid_n_ask_size_feat.loc[common_time_id].values , 'target_vol':target_st.loc[common_time_id,\"target\"].values})\n","#         corr_coef_lvl2_minus_lvl1_bid_n_ask_size_feat = plot_scatter_n_correlation(lvl2_minus_lvl1_bid_n_ask_size_feat_n_target_vol_df,st_id,method=corr_method)\n","#         lvl2_minus_lvl1_bid_n_ask_size_feat_n_target_vol_corr_coef[st_id] = corr_coef_lvl2_minus_lvl1_bid_n_ask_size_feat\n","#         bk_level1_2_size_imbalance_feat['lvl2_minus_lvl1_bid_n_ask_size_feat'] = pd.concat([ bk_level1_2_size_imbalance_feat['lvl2_minus_lvl1_bid_n_ask_size_feat'], st_lvl2_minus_lvl1_bid_n_ask_size_feat.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ####### Saving FIle #######\n","# ## The bk_level1_2_size_imbalance_feat is saved later below\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"kF4PIQC2R9tT"},"outputs":[],"source":["# import numpy as np\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# import plotly.express as px\n","# from itertools import combinations\n","# from numba import njit\n","\n","# subset_paths = train_book_paths\n","# level = 1  # set level 1 or 2 for book_train data\n","# corr_method = 'spearman'  # set 'pearson' or 'spearman'\n","\n","# bid_price = f\"bid_price{level}\"\n","# ask_price = f\"ask_price{level}\"\n","# bid_size = f\"bid_size{level}\"\n","# ask_size = f\"ask_size{level}\"\n","\n","# bk_level1_2_size_imbalance_feat = {\n","#     'bid_lvl2_min_lvl1_size_feat': pd.DataFrame(),\n","#     'ask_lvl2_min_lvl1_size_feat': pd.DataFrame(),\n","#     'lvl2_minus_lvl1_bid_n_ask_size_feat': pd.DataFrame()\n","# }\n","\n","# #@njit\n","# def calculate_features_numba(bid_size1, bid_size2, ask_size1, ask_size2):\n","#     bid_lvl2_min_lvl1_size_feat = np.minimum(bid_size2 - bid_size1, 0)\n","#     ask_lvl2_min_lvl1_size_feat = np.minimum(ask_size2 - ask_size1, 0)\n","#     lvl2_minus_lvl1_bid_n_ask_size_feat = np.minimum((bid_size2 + ask_size2) - (bid_size1 + ask_size1), 0)\n","#     return bid_lvl2_min_lvl1_size_feat, ask_lvl2_min_lvl1_size_feat, lvl2_minus_lvl1_bid_n_ask_size_feat\n","\n","# def calculate_features(book_train_st):\n","#     # Extract the necessary numpy arrays\n","#     bid_size1 = book_train_st[\"bid_size1\"].values\n","#     bid_size2 = book_train_st[\"bid_size2\"].values\n","#     ask_size1 = book_train_st[\"ask_size1\"].values\n","#     ask_size2 = book_train_st[\"ask_size2\"].values\n","\n","#     # Perform the calculations using numba\n","#     bid_feat, ask_feat, level_feat = calculate_features_numba(bid_size1, bid_size2, ask_size1, ask_size2)\n","\n","#     # Add the results back to the DataFrame\n","#     book_train_st[\"bid_lvl2_min_lvl1_size_feat\"] = bid_feat\n","#     book_train_st[\"ask_lvl2_min_lvl1_size_feat\"] = ask_feat\n","#     book_train_st[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = level_feat\n","\n","#     # Aggregate the features by time_id\n","#     bid_sum = book_train_st.groupby('time_id')[\"bid_lvl2_min_lvl1_size_feat\"].sum()\n","#     ask_sum = book_train_st.groupby('time_id')[\"ask_lvl2_min_lvl1_size_feat\"].sum()\n","#     level_sum = book_train_st.groupby('time_id')[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"].sum()\n","\n","#     # Transformation to make the data more normal\n","#     bid_sum = np.log1p((-bid_sum) ** 0.5)\n","#     ask_sum = np.log1p((-ask_sum) ** 0.5)\n","#     level_sum = np.log1p((-level_sum) ** 0.5)\n","\n","#     return bid_sum, ask_sum, level_sum\n","\n","# def process_path(path):\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     if st_id == -1:\n","#         return None  # skip if invalid stock id\n","\n","#     target_st = train_target.loc[train_target['stock_id'] == st_id].set_index(\"time_id\")\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     bid_sum, ask_sum, level_sum = calculate_features(book_train_st)\n","\n","#     result = {\n","#         'bid_lvl2_min_lvl1_size_feat': bid_sum.reindex(target_st.index).ffill().bfill(),\n","#         'ask_lvl2_min_lvl1_size_feat': ask_sum.reindex(target_st.index).ffill().bfill(),\n","#         'lvl2_minus_lvl1_bid_n_ask_size_feat': level_sum.reindex(target_st.index).ffill().bfill(),\n","#     }\n","#     return result\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_path)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             bk_level1_2_size_imbalance_feat[key] = pd.concat([bk_level1_2_size_imbalance_feat[key], value], axis=0)\n","\n","\n","# # ####### Saving FIle #######\n","# # ## The bk_level1_2_size_imbalance_feat is saved later below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hvrt7eGSK9G"},"outputs":[],"source":["\n","\n","# ## SET PARAMETERS HERE for trade_train.parquet files ONLY!!\n","# ## set the Correlation method\n","\n","# corr_method = 'spearman' # set 'pearson' or 'spearman'\n","# file = 'trade_train' # set 'book_train' or 'trade_train'\n","\n","# price = \"price\"\n","# size = \"size\"\n","# order_count = \"order_count\"\n","\n","# ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","# train_target = train\n","# subset_paths = train_trade_paths\n","# sum_size_n_target_vol_corr_coef = {}\n","# sum_order_count_n_target_vol_corr_coef = {}\n","# sum_size_per_order_count_n_target_vol_corr_coef = {}\n","\n","\n","# trade_sum_size_sum_order_count_sum_size_per_order_count = {}\n","# trade_sum_size_sum_order_count_sum_size_per_order_count['sum_size'] = pd.DataFrame()\n","# trade_sum_size_sum_order_count_sum_size_per_order_count['sum_order_count'] = pd.DataFrame()\n","# trade_sum_size_sum_order_count_sum_size_per_order_count['sum_size_per_order_count'] = pd.DataFrame()\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     #st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         trade_train_st = pd.read_parquet(path)\n","\n","#         st_sum_size = trade_train_st.groupby(by='time_id')[size].agg(['sum']) # mean\n","#         ## transformation to make the data more normal\n","#         st_sum_size = np.log(st_sum_size)\n","#         common_time_id_p = np.intersect1d(st_sum_size.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         sum_size_n_target_vol_df = pd.DataFrame({'sum_size':st_sum_size.loc[common_time_id_p].values.reshape((-1)) , 'target_vol':target_st.loc[common_time_id_p,\"target\"].values})\n","#         corr_coef_sum_size = plot_scatter_n_correlation(sum_size_n_target_vol_df,st_id,method=corr_method)\n","#         sum_size_n_target_vol_corr_coef[st_id] = corr_coef_sum_size\n","#         trade_sum_size_sum_order_count_sum_size_per_order_count['sum_size'] = pd.concat([ trade_sum_size_sum_order_count_sum_size_per_order_count['sum_size'], st_sum_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sum_order_count = trade_train_st.groupby(by='time_id')[order_count].agg(['sum']) # mean\n","#         ## transformation to make the data more normal\n","#         st_sum_order_count = np.log(st_sum_order_count)\n","#         common_time_id_p = np.intersect1d(st_sum_order_count.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         sum_order_count_n_target_vol_df = pd.DataFrame({'sum_size':st_sum_order_count.loc[common_time_id_p].values.reshape((-1)) , 'target_vol':target_st.loc[common_time_id_p,\"target\"].values})\n","#         corr_coef_sum_order_count = plot_scatter_n_correlation(sum_order_count_n_target_vol_df,st_id,method=corr_method)\n","#         sum_order_count_n_target_vol_corr_coef[st_id] = corr_coef_sum_order_count\n","#         trade_sum_size_sum_order_count_sum_size_per_order_count['sum_order_count'] = pd.concat([ trade_sum_size_sum_order_count_sum_size_per_order_count['sum_order_count'], st_sum_order_count.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         trade_train_st[\"size_per_order_count\"] = trade_train_st[\"size\"] / trade_train_st[\"order_count\"]\n","#         st_size_per_order_count = trade_train_st.groupby(by='time_id')[\"size_per_order_count\"].agg(['sum']) # mean\n","#         ## transformation to make the data more normal\n","#         st_size_per_order_count = np.log(st_size_per_order_count)\n","#         common_time_id_p = np.intersect1d(st_size_per_order_count.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         sum_size_per_order_count_n_target_vol_df = pd.DataFrame({'sum_size':st_size_per_order_count.loc[common_time_id_p].values.reshape((-1)) , 'target_vol':target_st.loc[common_time_id_p,\"target\"].values})\n","#         corr_coef_sum_size_per_order_count = plot_scatter_n_correlation(sum_size_per_order_count_n_target_vol_df,st_id,method=corr_method)\n","#         sum_size_per_order_count_n_target_vol_corr_coef[st_id] = corr_coef_sum_size_per_order_count\n","#         trade_sum_size_sum_order_count_sum_size_per_order_count['sum_size_per_order_count'] = pd.concat([ trade_sum_size_sum_order_count_sum_size_per_order_count['sum_size_per_order_count'], st_size_per_order_count.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","# ####### Saving FIle #######\n","# ## The trade_sum_size_sum_order_count_sum_size_per_order_count is saved later below\n","\n","\n","# \"\"\"\n","# New features end\n","# \"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZD8hnAajJFK"},"outputs":[],"source":["# ## SET PARAMETERS HERE for trade_train.parquet files ONLY!!\n","# ## set the Correlation method\n","\n","# ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","\n","# import numpy as np\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# from numba import njit\n","\n","\n","# subset_paths = train_trade_paths\n","# # Set parameters\n","# corr_method = 'spearman'  # set 'pearson' or 'spearman'\n","# file = 'trade_train'  # set 'book_train' or 'trade_train'\n","\n","# size = \"size\"\n","# order_count = \"order_count\"\n","\n","\n","# trade_sum_size_sum_order_count_sum_size_per_order_count = {\n","#     'sum_size': pd.DataFrame(),\n","#     'sum_order_count': pd.DataFrame(),\n","#     'sum_size_per_order_count': pd.DataFrame()\n","# }\n","\n","# #@njit\n","# def calculate_sum_and_normalize(values):\n","#     sum_values = np.sum(values)\n","#     normalized_sum = np.log(sum_values)\n","#     return normalized_sum\n","\n","# def process_stock_data(path):\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     if st_id == -1:\n","#         return None  # Skip if invalid stock id\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     trade_train_st = pd.read_parquet(path)\n","\n","#     # Compute sum and normalize\n","#     st_sum_size = trade_train_st.groupby(by='time_id')[size].apply(lambda x: calculate_sum_and_normalize(x.values))\n","#     st_sum_order_count = trade_train_st.groupby(by='time_id')[order_count].apply(lambda x: calculate_sum_and_normalize(x.values))\n","\n","#     trade_train_st[\"size_per_order_count\"] = trade_train_st[\"size\"] / trade_train_st[\"order_count\"]\n","#     st_size_per_order_count = trade_train_st.groupby(by='time_id')[\"size_per_order_count\"].apply(lambda x: calculate_sum_and_normalize(x.values))\n","\n","#     result = {\n","#         'sum_size': st_sum_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'sum_order_count': st_sum_order_count.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'sum_size_per_order_count': st_size_per_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","#     }\n","\n","#     return result\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_stock_data)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             trade_sum_size_sum_order_count_sum_size_per_order_count[key] = pd.concat([trade_sum_size_sum_order_count_sum_size_per_order_count[key], value], axis=0)\n","\n","# ####### Saving File #######\n","# ## The trade_sum_size_sum_order_count_sum_size_per_order_count is saved later below\n","\n","# \"\"\"\n","# New features end\n","# \"\"\"\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35KWyyeqSRHO"},"outputs":[],"source":["\n","# \"\"\"\n","# old features start\n","# \"\"\"\n","\n","# ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","# subset_paths = train_book_paths\n","# train_target = pd.read_csv('train.csv')\n","\n","\n","# min_bid_price_n_target_vol_corr_coef = {}\n","# max_bid_price_n_target_vol_corr_coef = {}\n","# min_ask_price_n_target_vol_corr_coef = {}\n","# max_ask_price_n_target_vol_corr_coef = {}\n","# min_bid_size_n_target_vol_corr_coef = {}\n","# max_bid_size_n_target_vol_corr_coef = {}\n","# min_ask_size_n_target_vol_corr_coef = {}\n","# max_ask_size_n_target_vol_corr_coef = {}\n","# range_ask_price_n_target_vol_corr_coef = {}\n","# range_bid_price_n_target_vol_corr_coef = {}\n","# range_ask_size_n_target_vol_corr_coef = {}\n","# range_bid_size_n_target_vol_corr_coef = {}\n","\n","\n","# bk_price_size_min_max_range = {}\n","\n","# bk_price_size_min_max_range['st_min_max_bid_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_min_max_ask_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_min_max_bid_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_min_max_ask_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_ask_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_bid_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_ask_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_bid_size'+str(level)] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","\n","#         st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min','max']).rename(columns={'min':'min_bid_price','max':'max_bid_price'})\n","#         common_time_id_mmbp = np.intersect1d(st_min_max_bid_price.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         min_bid_price_n_target_vol_df = pd.DataFrame({'min_bid_price':st_min_max_bid_price.loc[common_time_id_mmbp,\"min_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_mmbp,\"target\"].values})\n","#         corr_coef_min_bid_price = plot_scatter_n_correlation(min_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         min_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_min_bid_price\n","#         max_bid_price_n_target_vol_df = pd.DataFrame({'max_bid_price':st_min_max_bid_price.loc[common_time_id_mmbp,\"max_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_mmbp,\"target\"].values})\n","#         corr_coef_max_bid_price = plot_scatter_n_correlation(max_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         max_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_max_bid_price\n","#         bk_price_size_min_max_range['st_min_max_bid_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_bid_price'+str(level)], st_min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","#         st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min','max']).rename(columns={'min':'min_ask_price','max':'max_ask_price'})\n","#         common_time_id_mmap = np.intersect1d(st_min_max_ask_price.index.values, target_st['time_id'].values)\n","#         min_ask_price_n_target_vol_df = pd.DataFrame({'min_ask_price':st_min_max_ask_price.loc[common_time_id_mmap,\"min_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_mmap,\"target\"].values})\n","#         corr_coef_min_ask_price = plot_scatter_n_correlation(min_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         min_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_min_ask_price\n","#         max_ask_price_n_target_vol_df = pd.DataFrame({'max_ask_price':st_min_max_ask_price.loc[common_time_id_mmap,\"max_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_mmap,\"target\"].values})\n","#         corr_coef_max_ask_price = plot_scatter_n_correlation(max_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         max_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_max_ask_price\n","#         bk_price_size_min_max_range['st_min_max_ask_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_ask_price'+str(level)], st_min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min','max']).rename(columns={'min':'min_bid_size','max':'max_bid_size'})\n","#         common_time_id_mmbz = np.intersect1d(st_min_max_bid_size.index.values, target_st['time_id'].values)\n","#         min_bid_size_n_target_vol_df = pd.DataFrame({'min_bid_size':st_min_max_bid_size.loc[common_time_id_mmbz,\"min_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_mmbz,\"target\"].values})\n","#         corr_coef_min_bid_size = plot_scatter_n_correlation(min_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         min_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_min_bid_size\n","#         max_bid_size_n_target_vol_df = pd.DataFrame({'max_bid_size':st_min_max_bid_size.loc[common_time_id_mmbz,\"max_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_mmbz,\"target\"].values})\n","#         corr_coef_max_bid_size = plot_scatter_n_correlation(max_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         max_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_max_bid_size\n","#         bk_price_size_min_max_range['st_min_max_bid_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_bid_size'+str(level)], st_min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min','max']).rename(columns={'min':'min_ask_size','max':'max_ask_size'})\n","#         common_time_id_mmas = np.intersect1d(st_min_max_ask_size.index.values, target_st['time_id'].values)\n","#         min_ask_size_n_target_vol_df = pd.DataFrame({'min_ask_size':st_min_max_ask_size.loc[common_time_id_mmas,\"min_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_mmas,\"target\"].values})\n","#         corr_coef_min_ask_size = plot_scatter_n_correlation(min_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         min_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_min_ask_size\n","#         max_ask_size_n_target_vol_df = pd.DataFrame({'max_ask_size':st_min_max_ask_size.loc[common_time_id_mmas,\"max_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_mmas,\"target\"].values})\n","#         corr_coef_max_ask_size = plot_scatter_n_correlation(max_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         max_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_max_ask_size\n","#         bk_price_size_min_max_range['st_min_max_ask_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_ask_size'+str(level)], st_min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price:[my_range_price]}).rename(columns={ask_price:'range_ask_price'})\n","#         st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","#         common_time_id_rap = np.intersect1d(st_range_ask_price.index.values, target_st['time_id'].values)\n","#         range_ask_price_n_target_vol_df = pd.DataFrame({'range_ask_price':st_range_ask_price.loc[common_time_id_rap,\"range_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_rap,\"target\"].values})\n","#         corr_coef_range_ask_price = plot_scatter_n_correlation(range_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         range_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_range_ask_price\n","#         bk_price_size_min_max_range['st_range_ask_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_ask_price'+str(level)], st_range_ask_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price:[my_range_price]}).rename(columns={bid_price:'range_bid_price'})\n","#         st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","#         common_time_id_rbp = np.intersect1d(st_range_bid_price.index.values, target_st['time_id'].values)\n","#         range_bid_price_n_target_vol_df = pd.DataFrame({'range_bid_price':st_range_bid_price.loc[common_time_id_rbp,\"range_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_rbp,\"target\"].values})\n","#         corr_coef_range_bid_price = plot_scatter_n_correlation(range_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         range_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_range_bid_price\n","#         bk_price_size_min_max_range['st_range_bid_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_bid_price'+str(level)], st_range_bid_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size:[my_range_price]}).rename(columns={ask_size:'range_ask_size'})\n","#         st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","#         common_time_id_ras = np.intersect1d(st_range_ask_size.index.values, target_st['time_id'].values)\n","#         range_ask_size_n_target_vol_df = pd.DataFrame({'range_ask_size':st_range_ask_size.loc[common_time_id_ras,\"range_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_ras,\"target\"].values})\n","#         corr_coef_range_ask_size = plot_scatter_n_correlation(range_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         range_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_range_ask_size\n","#         bk_price_size_min_max_range['st_range_ask_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_ask_size'+str(level)], st_range_ask_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size:[my_range_price]}).rename(columns={bid_size:'range_bid_size'})\n","#         st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","#         common_time_id_rbs = np.intersect1d(st_range_bid_size.index.values, target_st['time_id'].values)\n","#         range_bid_size_n_target_vol_df = pd.DataFrame({'range_bid_size':st_range_bid_size.loc[common_time_id_rbs,\"range_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_rbs,\"target\"].values})\n","#         corr_coef_range_bid_size = plot_scatter_n_correlation(range_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         range_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_range_bid_size\n","#         bk_price_size_min_max_range['st_range_bid_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_bid_size'+str(level)], st_range_bid_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ######## SAVING FILE\n","# # This bk_price_size_min_max_range is saved below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYwKSlUZkG1C"},"outputs":[],"source":["# # \"\"\"\n","# # old features start\n","# # \"\"\"\n","\n","# # ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","\n","# import numpy as np\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# from numba import njit\n","\n","# level = 1\n","# subset_paths = train_book_paths\n","\n","# # Define your custom range function for price and size\n","# #@njit\n","# def my_range_price(values):\n","#     return np.max(values) - np.min(values)\n","\n","# # Initialize dictionaries\n","# bk_price_size_min_max_range = {\n","#     'st_min_max_bid_price'+str(level): pd.DataFrame(),\n","#     'st_min_max_ask_price'+str(level): pd.DataFrame(),\n","#     'st_min_max_bid_size'+str(level): pd.DataFrame(),\n","#     'st_min_max_ask_size'+str(level): pd.DataFrame(),\n","#     'st_range_ask_price'+str(level): pd.DataFrame(),\n","#     'st_range_bid_price'+str(level): pd.DataFrame(),\n","#     'st_range_ask_size'+str(level): pd.DataFrame(),\n","#     'st_range_bid_size'+str(level): pd.DataFrame()\n","# }\n","\n","# #@njit\n","# def calculate_min_max_range(values):\n","#     min_val = np.min(values)\n","#     max_val = np.max(values)\n","#     return min_val, max_val\n","\n","# def process_book_data(path):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     if st_id == -1:\n","#         return None  # Skip if invalid stock id\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     # Calculate min and max\n","#     bid_price_min_max = book_train_st.groupby(by='time_id')[bid_price].apply(lambda x: calculate_min_max_range(x.values))\n","#     ask_price_min_max = book_train_st.groupby(by='time_id')[ask_price].apply(lambda x: calculate_min_max_range(x.values))\n","#     bid_size_min_max = book_train_st.groupby(by='time_id')[bid_size].apply(lambda x: calculate_min_max_range(x.values))\n","#     ask_size_min_max = book_train_st.groupby(by='time_id')[ask_size].apply(lambda x: calculate_min_max_range(x.values))\n","\n","#     min_max_bid_price = pd.DataFrame(bid_price_min_max.tolist(), index=bid_price_min_max.index, columns=['min_bid_price', 'max_bid_price'])\n","#     min_max_ask_price = pd.DataFrame(ask_price_min_max.tolist(), index=ask_price_min_max.index, columns=['min_ask_price', 'max_ask_price'])\n","#     min_max_bid_size = pd.DataFrame(bid_size_min_max.tolist(), index=bid_size_min_max.index, columns=['min_bid_size', 'max_bid_size'])\n","#     min_max_ask_size = pd.DataFrame(ask_size_min_max.tolist(), index=ask_size_min_max.index, columns=['min_ask_size', 'max_ask_size'])\n","\n","#     # Calculate ranges\n","#     range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: lambda x: my_range_price(x.values)}).rename(columns={ask_price: 'range_ask_price'})\n","#     range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: lambda x: my_range_price(x.values)}).rename(columns={bid_price: 'range_bid_price'})\n","#     range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: lambda x: my_range_price(x.values)}).rename(columns={ask_size: 'range_ask_size'})\n","#     range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: lambda x: my_range_price(x.values)}).rename(columns={bid_size: 'range_bid_size'})\n","\n","#     # Reindex and concatenate\n","#     result = {\n","#         'st_min_max_bid_price'+str(level): min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_min_max_ask_price'+str(level): min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_min_max_bid_size'+str(level): min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_min_max_ask_size'+str(level): min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_ask_price'+str(level): range_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_bid_price'+str(level): range_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_ask_size'+str(level): range_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_bid_size'+str(level): range_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","#     }\n","\n","#     return result\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_book_data)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             bk_price_size_min_max_range[key] = pd.concat([bk_price_size_min_max_range[key], value], axis=0)\n","\n","# \"\"\"\n","# Old features end\n","# \"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNW5sur7STuw"},"outputs":[],"source":["\n","\n","\n","# ## 7f) - 7i) Check if  the sum of absolute differences is correlated with target\n","\n","# subset_paths = train_book_paths\n","\n","# sum_abs_dif_ask_price_n_target_vol_corr_coef = {}\n","# sum_abs_dif_ask_size_n_target_vol_corr_coef = {}\n","# sum_abs_dif_bid_price_n_target_vol_corr_coef = {}\n","# sum_abs_dif_bid_size_n_target_vol_corr_coef = {}\n","\n","# bk_price_size_sad = {}\n","\n","# bk_price_size_sad['st_sad_ask_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_sad['st_sad_ask_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_sad['st_sad_bid_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_sad['st_sad_bid_size'+str(level)] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","\n","#         st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price:[my_sum_abs_diff]}).rename(columns={ask_price:'sad_ask_price'})\n","#         st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","#         common_time_id_sadap = np.intersect1d(st_sad_ask_price.index.values, target_st['time_id'].values)\n","#         sad_ask_price_n_target_vol_df = pd.DataFrame({'sad_ask_price':st_sad_ask_price.loc[common_time_id_sadap,\"sad_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_sadap,\"target\"].values})\n","#         corr_coef_sad_ask_price = plot_scatter_n_correlation(sad_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_sad_ask_price\n","#         bk_price_size_sad['st_sad_ask_price'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_ask_price'+str(level)], st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size:[my_sum_abs_diff]}).rename(columns={ask_size:'sad_ask_size'})\n","#         st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","#         common_time_id_sadas = np.intersect1d(st_sad_ask_size.index.values, target_st['time_id'].values)\n","#         sad_ask_size_n_target_vol_df = pd.DataFrame({'sad_ask_size':st_sad_ask_size.loc[common_time_id_sadas,\"sad_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_sadas,\"target\"].values})\n","#         corr_coef_sad_ask_size = plot_scatter_n_correlation(sad_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_sad_ask_size\n","#         bk_price_size_sad['st_sad_ask_size'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_ask_size'+str(level)], st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price:[my_sum_abs_diff]}).rename(columns={bid_price:'sad_bid_price'})\n","#         st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","#         common_time_id_sadbp = np.intersect1d(st_sad_bid_price.index.values, target_st['time_id'].values)\n","#         sad_bid_price_n_target_vol_df = pd.DataFrame({'sad_bid_price':st_sad_bid_price.loc[common_time_id_sadbp,\"sad_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_sadbp,\"target\"].values})\n","#         corr_coef_sad_bid_price = plot_scatter_n_correlation(sad_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_sad_bid_price\n","#         bk_price_size_sad['st_sad_bid_price'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_bid_price'+str(level)], st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size:[my_sum_abs_diff]}).rename(columns={bid_size:'sad_bid_size'})\n","#         st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","#         common_time_id_sadbs = np.intersect1d(st_sad_bid_size.index.values, target_st['time_id'].values)\n","#         sad_bid_size_n_target_vol_df = pd.DataFrame({'sad_bid_size':st_sad_bid_size.loc[common_time_id_sadbs,\"sad_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_sadbs,\"target\"].values})\n","#         corr_coef_sad_bid_size = plot_scatter_n_correlation(sad_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_sad_bid_size\n","#         bk_price_size_sad['st_sad_bid_size'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_bid_size'+str(level)], st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ######## SAVING FILE\n","# #This bk_price_size_sad is saved later below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmEzT8yVp6zZ"},"outputs":[],"source":["# ## 7f) - 7i) Check if  the sum of absolute differences is correlated with target\n","\n","\n","# subset_paths = train_book_paths\n","\n","# import numpy as np\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# from numba import njit\n","\n","# # Define your custom SAD function\n","# #@njit\n","# def my_sum_abs_diff(values):\n","#     return np.sum(np.abs(np.diff(values)))\n","\n","# # Initialize dictionaries\n","# bk_price_size_sad = {\n","#     'st_sad_ask_price'+str(level): pd.DataFrame(),\n","#     'st_sad_ask_size'+str(level): pd.DataFrame(),\n","#     'st_sad_bid_price'+str(level): pd.DataFrame(),\n","#     'st_sad_bid_size'+str(level): pd.DataFrame()\n","# }\n","\n","# def process_book_data_sad(path):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     if st_id == -1:\n","#         return None  # Skip if invalid stock id\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     # Calculate SAD\n","#     def calculate_sad(series):\n","#         return my_sum_abs_diff(series.values)\n","\n","#     st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: calculate_sad}).rename(columns={ask_price: 'sad_ask_price'})\n","#     st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: calculate_sad}).rename(columns={ask_size: 'sad_ask_size'})\n","#     st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: calculate_sad}).rename(columns={bid_price: 'sad_bid_price'})\n","#     st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: calculate_sad}).rename(columns={bid_size: 'sad_bid_size'})\n","\n","#     # Reindex and concatenate\n","#     result = {\n","#         'st_sad_ask_price'+str(level): st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_sad_ask_size'+str(level): st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_sad_bid_price'+str(level): st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_sad_bid_size'+str(level): st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","#     }\n","\n","#     return result\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_book_data_sad)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             bk_price_size_sad[key] = pd.concat([bk_price_size_sad[key], value], axis=0)\n","\n","# ######## SAVING FILE\n","# # This bk_price_size_sad is saved later below\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6XOAeikSXBM"},"outputs":[],"source":["\n","# ## 7g) - 7j) Check if the correlation of any pair of bidsize1,bid_price1,asksize1,ask_price1 is correlated with target realized volatitlity for all the time_ids?\n","\n","\n","# subset_paths = train_book_paths\n","# train_target = pd.read_csv('train.csv')\n","\n","\n","# bs_bp_corr_n_target_vol_corr_coef = {}\n","# bs_as_corr_n_target_vol_corr_coef = {}\n","# bs_ap_corr_n_target_vol_corr_coef = {}\n","# bp_as_corr_n_target_vol_corr_coef = {}\n","# bp_ap_corr_n_target_vol_corr_coef = {}\n","# as_ap_corr_n_target_vol_corr_coef = {}\n","\n","# bk_size_price_corr = {}\n","\n","# bk_size_price_corr['st_bs_bp_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bs_as_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bs_ap_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bp_as_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bp_ap_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_as_ap_corr'+str(level)] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","\n","#         st_bs_bp_corr = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1]\n","#         st_bs_bp_corr.index = st_bs_bp_corr.index.droplevel(1)\n","#         common_time_id_bs_bp = np.intersect1d(st_bs_bp_corr.index.values, target_st['time_id'].values)\n","#         bs_bp_corr_n_target_vol_df = pd.DataFrame({'bs_bp_corr':st_bs_bp_corr.loc[common_time_id_bs_bp].values , 'target_vol':target_st.loc[common_time_id_bs_bp,\"target\"].values})\n","#         corr_coef_bs_bp = plot_scatter_n_correlation(bs_bp_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bs_bp_corr_n_target_vol_corr_coef[st_id] = corr_coef_bs_bp\n","#         bk_size_price_corr['st_bs_bp_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bs_bp_corr'+str(level)], st_bs_bp_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bs_as_corr = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1]\n","#         st_bs_as_corr.index = st_bs_as_corr.index.droplevel(1)\n","#         common_time_id_bs_as = np.intersect1d(st_bs_as_corr.index.values, target_st['time_id'].values)\n","#         bs_as_corr_n_target_vol_df = pd.DataFrame({'bs_as_corr':st_bs_as_corr.loc[common_time_id_bs_as].values , 'target_vol':target_st.loc[common_time_id_bs_as,\"target\"].values})\n","#         corr_coef_bs_as = plot_scatter_n_correlation(bs_as_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bs_as_corr_n_target_vol_corr_coef[st_id] = corr_coef_bs_as\n","#         bk_size_price_corr['st_bs_as_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bs_as_corr'+str(level)], st_bs_as_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bs_ap_corr = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1]\n","#         st_bs_ap_corr.index = st_bs_ap_corr.index.droplevel(1)\n","#         common_time_id_bs_ap = np.intersect1d(st_bs_ap_corr.index.values, target_st['time_id'].values)\n","#         bs_ap_corr_n_target_vol_df = pd.DataFrame({'bs_ap_corr':st_bs_ap_corr.loc[common_time_id_bs_ap].values , 'target_vol':target_st.loc[common_time_id_bs_ap,\"target\"].values})\n","#         corr_coef_bs_ap = plot_scatter_n_correlation(bs_ap_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bs_ap_corr_n_target_vol_corr_coef[st_id] = corr_coef_bs_ap\n","#         bk_size_price_corr['st_bs_ap_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bs_ap_corr'+str(level)], st_bs_ap_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bp_as_corr = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1]\n","#         st_bp_as_corr.index = st_bp_as_corr.index.droplevel(1)\n","#         common_time_id_bp_as = np.intersect1d(st_bp_as_corr.index.values, target_st['time_id'].values)\n","#         bp_as_corr_n_target_vol_df = pd.DataFrame({'bp_as_corr':st_bp_as_corr.loc[common_time_id_bp_as].values , 'target_vol':target_st.loc[common_time_id_bp_as,\"target\"].values})\n","#         corr_coef_bp_as = plot_scatter_n_correlation(bp_as_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bp_as_corr_n_target_vol_corr_coef[st_id] = corr_coef_bp_as\n","#         bk_size_price_corr['st_bp_as_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bp_as_corr'+str(level)], st_bp_as_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bp_ap_corr = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1]\n","#         st_bp_ap_corr.index = st_bp_ap_corr.index.droplevel(1)\n","#         common_time_id_bp_ap = np.intersect1d(st_bp_ap_corr.index.values, target_st['time_id'].values)\n","#         bp_ap_corr_n_target_vol_df = pd.DataFrame({'bp_ap_corr':st_bp_ap_corr.loc[common_time_id_bp_ap].values , 'target_vol':target_st.loc[common_time_id_bp_ap,\"target\"].values})\n","#         corr_coef_bp_ap = plot_scatter_n_correlation(bp_ap_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bp_ap_corr_n_target_vol_corr_coef[st_id] = corr_coef_bp_ap\n","#         bk_size_price_corr['st_bp_ap_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bp_ap_corr'+str(level)], st_bp_ap_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_as_ap_corr = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1]\n","#         st_as_ap_corr.index = st_as_ap_corr.index.droplevel(1)\n","#         common_time_id_as_ap = np.intersect1d(st_as_ap_corr.index.values, target_st['time_id'].values)\n","#         as_ap_corr_n_target_vol_df = pd.DataFrame({'as_ap_corr':st_as_ap_corr.loc[common_time_id_as_ap].values , 'target_vol':target_st.loc[common_time_id_as_ap,\"target\"].values})\n","#         corr_coef_as_ap = plot_scatter_n_correlation(as_ap_corr_n_target_vol_df,st_id,method=corr_method)\n","#         as_ap_corr_n_target_vol_corr_coef[st_id] = corr_coef_as_ap\n","#         bk_size_price_corr['st_as_ap_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_as_ap_corr'+str(level)], st_as_ap_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ######## saving FILE\n","# #This is bk_size_price_corr saved below\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4RmD9qmvvAk"},"outputs":[],"source":["\n","# ## 7g) - 7j) Check if the correlation of any pair of bidsize1,bid_price1,asksize1,ask_price1 is correlated with target realized volatitlity for all the time_ids?\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","# level=1\n","# subset_paths = train_book_paths\n","\n","# bid_price = f\"bid_price{level}\"\n","# ask_price = f\"ask_price{level}\"\n","# bid_size = f\"bid_size{level}\"\n","# ask_size = f\"ask_size{level}\"\n","\n","\n","# import pandas as pd\n","# import numpy as np\n","# from joblib import Parallel, delayed\n","# from numba import njit\n","\n","# # Define a function to compute correlation\n","\n","# # def compute_correlation(x, y):\n","# #     if len(x) < 2 or len(y) < 2:\n","# #         return np.nan\n","# #     mean_x = np.mean(x)\n","# #     mean_y = np.mean(y)\n","# #     std_x = np.std(x, ddof=0)\n","# #     std_y = np.std(y, ddof=0)\n","# #     covariance = np.mean((x - mean_x) * (y - mean_y))\n","# #     return covariance / (std_x * std_y) if std_x > 0 and std_y > 0 else np.nan\n","\n","# # def calculate_pairwise_correlation(df, col1, col2):\n","# #     correlations = df.groupby('time_id').apply(lambda x: compute_correlation(x[col1].values, x[col2].values))\n","# #     return correlations.rename(f'{col1}_{col2}_corr')\n","\n","# def process_book_data_corr(path):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     # # Calculate correlations\n","#     # st_bs_bp_corr = calculate_pairwise_correlation(book_train_st, bid_size, bid_price)\n","#     # st_bs_as_corr = calculate_pairwise_correlation(book_train_st, bid_size, ask_size)\n","#     # st_bs_ap_corr = calculate_pairwise_correlation(book_train_st, bid_size, ask_price)\n","#     # st_bp_as_corr = calculate_pairwise_correlation(book_train_st, bid_price, ask_size)\n","#     # st_bp_ap_corr = calculate_pairwise_correlation(book_train_st, bid_price, ask_price)\n","#     # st_as_ap_corr = calculate_pairwise_correlation(book_train_st, ask_size, ask_price)\n","\n","#     # # Reindex and concatenate\n","#     # result = {\n","#     #     'st_bs_bp_corr'+str(level): st_bs_bp_corr.reindex(target_st['time_id'].values).ffill().bfill(),\n","#     #     'st_bs_as_corr'+str(level): st_bs_as_corr.reindex(target_st['time_id'].values).ffill().bfill(),\n","#     #     'st_bs_ap_corr'+str(level): st_bs_ap_corr.reindex(target_st['time_id'].values).ffill().bfill(),\n","#     #     'st_bp_as_corr'+str(level): st_bp_as_corr.reindex(target_st['time_id'].values).ffill().bfill(),\n","#     #     'st_bp_ap_corr'+str(level): st_bp_ap_corr.reindex(target_st['time_id'].values).ffill().bfill(),\n","#     #     'st_as_ap_corr'+str(level): st_as_ap_corr.reindex(target_st['time_id'].values).ffill().bfill()\n","#     # }\n","\n","#     # return result\n","\n","#     # Calculate correlations and drop the multi-level index\n","#     corr_dict = {}\n","#     corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","#     # Reindex with target_st time_ids and apply forward and backward fill\n","#     for key in corr_dict.keys():\n","#         corr_dict[key] = corr_dict[key].reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     return corr_dict\n","\n","\n","# # Initialize dictionaries\n","# bk_size_price_corr = {\n","#     'st_bs_bp_corr'+str(level): pd.DataFrame(),\n","#     'st_bs_as_corr'+str(level): pd.DataFrame(),\n","#     'st_bs_ap_corr'+str(level): pd.DataFrame(),\n","#     'st_bp_as_corr'+str(level): pd.DataFrame(),\n","#     'st_bp_ap_corr'+str(level): pd.DataFrame(),\n","#     'st_as_ap_corr'+str(level): pd.DataFrame()\n","# }\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_book_data_corr)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             bk_size_price_corr[key+str(level)] = pd.concat([bk_size_price_corr[key+str(level)], value], axis=0)\n","\n","# ######## saving FILE\n","# # This bk_size_price_corr is saved later below\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUqk-zKWSZBW"},"outputs":[],"source":["\n","\n","# ## SET PARAMETERS HERE for trade_train.parquet files ONLY!!\n","# ## set the Correlation method\n","\n","# corr_method = 'pearson' # set 'pearson' or 'spearman'\n","# file = 'trade_train' # set 'book_train' or 'trade_train'\n","\n","# price = \"price\"\n","# size = \"size\"\n","# order_count = \"order_count\"\n","\n","# ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","# subset_paths = train_trade_paths\n","# train_target = pd.read_csv('train.csv')\n","\n","\n","# min_price_n_target_vol_corr_coef = {}\n","# max_price_n_target_vol_corr_coef = {}\n","# min_size_n_target_vol_corr_coef = {}\n","# max_size_n_target_vol_corr_coef = {}\n","# min_order_count_n_target_vol_corr_coef = {}\n","# max_order_count_n_target_vol_corr_coef = {}\n","# range_price_n_target_vol_corr_coef = {}\n","# range_size_n_target_vol_corr_coef = {}\n","# range_order_count_n_target_vol_corr_coef = {}\n","\n","\n","# trade_price_size_order_count_min_max_range = {}\n","\n","# trade_price_size_order_count_min_max_range['st_min_price'] = pd.DataFrame()\n","# trade_price_size_order_count_min_max_range['st_max_price'] = pd.DataFrame()\n","# #trade_price_size_order_count_min_max_range['st_min_size'] = pd.DataFrame()\n","# trade_price_size_order_count_min_max_range['st_max_size'] = pd.DataFrame()\n","# #trade_price_size_order_count_min_max_range['st_min_order_count'] = pd.DataFrame()\n","# trade_price_size_order_count_min_max_range['st_max_order_count'] = pd.DataFrame()\n","# trade_price_size_order_count_min_max_range['st_range_price'] = pd.DataFrame()\n","# trade_price_size_order_count_min_max_range['st_range_size'] = pd.DataFrame()\n","# trade_price_size_order_count_min_max_range['st_range_order_count'] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         trade_train_st = pd.read_parquet(path)\n","\n","#         st_min_max_price = trade_train_st.groupby(by='time_id')[price].agg(['min','max']).rename(columns={'min':'min_price','max':'max_price'})\n","#         common_time_id_p = np.intersect1d(st_min_max_price.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         min_price_n_target_vol_df = pd.DataFrame({'min_price':st_min_max_price.loc[common_time_id_p,\"min_price\"].values , 'target_vol':target_st.loc[common_time_id_p,\"target\"].values})\n","#         corr_coef_min_price = plot_scatter_n_correlation(min_price_n_target_vol_df,st_id,method=corr_method)\n","#         min_price_n_target_vol_corr_coef[st_id] = corr_coef_min_price\n","#         max_price_n_target_vol_df = pd.DataFrame({'max_price':st_min_max_price.loc[common_time_id_p,\"max_price\"].values , 'target_vol':target_st.loc[common_time_id_p,\"target\"].values})\n","#         corr_coef_max_price = plot_scatter_n_correlation(max_price_n_target_vol_df,st_id,method=corr_method)\n","#         max_price_n_target_vol_corr_coef[st_id] = corr_coef_max_price\n","#         trade_price_size_order_count_min_max_range['st_max_price'] = pd.concat([ trade_price_size_order_count_min_max_range['st_max_price'], st_min_max_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","#         trade_price_size_order_count_min_max_range['st_min_price'] = pd.concat([ trade_price_size_order_count_min_max_range['st_min_price'], st_min_max_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_min_max_size = trade_train_st.groupby(by='time_id')[size].agg(['min','max']).rename(columns={'min':'min_size','max':'max_size'})\n","#         common_time_id_s = np.intersect1d(st_min_max_size.index.values, target_st['time_id'].values)\n","#         min_size_n_target_vol_df = pd.DataFrame({'min_size':st_min_max_size.loc[common_time_id_s,\"min_size\"].values , 'target_vol':target_st.loc[common_time_id_s,\"target\"].values})\n","#         corr_coef_min_size = plot_scatter_n_correlation(min_size_n_target_vol_df,st_id,method=corr_method)\n","#         min_size_n_target_vol_corr_coef[st_id] = corr_coef_min_size\n","#         max_size_n_target_vol_df = pd.DataFrame({'max_size':st_min_max_size.loc[common_time_id_s,\"max_size\"].values , 'target_vol':target_st.loc[common_time_id_s,\"target\"].values})\n","#         corr_coef_max_size = plot_scatter_n_correlation(max_size_n_target_vol_df,st_id,method=corr_method)\n","#         max_size_n_target_vol_corr_coef[st_id] = corr_coef_max_size\n","#         #trade_price_size_order_count_min_max_range['st_min_size'] = pd.concat([ trade_price_size_order_count_min_max_range['st_min_size'], st_min_max_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","#         trade_price_size_order_count_min_max_range['st_max_size'] = pd.concat([ trade_price_size_order_count_min_max_range['st_max_size'], st_min_max_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_min_max_order_count = trade_train_st.groupby(by='time_id')[order_count].agg(['min','max']).rename(columns={'min':'min_order_count','max':'max_order_count'})\n","#         common_time_id_oc = np.intersect1d(st_min_max_order_count.index.values, target_st['time_id'].values)\n","#         min_order_count_n_target_vol_df = pd.DataFrame({'min_order_count':st_min_max_order_count.loc[common_time_id_oc,\"min_order_count\"].values , 'target_vol':target_st.loc[common_time_id_oc,\"target\"].values})\n","#         corr_coef_min_order_count = plot_scatter_n_correlation(min_order_count_n_target_vol_df,st_id,method=corr_method)\n","#         min_order_count_n_target_vol_corr_coef[st_id] = corr_coef_min_order_count\n","#         max_order_count_n_target_vol_df = pd.DataFrame({'max_order_count':st_min_max_order_count.loc[common_time_id_oc,\"max_order_count\"].values , 'target_vol':target_st.loc[common_time_id_oc,\"target\"].values})\n","#         corr_coef_max_order_count = plot_scatter_n_correlation(max_order_count_n_target_vol_df,st_id,method=corr_method)\n","#         max_order_count_n_target_vol_corr_coef[st_id] = corr_coef_max_order_count\n","#         #trade_price_size_order_count_min_max_range['st_min_order_count'] = pd.concat([ trade_price_size_order_count_min_max_range['st_min_order_count'], st_min_max_order_count.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","#         trade_price_size_order_count_min_max_range['st_max_order_count'] = pd.concat([ trade_price_size_order_count_min_max_range['st_max_order_count'], st_min_max_order_count.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_price = trade_train_st.groupby(by='time_id').agg({price:[my_range_price]}).rename(columns={price:'range_price'})\n","#         st_range_price.columns = st_range_price.columns.droplevel(1)\n","#         common_time_id_rp = np.intersect1d(st_range_price.index.values, target_st['time_id'].values)\n","#         range_price_n_target_vol_df = pd.DataFrame({'range_price':st_range_price.loc[common_time_id_rp,\"range_price\"].values , 'target_vol':target_st.loc[common_time_id_rp,\"target\"].values})\n","#         corr_coef_range_price = plot_scatter_n_correlation(range_price_n_target_vol_df,st_id,method=corr_method)\n","#         range_price_n_target_vol_corr_coef[st_id] = corr_coef_range_price\n","#         trade_price_size_order_count_min_max_range['st_range_price'] = pd.concat([ trade_price_size_order_count_min_max_range['st_range_price'], st_range_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_size = trade_train_st.groupby(by='time_id').agg({size:[my_range_price]}).rename(columns={size:'range_size'})\n","#         st_range_size.columns = st_range_size.columns.droplevel(1)\n","#         common_time_id_rs = np.intersect1d(st_range_size.index.values, target_st['time_id'].values)\n","#         range_size_n_target_vol_df = pd.DataFrame({'range_size':st_range_size.loc[common_time_id_rs,\"range_size\"].values , 'target_vol':target_st.loc[common_time_id_rs,\"target\"].values})\n","#         corr_coef_range_size = plot_scatter_n_correlation(range_size_n_target_vol_df,st_id,method=corr_method)\n","#         range_size_n_target_vol_corr_coef[st_id] = corr_coef_range_size\n","#         trade_price_size_order_count_min_max_range['st_range_size'] = pd.concat([ trade_price_size_order_count_min_max_range['st_range_size'], st_range_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_order_count = trade_train_st.groupby(by='time_id').agg({order_count:[my_range_price]}).rename(columns={order_count:'range_order_count'})\n","#         st_range_order_count.columns = st_range_order_count.columns.droplevel(1)\n","#         common_time_id_roc = np.intersect1d(st_range_order_count.index.values, target_st['time_id'].values)\n","#         range_order_count_n_target_vol_df = pd.DataFrame({'range_order_count':st_range_order_count.loc[common_time_id_roc,\"range_order_count\"].values , 'target_vol':target_st.loc[common_time_id_roc,\"target\"].values})\n","#         corr_coef_range_order_count = plot_scatter_n_correlation(range_order_count_n_target_vol_df,st_id,method=corr_method)\n","#         range_order_count_n_target_vol_corr_coef[st_id] = corr_coef_range_order_count\n","#         trade_price_size_order_count_min_max_range['st_range_order_count'] = pd.concat([ trade_price_size_order_count_min_max_range['st_range_order_count'], st_range_order_count.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ######## Saving File\n","# # This trade_price_size_order_count_min_max_range is saved below\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxd8AB2Mwtxc"},"outputs":[],"source":["\n","\n","# ## SET PARAMETERS HERE for trade_train.parquet files ONLY!!\n","# ## set the Correlation method\n","\n","# corr_method = 'pearson' # set 'pearson' or 'spearman'\n","# file = 'trade_train' # set 'book_train' or 'trade_train'\n","\n","# price = \"price\"\n","# size = \"size\"\n","# order_count = \"order_count\"\n","\n","# ## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","# subset_paths = train_trade_paths\n","\n","\n","# import pandas as pd\n","# import numpy as np\n","# from joblib import Parallel, delayed\n","# from numba import njit\n","\n","# # Define custom range function\n","\n","# def my_range_price(values):\n","#     return np.max(values) - np.min(values)\n","\n","# # Define a function to calculate min, max, and range\n","# def calculate_min_max_range(df, column, range_func):\n","#     min_max = df.groupby(by='time_id')[column].agg(['min', 'max']).rename(columns={'min': f'min_{column}', 'max': f'max_{column}'})\n","#     range_df = df.groupby(by='time_id').agg({column: [range_func]}).rename(columns={column: f'range_{column}'})\n","#     range_df.columns = range_df.columns.droplevel(1)\n","#     return min_max, range_df\n","\n","# def process_trade_data(path):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     if st_id == -1:\n","#         return None  # Skip if invalid stock id\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     trade_train_st = pd.read_parquet(path)\n","\n","#     # Calculate min, max, and range for each column\n","#     min_max_price, range_price = calculate_min_max_range(trade_train_st, price, my_range_price)\n","#     min_max_size, range_size = calculate_min_max_range(trade_train_st, size, my_range_price)\n","#     min_max_order_count, range_order_count = calculate_min_max_range(trade_train_st, order_count, my_range_price)\n","\n","#     # Reindex and concatenate\n","#     result = {\n","#         'st_min_price': min_max_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_max_price': min_max_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_min_size': min_max_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_max_size': min_max_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_min_order_count': min_max_order_count.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_max_order_count': min_max_order_count.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_price': range_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_size': range_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_range_order_count': range_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","#     }\n","\n","#     return result\n","\n","# # Initialize dictionaries\n","# trade_price_size_order_count_min_max_range = {\n","#     'st_min_price': pd.DataFrame(),\n","#     'st_max_price': pd.DataFrame(),\n","#     'st_min_size': pd.DataFrame(),\n","#     'st_max_size': pd.DataFrame(),\n","#     'st_min_order_count': pd.DataFrame(),\n","#     'st_max_order_count': pd.DataFrame(),\n","#     'st_range_price': pd.DataFrame(),\n","#     'st_range_size': pd.DataFrame(),\n","#     'st_range_order_count': pd.DataFrame()\n","# }\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_trade_data)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             trade_price_size_order_count_min_max_range[key] = pd.concat([trade_price_size_order_count_min_max_range[key], value], axis=0)\n","\n","# ######## Saving File\n","# # This trade_price_size_order_count_min_max_range is saved below\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlJPCqysSbE_"},"outputs":[],"source":["\n","# subset_paths = train_trade_paths\n","# sum_abs_dif_price_n_target_vol_corr_coef = {}\n","# sum_abs_dif_size_n_target_vol_corr_coef = {}\n","# sum_abs_dif_order_count_n_target_vol_corr_coef = {}\n","\n","# trade_price_size_order_count_sad = {}\n","\n","# trade_price_size_order_count_sad['st_sad_price'] = pd.DataFrame()\n","# trade_price_size_order_count_sad['st_sad_size'] = pd.DataFrame()\n","# trade_price_size_order_count_sad['st_sad_order_count'] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         trade_train_st = pd.read_parquet(path)\n","\n","#         st_sad_price = trade_train_st.groupby(by='time_id').agg({price:[my_sum_abs_diff]}).rename(columns={price:'sad_price'})\n","#         st_sad_price.columns = st_sad_price.columns.droplevel(1)\n","#         common_time_id_sad_p = np.intersect1d(st_sad_price.index.values, target_st['time_id'].values)\n","#         sad_price_n_target_vol_df = pd.DataFrame({'sad_price':st_sad_price.loc[common_time_id_sad_p,\"sad_price\"].values , 'target_vol':target_st.loc[common_time_id_sad_p,\"target\"].values})\n","#         corr_coef_sad_price = plot_scatter_n_correlation(sad_price_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_price_n_target_vol_corr_coef[st_id] = corr_coef_sad_price\n","#         trade_price_size_order_count_sad['st_sad_price'] = pd.concat([ trade_price_size_order_count_sad['st_sad_price'], st_sad_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_size = trade_train_st.groupby(by='time_id').agg({size:[my_sum_abs_diff]}).rename(columns={size:'sad_size'})\n","#         st_sad_size.columns = st_sad_size.columns.droplevel(1)\n","#         common_time_id_sad_s = np.intersect1d(st_sad_size.index.values, target_st['time_id'].values)\n","#         sad_size_n_target_vol_df = pd.DataFrame({'sad_size':st_sad_size.loc[common_time_id_sad_s,\"sad_size\"].values , 'target_vol':target_st.loc[common_time_id_sad_s,\"target\"].values})\n","#         corr_coef_sad_size = plot_scatter_n_correlation(sad_size_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_size_n_target_vol_corr_coef[st_id] = corr_coef_sad_size\n","#         trade_price_size_order_count_sad['st_sad_size'] = pd.concat([ trade_price_size_order_count_sad['st_sad_size'], st_sad_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_order_count = trade_train_st.groupby(by='time_id').agg({order_count:[my_sum_abs_diff]}).rename(columns={order_count:'sad_order_count'})\n","#         st_sad_order_count.columns = st_sad_order_count.columns.droplevel(1)\n","#         common_time_id_sad_oc = np.intersect1d(st_sad_order_count.index.values, target_st['time_id'].values)\n","#         sad_order_count_n_target_vol_df = pd.DataFrame({'sad_order_count':st_sad_order_count.loc[common_time_id_sad_oc,\"sad_order_count\"].values , 'target_vol':target_st.loc[common_time_id_sad_oc,\"target\"].values})\n","#         corr_coef_sad_order_count = plot_scatter_n_correlation(sad_order_count_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_order_count_n_target_vol_corr_coef[st_id] = corr_coef_sad_order_count\n","#         trade_price_size_order_count_sad['st_sad_order_count'] = pd.concat([ trade_price_size_order_count_sad['st_sad_order_count'], st_sad_order_count.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ######## saving file\n","# # This trade_price_size_order_count_sad is saved below\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Qxu0C_SxSK4"},"outputs":[],"source":["# import pandas as pd\n","# import numpy as np\n","# from joblib import Parallel, delayed\n","# from numba import njit\n","\n","# subset_paths = train_trade_paths\n","\n","# # Define custom sum of absolute differences function\n","\n","# def my_sum_abs_diff(values):\n","#     return np.sum(np.abs(np.diff(values)))\n","\n","# # Define a function to calculate sum of absolute differences\n","# def calculate_sad(df, column, sad_func):\n","#     sad_df = df.groupby(by='time_id').agg({column: [sad_func]}).rename(columns={column: f'sad_{column}'})\n","#     sad_df.columns = sad_df.columns.droplevel(1)\n","#     return sad_df\n","\n","# def process_trade_data(path):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     if st_id == -1:\n","#         return None  # Skip if invalid stock id\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     trade_train_st = pd.read_parquet(path)\n","\n","#     # Calculate sum of absolute differences for each column\n","#     sad_price = calculate_sad(trade_train_st, price, my_sum_abs_diff)\n","#     sad_size = calculate_sad(trade_train_st, size, my_sum_abs_diff)\n","#     sad_order_count = calculate_sad(trade_train_st, order_count, my_sum_abs_diff)\n","\n","#     # Reindex and concatenate\n","#     result = {\n","#         'st_sad_price': sad_price.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_sad_size': sad_size.reindex(target_st['time_id'].values).ffill().bfill(),\n","#         'st_sad_order_count': sad_order_count.reindex(target_st['time_id'].values).ffill().bfill()\n","#     }\n","\n","#     return result\n","\n","# # Initialize dictionaries\n","# trade_price_size_order_count_sad = {\n","#     'st_sad_price': pd.DataFrame(),\n","#     'st_sad_size': pd.DataFrame(),\n","#     'st_sad_order_count': pd.DataFrame()\n","# }\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_trade_data)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             trade_price_size_order_count_sad[key] = pd.concat([trade_price_size_order_count_sad[key], value], axis=0)\n","\n","# ######## Saving File\n","# # This trade_price_size_order_count_sad is saved below\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-biSfvuO1HMG"},"outputs":[],"source":["\n","# ## 7g) - 7j) Check if the correlation of any pair of bidsize1,bid_price1,asksize1,ask_price1 is correlated with target realized volatitlity for all the time_ids?\n","\n","# subset_paths = train_trade_paths\n","# price_size_corr_n_target_vol_corr_coef = {}\n","# price_order_count_n_target_vol_corr_coef = {}\n","# size_order_count_n_target_vol_corr_coef = {}\n","\n","# trade_price_size_order_count_corr = {}\n","\n","# #trade_price_size_order_count_corr['st_price_size_corr'] = pd.DataFrame()\n","# #trade_price_size_order_count_corr['st_price_order_count_corr'] = pd.DataFrame()\n","# trade_price_size_order_count_corr['st_size_order_count_corr'] = pd.DataFrame()\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         trade_train_st = pd.read_parquet(path)\n","\n","#         st_price_size_corr = trade_train_st.groupby('time_id')[[price, size]].corr().iloc[0::2, -1]\n","#         st_price_size_corr.index = st_price_size_corr.index.droplevel(1)\n","#         common_time_id_price_size = np.intersect1d(st_price_size_corr.index.values, target_st['time_id'].values)\n","#         price_size_corr_n_target_vol_df = pd.DataFrame({'price_size_corr':st_price_size_corr.loc[common_time_id_price_size].values , 'target_vol':target_st.loc[common_time_id_price_size,\"target\"].values})\n","#         corr_coef_price_size = plot_scatter_n_correlation(price_size_corr_n_target_vol_df,st_id,method=corr_method)\n","#         price_size_corr_n_target_vol_corr_coef[st_id] = corr_coef_price_size\n","#         #trade_price_size_order_count_corr['st_price_size_corr'] = pd.concat([ trade_price_size_order_count_corr['st_price_size_corr'], st_price_size_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_price_order_count_corr = trade_train_st.groupby('time_id')[[price, order_count]].corr().iloc[0::2, -1]\n","#         st_price_order_count_corr.index = st_price_order_count_corr.index.droplevel(1)\n","#         common_time_id_price_order_count = np.intersect1d(st_price_order_count_corr.index.values, target_st['time_id'].values)\n","#         price_order_count_corr_n_target_vol_df = pd.DataFrame({'price_order_count_corr':st_price_order_count_corr.loc[common_time_id_price_order_count].values , 'target_vol':target_st.loc[common_time_id_price_order_count,\"target\"].values})\n","#         corr_coef_price_order_count = plot_scatter_n_correlation(price_order_count_corr_n_target_vol_df,st_id,method=corr_method)\n","#         price_order_count_n_target_vol_corr_coef[st_id] = corr_coef_price_order_count\n","#         #trade_price_size_order_count_corr['st_price_order_count_corr'] = pd.concat([ trade_price_size_order_count_corr['st_price_order_count_corr'], st_price_order_count_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_size_order_count_corr = trade_train_st.groupby('time_id')[[size, order_count]].corr().iloc[0::2, -1]\n","#         st_size_order_count_corr.index = st_size_order_count_corr.index.droplevel(1)\n","#         common_time_id_size_order_count = np.intersect1d(st_size_order_count_corr.index.values, target_st['time_id'].values)\n","#         size_order_count_corr_n_target_vol_df = pd.DataFrame({'size_order_count_corr':st_size_order_count_corr.loc[common_time_id_size_order_count].values , 'target_vol':target_st.loc[common_time_id_size_order_count,\"target\"].values})\n","#         corr_coef_size_order_count = plot_scatter_n_correlation(size_order_count_corr_n_target_vol_df,st_id,method=corr_method)\n","#         size_order_count_n_target_vol_corr_coef[st_id] = corr_coef_size_order_count\n","#         trade_price_size_order_count_corr['st_size_order_count_corr'] = pd.concat([ trade_price_size_order_count_corr['st_size_order_count_corr'], st_size_order_count_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","# ######## Saving FILE\n","# # The trade_price_size_order_count_corr is saved below\n","\n","\n","\n","\n","\n","# \"\"\"\n","# old features end`\n","# \"\"\"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mh5KgyqMxoGy"},"outputs":[],"source":["# subset_paths = train_trade_paths\n","\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# import numpy as np\n","\n","# # Define a function to calculate correlation for given columns\n","# def calculate_corr(df, col1, col2):\n","#     corr_df = df.groupby('time_id')[[col1, col2]].corr().iloc[0::2, -1]\n","#     corr_df.index = corr_df.index.droplevel(1)\n","#     return corr_df\n","\n","# def process_trade_data(path):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     if st_id == -1:\n","#         return None  # Skip if invalid stock id\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = [target_st[\"time_id\"]]\n","#     trade_train_st = pd.read_parquet(path)\n","\n","#     # Calculate correlations\n","#     size_order_count_corr = calculate_corr(trade_train_st, size, order_count)\n","\n","#     # Reindex and concatenate\n","#     result = {\n","#         'st_size_order_count_corr': size_order_count_corr.reindex(target_st['time_id'].values).ffill().bfill()\n","#     }\n","\n","#     return result\n","\n","# # Initialize dictionary\n","# trade_price_size_order_count_corr = {\n","#     'st_size_order_count_corr': pd.DataFrame()\n","# }\n","\n","# # Parallel processing using joblib\n","# results = Parallel(n_jobs=-1)(delayed(process_trade_data)(path) for path in subset_paths)\n","\n","# # Combine results\n","# for result in results:\n","#     if result:\n","#         for key, value in result.items():\n","#             trade_price_size_order_count_corr[key] = pd.concat([trade_price_size_order_count_corr[key], value], axis=0)\n","\n","# ######## Saving File\n","# # This trade_price_size_order_count_corr is saved below\n","\n","\n","\n","\n","\n","\n","# ######## Saving FILE\n","# # The trade_price_size_order_count_corr is saved below\n","\n","\n","\n","\n","# \"\"\"\n","# old features end`\n","# \"\"\"\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xxeHwmpqKm-P"},"source":["## File Name: target_eda_within_stocks.ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WU27ga8sKsJD"},"outputs":[],"source":["#### All these features are covered in liquidity_features_linux.ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cefnFoXlLUvd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"q1rYjeLENrKz"},"source":["## File Name: target_eda_across_stocks.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iwar6IDRNv39"},"outputs":[],"source":["#### All these features are covered in liquidity_features_linux.ipynb"]},{"cell_type":"markdown","metadata":{"id":"NM0QF-_adMRZ"},"source":["## File Name: liquidity_features_linux.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yUZeNgFxdLIi"},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np # linear algebra\n","import glob\n","import os\n","import gc\n","import pickle\n","\n","from joblib import Parallel, delayed\n","\n","from sklearn import model_selection\n","from sklearn.metrics import r2_score\n","\n","\n","%matplotlib inline\n","# %matplotlib notebook\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import cm\n","#%matplotlib inline\n","import plotly.express as px\n","from IPython.display import display\n","import seaborn as sns\n","import plotly.subplots as sub_plots\n","import plotly.graph_objects as go\n","\n","\n","import statsmodels.api as sm\n","\n","from scipy.stats import binned_statistic\n","from scipy.stats import kurtosis, skew\n","\n","from numba import jit\n","\n","\n","#data_dir = '/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data'\n","data_dir = '/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/'\n","\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","import scipy as sp\n","from sklearn.metrics import silhouette_samples,silhouette_score\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","\n","\n","\n","import numba as nb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhP0P__adWh_"},"outputs":[],"source":["\n","# def find_equilibrium_price(book_data, lvl, iterations=22):\n","#     loga2 = np.array(book_data['log_ask_price2'])\n","#     loga1 = np.array(book_data['log_ask_price1'])\n","#     logb1 = np.array(book_data['log_bid_price1'])\n","#     logb2 = np.array(book_data['log_bid_price2'])\n","\n","#     asize2 = np.array(book_data['ask_size2'])\n","#     asize1 = np.array(book_data['ask_size1'])\n","#     bsize1 = np.array(book_data['bid_size1'])\n","#     bsize2 = np.array(book_data['bid_size2'])\n","\n","#     ub = loga1\n","#     lb = logb1\n","\n","#     s = (-1)**lvl\n","#     for iter in range(iterations):\n","#         mid_price = (ub + lb)/2.0\n","#         inv_diff_a2 = 1.0/( 1000*( mid_price - loga2 ) )\n","#         inv_diff_a1 = 1.0/( 1000*( mid_price - loga1 ) )\n","#         inv_diff_b1 = 1.0/( 1000*( mid_price - logb1 ) ) # negative\n","#         inv_diff_b2 = 1.0/( 1000*( mid_price - logb2 ) ) # negative\n","\n","#         f  = -(   ( bsize2*inv_diff_b2**(lvl+1) + bsize1*inv_diff_b1**(lvl+1) )\n","#               + s*( asize1*inv_diff_a1**(lvl+1) + asize2*inv_diff_a2**(lvl+1) ) )\n","\n","#         # when lvl = even, f is positive when buy side missing volume is larger than sell side missing volume\n","#         # when lvl = even, f is negative when sell side missing volume is larger than buy side missing volume\n","#         # when lvl = odd, f is positive when sell side missing volume is larger than buy side missing volume and vice versa\n","\n","#         dub = - (ub-lb)/2.0*(f>=0)\n","#         dlb =   (ub-lb)/2.0*(f< 0)\n","\n","#         # when f is positive, mid price moves towards buy side (bid_price) by reducing the upper bound\n","#         # when f is negative, mid price moves towards sell side (ask_price) by increasing the lower bound\n","#         ub = ub + dub\n","#         lb = lb + dlb\n","\n","#     equilibrium_price = (ub + lb)/2.0\n","\n","#     return equilibrium_price\n","\n","\n","\n","# def diff(list_stock_prices):\n","#     return list_stock_prices.diff()\n","\n","\n","\n","# @jit()\n","# def bucketized_summed_data(seconds_arr, time_id, data, buk_width, n_buks, time_ids_size):\n","#     z = np.zeros( (time_ids_size,n_buks) ) # 30 buckets for 600 seconds (10 minutes)\n","\n","#     t_id  = 0\n","#     for s in range(seconds_arr.shape[0]): # seconds.shape[0] is total size of the seconds column i.e. total rows in seconds column\n","\n","#         if time_id[s] != time_id[max(s-1,0)]:\n","#             t_id = t_id + 1\n","\n","#         z[t_id, int(seconds_arr[s]//buk_width)] += data[s]\n","\n","#     return z\n","\n","\n","# @jit(nopython=True)\n","# def end_bucket(buk_width, buk_sum:float, buk_weight:float, last_val, last_weight, last_time)->float:\n","#     dt = buk_width - last_time%buk_width\n","\n","#     buk_weight += 1.0*last_weight*dt\n","#     buk_sum    += 1.0*last_weight*dt*last_val\n","\n","#     return float(buk_sum/(buk_weight + 1e-8))\n","\n","\n","\n","# @jit()\n","# def bucketized_time_weighted_avg_data(seconds_arr, time_id_arr, data,weights, buk_width, n_buks, time_ids_size):\n","\n","#     z = np.zeros( (time_ids_size,n_buks) )\n","\n","#     prev_time   = 0\n","#     prev_weight = 0.0\n","#     prev_val    = 0.0\n","\n","#     buk_sum = 0.0\n","#     buk_weight = 0.0\n","\n","#     t_id  = 0  # time id\n","#     buk = 0  # bucket id\n","#     for idx in range(seconds_arr.shape[0]): # seconds.shape[0] is total size of the seconds column i.e. total rows in seconds column\n","\n","#         if time_id_arr[idx] != time_id_arr[max(idx-1,0)]: # transition to new time id\n","#             z[t_id, buk] = float(end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time))\n","#             buk += 1\n","\n","#             while buk < z.shape[1]:\n","#                 z[t_id, buk] = prev_val\n","#                 buk += 1\n","#             t_id += 1\n","#             buk = 0\n","\n","#             prev_time  = 0\n","#             buk_sum    = 0.0\n","#             buk_weight = 0.0\n","\n","#         if int(seconds_arr[idx]//buk_width) != int(prev_time//buk_width): # transition to new bucket\n","\n","#             z[t_id, buk] = float(end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time)) # end the previous bucket\n","#             buk += 1 # move to next bucket\n","\n","#             while buk < seconds_arr[idx]//buk_width:\n","#                 z[t_id, buk] = prev_val\n","#                 buk += 1\n","\n","#             prev_time  = buk_width*(seconds_arr[idx]//buk_width)\n","#             buk_sum    = 0.0\n","#             buk_weight = 0.0\n","\n","#         buk_sum    += prev_val*prev_weight*(seconds_arr[idx] - prev_time)  # in the same bucket\n","#         buk_weight +=          prev_weight*(seconds_arr[idx] - prev_time)  # in the same bucket\n","\n","#         prev_time   = seconds_arr[idx] # in the same bucket\n","#         prev_val    = data[idx] # in the same bucket\n","#         prev_weight = weights[idx] # in the same bucket\n","\n","#     z[t_id, buk] = end_bucket(buk_width, buk_sum, buk_weight, prev_val, prev_weight, prev_time)\n","\n","#     for buk in range(buk+1, z.shape[1]): # all buckets of the last time id\n","#         z[t_id, buk] = prev_val\n","\n","#     return z\n","\n","\n","\n","# def create_stock_data(st_id, dset):\n","\n","#     cols = ['st_id', 'time_id', 'seconds_in_bucket']\n","\n","#     ############################## BOOK DATA ##########################################\n","\n","#     book_data = pd.read_parquet(os.path.join(data_dir, 'book_{}.parquet/stock_id={}/'.format(dset, st_id)))\n","\n","#     book_data['st_id'] = st_id\n","\n","#     columns = cols + [col for col in book_data.columns if col not in cols]\n","#     book_data = book_data[columns]\n","#     # columns = 'st_id', 'time_id', 'seconds_in_bucket', 'bid_price1', 'ask_price1', 'bid_price2', 'ask_price2', 'bid_size1', 'ask_size1', 'bid_size2', 'ask_size2'\n","\n","#     # volume\n","#     book_data['ask_volume1'] = book_data['ask_price1']*book_data['ask_size1']\n","#     book_data['ask_volume2'] = book_data['ask_price2']*book_data['ask_size2']\n","#     book_data['bid_volume1'] = book_data['bid_price1']*book_data['bid_size1']\n","#     book_data['bid_volume2'] = book_data['bid_price2']*book_data['bid_size2']\n","\n","#     # becomes zero centered, reduces skew and kurtosis slightly bringing it slightly closer to normal for right skewed data, It is increases/worsens\n","#     # skew and kurtosis for left-skewed data\n","#     # correct way is to use box-cox transformation, variance stabilization\n","#     # Most stocks are right skewed only a few are left skewed?\n","#     book_data['log_ask_price1'] = np.log( book_data['ask_price1'] )\n","#     book_data['log_ask_price2'] = np.log( book_data['ask_price2'] )\n","#     book_data['log_bid_price1'] = np.log( book_data['bid_price1'] )\n","#     book_data['log_bid_price2'] = np.log( book_data['bid_price2'] )\n","\n","#     # redefining WAP using log prices\n","#     book_data['wap1_log_price'] = ( book_data['log_bid_price1'] * book_data['ask_size1'] + book_data['log_ask_price1'] * book_data['bid_size1'] ) / (book_data['bid_size1'] + book_data['ask_size1'])\n","#     book_data['wap2_log_price'] = ( book_data['log_bid_price2'] * book_data['ask_size2'] + book_data['log_ask_price2'] * book_data['bid_size2'] ) / (book_data['bid_size2'] + book_data['ask_size2'])\n","\n","#     # Find equilibrium price at which trades are likely to happen\n","#     # This price minimizes the missing total volume from buy and sell side\n","#     book_data['wap_eqi_price0'] = find_equilibrium_price( book_data, lvl=0)\n","#     book_data['wap_eqi_price1'] = find_equilibrium_price( book_data, lvl=1)\n","#     book_data['wap_eqi_price2'] = find_equilibrium_price( book_data, lvl=2)\n","#     # book_data['wap_eqi_price3'] = find_equilibrium_price( book_data, lvl=3)\n","#     # book_data['wap_eqi_price4'] = find_equilibrium_price( book_data, lvl=4)\n","\n","\n","#     # equilibrium price has converged closer to\n","#     book_data['liquidity0'] = (\n","#                   book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_bid_price1']) )\n","#                 + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_bid_price2']) )\n","#                 - book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_ask_price1']) )\n","#                 - book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price0'] - book_data['log_ask_price2']) )\n","#     )\n","\n","#     # liquidity 0 and liquidity 1 are negatively correlated with each other, if one has prices moving towards buy side, the other has price moving towards sell side\n","#     book_data['liquidity1'] = (\n","#                   book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price1']) )\n","#                 + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price2']) )\n","#                 - book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price1']) )\n","#                 - book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price2']) )\n","#     )\n","\n","#     book_data['liquidity2'] = (\n","#                   book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_bid_price1']) )**2\n","#                 + book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_bid_price2']) )**2\n","#                 + book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_ask_price1']) )**2\n","#                 + book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price2'] - book_data['log_ask_price2']) )**2\n","#     )\n","\n","#     book_data['spread']     = book_data['log_ask_price1'] - book_data['log_bid_price1']\n","#     book_data['inv_spread'] = (book_data['log_ask_price1'] - book_data['log_bid_price1'])**-2 # inverse of spread has the effect of amplifying low values and diminishing high values\n","#     book_data['log_spread'] = book_data['spread'].apply(np.log) # log of spread has the effect of amplifying low values and diminishing high values. It can normalize right skewed data\n","#     book_data['log_spread2'] = np.log(book_data['log_ask_price2'] - book_data['log_bid_price2'])\n","\n","#     book_data['book_size1'] = book_data['ask_volume1'] + book_data['bid_volume1']\n","#     book_data['book_size'] = book_data['ask_volume1'] + book_data['bid_volume1'] + book_data['ask_volume2'] + book_data['bid_volume2']\n","\n","#     # difference betweeen ask's level 1 and level 2 liquidity\n","#     # posiitive means level 2 ask liquidity is higher than level 1 ask liquidity\n","#     book_data['ask_liq1_diff'] = (\n","#                   book_data['ask_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price1']) )**1\n","#                -  book_data['ask_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_ask_price2']) )**1\n","#     )\n","\n","#     # difference betweeen bid's level 1 and level 2 liquidity\n","#     # posiitive means level 1 bid liquidity is higher than level 2 bid liquidity\n","#     book_data['bid_liq1_diff'] = (\n","#                   book_data['bid_volume1']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price1']) )**1\n","#                -  book_data['bid_volume2']/( 1000*(book_data['wap_eqi_price1'] - book_data['log_bid_price2']) )**1\n","#     )\n","\n","#     # simple returns on prices wap1_log_price,wap2_log_price,wap_eqi_price0,wap_eqi_price1\n","#     book_data['wap1_log_price_ret' ] = book_data.groupby(by = ['time_id'])['wap1_log_price'].apply(diff).fillna(0).values\n","#     book_data['wap2_log_price_ret' ] = book_data.groupby(by = ['time_id'])['wap2_log_price'].apply(diff).fillna(0).values\n","#     book_data['wap_eqi_price0_ret'] = book_data.groupby(by = ['time_id'])['wap_eqi_price0'].apply(diff).fillna(0).values\n","#     book_data['wap_eqi_price1_ret'] = book_data.groupby(by = ['time_id'])['wap_eqi_price1'].apply(diff).fillna(0).values\n","\n","#     # this indicates the changes in level 2 wap when level 1 wap does NOT change\n","#     # This happens when all orders in level 1 are filled and new orders are placed in level 2\n","#     # indication of liquidity as prices in level 2 are moving towards level 1\n","#     # Aggressive Market Orders, Imbalance in Market Depth, Execution of Large Orders, Liquidity Changes:\n","#     book_data['wap2_log_price_ret_changes_n_wap1_log_price_ret_constant'] = book_data['wap2_log_price_ret' ]*(book_data['wap1_log_price_ret' ]==0)\n","\n","#     # variance stabilization of right skewed data.\n","#     book_data['log_liquidity1'] = np.log(book_data['liquidity1'])\n","#     book_data['log_liquidity2'] = np.log(book_data['liquidity2'])\n","\n","#     # simple returns on liquidity / first order changes in liquidity\n","#     book_data['log_liquidity1_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity1'].apply(diff).fillna(0).values\n","#     book_data['log_liquidity2_ret'] = book_data.groupby(by = ['time_id'])['log_liquidity2'].apply(diff).fillna(0).values\n","#     # simple returns on log_spread / first order changes in log_spread\n","#     book_data['log_spread_ret'] = book_data.groupby(by = ['time_id'])['log_spread'].apply(diff).fillna(0).values\n","\n","#     # wap1 price returns when liquidity1 is positive/increases and negative/decreases\n","#     book_data['wap1_log_price_ret_pos_log_liq_ret'] = (book_data['log_liquidity1_ret']>0)*book_data['wap1_log_price_ret']\n","#     book_data['wap1_log_price_ret_neg_log_liq_ret'] = (book_data['log_liquidity1_ret']<0)*book_data['wap1_log_price_ret']\n","\n","\n","#     ids = np.array(book_data[['st_id', 'time_id']]) # single stock and all time_ids and seconda_in_bucket\n","#     ids = np.unique(ids, axis=0)\n","#     book_n_trade_data = {}\n","#     book_n_trade_data['time_id'] = ids[:,1:2]\n","\n","#     # bucketized data for book data\n","#     # Amount of wap1 price movements in a time bucket of 30 seconds, i.e. wap1 returns volatitlity in bucket\n","#     book_n_trade_data['wap1_log_price_ret_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.array(book_data['wap1_log_price_ret']),\n","#                                     20, 30, ids.shape[0])\n","\n","#     # Amount of absolute wap1 price movements in a time bucket of 30 seconds, i.e. ahsolute wap1 returns volatitlity in bucket\n","#     book_n_trade_data['wap1_log_price_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.abs(np.array(book_data['wap1_log_price_ret'])),\n","#                                     20, 30, ids.shape[0])\n","\n","#     # Amount of absolute wap2 price movements in a time bucket of 30 seconds, i.e.  ahsolute wap2 returns volatitlity in bucket\n","#     book_n_trade_data['wap2_log_price_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.abs(np.array(book_data['wap2_log_price_ret'])),\n","#                                     20, 30, ids.shape[0])\n","\n","#       # wap1 returns variance/ squared volatitlity in bucket\n","#     book_n_trade_data['wap1_log_price_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.array(book_data['wap1_log_price_ret'])**2,\n","#                                     20, 30, ids.shape[0])\n","#     # wap2 returns variance/ squared volatitlity in bucket\n","#     book_n_trade_data['wap2_log_price_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.array(book_data['wap2_log_price_ret'])**2,\n","#                                     20, 30, ids.shape[0])\n","\n","#     # squared wap1 returns volatitlity in bucket\n","#     book_n_trade_data['wap1_log_price_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.array(book_data['wap1_log_price_ret'])**2,\n","#                                     20, 30, ids.shape[0])**0.5\n","#     # squared wap2 returns volatitlity in bucket\n","#     book_n_trade_data['wap2_log_price_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.array(book_data['wap2_log_price_ret'])**2,\n","#                                     20, 30, ids.shape[0])**0.5\n","\n","#     # squared wap2_log_price_ret_changes_n_wap1_log_price_ret_constant volatitlity in bucket\n","#     book_n_trade_data['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                     np.array(book_data['time_id']),\n","#                                     np.array(book_data['wap2_log_price_ret_changes_n_wap1_log_price_ret_constant'])**2,\n","#                                     20, 30, ids.shape[0])**0.5\n","\n","#     # equilibrium price returns absolute volatitlity in bucket\n","#     book_n_trade_data['wap_eqi_price0_ret_abs_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.abs(np.array(book_data['wap_eqi_price0_ret'])),\n","#                                        20, 30, ids.shape[0])\n","\n","#     # squared equilibrium price returns volatitlity in bucket\n","#     book_n_trade_data['wap_eqi_price0_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['wap_eqi_price0_ret'])**2,\n","#                                        20, 30, ids.shape[0])**0.5\n","\n","#     # volatitlity in wap1_log_price_ret when liquidity1 return is positive. i.e. increases\n","#     book_n_trade_data['wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['wap1_log_price_ret_pos_log_liq_ret'])**2,\n","#                                        20, 30, ids.shape[0])**0.5\n","#     # volatitlity in wap1_log_price_ret when liquidity1 is negative. i.e. decreases\n","#     book_n_trade_data['wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['wap1_log_price_ret_neg_log_liq_ret'])**2,\n","#                                        20, 30, ids.shape[0])**0.5\n","\n","#     # squared wap equilibrium price 1 returns volatitlity in bucket\n","#     book_n_trade_data['wap_eqi_price1_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['wap_eqi_price1_ret'])**2,\n","#                                        20, 30, ids.shape[0])**0.5\n","\n","#     book_n_trade_data['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array((book_data['log_liquidity2_ret']*book_data['wap_eqi_price1_ret'])**2 ),\n","#                                        20, 30, ids.shape[0])\n","\n","#     # squared wap equilibrium price 1 returns volatitlity in bucket amplified (> 1) by positive/increasing liquidity returns (through exponent)\n","#     # and diminished ( < 1) by negative/decreasing liquidity returns (through exponent)\n","#     book_n_trade_data['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(( np.exp(book_data['log_liquidity1_ret'])*book_data['wap_eqi_price1_ret'])**2 ),\n","#                                        20, 30, ids.shape[0])\n","#     # copy of above\n","#     book_n_trade_data['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(( np.exp(book_data['log_liquidity1_ret'])*book_data['wap_eqi_price1_ret'])**2 ),\n","#                                        20, 30, ids.shape[0])\n","#     # variance/ squared volatitliy of wap1 price returns per unit of spread\n","#     # large value indicates volatilty\n","#     book_n_trade_data['wap1_log_price_ret_per_spread_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(( book_data['wap1_log_price_ret']/book_data['spread'])**2 ),\n","#                                        20, 30, ids.shape[0])\n","#     # variance/ squared volatitliy of wap1 price returns per unit of liquidity\n","#     # small value indicates volatilty?\n","#     book_n_trade_data['wap1_log_price_ret_per_liq2_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array( ( book_data['wap1_log_price_ret'])**2/book_data['liquidity2'] ),\n","#                                        20, 30, ids.shape[0])\n","\n","#     # measure of variance/ squared volatility of liquidity1 returns\n","#     book_n_trade_data['log_liquidity1_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['log_liquidity1_ret'])**2,\n","#                                        20, 30, ids.shape[0])\n","\n","#     # measure of variance/ squared volatility of liquidity2 returns\n","#     book_n_trade_data['log_liquidity2_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['log_liquidity2_ret'])**2,\n","#                                        20, 30, ids.shape[0])\n","\n","#     # measure of variance/ squared volatility of log spread returns\n","#     book_n_trade_data['log_spread_ret_sqr_vol_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                        np.array(book_data['log_spread_ret'])**2,\n","#                                        20, 30, ids.shape[0])\n","\n","#     # counting number of data points available in each time bucket\n","#     book_n_trade_data['book_delta_count_buks'] = bucketized_summed_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array(book_data['wap1_log_price_ret']*0+1.0),\n","#                                          20, 30, ids.shape[0])\n","\n","#     # time weighted average of wap1_log_price in each time bucket\n","#     book_n_trade_data['wap1_log_price_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.array(book_data['wap1_log_price']),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0])\n","\n","#     # time weighted average of wap2_log_price in each time bucket\n","#     book_n_trade_data['wap2_log_price_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.array(book_data['wap2_log_price']),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0])\n","\n","#     # time weighted average of wap_eqi_price0 equilibrium price in each time bucket\n","#     book_n_trade_data['wap_eqi_price0_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.array(book_data['wap_eqi_price0']),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0])\n","\n","#     # time weighted average of wap_eqi_price1 equilibrium price in each time bucket\n","#     book_n_trade_data['wap_eqi_price1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.array(book_data['wap_eqi_price1']),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0])\n","\n","\n","#     # filter out the extremely high and low prices of wap1_log_price by amplifying with postiive and negative exponential of wap1_log_price\n","#     # apply time weighted average to the amplified wap1_log_price\n","#     # what may be the physical meaning?\n","#     book_n_trade_data['wap1_log_price_amp_max_wavg'] = np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.exp( 4000*np.array(book_data['wap1_log_price'])),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0]) )/4000\n","#     book_n_trade_data['wap1_log_price_amp_min_wavg'] = -np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.exp(-4000*np.array(book_data['wap1_log_price'])),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0]) )/4000\n","#     # amplification of the difference between max and min\n","#     book_n_trade_data['wavg_wap1_log_price_amp_diff']  = np.exp(book_n_trade_data['wap1_log_price_amp_max_wavg'] - book_n_trade_data['wap1_log_price_amp_min_wavg'])\n","\n","#     # filter out the extremely high and low prices of wap_eqi_price0 by amplifying with postiive and negative exponential of wap_eqi_price0\n","#     # apply time weighted average to the amplified wap_eqi_price0\n","#     book_n_trade_data['wap_eqi_price0_amp_max_wavg'] = np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.exp( 4000*np.array(book_data['wap_eqi_price0'])),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0]) )/4000\n","\n","#     book_n_trade_data['wap_eqi_price0_amp_min_wavg'] = -np.log( bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                                    np.exp(-4000*np.array(book_data['wap_eqi_price0'])),\n","#                                                    np.ones((book_data.shape[0])),\n","#                                                    20, 30, ids.shape[0]) )/4000\n","#     # amplification of the difference between max and min\n","#     book_n_trade_data['wavg_wap_eqi_price0_amp_diff']  = np.exp(book_n_trade_data['wap_eqi_price0_amp_max_wavg'] - book_n_trade_data['wap_eqi_price0_amp_min_wavg'])\n","\n","#     del book_n_trade_data['wap1_log_price_amp_max_wavg'], book_n_trade_data['wap1_log_price_amp_min_wavg']\n","#     del book_n_trade_data['wap_eqi_price0_amp_max_wavg'], book_n_trade_data['wap_eqi_price0_amp_min_wavg']\n","\n","#     book_n_trade_data['liquidity1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['liquidity1'])),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","\n","#     book_n_trade_data['liquidity2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['liquidity2'])),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","\n","#     book_n_trade_data['root_liquidity2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['liquidity2']))**0.5,\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","\n","#     # time weighted average of spread in each time bucket\n","#     book_n_trade_data['spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['spread'])),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","#     # time weighted average of inverse spread in each time bucket\n","#     book_n_trade_data['inv_spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['spread']))**-1,\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","#     # time weighted average of log spread in each time bucket\n","#     book_n_trade_data['log_spread_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.log(np.array((book_data['spread']))),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","#     # time weighted average of log spread 2 in each time bucket\n","#     book_n_trade_data['log_spread2_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['log_spread2'])),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","#     # time weighted average of book size1 in each time bucket\n","#     book_n_trade_data['book_size1_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['book_size1'])),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","#     # time weighted average of book size in each time bucket\n","#     book_n_trade_data['book_size_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                        np.array(book_data['time_id']),\n","#                                          np.array((book_data['book_size'])),\n","#                                          np.ones((book_data.shape[0])),\n","#                                          20, 30, ids.shape[0])\n","\n","\n","#     ############################## TRADE DATA ##########################################\n","\n","#     trade_data =  pd.read_parquet(os.path.join(data_dir,'trade_{}.parquet/stock_id={}'.format( dset, st_id)))\n","#     trade_data['trade_volume'] = trade_data['size']*trade_data['price']\n","\n","#     # bucketized trade volume\n","#     book_n_trade_data['trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","#                                        np.array(trade_data['time_id']),\n","#                                          np.array(trade_data['trade_volume']),\n","#                                          20, 30, ids.shape[0])\n","#     # bucketized root of trade volume\n","#     book_n_trade_data['sqrt_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","#                                        np.array(trade_data['time_id']),\n","#                                          np.array(trade_data['trade_volume']**.5),\n","#                                          20, 30, ids.shape[0])\n","#     # bucketized cube root of trade volume\n","#     book_n_trade_data['cube_root_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","#                                        np.array(trade_data['time_id']),\n","#                                          np.array(trade_data['trade_volume']**(1/3)),\n","#                                          20, 30, ids.shape[0])\n","\n","#     # bucketized square of cube root of trade volume\n","#     book_n_trade_data['trade_volume_p2/3_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","#                                        np.array(trade_data['time_id']),\n","#                                          np.array(trade_data['trade_volume']**(2/3)),\n","#                                          20, 30, ids.shape[0])\n","\n","#     # bucketized quart root of trade volume\n","#     book_n_trade_data['quart_root_trade_volume_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","#                                        np.array(trade_data['time_id']),\n","#                                          np.array(trade_data['trade_volume']**.25),\n","#                                          20, 30, ids.shape[0])\n","\n","#     # count the number of trades in each time bucket\n","#     book_n_trade_data['trade_count_buks'] = bucketized_summed_data(np.array(trade_data['seconds_in_bucket']),\n","#                                        np.array(trade_data['time_id']),\n","#                                          np.array(trade_data['trade_volume']**0),\n","#                                          20, 30, ids.shape[0])\n","\n","\n","#     # trade volume per unit of liquidity1\n","#     book_n_trade_data['trade_volume_per_liquidity1_wavg_buks'] = book_n_trade_data['trade_volume_buks']/book_n_trade_data['liquidity1_wavg']\n","#     book_n_trade_data['trade_volume_per_liquidity2_wavg_buks'] = book_n_trade_data['trade_volume_buks']/book_n_trade_data['liquidity2_wavg']\n","\n","#     # time weighted average of difference betweeen ask's level 1 and level 2 liquidity\n","#     book_n_trade_data['ask_liq1_diff_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                         np.array(book_data['time_id']),\n","#                                         np.array((book_data['ask_liq1_diff'])),\n","#                                         np.ones((book_data.shape[0])),\n","#                                         20, 30, ids.shape[0])\n","#     # time weighted average of difference betweeen bid's level 1 and level 2 liquidity\n","#     book_n_trade_data['bid_liq1_diff_wavg'] = bucketized_time_weighted_avg_data(np.array(book_data['seconds_in_bucket']),\n","#                                         np.array(book_data['time_id']),\n","#                                         np.array((book_data['bid_liq1_diff'])),\n","#                                         np.ones((book_data.shape[0])),\n","#                                         20, 30, ids.shape[0])\n","\n","#     return book_n_trade_data\n","\n","\n","\n","\n","# @jit()\n","# def identify_missing_time_ids(all_time_ids, st_time_ids): # all_time_ids is all unique time_ids from all stocks, st_time_ids is time_ids for particular single stock\n","#     j = 0\n","#     z = 1 == np.zeros(  all_time_ids.shape[0]) # set all unique time_ids to False\n","#     for i in range(st_time_ids.shape[0]):\n","#         while all_time_ids[j] != st_time_ids[i]: # missing time id in the stock\n","#             z[j] = False # set the missing time id index in all unique time ids array z to False\n","#             j = j+1\n","#             if j >= all_time_ids.shape[0]:\n","#                 return z\n","#         z[j] = True\n","#         j = j+1\n","#     return z\n","\n","\n","\n","\n","# def create_dataSet(st_ids,dset):\n","\n","#     st_ids = sorted(st_ids)\n","\n","#     print('st_ids',st_ids)\n","\n","#     # a list contains all stock data each element of list is a dictionary of features for a particular stock\n","#     all_stock_data = Parallel(n_jobs = os.cpu_count() - 5)( delayed(create_stock_data)(st_id, dset) for st_id in st_ids)\n","\n","#     final_data = {}\n","\n","#     # get all unique time ids from all stocks. This is helpful to fill missing time ids.\n","#     t_ids = sum([list(ss['time_id']) for ss in all_stock_data], [] )\n","#     t_ids = list(np.unique(t_ids))\n","\n","#     num_buks = 30\n","#     t_ids_size = len(t_ids)\n","#     st_ids_size = len(st_ids)\n","\n","#     final_data['time_ids' ] = np.array(t_ids)\n","#     final_data['stock_ids'] = np.array(st_ids)\n","\n","\n","#     for key in all_stock_data[0].keys(): # common columns (features) to all stocks\n","#         if key == 'time_id':\n","#             continue\n","\n","#         Z = np.zeros(( t_ids_size, st_ids_size, num_buks))\n","\n","#         for st in range(st_ids_size):\n","#             ss = all_stock_data[st]\n","\n","#             #ts = index_into_set(np.array(time_ids), ss['time_id']).astype(int)\n","\n","#             b = identify_missing_time_ids(np.array(t_ids), ss['time_id']) # all unique time ids from all stocks and time ids of a particular stock are input\n","\n","#             #print(b)\n","#             #print(b.shape)\n","\n","#             Z[ b, st, :] = ss[key] # fill with features for avaialble time ids\n","\n","#             Z[~b, st, :] = np.nanmean(ss[key]) # fill with mean of features for missing time ids\n","\n","#             Z[:,st,:][np.isnan(Z[:,st,:])] = np.nanmean(Z[:,st,:]) # fill with mean of features for missing time ids and any missing bins\n","\n","#             #del ss[key]\n","\n","#         final_data[key] = Z\n","\n","#         gc.collect()\n","\n","\n","#     del all_stock_data\n","#     gc.collect()\n","\n","\n","#     # arbitrarily weighted average of wap1_log_price_ret_abs_vol_buks and wap2_log_price_ret_abs_vol_buks\n","#     final_data['wap1_log_price_ret_vol_buks'] = ( final_data['wap1_log_price_ret_vol_buks']**2 + .25*final_data['wap2_log_price_ret_vol_buks']**2)**0.5\n","\n","#     return final_data\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jhzOU_35gNnm"},"outputs":[],"source":["\n","# train_buckets = create_dataSet(st_ids = list(np.unique(train['stock_id'])), dset = 'train')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43PYWtunfPGm"},"outputs":[],"source":["# ## check for missing and extereme values\n","\n","# print('\\nNumber of missing values in all train_buckets columns\\n')\n","\n","# nan_sum = 0\n","# for k in train_buckets.keys():\n","#     #print(k, np.isnan(train_buckets[k]).sum())\n","#     nan_sum += np.isnan(train_buckets[k]).sum()\n","# print('Total number of missing values: ', nan_sum)\n","\n","# my_mins= []\n","# my_maxs = []\n","# print('\\nMin and Max values\\n')\n","# for k in train_buckets.keys():\n","#     print(k,' Min: ', np.min(train_buckets[k]))\n","#     my_mins.append(np.min(train_buckets[k]))\n","#     print(k,' Max: ', np.max(train_buckets[k]))\n","#     my_maxs.append(np.max(train_buckets[k]))\n","\n","\n","# ######## IDENTIFIED extreme values ########\n","\n","# ##wap1_log_price_ret_per_spread_sqr_vol_buks  Min:  0.0\n","# ##wap1_log_price_ret_per_spread_sqr_vol_buks  Max:  63893.42029636643\n","\n","# ## wavg_wap1_log_price_amp_diff  Min:  0.9999999999351177\n","# ## wavg_wap1_log_price_amp_diff  Max:  inf\n","# ## wavg_wap_eqi_price0_amp_diff  Min:  0.9999999999406891\n","# ## wavg_wap_eqi_price0_amp_diff  Max:  inf\n","# ## liquidity1_wavg  Min:  0.850008628347739\n","# ## liquidity1_wavg  Max:  2997063.2110014684\n","# ## liquidity2_wavg  Min:  0.1854020731394554\n","# ## liquidity2_wavg  Max:  108551400.92072429\n","# ## root_liquidity2_wavg  Min:  0.429666022039096\n","# ## root_liquidity2_wavg  Max:  9794.487019028536\n","# ## spread_wavg  Min:  2.514241494843901e-05\n","# ## spread_wavg  Max:  0.03775830443923237\n","# ## book_size1_wavg  Min:  1.9349169731140137\n","# ## book_size1_wavg  Max:  1058850.6744705746\n","# # book_size_wavg  Min:  3.9555423239957395\n","# # book_size_wavg  Max:  1427941.6492860292\n","# # trade_volume_buks  Min:  0.0\n","# # trade_volume_buks  Max:  740413.5085173845\n","# # sqrt_trade_volume_buks  Min:  0.0\n","# # sqrt_trade_volume_buks  Max:  2764.92093142591\n","# # cube_root_trade_volume_buks  Min:  0.0\n","# # cube_root_trade_volume_buks  Max:  503.86618878563735\n","# # trade_volume_p2/3_buks  Min:  0.0\n","# # trade_volume_p2/3_buks  Max:  15831.80523078871\n","# # ask_liq1_diff_wavg  Min:  -2925519.34853724\n","# # ask_liq1_diff_wavg  Max:  1525556.4992372217\n","# # bid_liq1_diff_wavg  Min:  -1590983.930454508\n","# # bid_liq1_diff_wavg  Max:  2545604.6237271978"]},{"cell_type":"markdown","metadata":{"id":"7JurF8e2fncp"},"source":["###### FEATURE ENGINEERING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFvu3MQKfQKW"},"outputs":[],"source":["# def get_cohesion_features(train_buckets, final_features, ffrom=0):\n","#     wap1_log_price_ret_buks = train_buckets['wap1_log_price_ret_buks'] # shape of (3830,112,30)\n","\n","#     if ffrom > 0:\n","#         buk = f'_:{ffrom}'\n","#     else:\n","#         buk = '_:0'\n","\n","#     # variance along time_id axis, mean along bucket axis and then square root\n","#     # basically standard deviation of wap1_log_price_ret_buks in each stock. This is like (overall) volatility over entire time period for each stock.\n","#     stocks_overall_wap1_log_price_ret_vol = np.mean( np.var(wap1_log_price_ret_buks, 0,keepdims=True), 2, keepdims=True)**0.5 # shape of (1,112,1)\n","\n","#     # normalize the variance of wap1_log_price_ret_buks by overall volatility, assume that mean of wap1_log_price_ret_buks is zero\n","#     wap1_log_price_ret_normalized = wap1_log_price_ret_buks[:,:,ffrom:]/stocks_overall_wap1_log_price_ret_vol # shape of (3830,112,30)\n","\n","#     # variance of wap1_log_price_ret_normalized along stock id axis, mean along bucket axis and then square root, shape of (3830,1,1) and then normalized by wap1_log_price_ret_vol (shape 3830 x 112, 1). dim 1 is broadcasted to dim 112\n","#     # multiply by stocks_overall_wap1_log_price_ret_vol (shape = (1,112,1) ) to get the original variance of wap1_log_price_ret_buks to get final shape of (3830,112,1)\n","#     # multiplication by stocks_overall_wap1_log_price_ret_vol (overall volatility) is the reverse of normalization\n","#     # Volatility across stocks at each time id divided by volatility at each stocks and time id = factor of overall volatility across stocks contributed by each stock in each time id.\n","#     # This is multiplied by overall volatility across time for each stock.\n","#     # It is just a scalar giving importance to amount of variance across all time.\n","#     # ati = across time, ast = across stock,     # shape of (3830,112,1),\n","#     final_features['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol'+ buk] = stocks_overall_wap1_log_price_ret_vol*np.mean(  np.var( wap1_log_price_ret_normalized, 1, keepdims=True), 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","#     # shape of (3830,112,1), market is np.mean(wap1_log_price_ret_normalized, 1, keepdims=True). IT is just mean over all stock ids.\n","#     final_features['wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol'+ buk] = stocks_overall_wap1_log_price_ret_vol*np.mean( (np.mean(wap1_log_price_ret_normalized, 1, keepdims=True) )**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","\n","#     # deviation from market is np.mean(wap1_log_price_ret_normalized, 1, keepdims=True)  minus wap1_log_price_ret_normalized\n","#     final_features['wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol' + buk] = stocks_overall_wap1_log_price_ret_vol*np.mean( (np.mean(wap1_log_price_ret_normalized, 1, keepdims=True) - wap1_log_price_ret_normalized)**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol']\n","\n","\n","\n","\n","\n","\n","# def cluster_agg(x, clusters, agg_fun):\n","#     r = 0*x\n","\n","#     for k in range(np.max(clusters)+1):\n","#         z = agg_fun(x[:,clusters==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","#         r[:,clusters==k,:] = np.repeat(z, repeats=int(np.sum(clusters==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","#     return r\n","\n","\n","\n","\n","# def get_misc_features(train_buckets, final_features):\n","\n","#     trade_volume_buks    = train_buckets['trade_volume_buks']\n","#     wap1_log_price_ret_vol_buks    = train_buckets['wap1_log_price_ret_vol_buks']\n","#     sqrt_trade_volume_buks   = train_buckets['sqrt_trade_volume_buks']\n","#     liquidity2_wavg   = train_buckets['liquidity2_wavg']\n","#     log_spread2_wavg  = train_buckets['log_spread2_wavg']\n","\n","#     # average out along the time ids and buckets axis\n","#     stocks_overall_trade_volume  = np.nanmean( trade_volume_buks, (0,2), keepdims=True) # shape of (1,112,1)\n","#     stocks_overall_sqrt_trade_volume = np.nanmean(sqrt_trade_volume_buks, (0,2), keepdims=True) # shape of (1,112,1)\n","#     stocks_overall_liquidity2 = np.nanmean(liquidity2_wavg, (0,2), keepdims=True) # shape of (1,112,1)\n","#     # average out along the buckets axis\n","#     stocks_overall_wap1_log_price_ret_vol = np.nanmean(wap1_log_price_ret_vol_buks**2, 2, keepdims=True)**.5 # shape of (3830,112,1)\n","\n","#     # (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)  = (vol[:,:, 0:] / s ) / (liq2[:,:, 0:]) / l2), standardized volume divided by standardized liquidity2\n","#     # (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8) , diminish the effect of outliers, or reduce large values\n","#     # np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True) , average over stock_id and buckets axis\n","#     # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8) , destandardize and then take the power of 8 to undo the effect of 1/8\n","#     # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5 , take the square root as it is liquidity of level 2\n","#     # (s/l2*np.nanmean( (vol[:,:, 0:]/liq2[:,:, 0:]*l2/s)**(1/8), (1,2), keepdims=True)**8)**.5/v1 , divide by wap1_log_price_ret_vol to check > 1\n","#     # for greater than wap1_log_price_ret_vol or wap1_log_price_ret_vol < 1 for less than wap1_log_price_ret_vol\n","#     # log detects > 1 or < 1\n","#     final_features['soft_stock_mean_tvpl2_:0'    ] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","#     # using only the last 20 buckets\n","#     final_features['soft_stock_mean_tvpl2_:10'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","#     # using only the last 10 buckets\n","#     final_features['soft_stock_mean_tvpl2_:20'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","#     # same as above but only using the last bucket of liquidity2\n","#     final_features['soft_stock_mean_tvpl2_liqf'       ] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","#     final_features['soft_stock_mean_tvpl2_liqf_volf10'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,10:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","#     final_features['soft_stock_mean_tvpl2_liqf_volf20'] = np.log( (stocks_overall_trade_volume/stocks_overall_liquidity2*np.nanmean( (trade_volume_buks[:,:,20:]/liquidity2_wavg[:,:,-1:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), (1,2), keepdims=True)**8)**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","#     # np.mean(vol1[:,:,25:]**2,2,keepdims=True) ,squared wap1_log_price_ret_vol_buks average for last 5 buckets\n","#     # np.mean(vol1[:,:,:15]**2,2,keepdims=True) , squared wap1_log_price_ret_vol_buks average for first 15 buckets\n","#     # np.nanmedian( np.mean(vol1[:,:,25:]**2,2,keepdims=True) / np.mean(vol1[:,:,:15]**2,2,keepdims=True),1,keepdims=True) , median of ratio  along stock_id axis , shape of (3830,1,1)\n","#     # square root to get back standard deviation / volatility\n","#     # log to detect > 1 or < 1\n","#     final_features['v1proj_25_15'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True) / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","#     # same as above but only for wap1_log_price_ret high correlation stocks\n","#     # final_features['v1proj_25_15_lr1_high_corr_stocks'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,25:]**2,2,keepdims=True)\n","#     #                                                             / np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","#     # same as above but only for wap1_log_price_ret_vol high correlation stocks\n","#     #final_features['v1proj_25_15_vol1_high_corr_stocks'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,25:]**2,2,keepdims=True)\n","#     #                                                             / np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,:15]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","#     # same as above but ratio of average of last 5 buckets to average of all buckets\n","#     #final_features['v1proj_25_lr1_high_corr_stocks']     = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,25:]**2,2,keepdims=True)\n","#     #                                                         / np.mean(wap1_log_price_ret_vol_buks[:,wap1_log_price_ret_high_corr_stocks,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","#     # same as above but ratio of average of last 5 buckets to average of all buckets\n","#     #final_features['v1proj_25_vol1_high_corr_stocks']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,25:]**2,2,keepdims=True)\n","#     #                                                          / np.mean(wap1_log_price_ret_vol_buks[:,log_wap1_log_price_ret_vol_high_corr_stocks,:]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","#     # average of log_spread_ret_sqr_vol_buks over all buckets\n","#     final_features['lsvol'] = np.log( np.nanmean(train_buckets['log_spread_ret_sqr_vol_buks'], 2, keepdims=True))\n","\n","\n","#     # average of log_liquidity1_ret_sqr_vol_buks over all buckets\n","#     final_features['liqvol1'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], 2, keepdims=True))\n","#     # average of log_liquidity2_ret_sqr_vol_buks over all stock ids and buckets\n","#     final_features['liqvol1_smean'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], (1,2), keepdims=True)) # shape of (3830,1,1)\n","\n","#     # average of log_liquidity1_ret_sqr_vol_buks over all buckets for each cluster\n","#     # is grouped by cluster and then median is taken. repeat the median value for each stock in the same cluster. shape of (3830,112,1)\n","#     #### final_features['liqvol1_smean_c3'] = np.log( cluster_agg(np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'], 2, keepdims=True), wap1_log_price_ret_cluster3 ,np.nanmedian))\n","\n","#     # average of log_liquidity2_ret_sqr_vol_buks over all buckets\n","#     final_features['liqvol2'] = np.log( np.nanmean(train_buckets['log_liquidity2_ret_sqr_vol_buks'], 2, keepdims=True))\n","\n","#     # ratio of average log_liquidity1_ret_sqr_vol_buks in last 15 buckets to first 15 buckets\n","#     final_features['liqvol1_15_15'] = np.log( np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'][:,:,15:  ], 2, keepdims=True)\n","#                                             /np.nanmean(train_buckets['log_liquidity1_ret_sqr_vol_buks'][:,:,  :15], 2, keepdims=True))\n","\n","#     # average of trade_count over all buckets\n","#     final_features['trade_count']      = np.log( np.nanmean(train_buckets['trade_count_buks']    , 2, keepdims=True))\n","#     # average of squre root of trade_count over all buckets\n","#     final_features['root_trade_count'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","\n","#     # average of squre root of trade_count over all stock ids and all buckets\n","#     final_features['root_trade_count_smean'] = np.log( np.nanmean(train_buckets['trade_count_buks']**.5, (1,2), keepdims=True))\n","\n","#     # average of squre root of number of data points in a bucket over all buckets\n","#     final_features['root_book_delta_count'] = np.log( np.nanmean(train_buckets['book_delta_count_buks']**.5, 2, keepdims=True))\n","\n","#     # average of square root of trade_count over all buckets for each cluster\n","#     # is grouped by cluster 1 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","#     #### final_features['root_trade_count_smean_c1'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","#     ####                                                                 wap1_log_price_ret_cluster1,np.nanmean))\n","\n","#     # average of square root of trade_count over all buckets for each cluster\n","#     # is grouped by cluster 2 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","#     #### final_features['root_trade_count_smean_c2'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","#     ####                                                                 wap1_log_price_ret_cluster2,np.nanmean))\n","\n","#     # average of square root of trade_count over all buckets for each cluster\n","#     # is grouped by cluster 3 and then mean is taken. repeat the mean value for each stock in the same cluster. shape of (3830,112,1)\n","#     #### final_features['root_trade_count_smean_c3'] = np.log( cluster_agg(np.nanmean(train_buckets['trade_count_buks']**0.5, 2, keepdims=True),\n","#     ####                                                                 wap1_log_price_ret_cluster3,np.nanmean))\n","\n","#     # variance of square root of trade_count over all buckets for each cluster\n","#     final_features['root_trade_count_var'] = np.log( np.nanvar(train_buckets['trade_count_buks']**.5, 2, keepdims=True))\n","\n","#     # ratio of average trade_count in last 15 buckets to first 15 buckets\n","#     final_features['trade_count_15_15']      = np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ], 2, keepdims=True)/\n","#                                                       np.nanmean(train_buckets['trade_count_buks'][:,:,  :15], 2, keepdims=True))\n","\n","#     # ratio of average square root trade_count in last 15 buckets to first 15 buckets\n","#     final_features['root_trade_count_15_15'] =  np.log( np.nanmean(train_buckets['trade_count_buks'][:,:,15:  ]**.5, 2, keepdims=True)/\n","#                                                        np.nanmean(train_buckets['trade_count_buks'][:,:,  :15]**.5, 2, keepdims=True))\n","\n","#     # median of ratio of mean wap1_log_price_ret_vol_buks in last bucket to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_15'] = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,29:  ]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True),1,keepdims=True)**.5)\n","\n","#     # median of ratio of mean wap1_log_price_ret_vol_buks in last 10 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_20']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,20:  ]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","#     # median of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","#     # median of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29']    = np.log( np.nanmedian( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,  :  ]**2,2,keepdims=True),1,keepdims=True)**.5 )\n","\n","#     # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","#     # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","#     # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","#     # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                         / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","#     # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_15_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","#                                                            / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","#     # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_15_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,28:  ]**2,2,keepdims=True)\n","#                                                            / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","#     # 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25_15_q1'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","#                                                            / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.25, 1,keepdims=True)**.5 )\n","\n","#     # 75% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25_15_q3'] = np.log( np.quantile( np.mean(wap1_log_price_ret_vol_buks[:,:,25:  ]**2,2,keepdims=True)\n","#                                                            / np.mean(wap1_log_price_ret_vol_buks[:,:,  :15]**2,2,keepdims=True), 0.75, 1,keepdims=True)**.5 )\n","\n","\n","#     # std of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                                / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n","\n","#     # std of ratio of mean wap1_log_price_ret_vol_buks in last bucket to first 15 buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_15_std'] = np.log( np.nanstd( np.log( np.mean(wap1_log_price_ret_vol_buks[:,:,29:]**2,2,keepdims=True)\n","#                                                                / np.mean(wap1_log_price_ret_vol_buks[:,:,:15]**2,2,keepdims=True)),1,keepdims=True)**.5 )\n","\n","#     # std of ratio of mean wap1_log_price_ret_vol_buks in last 10 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_20_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,20:]**2,2,keepdims=True)\n","#                                                             /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","#     # std of ratio of mean wap1_log_price_ret_vol_buks in last 5 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_25_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","#                                                             /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","#     # std of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_std'] = np.log( np.nanstd( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","#                                                             /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),1,keepdims=True) )\n","\n","#     # difference between 75% and 25% quantile of ratio of mean wap1_log_price_ret_vol_buks in last 2 buckets to all buckets along stock_id axis, shape of (3830,1,1)\n","#     final_features['v1proj_29_q3q1'] = np.log(np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","#                                                             /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True))\n","#                                                     ,0.75, axis=1,keepdims=True)\n","#                                                 -\n","#                                                 np.quantile( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,28:]**2,2,keepdims=True)\n","#                                                             /np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True))\n","#                                                     ,0.25, axis=1,keepdims=True))\n","\n","\n","#     # CHECK IF there are more than 100 stocks\n","#     # Basically, all features calcualted above are repeated and aggregated for each cluster using mean, median and std etc..\n","# #     if wap1_log_price_ret_vol_buks.shape[1]>100:\n","\n","\n","# #         final_features['v1proj_25_c1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                              / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True), wap1_log_price_ret_cluster1, np.median)**0.5 )\n","\n","\n","# #         final_features['v1proj_25_c2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         wap1_log_price_ret_cluster2, np.median)**0.5 )\n","# #         final_features['v1proj_25_c3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         wap1_log_price_ret_cluster3, np.median)**0.5 )\n","# #         final_features['v1proj_25_c4'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                        wap1_log_price_ret_cluster4, np.median)**0.5 )\n","# #         final_features['v1proj_25_c5'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         wap1_log_price_ret_cluster5, np.median)**0.5 )\n","\n","\n","\n","# #         final_features['soft_stock_mean_tvpl2_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","# #         final_features['soft_stock_mean_tvpl2_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","# #         final_features['soft_stock_mean_tvpl2_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 0:]/liquidity2_wavg[:,:, 0:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","# #         final_features['soft_stock_mean_tvpl2_10_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","# #         final_features['soft_stock_mean_tvpl2_10_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","# #         final_features['soft_stock_mean_tvpl2_10_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 10:]/liquidity2_wavg[:,:,10:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","# #         final_features['soft_stock_mean_tvpl2_20_c1'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster1, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","# #         final_features['soft_stock_mean_tvpl2_20_c2'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster2, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","# #         final_features['soft_stock_mean_tvpl2_20_c3'] = np.log( stocks_overall_trade_volume/stocks_overall_liquidity2*cluster_agg(\n","# #             np.nanmean( (trade_volume_buks[:,:, 20:]/liquidity2_wavg[:,:,20:]*stocks_overall_liquidity2/stocks_overall_trade_volume)**(1/8), 2, keepdims=True),\n","# #                 wap1_log_price_ret_cluster3, np.mean)**8**.5/stocks_overall_wap1_log_price_ret_vol)\n","\n","\n","# #         final_features['v1proj_25_c1_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","# #                                                         wap1_log_price_ret_cluster1, np.nanstd)**0.5  )\n","# #         final_features['v1proj_25_c2_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","# #                                                         wap1_log_price_ret_cluster2, np.nanstd)**0.5 )\n","# #         final_features['v1proj_25_c3_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","# #                                                         wap1_log_price_ret_cluster3, np.nanstd)**0.5 )\n","# #         final_features['v1proj_25_c4_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","# #                                                         wap1_log_price_ret_cluster4, np.nanstd)**0.5 )\n","# #         final_features['v1proj_25_c5_std'] = np.log( cluster_agg( np.log(np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True)),\n","# #                                                         wap1_log_price_ret_cluster5, np.nanstd)**0.5 )\n","\n","\n","# #         final_features['v1proj_25_vc1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_wap1_log_price_ret_vol_clusters1, np.median)**0.5  )\n","# #         final_features['v1proj_25_vc2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_wap1_log_price_ret_vol_clusters2, np.median)**0.5 )\n","# #         final_features['v1proj_25_vc3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_wap1_log_price_ret_vol_clusters3, np.median)**0.5 )\n","\n","# #         final_features['v1proj_25_vc4'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_wap1_log_price_ret_vol_clusters4, np.median)**0.5 )\n","\n","\n","\n","\n","# #         final_features['v1proj_25_vvc1'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_quart_trade_volume_clusters1, np.median)**0.5  )\n","# #         final_features['v1proj_25_vvc2'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_quart_trade_volume_clusters2, np.median)**0.5 )\n","# #         final_features['v1proj_25_vvc3'] = np.log( cluster_agg( np.mean(wap1_log_price_ret_vol_buks[:,:,25:]**2,2,keepdims=True)\n","# #                                                        / np.mean(wap1_log_price_ret_vol_buks[:,:,:  ]**2,2,keepdims=True),\n","# #                                                         log_quart_trade_volume_clusters3, np.median)**0.5 )\n","\n","\n","\n","# #         final_features['v1spprojt15f25_c1'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            wap1_log_price_ret_cluster1, np.median) )\n","# #         final_features['v1spprojt15f25_c2'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            wap1_log_price_ret_cluster2, np.median) )\n","# #         final_features['v1spprojt15f25_c3'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            wap1_log_price_ret_cluster3, np.median) )\n","# #         final_features['v1spprojt15f25_c4'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            wap1_log_price_ret_cluster4, np.median) )\n","\n","# #         final_features['v1spprojt15f25_vc1'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            log_wap1_log_price_ret_vol_clusters1, np.median) )\n","# #         final_features['v1spprojt15f25_vc2'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            log_wap1_log_price_ret_vol_clusters2, np.median) )\n","# #         final_features['v1spprojt15f25_vc3'] = np.log( cluster_agg(\n","# #                           - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True)\n","# #                           + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True),\n","# #                            log_wap1_log_price_ret_vol_clusters3, np.median) )\n","\n","\n","#     # ratio of average of all buckets trade_volume to average of all buckets liquidity2, shape of (3830,1,1)\n","#     final_features['tvpl2_rmed2v1']     = np.log( np.median( ( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","#                                                     / np.mean( liquidity2_wavg[:,:, 0:]**.5, 2, keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","#     # ratio of average of all buckets trade_volume to average of last 5 buckets liquidity2, shape of (3830,1,1)\n","#     final_features['tvpl2_rmed2v1lf25'] = np.log( np.median(( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","#                                                     / np.mean( (liquidity2_wavg[:,:, 25:])**.5, (2), keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","#     # ratio of average of all buckets trade_volume to average of last 1 buckets liquidity2, shape of (3830,1,1)\n","#     final_features['tvpl2_rmed2v1lf29'] = np.log( np.median(( np.mean( trade_volume_buks, 2, keepdims=True)**.5\n","#                                                     / np.mean( (liquidity2_wavg[:,:, 29:])**.5, (2), keepdims=True))/stocks_overall_wap1_log_price_ret_vol, 1, keepdims=True))\n","\n","\n","\n","#     # ratio of average of all buckets trade_volume to average of    all buckets liquidity2, shape of (3830,112,1)\n","#     final_features['tvpl2']        = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg)**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","#     # ratio of average of all buckets trade_volume to average of last 20 buckets liquidity2, shape of (3830,112,1)\n","#     final_features['tvpl2_liqf10'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","#     # ratio of average of all buckets trade_volume to average of last 10 buckets liquidity2, shape of (3830,112,1)\n","#     final_features['tvpl2_liqf20'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","#     # ratio of average of all buckets trade_volume to average of last 1 bucket liquidity2, shape of (3830,112,1)\n","#     final_features['tvpl2_liqf29'] = np.log( np.mean( trade_volume_buks, 2, keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","\n","#     # ratio of average of all time ids and buckets trade_volume to average of all bucket liquidity2, shape of (3830, 112, 1)\n","#     final_features['tvpl2_smean_vol'       ] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:, 0:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","#     # ratio of average of all time ids and buckets trade_volume to average of last 20 bucket liquidity2, shape of (3830, 112, 1)\n","#     final_features['tvpl2_smean_vol_liqf10'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,10:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","#     # ratio of average of all time ids and buckets trade_volume to average of last 10 bucket liquidity2, shape of (3830, 112, 1)\n","#     final_features['tvpl2_smean_vol_liqf20'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,20:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","#     # ratio of average of all time ids and buckets trade_volume to average of last 1 bucket liquidity2, shape of (3830, 112, 1)\n","#     final_features['tvpl2_smean_vol_liqf29'] = np.log( np.mean( trade_volume_buks, (0,2), keepdims=True)**.5/np.mean( (liquidity2_wavg[:,:,29:])**.5, (2), keepdims=True)/stocks_overall_wap1_log_price_ret_vol)\n","\n","\n","#     #\n","#     final_features['v1liq2projt5'] = np.log( ( np.mean( liquidity2_wavg[:,:,  : 5]**(1/8), 2, keepdims=True)**8\n","#                                             / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","#     #\n","#     final_features['v1liq2projt10'] = np.log( ( np.mean( liquidity2_wavg[:,:,  :10]**(1/8), 2, keepdims=True)**8\n","#                                              / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","\n","#     final_features['v1liq2projt20'] = np.log( ( np.mean( liquidity2_wavg[:,:,  :20]**(1/8), 2, keepdims=True)**8\n","#                                              / np.mean( liquidity2_wavg[:,:,28:  ]       , 2, keepdims=True) )**(1/2) )\n","\n","#     # ratio of average of first 10 buckets liquidity2_wavg to average of last 1 buckets liquidity2, shape of (3830,112,1)\n","#     final_features['liqt10rf29'] = np.log( np.mean( liquidity2_wavg[:,:,:10]**.5, (2), keepdims=True)**2 / liquidity2_wavg[:,:,29:] )\n","#     # ratio of average of first 20 buckets liquidity2_wavg to average of last 1 buckets liquidity2, shape of (3830,112,1)\n","#     final_features['liqt20rf29'] = np.log( np.mean( liquidity2_wavg[:,:,:20]**.5, (2), keepdims=True)**2 / liquidity2_wavg[:,:,29:] )\n","\n","\n","\n","#     # median along stock id axis, how liquidity2_wavg changes/ratio in first 10 bins to last 5 bins. shape of (3830,1,1)\n","#     final_features['v1liq2sprojt10f25'] = np.log( np.median(\n","#                           np.mean(liquidity2_wavg[:,:,:10]**.125, (2),keepdims=True)**8/\n","#                           np.mean(liquidity2_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","#                         , 1, keepdims=True)**(1/2) )\n","\n","#     final_features['v1liq2sprojt5f25'] = np.log( np.median(\n","#                           np.mean(liquidity2_wavg[:,:,  : 5]**.125, (2),keepdims=True)**8/\n","#                           np.mean(liquidity2_wavg[:,:,25:  ]**.125, (2),keepdims=True)**8\n","#                         , 1, keepdims=True)**(1/2) )\n","\n","#     # median along stock id axis, of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt10f29'] = np.median( - np.mean(log_spread2_wavg[:,:,  :  ], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:  ], (2),keepdims=True) , 1, keepdims=True)\n","#     # median along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt15f25'] = np.median( - np.mean(log_spread2_wavg[:,:,  :15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:  ], (2),keepdims=True) , 1, keepdims=True)\n","#     # median along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt15f29'] = np.median( - np.mean(log_spread2_wavg[:,:,  :15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:  ], (2),keepdims=True), 1, keepdims=True)\n","\n","#     # 25% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt15f29_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","#     # 75% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt15f29_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","#     # 25% quantile along stock id axis, of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt15f25_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","#     # 75% quantile along stock id axis,of ratio of mean of first 15 buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojt15f25_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:15], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","#     # 25% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojtf29_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","#     # 75% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 1 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojtf29_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,29:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","#     # 25% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojtf25_q1'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.25, 1, keepdims=True)\n","#     # 75% quantile along stock id axis,of ratio of mean of all buckets log_spread2_wavg to mean of last 5 bucket log_spread2_wavg , shape of (3830,1,1)\n","#     final_features['v1spprojtf25_q3'] = np.quantile( - np.mean(log_spread2_wavg[:,:,:], (2),keepdims=True) + np.mean(log_spread2_wavg[:,:,25:], (2),keepdims=True) ,0.75, 1, keepdims=True)\n","\n","\n","#     return\n","\n","\n","\n","\n","\n","# def get_simple_features(binned_features, final_features, name, ffrom=0):\n","#     wap1_log_price_ret_vol = binned_features[name+'_buks']\n","\n","#     suffix = f'_from_{ffrom}'\n","\n","#     # average of wap1_log_price_ret_buks in all buckets of a time id. Then average over all time ids. Then take square root, shape of (1,112,1)\n","#     stocks_overall_wap1_log_price_ret_mean  = np.mean(np.mean(wap1_log_price_ret_vol**2, (2), keepdims=True)**.5, 0, keepdims=True)\n","\n","#     # wap1_log_price_ret_vol and then normalized by wap1_log_price_ret_vol (shape 3830 x 112, 1), is it 1 for ffrom=0?\n","#     final_features[name + suffix] =            np.log(             np.mean(wap1_log_price_ret_vol[:,:,ffrom:]**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","#     #\n","#     final_features[name+'stock_mean'+suffix] = np.log(stocks_overall_wap1_log_price_ret_mean*np.median( np.mean( wap1_log_price_ret_vol[:,:,ffrom:]/stocks_overall_wap1_log_price_ret_mean, 2, keepdims=True), 1, keepdims=True)/final_features['wap1_log_price_ret_vol'])\n","\n","\n","\n","\n","\n","\n","\n","# def generate_features(train_buckets):\n","\n","#     final_features = {}\n","\n","#     final_features['time_ids'] = (1*train_buckets['time_ids'][:,np.newaxis] + 0*train_buckets['stock_ids'][np.newaxis,:])[:,:,np.newaxis] # (3830, 112, 1) containing only repeated time ids along stock axis\n","#     final_features['stock_ids'] = (0*train_buckets['time_ids'][:,np.newaxis] + 1*train_buckets['stock_ids'][np.newaxis,:])[:,:,np.newaxis] # (3830, 112, 1) containing only repeated stock ids along time axis\n","\n","#     # average out along the buckets axis\n","#     final_features['wap1_log_price_ret_vol'] = np.mean(train_buckets['wap1_log_price_ret_vol_buks']**2, 2, keepdims=True)**0.5\n","\n","#     # (log liquidity ret x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","#     # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","#     # check if greater than or less than wap1 price volatility\n","#     final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol'] = np.log(np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","#     final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol'] = np.log(np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","#     final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2'] = np.log(np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","\n","#     # average out along the buckets axis\n","#     final_features['wap1_log_price_ret_per_liq2_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks']**2, 2, keepdims=True)**0.5)\n","#     final_features['wap1_log_price_ret_per_spread_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks']**2, 2, keepdims=True)**0.5)\n","\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = np.log( np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                           /np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","#     final_features['wap1_log_price_ret_per_liq2_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                           /np.mean(train_buckets['wap1_log_price_ret_per_liq2_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                           /np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = np.log( np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                           /np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     final_features['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio'] = np.log( np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                           /np.mean(train_buckets['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_buks_2'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio'] = np.log( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                           /np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5)\n","\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     # median across all stocks (dimension 1) for that time id.\n","#     final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                                     / np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     # median across all stocks (dimension 1) for that time id.\n","#     final_features['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                                     / np.mean(train_buckets['log_liq2_ret_*_wap_eqi_price1_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","#     # ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend\n","#     # median across all stocks (dimension 1) for that time id.\n","#     final_features['wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock'] = np.log(np.median( np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)**0.5\n","#                                                     / np.mean(train_buckets['wap1_log_price_ret_per_spread_sqr_vol_buks'][:,:,  :15]**2, 2, keepdims=True)**0.5,1,keepdims=True))\n","\n","#     # (liquidity x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","#     # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","#     # check if greater than or less than wap1 price volatility\n","#     final_features['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol'] = np.log(np.mean(train_buckets['wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'])\n","\n","#     # (liquidity x equi_price1_volatility) / wap1 price volatitliy = a kind of liquidity adjusted volatility\n","#     # take log as the value is around 1. log gives negative sign to values less than 1 and positive sign to values greater than 1\n","#     # check if greater than or less than wap1 price volatility\n","#     # volatitlity in wap1_log_price_ret when liquidity1 is negative. i.e. decreases divided by wap1_log_price_ret_vol\n","#     final_features['wap1_log_price_ret_neg_log_liq_ret_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_neg_log_liq_ret_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'] )\n","#     # volatitlity in wap1_log_price_ret when liquidity1 is positive. i.e. increases divided by wap1_log_price_ret_vol\n","#     final_features['wap1_log_price_ret_pos_log_liq_ret_sqr_vol'] = np.log(np.mean(train_buckets['wap1_log_price_ret_pos_log_liq_ret_sqr_vol_buks']**2, 2, keepdims=True)**0.5/final_features['wap1_log_price_ret_vol'] )\n","#     # difference between volatitlity in wap1_log_price_ret when liquidity1 is positive and negative. i.e. increases minus decreases\n","#     final_features['wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol'] = final_features['wap1_log_price_ret_pos_log_liq_ret_sqr_vol'] - final_features['wap1_log_price_ret_neg_log_liq_ret_sqr_vol']\n","\n","\n","#     get_cohesion_features(train_buckets, final_features, ffrom=0) # uses wap1_log_price_ret_buks from bucket 0 to 30 of a time id, all\n","#     get_cohesion_features(train_buckets, final_features, ffrom=10) # uses wap1_log_price_ret_buks from bucket 10 to 30 of a time id, last two thirds\n","#     get_cohesion_features(train_buckets, final_features, ffrom=20) # uses wap1_log_price_ret_buks from bucket 20 to 30 of a time id, last one third\n","\n","#     get_misc_features(train_buckets, final_features)\n","\n","\n","#     get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=0)\n","#     get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=10)\n","#     get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=20)\n","#     get_simple_features(train_buckets, final_features, 'wap1_log_price_ret_vol', ffrom=25)\n","\n","#     # standardize wap1_log_price_ret_vol by dividing by mean of wap1_log_price_ret_vol\n","#     final_features['vol1_mean'] = np.log(final_features['wap1_log_price_ret_vol']/np.nanmean(final_features['wap1_log_price_ret_vol'], 0, keepdims=True))\n","\n","#     # std of ratio of mean of last 15 buckets to mean of first 15 buckets within a time id. ratio > 1 means increasing trend. ratio < 1 means decreasing trend, shape of (1,112,1)\n","#     final_features['mean_half_delta'] = np.nanstd( np.log( np.mean( train_buckets['wap1_log_price_ret_vol_buks'][:,:,15:  ]**2, 2, keepdims=True)\n","#                                                          / np.mean( train_buckets['wap1_log_price_ret_vol_buks'][:,:,  :15]**2, 2, keepdims=True) ) , 0, keepdims=True)\n","\n","#     # std of difference between last bin and first bin of log_spread_wavg,  shape of (1,112,1)\n","#     final_features['mean_half_delta_lsprd'] = np.log(np.nanstd( (  train_buckets['log_spread_wavg'][:,:,-1: ]\n","#                                                          -  train_buckets['log_spread_wavg'][:,:,  :1] ), 0, keepdims=True) )\n","\n","#     # take log\n","#     final_features['log_wap1_log_price_ret_vol'] = np.log(final_features['wap1_log_price_ret_vol'])\n","\n","#     return final_features\n","\n","\n","\n","\n","\n","\n","\n","# def read_targets_from_df(df, s, t):\n","#     # returns a matrix of shape (len(t), len(s)) with target values as entries\n","#     t = list(t)\n","#     s = list(s)\n","\n","\n","#     Z = np.zeros((len(t), len(s)))\n","\n","#     dft = np.array(df['time_id'])\n","#     dfs = np.array(df['stock_id'])\n","#     dfr = np.array(df['target'])\n","\n","\n","#     for k in range(df.shape[0]):\n","#         Z[t.index(dft[k]), s.index(dfs[k])] = dfr[k]\n","#     return Z\n","\n","\n","\n","\n","\n","\n","\n","# def merge_features_to_df(fdict, df, features):\n","\n","#     T = fdict['time_ids'][:,:,0]\n","#     S = fdict['stock_ids'][:,:,0]\n","\n","#     T = np.reshape(T, [T.shape[0]*T.shape[1]])\n","#     S = np.reshape(S, [S.shape[0]*S.shape[1]])\n","\n","#     sh = np.max( [fdict[f].shape[1] for f in features] )\n","#     sq = np.max( [fdict[f].shape[0] for f in features] )\n","\n","#     for f in features:\n","#         if fdict[f].shape[1]==1:\n","#             print(f, 'shape[1]==1')\n","#             fdict[f] = np.repeat(fdict[f], repeats=sh, axis=1)\n","#         if fdict[f].shape[0]==1:\n","#             print(f, 'shape[0]==1')\n","#             fdict[f] = np.repeat(fdict[f], repeats=sq, axis=0)\n","\n","#     reshaped_features = [np.reshape( fdict[feature], [fdict[feature].shape[0]*fdict[feature].shape[1]] )\n","#                          for feature in features]\n","\n","#     dfz = pd.DataFrame(data=np.vstack([S,T] + reshaped_features  ).T, columns=['stock_id', 'time_id']+features)\n","#     dfz['time_id' ] = dfz['time_id' ].astype(int)\n","#     dfz['stock_id'] = dfz['stock_id'].astype(int)\n","\n","#     dfz = df.merge(dfz, on=['stock_id', 'time_id'], how='left')\n","#     # some time ids are missing in the df, so df is smaller than dfz\n","#     # 428960 to 428932, 151 to 152\n","#     # suffix _x and _y are added for clashing columns during merge\n","#     return dfz\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ymZ4WfHagKDg"},"outputs":[],"source":["# train_features = generate_features(train_buckets)\n","# train_features['target'] = read_targets_from_df(train, train_buckets['stock_ids'], train_buckets['time_ids'])\n","\n","# train_feat_df = merge_features_to_df(train_features, train, [f for f in list(train_features.keys()) if ('time_id' not in f)])\n","# del train_features\n","\n","\n","# train_feat_df['target'] = train_feat_df['target_y']\n","# train_feat_df.drop(columns=['target_x', 'target_y'], inplace=True)\n","\n","# print('train_feat_df columns', train_feat_df.columns)\n","\n","\n","\n","# # Define the Numba-accelerated function for WAP1 price calculation\n","# @nb.njit\n","# def compute_wap1(bid_price1, ask_price1, bid_size1, ask_size1):\n","#     return (bid_price1 * ask_size1 + ask_price1 * bid_size1) / (bid_size1 + ask_size1)\n","\n","# # Function to process trade parquet files\n","# def process_trade_file(path):\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     trade_train_st = pd.read_parquet(path)\n","#     trade_train_st['stock_id'] = st_id\n","#     return trade_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'price']].rename(columns={'price': 'trade_price'})\n","\n","# # Function to process book parquet files\n","# def process_book_file(path):\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     book_train_st = pd.read_parquet(path)\n","#     book_train_st['stock_id'] = st_id\n","#     # Apply Numba-accelerated WAP1 calculation\n","#     book_train_st['wap1_price'] = compute_wap1(\n","#         book_train_st['bid_price1'].values,\n","#         book_train_st['ask_price1'].values,\n","#         book_train_st['bid_size1'].values,\n","#         book_train_st['ask_size1'].values\n","#     )\n","#     return book_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'wap1_price']]\n","\n","\n","# # Use joblib's Parallel and delayed to process trade files in parallel\n","# trade_dfs = Parallel(n_jobs=-1)(delayed(process_trade_file)(path) for path in train_trade_paths)\n","\n","# # Use joblib's Parallel and delayed to process book files in parallel\n","# book_dfs = Parallel(n_jobs=-1)(delayed(process_book_file)(path) for path in train_book_paths)\n","\n","# # Concatenate the DataFrames into the final DataFrames\n","# trade_price_df = pd.concat(trade_dfs, axis=0)\n","# book_price_df = pd.concat(book_dfs, axis=0)\n","\n","# ##### merge trade_price and book_price data and calculate trade_price_n_wap1_deviation #####\n","# trade_price_n_wap1_deviation = pd.DataFrame()\n","\n","# # Group by 'stock_id' and perform operations in one go\n","# for st, trade_price_st in trade_price_df.groupby('stock_id'):\n","#     book_price_st = book_price_df[book_price_df['stock_id'] == st]\n","\n","#     # Ensure consistent types for merging using .loc to avoid SettingWithCopyWarning\n","#     trade_price_st.loc[:, 'time_id'] = trade_price_st['time_id'].astype(int)\n","#     book_price_st.loc[:, 'time_id'] = book_price_st['time_id'].astype(int)\n","#     trade_price_st.loc[:, 'seconds_in_bucket'] = trade_price_st['seconds_in_bucket'].astype(int)\n","#     book_price_st.loc[:, 'seconds_in_bucket'] = book_price_st['seconds_in_bucket'].astype(int)\n","\n","#     # Merge trade and book data on 'stock_id', 'time_id', 'seconds_in_bucket'\n","#     merged = trade_price_st.merge(book_price_st, on=['stock_id', 'time_id', 'seconds_in_bucket'], how='inner')\n","\n","#     # Calculate the ratio with vectorized operations\n","#     merged['ratio'] = (merged['wap1_price'] / merged['trade_price'])**0.5\n","\n","#     # Group by 'stock_id' and 'time_id', and calculate standard deviation of 'ratio'\n","#     temp_df1 = merged.groupby(['stock_id', 'time_id'])['ratio'].std().reset_index()\n","\n","#     # Append to the final DataFrame\n","#     trade_price_n_wap1_deviation = pd.concat([trade_price_n_wap1_deviation, temp_df1], axis=0)\n","\n","# ####### Calculate correlation between trade_price_n_wap1_deviation and target ########\n","# #merged_df = train.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","# trade_price_n_wap1_deviation_df = train.merge(trade_price_n_wap1_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","# del book_price_df, trade_dfs, book_dfs, trade_price_st, book_price_st, merged\n","\n","\n","\n","\n","\n","# import glob\n","# import pandas as pd\n","# import numpy as np\n","# from joblib import Parallel, delayed\n","\n","# # Assuming find_equilibrium_price is already defined elsewhere\n","\n","# # Function to process book parquet files and calculate WAP equilibrium price\n","# def process_book_eqi_file(path):\n","#     st_id = int(path.split('/stock_id=')[1])\n","#     book_train_st = pd.read_parquet(path)\n","#     book_train_st['stock_id'] = st_id\n","\n","#     # Avoid log(0) errors with a small adjustment to prices if necessary\n","#     book_train_st['log_ask_price2'] = book_train_st['ask_price2'] #.replace(0, np.nan)\n","#     book_train_st['log_bid_price2'] = book_train_st['bid_price2'] #.replace(0, np.nan)\n","#     book_train_st['log_ask_price1'] = book_train_st['ask_price1'] #.replace(0, np.nan)\n","#     book_train_st['log_bid_price1'] = book_train_st['bid_price1'] #.replace(0, np.nan)\n","\n","#     # Calculate the equilibrium price\n","#     book_train_st['wap_eqi_price0'] = find_equilibrium_price(book_train_st, 0)\n","\n","#     return book_train_st[['stock_id', 'time_id', 'seconds_in_bucket', 'wap_eqi_price0']]\n","\n","\n","# # Use joblib's Parallel and delayed to process book files in parallel\n","# book_eqi_dfs = Parallel(n_jobs=-1)(delayed(process_book_eqi_file)(path) for path in train_book_paths)\n","\n","# # Concatenate the DataFrames into the final DataFrame\n","# book_eqi_price_df = pd.concat(book_eqi_dfs, axis=0)\n","\n","# ##### Merge trade_price and book_price data and calculate trade_price_n_wap_eqi_price0_deviation #####\n","# trade_price_n_wap_eqi_price0_deviation = pd.DataFrame()\n","\n","# # Group by 'stock_id' and perform operations in one go\n","# for st, trade_price_st in trade_price_df.groupby('stock_id'):\n","#     book_price_st = book_eqi_price_df[book_eqi_price_df['stock_id'] == st]\n","\n","#     # Ensure consistent types for merging using .loc to avoid SettingWithCopyWarning\n","#     trade_price_st.loc[:, 'time_id'] = trade_price_st['time_id'].astype(int)\n","#     book_price_st.loc[:, 'time_id'] = book_price_st['time_id'].astype(int)\n","#     trade_price_st.loc[:, 'seconds_in_bucket'] = trade_price_st['seconds_in_bucket'].astype(int)\n","#     book_price_st.loc[:, 'seconds_in_bucket'] = book_price_st['seconds_in_bucket'].astype(int)\n","\n","#     # Merge trade and book data on 'stock_id', 'time_id', 'seconds_in_bucket'\n","#     merged1 = trade_price_st.merge(book_price_st, on=['stock_id', 'time_id', 'seconds_in_bucket'], how='inner')\n","\n","#     # Calculate the ratio with vectorized operations\n","#     merged1['ratio'] = (merged1['wap_eqi_price0'] / (merged1['trade_price'] ) )**0.5\n","\n","#     # Group by 'stock_id' and 'time_id', and calculate standard deviation of 'ratio'\n","#     temp_df1 = merged1.groupby(['stock_id', 'time_id'])['ratio'].std().reset_index()\n","\n","#     # Append to the final DataFrame\n","#     trade_price_n_wap_eqi_price0_deviation = pd.concat([trade_price_n_wap_eqi_price0_deviation, temp_df1], axis=0)\n","\n","# ####### Calculate correlation between trade_price_n_wap_eqi_price0_deviation and target ########\n","# trade_price_n_wap_eqi_price0_deviation_df = train.merge(trade_price_n_wap_eqi_price0_deviation, on=['stock_id', 'time_id'], how='left').ffill().bfill()\n","# del trade_price_df, trade_price_st, book_price_st, merged1, temp_df1, book_eqi_price_df, book_eqi_dfs\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"us3Z1nc7MG19"},"outputs":[],"source":["\n","\n","# for key in bk_level1_2_size_imbalance_feat.keys():\n","#     bk_level1_2_size_imbalance_feat[key].index = range(bk_level1_2_size_imbalance_feat[key].shape[0])\n","#     bk_level1_2_size_imbalance_feat[key].columns = [key]\n","#     train_feat_df = train_feat_df.merge(bk_level1_2_size_imbalance_feat[key], left_index=True, right_index=True)\n","# del bk_level1_2_size_imbalance_feat\n","\n","# for key in trade_sum_size_sum_order_count_sum_size_per_order_count.keys():\n","#     trade_sum_size_sum_order_count_sum_size_per_order_count[key].index = range(trade_sum_size_sum_order_count_sum_size_per_order_count[key].shape[0])\n","#     trade_sum_size_sum_order_count_sum_size_per_order_count[key].columns = [key]\n","#     train_feat_df = train_feat_df.merge(trade_sum_size_sum_order_count_sum_size_per_order_count[key], left_index=True, right_index=True)\n","# del trade_sum_size_sum_order_count_sum_size_per_order_count\n","\n","# trade_price_n_wap1_deviation_df[\"trade_price_n_wap1_dev\"] = trade_price_n_wap1_deviation_df[\"ratio\"]**0.5\n","# trade_price_n_wap1_deviation_df.drop(columns=[\"ratio\"], inplace=True)\n","# train_feat_df = train_feat_df.merge(trade_price_n_wap1_deviation_df, on=['stock_id', 'time_id'], how='left')\n","# del trade_price_n_wap1_deviation_df\n","\n","# trade_price_n_wap_eqi_price0_deviation_df[\"trade_price_n_wap_eqi_price0_dev\"] = trade_price_n_wap_eqi_price0_deviation_df[\"ratio\"]**0.5\n","# trade_price_n_wap_eqi_price0_deviation_df.drop(columns=[\"ratio\"], inplace=True)\n","# train_feat_df = train_feat_df.merge(trade_price_n_wap_eqi_price0_deviation_df, on=['stock_id', 'time_id'], how='left')\n","# del trade_price_n_wap_eqi_price0_deviation_df\n","\n"]},{"cell_type":"markdown","metadata":{"id":"l4aTzBN1jvdE"},"source":["##### Older features"]},{"cell_type":"markdown","metadata":{"id":"TXmTXn9UlA-m"},"source":["##### File Name: data_munging.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzMBVtYy5GP4"},"outputs":[],"source":["# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_partial.pkl','wb') as fb:\n","#     pickle.dump( train_feat_df,fb)\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_partial.pkl','rb') as fb:\n","#     train_feat_df = pickle.load( fb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"heXHKkJSpXgc"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sio7S0xcMokU"},"outputs":[],"source":["# # remove duplicate columns\n","# train_feat_df = train_feat_df.loc[:,~train_feat_df.columns.duplicated()].copy()\n","\n","# train_feat_df.drop(columns=['stock_ids'], inplace=True) # drop this redundant column\n","\n","# train_feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBaHvlATjxMo"},"outputs":[],"source":["# \"\"\" Realized volatility calculation of first 10 mins\"\"\"\n","\n","\n","# # def log_return(list_stock_prices):\n","# #     return np.log(list_stock_prices).diff()\n","\n","# # def realized_volatility(series_log_return):\n","# #     return np.sqrt(np.sum(series_log_return**2))\n","\n","# # def realized_volatility_per_time_id(file_path, prediction_column_name):\n","# #     df_book_data = pd.read_parquet(file_path)\n","# #     df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\n","# #                                       df_book_data['bid_size1']+ df_book_data[\n","# #                                   'ask_size1'])\n","# #     df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].apply(log_return).values\n","# #     df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n","# #     df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n","# #     df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n","# #     stock_id = file_path.split('=')[1]\n","# #     df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n","# #     return df_realized_vol_per_stock[['row_id',prediction_column_name]]\n","\n","\n","# # def past_realized_volatility_per_stock(list_file,prediction_column_name):\n","# #     df_past_realized = pd.DataFrame()\n","# #     for file in list_file:\n","# #         df_past_realized = pd.concat([df_past_realized,\n","# #                                      realized_volatility_per_time_id(file,prediction_column_name)])\n","# #     return df_past_realized\n","\n","# # list_order_book_file_train = train_book_paths\n","\n","# # df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n","# #                                                            prediction_column_name='first_10_min_vol')\n","\n","# # train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n","# # train = train[['row_id','target']]\n","# # #df_joined = train.merge(df_past_realized_train[['row_id','first_10_min_vol']], on = ['row_id'], how = 'left')\n","# # df_20_min_volatility = train.merge(df_past_realized_train[['row_id','first_10_min_vol']], on = ['row_id'], how = 'left')\n","# # df_20_min_volatility['stock_id'] = df_20_min_volatility['row_id'].transform(lambda x: x.split('-')[0])\n","# # df_20_min_volatility['time_id'] = df_20_min_volatility['row_id'].transform(lambda x: x.split('-')[1])\n","\n","# # # ## save all stocks 20-min vol in data folder\n","# # # df_joined.to_parquet('20_min_volatility.parquet\\\\20_min_volatility_all_stocks.parquet')\n","\n","\n","\n","\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/')\n","\n","\n","# import pandas as pd\n","# import numpy as np\n","# from joblib import Parallel, delayed\n","\n","# def log_return(series):\n","#     return np.log(series).diff()\n","\n","# def realized_volatility(series_log_return):\n","#     return np.sqrt(np.sum(series_log_return**2))\n","\n","# def realized_volatility_per_time_id(file_path, prediction_column_name):\n","#     df_book_data = pd.read_parquet(file_path)\n","\n","#     # Calculate WAP\n","#     df_book_data['wap'] = (df_book_data['bid_price1'] * df_book_data['ask_size1'] +\n","#                            df_book_data['ask_price1'] * df_book_data['bid_size1']) / (\n","#                            df_book_data['bid_size1'] + df_book_data['ask_size1'])\n","\n","#     # Calculate log return\n","#     df_book_data['log_return'] = df_book_data.groupby('time_id')['wap'].transform(log_return)\n","\n","#     # Remove rows with NaN log returns\n","#     df_book_data = df_book_data.dropna(subset=['log_return'])\n","\n","#     # Calculate realized volatility per time_id\n","#     df_realized_vol_per_stock = (df_book_data.groupby('time_id')['log_return']\n","#                                  .agg(realized_volatility)\n","#                                  .reset_index(name=prediction_column_name))\n","\n","#     stock_id = file_path.split('=')[1]\n","#     df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x: f'{stock_id}-{x}')\n","\n","#     return df_realized_vol_per_stock[['row_id', prediction_column_name]]\n","\n","# def past_realized_volatility_per_stock(list_file, prediction_column_name):\n","#     # Parallel processing of files\n","#     results = Parallel(n_jobs=-1)(delayed(realized_volatility_per_time_id)(file, prediction_column_name) for file in list_file)\n","#     df_past_realized = pd.concat(results, ignore_index=True)\n","#     return df_past_realized\n","\n","# list_order_book_file_train = train_book_paths\n","\n","# df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n","#                                                            prediction_column_name='first_10_min_vol')\n","\n","# # Prepare train DataFrame\n","# train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n","# train = train[['row_id', 'target']]\n","\n","# # Merge with past realized volatility\n","# df_20_min_volatility = train.merge(df_past_realized_train[['row_id', 'first_10_min_vol']],\n","#                                    on='row_id', how='left')\n","\n","# # Extract stock_id and time_id\n","# df_20_min_volatility[['stock_id', 'time_id']] = df_20_min_volatility['row_id'].str.split('-', expand=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwOapjsbPQTc"},"outputs":[],"source":["df_20_min_volatility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAvgw2eLQM8R"},"outputs":[],"source":["train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MH5IcUclcgw"},"outputs":[],"source":["###  std., realized volatility of trade execution price, std, mean of size, std, mean of order_count at the available times.\n","\n","\n","# subset_paths = train_trade_paths\n","\n","# trade_price_std_arr = np.array([])\n","# trade_price_real_vol_arr = np.array([])\n","# trade_size_std_arr = np.array([])\n","# trade_size_mean_arr = np.array([])\n","# trade_order_count_std_arr = np.array([])\n","# trade_order_count_mean_arr = np.array([])\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     trade_train_st = pd.read_parquet(path)\n","\n","#     train_idx = train[ train['stock_id']==st_id ]['time_id']\n","#     train_idx.index = train_idx.values\n","\n","#     ## std. of price\n","#     std_price = trade_train_st.groupby(by='time_id')['price'].std()\n","#     std_price = pd.merge(train_idx, std_price, left_index=True, right_index=True, how='left').fillna(np.nanmean(std_price)).drop(columns=['time_id'])\n","#     trade_price_std_arr = np.append(trade_price_std_arr, std_price['price'].values)\n","\n","\n","#     ## reaized volatility of price\n","#     trade_train_st['log_ret_price'] = trade_train_st.groupby(by='time_id')['price'].apply(log_return).values\n","#     trade_train_st = trade_train_st[~trade_train_st['log_ret_price'].isnull()]\n","#     df_realized_vol_per_stock =  pd.DataFrame(trade_train_st.groupby(['time_id'])['log_ret_price'].agg(realized_volatility)).rename(columns={'log_ret_price':'real_vol_price'})\n","\n","#     df_realized_vol_per_stock = pd.merge(train_idx, df_realized_vol_per_stock, left_index=True, right_index=True, how='left').fillna(np.nanmean(df_realized_vol_per_stock)).drop(columns=['time_id'])\n","#     trade_price_real_vol_arr = np.append(trade_price_real_vol_arr, df_realized_vol_per_stock['real_vol_price'].values)\n","\n","#     ## std. of size\n","#     std_size = trade_train_st.groupby(by='time_id')['size'].std()\n","#     std_size = pd.merge(train_idx, std_size, left_index=True, right_index=True, how='left').fillna(np.nanmean(std_size)).drop(columns=['time_id'])\n","#     trade_size_std_arr = np.append(trade_size_std_arr, std_size['size'].values)\n","\n","#     ## mean of size\n","#     mean_size = trade_train_st.groupby(by='time_id')['size'].mean()\n","#     mean_size = pd.merge(train_idx, mean_size, left_index=True, right_index=True, how='left').fillna(np.nanmean(mean_size)).drop(columns=['time_id'])\n","#     trade_size_mean_arr = np.append(trade_size_mean_arr, mean_size['size'].values)\n","\n","\n","#     ## Std of order_count\n","#     std_order_count = trade_train_st.groupby(by='time_id')['order_count'].std()\n","#     std_order_count = pd.merge(train_idx, std_order_count, left_index=True, right_index=True, how='left').fillna(np.nanmean(std_order_count)).drop(columns=['time_id'])\n","#     trade_order_count_std_arr = np.append(trade_order_count_std_arr, std_order_count['order_count'].values)\n","\n","#     ## mean of order_count\n","#     mean_order_count = trade_train_st.groupby(by='time_id')['order_count'].mean()\n","#     mean_order_count = pd.merge(train_idx, mean_order_count, left_index=True, right_index=True, how='left').fillna(np.nanmean(mean_order_count)).drop(columns=['time_id'])\n","#     trade_order_count_mean_arr = np.append(trade_order_count_mean_arr, mean_order_count['order_count'].values)\n","\n","\n","# trade_price_std_df = pd.DataFrame({'trade_price_std':trade_price_std_arr, 'trade_price_real_vol':trade_price_real_vol_arr, 'trade_size_std':trade_size_std_arr, 'trade_size_mean':trade_size_mean_arr, 'trade_order_count_std':trade_order_count_std_arr, 'trade_order_count_mean':trade_order_count_mean_arr})\n","# train_feat_df = train_feat_df.merge(trade_price_std_df, left_index=True, right_index=True)\n","\n","\n","\n","\n","\n","\n","\n","# subset_paths = trade_paths\n","# train = pd.read_csv('train.csv')\n","\n","# import pandas as pd\n","# import numpy as np\n","\n","# def process_trade_data(path, train_idx):\n","#     trade_train_st = pd.read_parquet(path)\n","\n","#     # Calculate statistics for price\n","#     std_price = trade_train_st.groupby('time_id')['price'].std()\n","#     std_price = pd.merge(train_idx, std_price, left_index=True, right_index=True, how='left')\n","#     std_price.fillna(std_price['price'].mean(), inplace=True)\n","\n","#     log_ret_price = trade_train_st.groupby('time_id')['price'].apply(log_return)\n","#     trade_train_st['log_ret_price'] = trade_train_st.groupby('time_id')['price'].transform(log_return)\n","#     trade_train_st = trade_train_st[~trade_train_st['log_ret_price'].isnull()]\n","#     real_vol_price = trade_train_st.groupby('time_id')['log_ret_price'].agg(realized_volatility)\n","#     real_vol_price = pd.merge(train_idx, real_vol_price, left_index=True, right_index=True, how='left')\n","#     real_vol_price.fillna(real_vol_price['log_ret_price'].mean(), inplace=True)\n","\n","#     # Calculate statistics for size\n","#     std_size = trade_train_st.groupby('time_id')['size'].std()\n","#     std_size = pd.merge(train_idx, std_size, left_index=True, right_index=True, how='left')\n","#     std_size.fillna(std_size['size'].mean(), inplace=True)\n","\n","#     mean_size = trade_train_st.groupby('time_id')['size'].mean()\n","#     mean_size = pd.merge(train_idx, mean_size, left_index=True, right_index=True, how='left')\n","#     mean_size.fillna(mean_size['size'].mean(), inplace=True)\n","\n","#     # Calculate statistics for order_count\n","#     std_order_count = trade_train_st.groupby('time_id')['order_count'].std()\n","#     std_order_count = pd.merge(train_idx, std_order_count, left_index=True, right_index=True, how='left')\n","#     std_order_count.fillna(std_order_count['order_count'].mean(), inplace=True)\n","\n","#     mean_order_count = trade_train_st.groupby('time_id')['order_count'].mean()\n","#     mean_order_count = pd.merge(train_idx, mean_order_count, left_index=True, right_index=True, how='left')\n","#     mean_order_count.fillna(mean_order_count['order_count'].mean(), inplace=True)\n","\n","#     # Return results\n","#     return {\n","#         'trade_price_std': std_price['price'].values,\n","#         'trade_price_real_vol': real_vol_price['log_ret_price'].values,\n","#         'trade_size_std': std_size['size'].values,\n","#         'trade_size_mean': mean_size['size'].values,\n","#         'trade_order_count_std': std_order_count['order_count'].values,\n","#         'trade_order_count_mean': mean_order_count['order_count'].values\n","#     }\n","\n","# subset_paths = trade_paths\n","# train_idx = train.set_index('time_id')[['stock_id']]\n","\n","# results = []\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     if st_id in train['stock_id'].values:\n","#         result = process_trade_data(path, train_idx[train_idx['stock_id'] == st_id])\n","#         results.append(result)\n","\n","# # Concatenate results\n","# trade_price_std_arr = np.concatenate([result['trade_price_std'] for result in results])\n","# trade_price_real_vol_arr = np.concatenate([result['trade_price_real_vol'] for result in results])\n","# trade_size_std_arr = np.concatenate([result['trade_size_std'] for result in results])\n","# trade_size_mean_arr = np.concatenate([result['trade_size_mean'] for result in results])\n","# trade_order_count_std_arr = np.concatenate([result['trade_order_count_std'] for result in results])\n","# trade_order_count_mean_arr = np.concatenate([result['trade_order_count_mean'] for result in results])\n","\n","# # Create DataFrame and merge\n","# trade_price_std_df = pd.DataFrame({\n","#     'trade_price_std': trade_price_std_arr,\n","#     'trade_price_real_vol': trade_price_real_vol_arr,\n","#     'trade_size_std': trade_size_std_arr,\n","#     'trade_size_mean': trade_size_mean_arr,\n","#     'trade_order_count_std': trade_order_count_std_arr,\n","#     'trade_order_count_mean': trade_order_count_mean_arr\n","# })\n","\n","# train_feat_df = train_feat_df.merge(trade_price_std_df, left_index=True, right_index=True)\n","# del trade_price_std_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7fcHXxCn4VK"},"outputs":[],"source":["# # reshape target to shape 3830 x 112 x 1\n","\n","# first_10_min_vol_df = df_20_min_volatility\n","\n","# all_stocks_first_10_min_vol_df = pd.DataFrame()\n","# #all_stocks_first_10_min_vol_df['time_id'] = all_uniq_time_ids['time_id']\n","\n","# for st_id in unique_stock_ids:\n","#     st_df = pd.DataFrame()\n","#     subset = first_10_min_vol_df[first_10_min_vol_df['stock_id'] == st_id]\n","#     st_df['time_id'] = subset['time_id']\n","#     st_df['first_10_min_vol'] = subset['first_10_min_vol']\n","#     st_df = all_uniq_time_ids.merge(st_df, on='time_id', how='left').ffill().bfill()\n","#     all_stocks_first_10_min_vol_df[st_id] = st_df['first_10_min_vol']\n","\n","# all_stocks_first_10_min_vol_df = all_stocks_first_10_min_vol_df.to_numpy()[:,:,np.newaxis] # change dimension from 380 x 112 to 3830 x 112 x 1\n","\n","\n","\n","# import numpy as np\n","# import pandas as pd\n","\n","# # Initialize an empty list to hold the data for each stock\n","# all_stocks_data = []\n","\n","# # Iterate through each stock ID and collect the data\n","# for st_id in unique_stock_ids:\n","#     subset = first_10_min_vol_df[first_10_min_vol_df['stock_id'] == st_id]\n","#     st_df = pd.DataFrame({'time_id': subset['time_id'], 'first_10_min_vol': subset['first_10_min_vol']})\n","\n","#     # Merge with all unique time IDs and handle missing data\n","#     st_df = all_uniq_time_ids.merge(st_df, on='time_id', how='left').ffill().bfill()\n","\n","#     # Append the first_10_min_vol column to the list\n","#     all_stocks_data.append(st_df['first_10_min_vol'].to_numpy())\n","\n","# # Convert the list to a NumPy array and then reshape it\n","# all_stocks_first_10_min_vol_array = np.stack(all_stocks_data, axis=1)\n","\n","# # Change dimension from 380 x 112 to 3830 x 112 x 1\n","# all_stocks_first_10_min_vol_array = all_stocks_first_10_min_vol_array[:, :, np.newaxis]\n","\n","# all_stocks_first_10_min_vol_df = all_stocks_first_10_min_vol_array\n","\n","\n","\n","\n","\n","\n","# import numpy as np\n","# import pandas as pd\n","\n","# first_10_min_vol_df = df_20_min_volatility\n","\n","# # Initialize an array with NaN values\n","# all_stocks_first_10_min_vol_array = np.full((len(all_uniq_time_ids), len(unique_stock_ids)), np.nan)\n","\n","# # Create a dictionary for quick access to stock index\n","# stock_id_to_index = {st_id: idx for idx, st_id in enumerate(unique_stock_ids)}\n","\n","# # Fill the array with data\n","# for st_id, subset in first_10_min_vol_df.groupby('stock_id'):\n","#     time_id_indices = all_uniq_time_ids['time_id'].searchsorted(subset['time_id'].astype(int))\n","#     all_stocks_first_10_min_vol_array[time_id_indices, stock_id_to_index[int(st_id)]] = subset['first_10_min_vol'].values\n","\n","# # Forward fill and backward fill missing values\n","# all_stocks_first_10_min_vol_array = pd.DataFrame(all_stocks_first_10_min_vol_array).ffill().bfill().to_numpy()\n","\n","# # Reshape to the desired shape\n","# all_stocks_first_10_min_vol_array = all_stocks_first_10_min_vol_array[:, :, np.newaxis]\n","\n","# all_stocks_first_10_min_vol_df = all_stocks_first_10_min_vol_array\n","\n","# train_feat_df['first_10_min_vol'] = first_10_min_vol_df['first_10_min_vol']\n","# del first_10_min_vol_df\n"]},{"cell_type":"markdown","metadata":{"id":"VnXnopV-xFlP"},"source":["##### File name: target_eda_across_stocks.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MbOdX41xlO7"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import plotly.express as px\n","import plotly.subplots as sp\n","import plotly.graph_objects as go\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","from ipywidgets import interact, interactive, fixed, interact_manual\n","#import hdbscan\n","import seaborn as sns\n","from matplotlib.pyplot import get_cmap\n","from sklearn.metrics import silhouette_samples,silhouette_score\n","from matplotlib.pyplot import cm\n","from sklearn.cluster import SpectralClustering, MiniBatchKMeans, MeanShift,AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","import sys\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from scipy.spatial.distance import squareform\n","from scipy.stats import skew, kurtosis\n","import pickle\n","import os\n","import ipywidgets as widgets\n","from matplotlib.patches import Rectangle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aB-qjjPZz5el"},"outputs":[],"source":["\n","\n","# def get_alternating_cluster_coloring(labels):\n","\n","#     sorted_labels = np.sort(labels)\n","#     colors = ['yellow', 'magenta']\n","#     coloring = []\n","\n","#     sorted_labels = np.insert(sorted_labels, 0, sorted_labels[0], axis=0)\n","#     for i in range(len(sorted_labels)-1):\n","#         if sorted_labels[i+1] == sorted_labels[i]:\n","#             coloring.append(colors[0])\n","#         else:\n","#             colors = colors[::-1]\n","#             coloring.append(colors[0])\n","#     return coloring"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3RGRsLGOzhoV"},"outputs":[],"source":["# ## verify the clustering using silhouette score for particular clustring parameters\n","# def plot_silhouette_scores(X_train,labels,metric,linkage,clustering_type):\n","#     unique_clusters = np.unique(labels)\n","#     num_clusters = len(unique_clusters)\n","#     sample_silhouette_values = silhouette_samples(X=X_train, labels=labels,metric=metric)\n","#     silhouette_avg = silhouette_score(X=X_train, labels=labels,metric=metric)\n","#     fig, ax1 = plt.subplots()\n","#     y_lower = 10\n","#     mean_num_zones_dev_in_clusters = []\n","\n","#     mean_num_zones_in_clusters = X_train.shape[0] /num_clusters\n","\n","#     for i in range(num_clusters): # exclude the\n","#         # Aggregate the silhouette scores for samples belonging to\n","#         # cluster i, and sort them\n","#         ith_cluster_silhouette_values = sample_silhouette_values[labels == i+1]\n","#         ith_cluster_silhouette_values.sort()\n","#         size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","#         y_upper = y_lower + size_cluster_i\n","#         color = cm.nipy_spectral(float(i+1) / num_clusters)\n","#         ax1.fill_betweenx(\n","#             np.arange(y_lower, y_upper),\n","#             0,\n","#             ith_cluster_silhouette_values,\n","#             facecolor=color,\n","#             edgecolor=color,\n","#             alpha=0.7,\n","#         )\n","#         # Label the silhouette plots with their cluster numbers at the middle\n","#         ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))\n","#         # Compute the new y_lower for next plot\n","#         y_lower = y_upper + 10  # 10 for the 0 samples\n","#         mean_num_zones_dev_in_clusters.append(abs(mean_num_zones_in_clusters - np.sum(labels == i+1) ))\n","\n","#     num_neg_silhouette_scores = np.sum(sample_silhouette_values < 0 )\n","#     min_silhouette_score = min(sample_silhouette_values)\n","#     dev_from_mean_num_zone = np.sum(np.array(mean_num_zones_dev_in_clusters))\n","\n","#     print(\"[\",num_clusters,',',silhouette_avg,',',num_neg_silhouette_scores,',',min_silhouette_score,\"]\")\n","#     ax1.set_title(\"met: \"+metric+\", link: \"+linkage+\", sil_avg: \"+str(silhouette_avg)+\",\\n num_neg_sil_score: \"+str(num_neg_silhouette_scores)+\", min_sil_score: \"+str(min_silhouette_score)+\",\\n num_clust: \"+str(num_clusters)+\", Clust_type : \"+clustering_type )\n","#     ax1.set_xlabel(\"The silhouette coefficient values\")\n","#     ax1.set_ylabel(\"Cluster label\")\n","#     # The vertical line for average silhouette score of all the values\n","#     ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","#     ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","#     ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","\n","#     return silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OE8vQ7CywST"},"outputs":[],"source":["# def find_unique_time_ids_in_all_stocks(train):\n","#     all_time_ids = np.array([])\n","#     for st_id in unique_stock_ids:\n","#         all_time_ids = np.append(all_time_ids,train[train['stock_id'] == st_id]['time_id'])\n","#     return np.unique(all_time_ids)\n","\n","# unique_time_ids = find_unique_time_ids_in_all_stocks(train)\n","\n","\n","# train_common_time_ids_df = pd.DataFrame()\n","# for st_id in unique_stock_ids:\n","#     temp_df = pd.DataFrame()\n","#     temp_df[str(st_id)] = train[train['stock_id'] == st_id]['target']\n","#     temp_df.index = train[train['stock_id'] == st_id]['time_id']\n","#     temp_df = temp_df.reindex(unique_time_ids).ffill().bfill() ## forward and backward fill the missing values so that data is available at all time_id\n","#     train_common_time_ids_df = pd.concat([train_common_time_ids_df,temp_df],axis=1)\n","\n","# train_common_time_ids_df\n","\n","\n","# target_corr_mat = train_common_time_ids_df.corr()\n","# target_corr_mat\n","\n","\n","# if ~(np.any(np.where(target_corr_mat < 0))):\n","#     dissimilarity = 1 - abs(target_corr_mat)\n","\n","# # create map of labels_order (0,1,..., 111, 112) to stock_id_dict (0,1,..., 125, 126)\n","# map_labels_order_to_stock_id_dict = {x: target_corr_mat.columns[x] for x in range(112)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONvSzNf8xL0_"},"outputs":[],"source":["# ## Best clusterings on pearson correlation matrix using complete for best small number of clusters\n","\n","# clustering_type = 'Agg. Hier. Clustering'\n","# Best_silhouette_parameters = {}\n","# metrics = ['null']\n","\n","# final_pear_corr_target_vol_clusters = pd.DataFrame(columns=['stock_id'])\n","# final_pear_corr_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","\n","\n","# c = [3] # number of clusters\n","# i=0\n","# for metric in metrics:\n","#     for method in ['complete']:\n","#         Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","#         for t in [0.5]: #[0.2,0.45]:\n","#             # Calculate the cluster\n","#             labels = fcluster(Z, t, criterion='distance')\n","#             # Keep the indices to sort labels\n","#             labels_order = np.argsort(labels)\n","#             stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","#             final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#             i+=1\n","\n","#             plt.figure(figsize=(20,5))\n","#             plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#             dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","#             plt.show()\n","\n","#             silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","#             Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","#             # Build a new dataframe with the sorted columns\n","#             #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","#             clustered = train_common_time_ids_df[stock_id_order]\n","\n","#             # Plot the correlation heatmap\n","#             correlations = clustered.corr()\n","#             fig, ax = plt.subplots(figsize=(20,20))\n","#             sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","#                         xticklabels=stock_id_order, yticklabels=stock_id_order,ax= ax)\n","\n","#             ## show clusters as alternating colors on the correlation heatmap\n","#             cluster_colorings = get_alternating_cluster_coloring(labels)\n","#             for i in range(len(cluster_colorings)):\n","#                 ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","#             plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","#             plt.show()\n","\n","# max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","# print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OpQfTyE7yCBx"},"outputs":[],"source":["# ## Best clusterings on pearson correlation matrix with weighted linkage\n","\n","# clustering_type = 'Agg. Hier. Clustering'\n","# Best_silhouette_parameters = {}\n","# metrics = ['null']\n","\n","# c = [49] # number of clusters\n","# i=0\n","# for metric in metrics:\n","#     for method in ['weighted']:\n","#         Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","#         for t in [0.15]:\n","#             # Calculate the cluster\n","#             labels = fcluster(Z, t, criterion='distance')\n","#             # Keep the indices to sort labels\n","#             labels_order = np.argsort(labels)\n","#             stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","#             final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#             i+=1\n","\n","#             plt.figure(figsize=(20,5))\n","#             plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#             dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","#             plt.show()\n","\n","#             silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","#             Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","#             # Build a new dataframe with the sorted columns\n","#             #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","#             clustered = train_common_time_ids_df[stock_id_order]\n","\n","#             # Plot the correlation heatmap\n","#             correlations = clustered.corr()\n","#             fig, ax = plt.subplots(figsize=(20,20))\n","#             sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","#                         xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","#             ## show clusters as alternating colors on the correlation heatmap\n","#             cluster_colorings = get_alternating_cluster_coloring(labels)\n","#             for i in range(len(cluster_colorings)):\n","#                 ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","#             plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","#             plt.show()\n","\n","# max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","# print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmYqWZ2gyI9s"},"outputs":[],"source":["# ## Best clusterings on pearson correlation matrix using ward\n","\n","# clustering_type = 'Agg. Hier. Clustering'\n","# Best_silhouette_parameters = {}\n","\n","# c=[90] # number of clusters\n","# i=0\n","# metrics = ['nil']\n","# for metric in metrics:\n","#     for method in ['ward']:\n","#         Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","#         for t in [0.1]:\n","#             # Calculate the cluster\n","#             labels = fcluster(Z, t, criterion='distance')\n","#             # Keep the indices to sort labels\n","#             labels_order = np.argsort(labels)\n","#             stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","#             final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#             i+=1\n","\n","#             plt.figure(figsize=(20,5))\n","#             plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#             dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","#             plt.show()\n","\n","#             silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","#             Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","#             # Build a new dataframe with the sorted columns\n","#             #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","#             clustered = train_common_time_ids_df[stock_id_order]\n","\n","#             # Plot the correlation heatmap\n","#             correlations = clustered.corr()\n","#             fig, ax = plt.subplots(figsize=(20,20))\n","#             sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","#                         xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","#             ## show clusters as alternating colors on the correlation heatmap\n","#             cluster_colorings = get_alternating_cluster_coloring(labels)\n","#             for i in range(len(cluster_colorings)):\n","#                 ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","#             plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","#             plt.show()\n","\n","\n","# max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","# print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"747F8CHmyOVP"},"outputs":[],"source":["# ## Best clusterings on pearson correlation matrix using median\n","\n","# clustering_type = 'Agg. Hier. Clustering'\n","# Best_silhouette_parameters = {}\n","\n","# c=[10] # number of clusters\n","# i=0\n","# metrics = ['nil']\n","# for metric in metrics:\n","#     for method in ['median']:\n","#         Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","#         for t in [0.25]:\n","#             # Calculate the cluster\n","#             labels = fcluster(Z, t, criterion='distance')\n","#             # Keep the indices to sort labels\n","#             labels_order = np.argsort(labels)\n","#             stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","#             final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#             i+=1\n","\n","#             plt.figure(figsize=(20,5))\n","#             plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#             dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","#             plt.show()\n","\n","#             silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","#             Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","#             # Build a new dataframe with the sorted columns\n","#             #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","#             clustered = train_common_time_ids_df[stock_id_order]\n","\n","#             # Plot the correlation heatmap\n","#             correlations = clustered.corr()\n","#             fig, ax = plt.subplots(figsize=(20,20))\n","#             sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","#                         xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","#             ## show clusters as alternating colors on the correlation heatmap\n","#             cluster_colorings = get_alternating_cluster_coloring(labels)\n","#             for i in range(len(cluster_colorings)):\n","#                 ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","#             plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","#             plt.show()\n","\n","\n","# max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","# print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkKu6b7yyTha"},"outputs":[],"source":["# ## Best clusterings on pearson correlation matrix using average\n","\n","# clustering_type = 'Agg. Hier. Clustering'\n","# Best_silhouette_parameters = {}\n","\n","# c=[26] # number of clusters\n","# i=0\n","# metrics = ['nil']\n","# for metric in metrics:\n","#     for method in ['average']:\n","#         Z = linkage(y=squareform(dissimilarity), method=method, optimal_ordering=True)\n","\n","#         for t in [0.2]:\n","#             # Calculate the cluster\n","#             labels = fcluster(Z, t, criterion='distance')\n","#             # Keep the indices to sort labels\n","#             labels_order = np.argsort(labels)\n","#             stock_id_order = list(map(map_labels_order_to_stock_id_dict.get,labels_order))\n","\n","#             final_pear_corr_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#             i+=1\n","\n","#             plt.figure(figsize=(20,5))\n","#             plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#             dendrogram(Z,color_threshold=t,labels=dissimilarity.columns)\n","#             plt.show()\n","\n","#             silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(dissimilarity,labels,'precomputed',method,clustering_type)\n","#             Best_silhouette_parameters[silhouette_avg] = [metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone]\n","\n","#             # Build a new dataframe with the sorted columns\n","#             #clustered = pd.concat([train_common_time_ids_df[i] for i in train_common_time_ids_df.columns[labels_order]], axis=1)\n","#             clustered = train_common_time_ids_df[stock_id_order]\n","\n","#             # Plot the correlation heatmap\n","#             correlations = clustered.corr()\n","#             fig, ax = plt.subplots(figsize=(20,20))\n","#             sns.heatmap(round(correlations, 2), cmap='RdBu', vmin=0.28, vmax=1,\n","#                         xticklabels=stock_id_order, yticklabels=stock_id_order)\n","\n","#             ## show clusters as alternating colors on the correlation heatmap\n","#             cluster_colorings = get_alternating_cluster_coloring(labels)\n","#             for i in range(len(cluster_colorings)):\n","#                 ax.add_patch(Rectangle((i, i), 1, 1, fill=True, color=cluster_colorings[i]))\n","\n","#             plt.title(f'Clustered target Corr matrix, method: {method}, threshold: {t}')\n","#             plt.show()\n","\n","\n","# max_silhouette_avg = max(Best_silhouette_parameters.keys())\n","# print(f'Best silhouette score: {max_silhouette_avg} ,Best_silhouette_parameters: {Best_silhouette_parameters[max_silhouette_avg]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HV4BSMGA2McZ"},"outputs":[],"source":["# # ## create feature matrix from individual feature dictionaries\n","# # def create_feature_matrix(args):\n","# #     X_feat = np.zeros((len(args[1]),len(args)))\n","# #     for f in range(len(args)):\n","# #         X_feat[:,f] = np.array(list(args[f].values()))\n","# #     return X_feat\n","\n","\n","# # class feat_transformation():\n","# #     def __init__(self,):\n","# #         return\n","# #     def feature_normalization(self, X_train, transform_type='minmax'):\n","# #         # Default to MinMaxScaler unless 'standard' is specified\n","# #         if transform_type == 'standard':\n","# #             self.scaler = StandardScaler()\n","# #         else:\n","# #             self.scaler = MinMaxScaler()\n","\n","# #         self.scaler.fit(X_train)\n","# #         X_train_normalized = self.scaler.transform(X_train)\n","# #         return X_train_normalized\n","\n","\n","# #     def inv_feature_normalization(self,X_train_normalized):\n","# #         # ensure that features of different scale are comparable using euclidean distance\n","# #         X_train = self.scaler.inverse_transform(X_train_normalized)\n","# #         return X_train\n","\n","\n","# # ## summary statistics of each stocks target column\n","# # total_time_id ={} # total number of time ids in each stock\n","# # mean_vol = {}\n","# # std_vol = {}\n","# # min_vol = {}\n","# # p25_vol = {}\n","# # median_vol = {}\n","# # p75_vol = {}\n","# # max_vol = {}\n","# # skew_vol = {}\n","# # kurt_vol = {}\n","\n","# # for st_id in train['stock_id'].unique():\n","# #     sum_stats = train[train['stock_id'] == st_id]['target'].describe()\n","# #     total_time_id[st_id] = sum_stats[0]\n","# #     mean_vol[st_id] = sum_stats[1]\n","# #     std_vol[st_id] = sum_stats[2]\n","# #     min_vol[st_id] = sum_stats[3]\n","# #     p25_vol[st_id] = sum_stats[4]\n","# #     median_vol[st_id] = sum_stats[5]\n","# #     p75_vol[st_id] = sum_stats[6]\n","# #     max_vol[st_id] = sum_stats[7]\n","# #     skew_vol[st_id] = skew(train[train['stock_id'] == st_id]['target'])\n","# #     kurt_vol[st_id] = kurtosis(train[train['stock_id'] == st_id]['target'])\n","\n","\n","\n","\n","\n","# import numpy as np\n","# import pandas as pd\n","# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","# from scipy.stats import skew, kurtosis\n","\n","# def create_feature_matrix(feature_dicts):\n","#     \"\"\"\n","#     Create a feature matrix from a list of dictionaries containing feature values.\n","#     \"\"\"\n","#     n_samples = len(feature_dicts[0])\n","#     n_features = len(feature_dicts)\n","#     X_feat = np.zeros((n_samples, n_features))\n","\n","#     for i, feature_dict in enumerate(feature_dicts):\n","#         X_feat[:, i] = np.array(list(feature_dict.values()))\n","\n","#     return X_feat\n","\n","\n","# class FeatureTransformation:\n","#     def __init__(self):\n","#         self.scaler = None\n","\n","#     def feature_normalization(self, X_train, transform_type='minmax'):\n","#         \"\"\"\n","#         Normalize features using MinMaxScaler or StandardScaler.\n","#         \"\"\"\n","#         self.scaler = StandardScaler() if transform_type == 'standard' else MinMaxScaler()\n","#         X_train_normalized = self.scaler.fit_transform(X_train)\n","#         return X_train_normalized\n","\n","#     def inv_feature_normalization(self, X_train_normalized):\n","#         \"\"\"\n","#         Inverse transform normalized features to their original scale.\n","#         \"\"\"\n","#         return self.scaler.inverse_transform(X_train_normalized)\n","\n","\n","# def calculate_summary_statistics(train_df):\n","#     \"\"\"\n","#     Calculate summary statistics for each stock's target column.\n","#     \"\"\"\n","#     summary_stats = {\n","#         'total_time_id': {},\n","#         'mean_vol': {},\n","#         'std_vol': {},\n","#         'min_vol': {},\n","#         'p25_vol': {},\n","#         'median_vol': {},\n","#         'p75_vol': {},\n","#         'max_vol': {},\n","#         'skew_vol': {},\n","#         'kurt_vol': {}\n","#     }\n","\n","#     for st_id, group in train_df.groupby('stock_id'):\n","#         target = group['target']\n","#         summary_stats['total_time_id'][st_id] = len(target)\n","#         summary_stats['mean_vol'][st_id] = target.mean()\n","#         summary_stats['std_vol'][st_id] = target.std()\n","#         summary_stats['min_vol'][st_id] = target.min()\n","#         summary_stats['p25_vol'][st_id] = target.quantile(0.25)\n","#         summary_stats['median_vol'][st_id] = target.median()\n","#         summary_stats['p75_vol'][st_id] = target.quantile(0.75)\n","#         summary_stats['max_vol'][st_id] = target.max()\n","#         summary_stats['skew_vol'][st_id] = skew(target)\n","#         summary_stats['kurt_vol'][st_id] = kurtosis(target)\n","\n","#     return summary_stats\n","\n","\n","# # Usage\n","# # train = pd.read_csv('your_train_data.csv')  # Assuming 'train' is your DataFrame\n","# summary_stats = calculate_summary_statistics(train)\n","\n","\n","# std_vol = summary_stats['std_vol']\n","# mean_vol = summary_stats['mean_vol']\n","# min_vol = summary_stats['min_vol']\n","# median_vol = summary_stats['median_vol']\n","# max_vol = summary_stats['max_vol']\n","# kurt_vol = summary_stats['kurt_vol']\n","# p25_vol = summary_stats['p25_vol']\n","# p75_vol = summary_stats['p75_vol']\n","# skew_vol = summary_stats['skew_vol']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2gbRZvV0R04"},"outputs":[],"source":["\n","# ####### NON-ROBUST features #######\n","\n","\n","# new_X_feat = [std_vol,min_vol,median_vol,max_vol,kurt_vol]\n","# X_feat = create_feature_matrix(new_X_feat)\n","\n","# feat_transform = FeatureTransformation()\n","# X_feat_normalized = feat_transform.feature_normalization(X_feat)\n","\n","\n","# ## Select the BEST agglomerative hierarchical clustering for features: std_vol,min_vol,median_vol,max_vol,kurt_vol\n","\n","# #['single','complete','average','weighted','centroid','median','ward']\n","\n","# final_sum_stats_target_vol_clusters = pd.DataFrame(columns=['stock_id','4_clusters', '10_clusters', '16_clusters', '30_clusters'])\n","# final_sum_stats_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","# c = [4,10,16,30]\n","# clustering_type = 'Agg. Hier. Clustering'\n","# metric = 'euclidean'\n","# for method in ['complete']:\n","#     Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","#     i=0\n","\n","#     for t in [1,0.5,0.4,0.25]:#np.arange(0.2,0.3,0.1):\n","#         # Calculate the cluster\n","#         labels = fcluster(Z, t, criterion='distance')\n","#         # Keep the indices to sort labels\n","#         final_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#         i+=1\n","#         labels_order = np.argsort(labels)\n","\n","\n","#         plt.figure(figsize=(20,5))\n","#         plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#         dendrogram(Z,color_threshold=t)\n","#         plt.show()\n","\n","#         silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","# print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","\n","\n","\n","# # # others that can be tried are\n","# # Num_clusters = 10, linkage = ward,threshold = 0.7\n","# # Num_clusters = 15, linkage = ward,threshold = 0.5\n","# # Num_clusters = 5, linkage = average,threshold = 0.5\n","# # Num_clusters = 20, linkage = average,threshold = 0.25\n","# # Num_clusters = 4, linkage =weighted,threshold = 0.6\n","# # Num_clusters = 8, linkage =weighted,threshold = 0.4\n","# # Num_clusters = 16, linkage =weighted,threshold = 0.3\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYvkQ_h61jrg"},"outputs":[],"source":["# ####### ROBUST features #######\n","\n","# new_X_feat = [std_vol,p25_vol,median_vol,p75_vol,skew_vol,kurt_vol]\n","# X_feat = create_feature_matrix(new_X_feat)\n","\n","# feat_transform = FeatureTransformation()\n","# X_feat_normalized = feat_transform.feature_normalization(X_feat, transform_type=\"standard\")\n","\n","\n","# ## Select the BEST agglomerative hierarchical clustering for features : std_vol,p25_vol,median_vol,p75_vol,skew_vol,kurt_vol\n","\n","# #['single','complete','average','weighted','centroid','median','ward']\n","\n","# final_robust_sum_stats_target_vol_clusters = pd.DataFrame(columns=['stock_id','2_clusters', '4_clusters', '14_clusters','20_clusters','32_clusters', '60_clusters' ])\n","# final_robust_sum_stats_target_vol_clusters['stock_id'] = train['stock_id'].unique()\n","# c = [20,32,60]\n","# clustering_type = 'Agg. Hier. Clustering'\n","# metric = 'euclidean'\n","# for method in ['complete']:\n","#     Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","#     i=0\n","\n","#     for t in [1.5,1,0.5]:#np.arange(0.2,0.3,0.1):\n","#         # Calculate the cluster\n","#         labels = fcluster(Z, t, criterion='distance')\n","#         # Keep the indices to sort labels\n","#         final_robust_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#         i+=1\n","#         labels_order = np.argsort(labels)\n","\n","\n","#         plt.figure(figsize=(20,5))\n","#         plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#         dendrogram(Z,color_threshold=t)\n","#         plt.show()\n","\n","#         silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","# print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","# c = [2,4,14]\n","# metric = 'euclidean'\n","# for method in ['ward']:\n","#     Z = linkage(y=X_feat_normalized, method=method, metric=metric,optimal_ordering=True)\n","#     i=0\n","\n","#     for t in [20,10,3]:#np.arange(0.2,0.3,0.1):\n","#         # Calculate the cluster\n","#         labels = fcluster(Z, t, criterion='distance')\n","#         # Keep the indices to sort labels\n","#         final_robust_sum_stats_target_vol_clusters[str(c[i])+ '_clusters'] = labels\n","#         i+=1\n","#         labels_order = np.argsort(labels)\n","\n","\n","#         plt.figure(figsize=(20,5))\n","#         plt.title(\"Method: {}, threshold: {}\".format(method,t))\n","#         dendrogram(Z,color_threshold=t)\n","#         plt.show()\n","\n","#         silhouette_avg, num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone = plot_silhouette_scores(X_feat_normalized,labels,'euclidean',method,clustering_type)\n","\n","# print(f'Best silhouette score: {silhouette_avg} ,Best_silhouette_parameters: {metric,method,t,num_neg_silhouette_scores, min_silhouette_score,dev_from_mean_num_zone}')\n","\n","\n","\n","# # # others that can be tried are\n","# # Num_clusters = 10, linkage = ward,threshold = 0.7\n","# # Num_clusters = 15, linkage = ward,threshold = 0.5\n","# # Num_clusters = 5, linkage = average,threshold = 0.5\n","# # Num_clusters = 20, linkage = average,threshold = 0.25\n","# # Num_clusters = 4, linkage =weighted,threshold = 0.6\n","# # Num_clusters = 8, linkage =weighted,threshold = 0.4\n","# # Num_clusters = 16, linkage =weighted,threshold = 0.3\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og-S_FUfXERz"},"outputs":[],"source":["# def cluster_agg(x, clusters, agg_fun):\n","#     r = 0*x\n","\n","#     for k in range(np.max(clusters)+1):\n","#         z = agg_fun(x[:,clusters==k,:], 1, keepdims=True) # all stocks in a cluster are aggregated along stock_id axis\n","#         r[:,clusters==k,:] = np.repeat(z, repeats=int(np.sum(clusters==k)), axis=1) # repeat the aggregated value for each stock in the cluster\n","#     return r"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"navSnl7J4VzS"},"outputs":[],"source":["# final_sum_stats_target_vol_clusters = final_sum_stats_target_vol_clusters.drop(columns=['stock_id'])\n","# final_robust_sum_stats_target_vol_clusters = final_robust_sum_stats_target_vol_clusters.drop(columns=['stock_id'])\n","\n","# final_pear_corr_target_vol_clusters = final_pear_corr_target_vol_clusters.drop(columns=['stock_id'])\n","\n","\n","\n","# final_sum_stats_target_vol_dict = {}\n","# final_robust_sum_stats_target_vol_clusters_dict = {}\n","# final_pear_corr_target_vol_dict = {}\n","\n","# for c in final_sum_stats_target_vol_clusters.columns:\n","#     labels = final_sum_stats_target_vol_clusters[c]\n","#     labels = labels + 1 # shift to remove 0 labels\n","#     final_sum_stats_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","# for c in final_robust_sum_stats_target_vol_clusters.columns:\n","#     labels = final_robust_sum_stats_target_vol_clusters[c]\n","#     labels = labels + 1 # shift to remove 0 labels\n","#     final_robust_sum_stats_target_vol_clusters_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","# for c in final_pear_corr_target_vol_clusters.columns:\n","#     labels = final_pear_corr_target_vol_clusters[c]\n","#     labels = labels + 1 # shift to remove 0 labels\n","#     final_pear_corr_target_vol_dict[c] = cluster_agg(all_stocks_first_10_min_vol_df, labels, np.nanmean) # can try np.nanmedian\n","\n","\n","\n","\n","# final_sum_stats_target_vol_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","# final_sum_stats_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","\n","# sum_stats_df = merge_features_to_df(final_sum_stats_target_vol_dict, train,   [f for f in list(final_sum_stats_target_vol_dict.keys()) ]   )\n","# sum_stats_df.shape\n","\n","# final_robust_sum_stats_target_vol_clusters_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","# final_robust_sum_stats_target_vol_clusters_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","\n","# robust_sum_stats_df = merge_features_to_df(final_robust_sum_stats_target_vol_clusters_dict, train,   [f for f in list(final_robust_sum_stats_target_vol_clusters_dict.keys()) ]   )\n","# robust_sum_stats_df.shape\n","\n","\n","\n","# final_pear_corr_target_vol_dict['time_ids'] = np.repeat(all_uniq_time_ids, repeats=112, axis=1)[:,:,np.newaxis]\n","# final_pear_corr_target_vol_dict['stock_ids'] = np.repeat([unique_stock_ids], repeats=3830, axis=0)[:,:,np.newaxis]\n","\n","# pear_corr_df = merge_features_to_df(final_pear_corr_target_vol_dict, train,   [f for f in list(final_pear_corr_target_vol_dict.keys()) ]   )\n","# pear_corr_df.shape\n","\n","\n","\n","# final_sum_stats_target_vol_df = sum_stats_df[['4_clusters','10_clusters','16_clusters','30_clusters']]\n","# final_sum_stats_target_vol_df.rename(columns={'4_clusters':'target_vol_sum_stats_4_clusters','10_clusters':'target_vol_sum_stats_10_clusters',\n","#                                               '16_clusters':'target_vol_sum_stats_16_clusters','30_clusters':'target_vol_sum_stats_30_clusters'}, inplace=True)\n","\n","# final_robust_sum_stats_target_vol_df = robust_sum_stats_df[[\"2_clusters\",\"4_clusters\",\"14_clusters\",\"20_clusters\",\"32_clusters\",\"60_clusters\"]]\n","# final_robust_sum_stats_target_vol_df.rename(columns={'2_clusters':'target_vol_robust_sum_stats_2_clusters','4_clusters':'target_vol_robust_sum_stats_4_clusters',\n","#                                                       '14_clusters':'target_vol_robust_sum_stats_14_clusters','20_clusters':'target_vol_robust_sum_stats_20_clusters',\n","#                                                       '32_clusters':'target_vol_robust_sum_stats_32_clusters','60_clusters':'target_vol_robust_sum_stats_60_clusters'}, inplace=True)\n","\n","# final_pear_corr_target_vol_df = pear_corr_df[['3_clusters','49_clusters','90_clusters','10_clusters','26_clusters']]\n","# final_pear_corr_target_vol_df.rename(columns={'3_clusters':'target_vol_pcorr_3_clusters','49_clusters': 'target_vol_pcorr_49_clusters',\n","#                                                 '90_clusters':'target_vol_pcorr_90_clusters','10_clusters':'target_vol_pcorr_10_clusters',\n","#                                                 '26_clusters':'target_vol_pcorr_26_clusters'}, inplace=True)\n","\n","# train_feat_df = train_feat_df.merge(final_sum_stats_target_vol_df, left_index=True, right_index=True)\n","# del final_sum_stats_target_vol_df\n","# train_feat_df = train_feat_df.merge(final_robust_sum_stats_target_vol_df, left_index=True, right_index=True)\n","# del final_robust_sum_stats_target_vol_df\n","# train_feat_df = train_feat_df.merge(final_pear_corr_target_vol_df, left_index=True, right_index=True)\n","# del final_pear_corr_target_vol_df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSrq5pXe5Y9A"},"outputs":[],"source":["####### bk_price_size_min_max_range\n","\n","\n","# bk_price_size_min_max_range['st_min_max_bid_price1'].rename(columns={'min_bid_price':'min_bid_price1', 'max_bid_price':'max_bid_price1'}, inplace=True)\n","# bk_price_size_min_max_range['st_min_max_bid_price1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_min_max_bid_price1']], axis=1)\n","# train_feat_df.columns\n","\n","# bk_price_size_min_max_range['st_min_max_ask_price1'].rename(columns={'min_ask_price':'min_ask_price1', 'max_ask_price':'max_ask_price1'}, inplace=True)\n","# bk_price_size_min_max_range['st_min_max_ask_price1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_min_max_ask_price1']], axis=1)\n","# train_feat_df.columns\n","\n","# bk_price_size_min_max_range['st_min_max_bid_size1'].rename(columns={'min_bid_size':'min_bid_size1', 'max_bid_size':'max_bid_size1'}, inplace=True)\n","# bk_price_size_min_max_range['st_min_max_bid_size1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_min_max_bid_size1']], axis=1)\n","# train_feat_df.columns\n","\n","# bk_price_size_min_max_range['st_min_max_ask_size1'].rename(columns={'min_ask_size':'min_ask_size1', 'max_ask_size':'max_ask_size1'}, inplace=True)\n","# bk_price_size_min_max_range['st_min_max_ask_size1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_min_max_ask_size1']], axis=1)\n","\n","# bk_price_size_min_max_range['st_range_ask_price1'].rename(columns={'range_ask_price':'range_ask_price1'}, inplace=True)\n","# bk_price_size_min_max_range['st_range_ask_price1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_range_ask_price1']], axis=1)\n","\n","# bk_price_size_min_max_range['st_range_bid_price1'].rename(columns={'range_bid_price':'range_bid_price1'}, inplace=True)\n","# bk_price_size_min_max_range['st_range_bid_price1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_range_bid_price1']], axis=1)\n","\n","\n","# bk_price_size_min_max_range['st_range_ask_size1'].rename(columns={'range_ask_size':'range_ask_size1'}, inplace=True)\n","# bk_price_size_min_max_range['st_range_ask_size1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_range_ask_size1']], axis=1)\n","\n","# bk_price_size_min_max_range['st_range_bid_size1'].rename(columns={'range_bid_size':'range_bid_size1'}, inplace=True)\n","# bk_price_size_min_max_range['st_range_bid_size1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range['st_range_bid_size1']], axis=1)\n","# train_feat_df.columns\n","# del bk_price_size_min_max_range\n","\n","# ####### bk_price_size_sad\n","\n","# bk_price_size_sad['st_sad_ask_price1'].rename(columns={'sad_ask_price':'sad_ask_price1'}, inplace=True)\n","# bk_price_size_sad['st_sad_ask_price1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad['st_sad_ask_price1']], axis=1)\n","\n","# bk_price_size_sad['st_sad_ask_size1'].rename(columns={'sad_ask_size':'sad_ask_size1'}, inplace=True)\n","# bk_price_size_sad['st_sad_ask_size1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad['st_sad_ask_size1']], axis=1)\n","\n","# bk_price_size_sad['st_sad_bid_price1'].rename(columns={'sad_bid_price':'sad_bid_price1'}, inplace=True)\n","# bk_price_size_sad['st_sad_bid_price1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad['st_sad_bid_price1']], axis=1)\n","\n","# bk_price_size_sad['st_sad_bid_size1'].rename(columns={'sad_bid_size':'sad_bid_size1'}, inplace=True)\n","# bk_price_size_sad['st_sad_bid_size1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad['st_sad_bid_size1']], axis=1)\n","# del bk_price_size_sad\n","\n","\n","# ## bk_size_price_corr\n","\n","# bk_size_price_corr['st_bs_bp_corr1'].rename(columns={0:'bs_bp_corr1'}, inplace=True)\n","# bk_size_price_corr['st_bs_bp_corr1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr['st_bs_bp_corr1']], axis=1)\n","\n","# bk_size_price_corr['st_bs_as_corr1'].rename(columns={0:'bs_as_corr1'}, inplace=True)\n","# bk_size_price_corr['st_bs_as_corr1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr['st_bs_as_corr1']], axis=1)\n","\n","# bk_size_price_corr['st_bs_ap_corr1'].rename(columns={0:'bs_ap_corr1'}, inplace=True)\n","# bk_size_price_corr['st_bs_ap_corr1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr['st_bs_ap_corr1']], axis=1)\n","\n","# bk_size_price_corr['st_bp_as_corr1'].rename(columns={0:'bp_as_corr1'}, inplace=True)\n","# bk_size_price_corr['st_bp_as_corr1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr['st_bp_as_corr1']], axis=1)\n","\n","# bk_size_price_corr['st_bp_ap_corr1'].rename(columns={0:'bp_ap_corr1'}, inplace=True)\n","# bk_size_price_corr['st_bp_ap_corr1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr['st_bp_ap_corr1']], axis=1)\n","\n","# bk_size_price_corr['st_as_ap_corr1'].rename(columns={0:'as_ap_corr1'}, inplace=True)\n","# bk_size_price_corr['st_as_ap_corr1'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr['st_as_ap_corr1']], axis=1)\n","# del bk_size_price_corr\n","\n","\n","# ## trade_price_size_order_count_min_max_range\n","\n","# trade_price_size_order_count_min_max_range['st_min_price'].rename(columns={'min_price':'min_price1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_min_price'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_min_price']], axis=1)\n","\n","# trade_price_size_order_count_min_max_range['st_max_price'].rename(columns={'max_price':'max_price1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_max_price'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_max_price']], axis=1)\n","\n","# trade_price_size_order_count_min_max_range['st_max_size'].rename(columns={'min_size':'min_size1', 'max_size':'max_size1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_max_size'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_max_size']], axis=1)\n","\n","# trade_price_size_order_count_min_max_range['st_max_order_count'].rename(columns={'min_order_count':'min_order_count1', 'max_order_count':'max_order_count1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_max_order_count'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_max_order_count']], axis=1)\n","\n","# trade_price_size_order_count_min_max_range['st_range_price'].rename(columns={'range_price':'range_price1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_range_price'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_range_price']], axis=1)\n","\n","# trade_price_size_order_count_min_max_range['st_range_size'].rename(columns={'range_size':'range_size1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_range_size'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_range_size']], axis=1)\n","\n","# trade_price_size_order_count_min_max_range['st_range_order_count'].rename(columns={'range_order_count':'range_order_count1'}, inplace=True)\n","# trade_price_size_order_count_min_max_range['st_range_order_count'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_min_max_range['st_range_order_count']], axis=1)\n","# del trade_price_size_order_count_min_max_range\n","\n","\n","# ## trade_price_size_order_count_sad\n","\n","# trade_price_size_order_count_sad['st_sad_price'].rename(columns={'sad_price':'sad_price1'}, inplace=True)\n","# trade_price_size_order_count_sad['st_sad_price'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_sad['st_sad_price']], axis=1)\n","\n","# trade_price_size_order_count_sad['st_sad_size'].rename(columns={'sad_size':'sad_size1'}, inplace=True)\n","# trade_price_size_order_count_sad['st_sad_size'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_sad['st_sad_size']], axis=1)\n","\n","# trade_price_size_order_count_sad['st_sad_order_count'].rename(columns={'sad_order_count':'sad_order_count1'}, inplace=True)\n","# trade_price_size_order_count_sad['st_sad_order_count'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_sad['st_sad_order_count']], axis=1)\n","# del trade_price_size_order_count_sad\n","\n","# ## trade_price_size_order_count_corr\n","\n","# trade_price_size_order_count_corr['st_size_order_count_corr'].rename(columns={0:'size_order_count_corr1'}, inplace=True)\n","# trade_price_size_order_count_corr['st_size_order_count_corr'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, trade_price_size_order_count_corr['st_size_order_count_corr']], axis=1)\n","# del trade_price_size_order_count_corr\n","\n","\n","\n","\n","# ## apply summary statistics and pearson correlated cluster labels to stock ids, similar labels represent similarity in stocks\n","# ## These will be considered as categorical features\n","# ## we can also input our own feature clustering this way as well instead of taking median of the first_10_min_target_volatility for each cluster\n","\n","\n","# for c in final_sum_stats_target_vol_clusters.columns:\n","#     labels = final_sum_stats_target_vol_clusters[c]\n","#     train_feat_df['sum_stats_'+c+'_labels'] = train_feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","# del final_sum_stats_target_vol_clusters\n","\n","# for c in final_robust_sum_stats_target_vol_clusters.columns:\n","#     labels = final_robust_sum_stats_target_vol_clusters[c]\n","#     train_feat_df['robust_sum_stats_'+c+'_labels'] = train_feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","# del final_robust_sum_stats_target_vol_clusters\n","\n","# for c in final_pear_corr_target_vol_clusters.columns:\n","#     labels = final_pear_corr_target_vol_clusters[c]\n","#     train_feat_df['pear_corr_'+c+'_labels'] = train_feat_df['stock_id'].apply(lambda x: labels[int(np.argmax(np.array(unique_stock_ids)==x))  ])\n","# del final_pear_corr_target_vol_clusters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"279ZDxRN5lt8"},"outputs":[],"source":["\n","## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","# level = 2\n","# subset_paths = train_book_paths\n","\n","# train_target = pd.read_csv('train.csv')\n","\n","\n","# min_bid_price_n_target_vol_corr_coef = {}\n","# max_bid_price_n_target_vol_corr_coef = {}\n","# min_ask_price_n_target_vol_corr_coef = {}\n","# max_ask_price_n_target_vol_corr_coef = {}\n","# min_bid_size_n_target_vol_corr_coef = {}\n","# max_bid_size_n_target_vol_corr_coef = {}\n","# min_ask_size_n_target_vol_corr_coef = {}\n","# max_ask_size_n_target_vol_corr_coef = {}\n","# range_ask_price_n_target_vol_corr_coef = {}\n","# range_bid_price_n_target_vol_corr_coef = {}\n","# range_ask_size_n_target_vol_corr_coef = {}\n","# range_bid_size_n_target_vol_corr_coef = {}\n","\n","\n","# bk_price_size_min_max_range = {}\n","\n","# bk_price_size_min_max_range['st_min_max_bid_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_min_max_ask_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_min_max_bid_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_min_max_ask_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_ask_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_bid_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_ask_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_min_max_range['st_range_bid_size'+str(level)] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","\n","#         st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min','max']).rename(columns={'min':'min_bid_price','max':'max_bid_price'})\n","#         common_time_id_mmbp = np.intersect1d(st_min_max_bid_price.index.values, target_st['time_id'].values) # ensure that only common time ids in trade_train and target are used.\n","#         min_bid_price_n_target_vol_df = pd.DataFrame({'min_bid_price':st_min_max_bid_price.loc[common_time_id_mmbp,\"min_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_mmbp,\"target\"].values})\n","#         corr_coef_min_bid_price = plot_scatter_n_correlation(min_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         min_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_min_bid_price\n","#         max_bid_price_n_target_vol_df = pd.DataFrame({'max_bid_price':st_min_max_bid_price.loc[common_time_id_mmbp,\"max_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_mmbp,\"target\"].values})\n","#         corr_coef_max_bid_price = plot_scatter_n_correlation(max_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         max_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_max_bid_price\n","#         bk_price_size_min_max_range['st_min_max_bid_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_bid_price'+str(level)], st_min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","\n","#         st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min','max']).rename(columns={'min':'min_ask_price','max':'max_ask_price'})\n","#         common_time_id_mmap = np.intersect1d(st_min_max_ask_price.index.values, target_st['time_id'].values)\n","#         min_ask_price_n_target_vol_df = pd.DataFrame({'min_ask_price':st_min_max_ask_price.loc[common_time_id_mmap,\"min_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_mmap,\"target\"].values})\n","#         corr_coef_min_ask_price = plot_scatter_n_correlation(min_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         min_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_min_ask_price\n","#         max_ask_price_n_target_vol_df = pd.DataFrame({'max_ask_price':st_min_max_ask_price.loc[common_time_id_mmap,\"max_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_mmap,\"target\"].values})\n","#         corr_coef_max_ask_price = plot_scatter_n_correlation(max_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         max_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_max_ask_price\n","#         bk_price_size_min_max_range['st_min_max_ask_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_ask_price'+str(level)], st_min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min','max']).rename(columns={'min':'min_bid_size','max':'max_bid_size'})\n","#         common_time_id_mmbz = np.intersect1d(st_min_max_bid_size.index.values, target_st['time_id'].values)\n","#         min_bid_size_n_target_vol_df = pd.DataFrame({'min_bid_size':st_min_max_bid_size.loc[common_time_id_mmbz,\"min_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_mmbz,\"target\"].values})\n","#         corr_coef_min_bid_size = plot_scatter_n_correlation(min_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         min_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_min_bid_size\n","#         max_bid_size_n_target_vol_df = pd.DataFrame({'max_bid_size':st_min_max_bid_size.loc[common_time_id_mmbz,\"max_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_mmbz,\"target\"].values})\n","#         corr_coef_max_bid_size = plot_scatter_n_correlation(max_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         max_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_max_bid_size\n","#         bk_price_size_min_max_range['st_min_max_bid_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_bid_size'+str(level)], st_min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min','max']).rename(columns={'min':'min_ask_size','max':'max_ask_size'})\n","#         common_time_id_mmas = np.intersect1d(st_min_max_ask_size.index.values, target_st['time_id'].values)\n","#         min_ask_size_n_target_vol_df = pd.DataFrame({'min_ask_size':st_min_max_ask_size.loc[common_time_id_mmas,\"min_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_mmas,\"target\"].values})\n","#         corr_coef_min_ask_size = plot_scatter_n_correlation(min_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         min_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_min_ask_size\n","#         max_ask_size_n_target_vol_df = pd.DataFrame({'max_ask_size':st_min_max_ask_size.loc[common_time_id_mmas,\"max_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_mmas,\"target\"].values})\n","#         corr_coef_max_ask_size = plot_scatter_n_correlation(max_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         max_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_max_ask_size\n","#         bk_price_size_min_max_range['st_min_max_ask_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_min_max_ask_size'+str(level)], st_min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price:[my_range_price]}).rename(columns={ask_price:'range_ask_price'})\n","#         st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","#         common_time_id_rap = np.intersect1d(st_range_ask_price.index.values, target_st['time_id'].values)\n","#         range_ask_price_n_target_vol_df = pd.DataFrame({'range_ask_price':st_range_ask_price.loc[common_time_id_rap,\"range_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_rap,\"target\"].values})\n","#         corr_coef_range_ask_price = plot_scatter_n_correlation(range_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         range_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_range_ask_price\n","#         bk_price_size_min_max_range['st_range_ask_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_ask_price'+str(level)], st_range_ask_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price:[my_range_price]}).rename(columns={bid_price:'range_bid_price'})\n","#         st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","#         common_time_id_rbp = np.intersect1d(st_range_bid_price.index.values, target_st['time_id'].values)\n","#         range_bid_price_n_target_vol_df = pd.DataFrame({'range_bid_price':st_range_bid_price.loc[common_time_id_rbp,\"range_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_rbp,\"target\"].values})\n","#         corr_coef_range_bid_price = plot_scatter_n_correlation(range_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         range_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_range_bid_price\n","#         bk_price_size_min_max_range['st_range_bid_price'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_bid_price'+str(level)], st_range_bid_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size:[my_range_price]}).rename(columns={ask_size:'range_ask_size'})\n","#         st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","#         common_time_id_ras = np.intersect1d(st_range_ask_size.index.values, target_st['time_id'].values)\n","#         range_ask_size_n_target_vol_df = pd.DataFrame({'range_ask_size':st_range_ask_size.loc[common_time_id_ras,\"range_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_ras,\"target\"].values})\n","#         corr_coef_range_ask_size = plot_scatter_n_correlation(range_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         range_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_range_ask_size\n","#         bk_price_size_min_max_range['st_range_ask_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_ask_size'+str(level)], st_range_ask_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size:[my_range_price]}).rename(columns={bid_size:'range_bid_size'})\n","#         st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","#         common_time_id_rbs = np.intersect1d(st_range_bid_size.index.values, target_st['time_id'].values)\n","#         range_bid_size_n_target_vol_df = pd.DataFrame({'range_bid_size':st_range_bid_size.loc[common_time_id_rbs,\"range_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_rbs,\"target\"].values})\n","#         corr_coef_range_bid_size = plot_scatter_n_correlation(range_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         range_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_range_bid_size\n","#         bk_price_size_min_max_range['st_range_bid_size'+str(level)] = pd.concat([ bk_price_size_min_max_range['st_range_bid_size'+str(level)], st_range_bid_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","# bk_price_size_min_max_range_2 = bk_price_size_min_max_range\n","\n","\n","\n","\n","\n","\n","## 7a) - 7e) Check if minimum/maximum/range of bidsize1/bid_price1 and asksize1/ask_price1 in a time_id correlated with target realized volatitlity for the same time_id?\n","\n","# level = 2\n","# subset_paths = train_book_paths\n","\n","# bid_price = f\"bid_price{level}\"\n","# ask_price = f\"ask_price{level}\"\n","# bid_size = f\"bid_size{level}\"\n","# ask_size = f\"ask_size{level}\"\n","\n","\n","# train_target = pd.read_csv('train.csv')\n","\n","# import pandas as pd\n","# import numpy as np\n","# from joblib import Parallel, delayed\n","\n","# def process_stock_data(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = target_st['time_id']\n","\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     result = {}\n","\n","#     # Min-Max Bid Price\n","#     st_min_max_bid_price = book_train_st.groupby(by='time_id')[bid_price].agg(['min', 'max']).rename(columns={'min': 'min_bid_price', 'max': 'max_bid_price'})\n","#     result['st_min_max_bid_price'] = st_min_max_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Min-Max Ask Price\n","#     st_min_max_ask_price = book_train_st.groupby(by='time_id')[ask_price].agg(['min', 'max']).rename(columns={'min': 'min_ask_price', 'max': 'max_ask_price'})\n","#     result['st_min_max_ask_price'] = st_min_max_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Min-Max Bid Size\n","#     st_min_max_bid_size = book_train_st.groupby(by='time_id')[bid_size].agg(['min', 'max']).rename(columns={'min': 'min_bid_size', 'max': 'max_bid_size'})\n","#     result['st_min_max_bid_size'] = st_min_max_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Min-Max Ask Size\n","#     st_min_max_ask_size = book_train_st.groupby(by='time_id')[ask_size].agg(['min', 'max']).rename(columns={'min': 'min_ask_size', 'max': 'max_ask_size'})\n","#     result['st_min_max_ask_size'] = st_min_max_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Range Ask Price\n","#     st_range_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_range_price]}).rename(columns={ask_price: 'range_ask_price'})\n","#     st_range_ask_price.columns = st_range_ask_price.columns.droplevel(1)\n","#     result['st_range_ask_price'] = st_range_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Range Bid Price\n","#     st_range_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_range_price]}).rename(columns={bid_price: 'range_bid_price'})\n","#     st_range_bid_price.columns = st_range_bid_price.columns.droplevel(1)\n","#     result['st_range_bid_price'] = st_range_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Range Ask Size\n","#     st_range_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_range_price]}).rename(columns={ask_size: 'range_ask_size'})\n","#     st_range_ask_size.columns = st_range_ask_size.columns.droplevel(1)\n","#     result['st_range_ask_size'] = st_range_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     # Range Bid Size\n","#     st_range_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_range_price]}).rename(columns={bid_size: 'range_bid_size'})\n","#     st_range_bid_size.columns = st_range_bid_size.columns.droplevel(1)\n","#     result['st_range_bid_size'] = st_range_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     return result\n","\n","# # Parallel processing\n","# results = Parallel(n_jobs=-1)(delayed(process_stock_data)(path, level, train_target, bid_price, ask_price, bid_size, ask_size, my_range_price) for path in subset_paths)\n","\n","# # Combine the results\n","# bk_price_size_min_max_range_2 = {\n","#     'st_min_max_bid_price'+str(level): pd.concat([res['st_min_max_bid_price'] for res in results if res is not None], axis=0),\n","#     'st_min_max_ask_price'+str(level): pd.concat([res['st_min_max_ask_price'] for res in results if res is not None], axis=0),\n","#     'st_min_max_bid_size'+str(level): pd.concat([res['st_min_max_bid_size'] for res in results if res is not None], axis=0),\n","#     'st_min_max_ask_size'+str(level): pd.concat([res['st_min_max_ask_size'] for res in results if res is not None], axis=0),\n","#     'st_range_ask_price'+str(level): pd.concat([res['st_range_ask_price'] for res in results if res is not None], axis=0),\n","#     'st_range_bid_price'+str(level): pd.concat([res['st_range_bid_price'] for res in results if res is not None], axis=0),\n","#     'st_range_ask_size'+str(level): pd.concat([res['st_range_ask_size'] for res in results if res is not None], axis=0),\n","#     'st_range_bid_size'+str(level): pd.concat([res['st_range_bid_size'] for res in results if res is not None], axis=0),\n","# }\n","\n","\n","\n","# ####### bk_price_size_min_max_range_2\n","\n","# bk_price_size_min_max_range_2['st_min_max_bid_price2'].rename(columns={'min_bid_price':'min_bid_price2', 'max_bid_price':'max_bid_price2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_min_max_bid_price2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_min_max_bid_price2']], axis=1)\n","\n","# bk_price_size_min_max_range_2['st_min_max_ask_price2'].rename(columns={'min_ask_price':'min_ask_price2', 'max_ask_price':'max_ask_price2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_min_max_ask_price2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_min_max_ask_price2']], axis=1)\n","\n","# bk_price_size_min_max_range_2['st_min_max_bid_size2'].rename(columns={'min_bid_size':'min_bid_size2', 'max_bid_size':'max_bid_size2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_min_max_bid_size2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_min_max_bid_size2']], axis=1)\n","\n","# bk_price_size_min_max_range_2['st_min_max_ask_size2'].rename(columns={'min_ask_size':'min_ask_size2', 'max_ask_size':'max_ask_size2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_min_max_ask_size2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_min_max_ask_size2']], axis=1)\n","\n","# bk_price_size_min_max_range_2['st_range_ask_price2'].rename(columns={'range_ask_price':'range_ask_price2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_range_ask_price2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_range_ask_price2']], axis=1)\n","\n","# bk_price_size_min_max_range_2['st_range_bid_price2'].rename(columns={'range_bid_price':'range_bid_price2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_range_bid_price2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_range_bid_price2']], axis=1)\n","\n","# bk_price_size_min_max_range_2['st_range_ask_size2'].rename(columns={'range_ask_size':'range_ask_size2'}, inplace=True)\n","# bk_price_size_min_max_range_2['st_range_ask_size2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_min_max_range_2['st_range_ask_size2']], axis=1)\n","# del bk_price_size_min_max_range_2\n","\n","\n","\n","\n","\n","\n","\n","\n","## 7f) - 7i) Check if  the sum of absolute differences is correlated with target\n","\n","# level = 2\n","# subset_paths = train_book_paths\n","\n","\n","# sum_abs_dif_ask_price_n_target_vol_corr_coef = {}\n","# sum_abs_dif_ask_size_n_target_vol_corr_coef = {}\n","# sum_abs_dif_bid_price_n_target_vol_corr_coef = {}\n","# sum_abs_dif_bid_size_n_target_vol_corr_coef = {}\n","\n","# bk_price_size_sad = {}\n","\n","# bk_price_size_sad['st_sad_ask_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_sad['st_sad_ask_size'+str(level)] = pd.DataFrame()\n","# bk_price_size_sad['st_sad_bid_price'+str(level)] = pd.DataFrame()\n","# bk_price_size_sad['st_sad_bid_size'+str(level)] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","\n","#         st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price:[my_sum_abs_diff]}).rename(columns={ask_price:'sad_ask_price'})\n","#         st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","#         common_time_id_sadap = np.intersect1d(st_sad_ask_price.index.values, target_st['time_id'].values)\n","#         sad_ask_price_n_target_vol_df = pd.DataFrame({'sad_ask_price':st_sad_ask_price.loc[common_time_id_sadap,\"sad_ask_price\"].values , 'target_vol':target_st.loc[common_time_id_sadap,\"target\"].values})\n","#         corr_coef_sad_ask_price = plot_scatter_n_correlation(sad_ask_price_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_ask_price_n_target_vol_corr_coef[st_id] = corr_coef_sad_ask_price\n","#         bk_price_size_sad['st_sad_ask_price'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_ask_price'+str(level)], st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size:[my_sum_abs_diff]}).rename(columns={ask_size:'sad_ask_size'})\n","#         st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","#         common_time_id_sadas = np.intersect1d(st_sad_ask_size.index.values, target_st['time_id'].values)\n","#         sad_ask_size_n_target_vol_df = pd.DataFrame({'sad_ask_size':st_sad_ask_size.loc[common_time_id_sadas,\"sad_ask_size\"].values , 'target_vol':target_st.loc[common_time_id_sadas,\"target\"].values})\n","#         corr_coef_sad_ask_size = plot_scatter_n_correlation(sad_ask_size_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_ask_size_n_target_vol_corr_coef[st_id] = corr_coef_sad_ask_size\n","#         bk_price_size_sad['st_sad_ask_size'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_ask_size'+str(level)], st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price:[my_sum_abs_diff]}).rename(columns={bid_price:'sad_bid_price'})\n","#         st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","#         common_time_id_sadbp = np.intersect1d(st_sad_bid_price.index.values, target_st['time_id'].values)\n","#         sad_bid_price_n_target_vol_df = pd.DataFrame({'sad_bid_price':st_sad_bid_price.loc[common_time_id_sadbp,\"sad_bid_price\"].values , 'target_vol':target_st.loc[common_time_id_sadbp,\"target\"].values})\n","#         corr_coef_sad_bid_price = plot_scatter_n_correlation(sad_bid_price_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_bid_price_n_target_vol_corr_coef[st_id] = corr_coef_sad_bid_price\n","#         bk_price_size_sad['st_sad_bid_price'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_bid_price'+str(level)], st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size:[my_sum_abs_diff]}).rename(columns={bid_size:'sad_bid_size'})\n","#         st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","#         common_time_id_sadbs = np.intersect1d(st_sad_bid_size.index.values, target_st['time_id'].values)\n","#         sad_bid_size_n_target_vol_df = pd.DataFrame({'sad_bid_size':st_sad_bid_size.loc[common_time_id_sadbs,\"sad_bid_size\"].values , 'target_vol':target_st.loc[common_time_id_sadbs,\"target\"].values})\n","#         corr_coef_sad_bid_size = plot_scatter_n_correlation(sad_bid_size_n_target_vol_df,st_id,method=corr_method)\n","#         sum_abs_dif_bid_size_n_target_vol_corr_coef[st_id] = corr_coef_sad_bid_size\n","#         bk_price_size_sad['st_sad_bid_size'+str(level)] = pd.concat([ bk_price_size_sad['st_sad_bid_size'+str(level)], st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","# bk_price_size_sad_2 = bk_price_size_sad\n","\n","\n","\n","# level = 2\n","# subset_paths = train_book_paths\n","\n","# bid_price = f\"bid_price{level}\"\n","# ask_price = f\"ask_price{level}\"\n","# bid_size = f\"bid_size{level}\"\n","# ask_size = f\"ask_size{level}\"\n","\n","\n","# from joblib import Parallel, delayed\n","# import pandas as pd\n","\n","# # Function to process each stock ID\n","# def process_stock(path, level):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = target_st[\"time_id\"]\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     # Aggregate features\n","#     st_sad_ask_price = book_train_st.groupby(by='time_id').agg({ask_price: [my_sum_abs_diff]}).rename(columns={ask_price: 'sad_ask_price'})\n","#     st_sad_ask_price.columns = st_sad_ask_price.columns.droplevel(1)\n","#     st_sad_ask_price = st_sad_ask_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     st_sad_ask_size = book_train_st.groupby(by='time_id').agg({ask_size: [my_sum_abs_diff]}).rename(columns={ask_size: 'sad_ask_size'})\n","#     st_sad_ask_size.columns = st_sad_ask_size.columns.droplevel(1)\n","#     st_sad_ask_size = st_sad_ask_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     st_sad_bid_price = book_train_st.groupby(by='time_id').agg({bid_price: [my_sum_abs_diff]}).rename(columns={bid_price: 'sad_bid_price'})\n","#     st_sad_bid_price.columns = st_sad_bid_price.columns.droplevel(1)\n","#     st_sad_bid_price = st_sad_bid_price.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     st_sad_bid_size = book_train_st.groupby(by='time_id').agg({bid_size: [my_sum_abs_diff]}).rename(columns={bid_size: 'sad_bid_size'})\n","#     st_sad_bid_size.columns = st_sad_bid_size.columns.droplevel(1)\n","#     st_sad_bid_size = st_sad_bid_size.reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     return (st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size)\n","\n","\n","# # Run the function in parallel\n","# results = Parallel(n_jobs=-1)(delayed(process_stock)(path, level) for path in subset_paths)\n","\n","# # Initialize empty DataFrames to store the results\n","# bk_price_size_sad_2 = {\n","#     'st_sad_ask_price' + str(level): pd.DataFrame(),\n","#     'st_sad_ask_size' + str(level): pd.DataFrame(),\n","#     'st_sad_bid_price' + str(level): pd.DataFrame(),\n","#     'st_sad_bid_size' + str(level): pd.DataFrame()\n","# }\n","\n","# # Collect results into the final DataFrames\n","# for result in results:\n","#     if result is not None:\n","#         st_sad_ask_price, st_sad_ask_size, st_sad_bid_price, st_sad_bid_size = result\n","#         bk_price_size_sad_2['st_sad_ask_price' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_ask_price' + str(level)], st_sad_ask_price], axis=0)\n","#         bk_price_size_sad_2['st_sad_ask_size' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_ask_size' + str(level)], st_sad_ask_size], axis=0)\n","#         bk_price_size_sad_2['st_sad_bid_price' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_bid_price' + str(level)], st_sad_bid_price], axis=0)\n","#         bk_price_size_sad_2['st_sad_bid_size' + str(level)] = pd.concat([bk_price_size_sad_2['st_sad_bid_size' + str(level)], st_sad_bid_size], axis=0)\n","\n","# # The result is stored in bk_price_size_sad\n","\n","\n","\n","# ####### bk_price_size_sad_2\n","\n","# bk_price_size_sad_2['st_sad_ask_price2'].rename(columns={'sad_ask_price':'sad_ask_price2'}, inplace=True)\n","# bk_price_size_sad_2['st_sad_ask_price2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad_2['st_sad_ask_price2']], axis=1)\n","\n","# bk_price_size_sad_2['st_sad_ask_size2'].rename(columns={'sad_ask_size':'sad_ask_size2'}, inplace=True)\n","# bk_price_size_sad_2['st_sad_ask_size2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad_2['st_sad_ask_size2']], axis=1)\n","\n","# bk_price_size_sad_2['st_sad_bid_price2'].rename(columns={'sad_bid_price':'sad_bid_price2'}, inplace=True)\n","# bk_price_size_sad_2['st_sad_bid_price2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_price_size_sad_2['st_sad_bid_price2']], axis=1)\n","# del bk_price_size_sad_2\n","\n","\n","\n","\n","\n","\n","\n","## 7g) - 7j) Check if the correlation of any pair of bidsize1,bid_price1,asksize1,ask_price1 is correlated with target realized volatitlity for all the time_ids?\n","\n","# level=2\n","# subset_paths = train_book_paths\n","\n","# train_target = pd.read_csv('train.csv')\n","\n","\n","# bs_bp_corr_n_target_vol_corr_coef = {}\n","# bs_as_corr_n_target_vol_corr_coef = {}\n","# bs_ap_corr_n_target_vol_corr_coef = {}\n","# bp_as_corr_n_target_vol_corr_coef = {}\n","# bp_ap_corr_n_target_vol_corr_coef = {}\n","# as_ap_corr_n_target_vol_corr_coef = {}\n","\n","# bk_size_price_corr = {}\n","\n","# bk_size_price_corr['st_bs_bp_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bs_as_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bs_ap_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bp_as_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_bp_ap_corr'+str(level)] = pd.DataFrame()\n","# bk_size_price_corr['st_as_ap_corr'+str(level)] = pd.DataFrame()\n","\n","\n","# for path in subset_paths:\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","#     #st_id = int(path.split('\\\\')[1].split('_')[1].split('=')[1])\n","#     # stock ids in [103,18,31,37] have DIFFERENT length of time_id compared to target so we exclude them for now\n","#     if st_id != -1:#not in [103,18,31,37,110]: # select stock id here\n","#         target_st = train_target[train_target['stock_id']==st_id]\n","#         target_st.index = [target_st[\"time_id\"]]\n","#         book_train_st = pd.read_parquet(path)\n","\n","#         st_bs_bp_corr = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1]\n","#         st_bs_bp_corr.index = st_bs_bp_corr.index.droplevel(1)\n","#         common_time_id_bs_bp = np.intersect1d(st_bs_bp_corr.index.values, target_st['time_id'].values)\n","#         bs_bp_corr_n_target_vol_df = pd.DataFrame({'bs_bp_corr':st_bs_bp_corr.loc[common_time_id_bs_bp].values , 'target_vol':target_st.loc[common_time_id_bs_bp,\"target\"].values})\n","#         corr_coef_bs_bp = plot_scatter_n_correlation(bs_bp_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bs_bp_corr_n_target_vol_corr_coef[st_id] = corr_coef_bs_bp\n","#         bk_size_price_corr['st_bs_bp_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bs_bp_corr'+str(level)], st_bs_bp_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bs_as_corr = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1]\n","#         st_bs_as_corr.index = st_bs_as_corr.index.droplevel(1)\n","#         common_time_id_bs_as = np.intersect1d(st_bs_as_corr.index.values, target_st['time_id'].values)\n","#         bs_as_corr_n_target_vol_df = pd.DataFrame({'bs_as_corr':st_bs_as_corr.loc[common_time_id_bs_as].values , 'target_vol':target_st.loc[common_time_id_bs_as,\"target\"].values})\n","#         corr_coef_bs_as = plot_scatter_n_correlation(bs_as_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bs_as_corr_n_target_vol_corr_coef[st_id] = corr_coef_bs_as\n","#         bk_size_price_corr['st_bs_as_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bs_as_corr'+str(level)], st_bs_as_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bs_ap_corr = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1]\n","#         st_bs_ap_corr.index = st_bs_ap_corr.index.droplevel(1)\n","#         common_time_id_bs_ap = np.intersect1d(st_bs_ap_corr.index.values, target_st['time_id'].values)\n","#         bs_ap_corr_n_target_vol_df = pd.DataFrame({'bs_ap_corr':st_bs_ap_corr.loc[common_time_id_bs_ap].values , 'target_vol':target_st.loc[common_time_id_bs_ap,\"target\"].values})\n","#         corr_coef_bs_ap = plot_scatter_n_correlation(bs_ap_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bs_ap_corr_n_target_vol_corr_coef[st_id] = corr_coef_bs_ap\n","#         bk_size_price_corr['st_bs_ap_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bs_ap_corr'+str(level)], st_bs_ap_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bp_as_corr = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1]\n","#         st_bp_as_corr.index = st_bp_as_corr.index.droplevel(1)\n","#         common_time_id_bp_as = np.intersect1d(st_bp_as_corr.index.values, target_st['time_id'].values)\n","#         bp_as_corr_n_target_vol_df = pd.DataFrame({'bp_as_corr':st_bp_as_corr.loc[common_time_id_bp_as].values , 'target_vol':target_st.loc[common_time_id_bp_as,\"target\"].values})\n","#         corr_coef_bp_as = plot_scatter_n_correlation(bp_as_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bp_as_corr_n_target_vol_corr_coef[st_id] = corr_coef_bp_as\n","#         bk_size_price_corr['st_bp_as_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bp_as_corr'+str(level)], st_bp_as_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_bp_ap_corr = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1]\n","#         st_bp_ap_corr.index = st_bp_ap_corr.index.droplevel(1)\n","#         common_time_id_bp_ap = np.intersect1d(st_bp_ap_corr.index.values, target_st['time_id'].values)\n","#         bp_ap_corr_n_target_vol_df = pd.DataFrame({'bp_ap_corr':st_bp_ap_corr.loc[common_time_id_bp_ap].values , 'target_vol':target_st.loc[common_time_id_bp_ap,\"target\"].values})\n","#         corr_coef_bp_ap = plot_scatter_n_correlation(bp_ap_corr_n_target_vol_df,st_id,method=corr_method)\n","#         bp_ap_corr_n_target_vol_corr_coef[st_id] = corr_coef_bp_ap\n","#         bk_size_price_corr['st_bp_ap_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_bp_ap_corr'+str(level)], st_bp_ap_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","#         st_as_ap_corr = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1]\n","#         st_as_ap_corr.index = st_as_ap_corr.index.droplevel(1)\n","#         common_time_id_as_ap = np.intersect1d(st_as_ap_corr.index.values, target_st['time_id'].values)\n","#         as_ap_corr_n_target_vol_df = pd.DataFrame({'as_ap_corr':st_as_ap_corr.loc[common_time_id_as_ap].values , 'target_vol':target_st.loc[common_time_id_as_ap,\"target\"].values})\n","#         corr_coef_as_ap = plot_scatter_n_correlation(as_ap_corr_n_target_vol_df,st_id,method=corr_method)\n","#         as_ap_corr_n_target_vol_corr_coef[st_id] = corr_coef_as_ap\n","#         bk_size_price_corr['st_as_ap_corr'+str(level)] = pd.concat([ bk_size_price_corr['st_as_ap_corr'+str(level)], st_as_ap_corr.reindex(target_st['time_id'].values).ffill().bfill() ],axis=0)\n","\n","# bk_size_price_corr_2 = bk_size_price_corr\n","\n","\n","\n","\n","\n","\n","\n","# level=2\n","# subset_paths = train_book_paths\n","\n","# bid_price = f\"bid_price{level}\"\n","# ask_price = f\"ask_price{level}\"\n","# bid_size = f\"bid_size{level}\"\n","# ask_size = f\"ask_size{level}\"\n","\n","\n","# train_target = pd.read_csv('train.csv')\n","\n","# from joblib import Parallel, delayed\n","# import pandas as pd\n","\n","# # Function to process each stock ID\n","# def process_stock(path, level):\n","#     st_id = int(path.split('/')[1].split('_')[1].split('=')[1])\n","\n","#     target_st = train_target[train_target['stock_id'] == st_id]\n","#     target_st.index = target_st[\"time_id\"]\n","#     book_train_st = pd.read_parquet(path)\n","\n","#     # Calculate correlations and drop the multi-level index\n","#     corr_dict = {}\n","#     corr_dict['st_bs_bp_corr'] = book_train_st.groupby('time_id')[[bid_size, bid_price]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bs_as_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bs_ap_corr'] = book_train_st.groupby('time_id')[[bid_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bp_as_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_size]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_bp_ap_corr'] = book_train_st.groupby('time_id')[[bid_price, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","#     corr_dict['st_as_ap_corr'] = book_train_st.groupby('time_id')[[ask_size, ask_price]].corr().iloc[0::2, -1].droplevel(1)\n","\n","#     # Reindex with target_st time_ids and apply forward and backward fill\n","#     for key in corr_dict.keys():\n","#         corr_dict[key] = corr_dict[key].reindex(target_st['time_id'].values).ffill().bfill()\n","\n","#     return corr_dict\n","\n","\n","\n","# # Run the function in parallel\n","# results = Parallel(n_jobs=-1)(delayed(process_stock)(path, level) for path in subset_paths)\n","\n","# # Initialize empty DataFrames to store the results\n","# bk_size_price_corr_2 = {\n","#     'st_bs_bp_corr' + str(level): pd.DataFrame(),\n","#     'st_bs_as_corr' + str(level): pd.DataFrame(),\n","#     'st_bs_ap_corr' + str(level): pd.DataFrame(),\n","#     'st_bp_as_corr' + str(level): pd.DataFrame(),\n","#     'st_bp_ap_corr' + str(level): pd.DataFrame(),\n","#     'st_as_ap_corr' + str(level): pd.DataFrame()\n","# }\n","\n","# # Collect results into the final DataFrames\n","# for result in results:\n","#     if result is not None:\n","#         for key in result.keys():\n","#             bk_size_price_corr_2[key + str(level)] = pd.concat([bk_size_price_corr_2[key + str(level)], result[key]], axis=0)\n","\n","# The result is stored in bk_size_price_corr\n","\n","\n","####### bk_size_price_corr_2\n","\n","# bk_size_price_corr_2['st_bs_bp_corr2'].rename(columns={0:'bs_bp_corr2'}, inplace=True)\n","# bk_size_price_corr_2['st_bs_bp_corr2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr_2['st_bs_bp_corr2']], axis=1)\n","\n","# bk_size_price_corr_2['st_bs_as_corr2'].rename(columns={0:'bs_as_corr2'}, inplace=True)\n","# bk_size_price_corr_2['st_bs_as_corr2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr_2['st_bs_as_corr2']], axis=1)\n","\n","# bk_size_price_corr_2['st_bs_ap_corr2'].rename(columns={0:'bs_ap_corr2'}, inplace=True)\n","# bk_size_price_corr_2['st_bs_ap_corr2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr_2['st_bs_ap_corr2']], axis=1)\n","\n","# bk_size_price_corr_2['st_bp_as_corr2'].rename(columns={0:'bp_as_corr2'}, inplace=True)\n","# bk_size_price_corr_2['st_bp_as_corr2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr_2['st_bp_as_corr2']], axis=1)\n","\n","# bk_size_price_corr_2['st_bp_ap_corr2'].rename(columns={0:'bp_ap_corr2'}, inplace=True)\n","# bk_size_price_corr_2['st_bp_ap_corr2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr_2['st_bp_ap_corr2']], axis=1)\n","\n","# bk_size_price_corr_2['st_as_ap_corr2'].rename(columns={0:'as_ap_corr2'}, inplace=True)\n","# bk_size_price_corr_2['st_as_ap_corr2'].reset_index(drop=True, inplace=True)\n","# train_feat_df = pd.concat([train_feat_df, bk_size_price_corr_2['st_as_ap_corr2']], axis=1)\n","# del bk_size_price_corr_2\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqiXlsHb5cSF"},"outputs":[],"source":["# # check for duplicate columns in train_feat_df\n","# # max_price1 and min_price1 were added twice so we check for duplicate columns\n","\n","# uniq_cols = np.unique(train_feat_df.columns)\n","\n","# ## check for missing values\n","# (train_feat_df.columns).shape\n","\n","# twice_ctr = {} # twice occurance counter\n","# for c in uniq_cols:\n","#     twice_ctr[c] = 0\n","\n","# for c in train_feat_df.columns:\n","#     twice_ctr[c]+=1\n","\n","# idx = np.where(np.array(list(twice_ctr.values())) == 2) ## col 41 and 54 in uniq_cols are repeated\n","\n","# np.array(list(twice_ctr.keys()))[41]\n","# np.array(list(twice_ctr.keys()))[54]\n","\n","# # remove duplicate columns\n","# train_feat_df = train_feat_df.loc[:,~train_feat_df.columns.duplicated()].copy()\n","\n","# #train_feat_df.drop(columns=['stock_ids'], inplace=True) # drop this redundant column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnjIimlQe2nW"},"outputs":[],"source":["# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_full.pkl','wb') as fb:\n","#       pickle.dump(train_feat_df, fb)\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_full.pkl','rb') as fb:\n","#      train_feat_df = pickle.load( fb)"]},{"cell_type":"markdown","metadata":{"id":"291WnKnD13mh"},"source":["# Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNBzT9Yw188t"},"outputs":[],"source":["# #### Check that no two columns have the same values\n","# for c in train_feat_df.columns:\n","#     ctr=0\n","#     for c1 in train_feat_df.columns:\n","#         if np.array_equal(train_feat_df[c], train_feat_df[c1]) and c!=c1:\n","#             ctr+=1\n","#             print(c, c1)\n","#         # if (ctr>1):\n","#         #     print(c, c1)\n","\n","# ## Drop the columns that have the same values\n","# train_feat_df.drop(columns=['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio\", \"min_price\", \"max_price\", \"target_x\",\"target_y\" ], inplace=True)\n","# train_feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TwHKJSw2C6A"},"outputs":[],"source":["# # Check for NULL and INF values in train_feat_df\n","\n","# nan_cols = []\n","# pos_inf_cols = []\n","# neg_inf_cols = []\n","\n","# float_cols = [ c for c in train_feat_df.columns if train_feat_df[c].dtype != 'category']\n","\n","# for c in float_cols:\n","#     if np.isinf(train_feat_df[c]).any():\n","#         print(\"pos INF: \",c,': ', np.isinf(train_feat_df[c]).sum())\n","#         pos_inf_cols.append(c)\n","\n","#     if np.isneginf(train_feat_df[c]).any():\n","#         print(\"neg INF: \",c,': ', np.isneginf(train_feat_df[c]).sum())\n","#         neg_inf_cols.append(c)\n","\n","#     if np.isnan(train_feat_df[c]).any():\n","#         print(\"NaN: \", c ,': ', np.isnan(train_feat_df[c]).sum())\n","#         nan_cols.append(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQZsBlyo2TK6"},"outputs":[],"source":["# ####### Drop any feature with large number of nan, positive and negative infinity\n","\n","# for c in train_feat_df.columns:\n","#     if  np.isinf(train_feat_df[c]).sum() > 1000 or np.isneginf(train_feat_df[c]).sum() > 1000 or np.isnan(train_feat_df[c]).sum() > 1000:\n","#         train_feat_df.drop(columns=[c], inplace=True)\n","#         print(c)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nW5moSlu2VCn"},"outputs":[],"source":["# ####### HANDLING NULL and INF VALUES ########\n","\n","# ### drop columns with very high fraction of NaN values\n","# # train_feat_df.drop(columns=[\"v1proj_29_15_std\"], inplace=True)\n","\n","# # ### groupby stock id and fill NaN values with median of the stock id or leave it the same\n","# # for c in nan_cols:\n","# #     train_feat_df[c] = train_feat_df.groupby('stock_id')[c].transform(lambda x: x.fillna(np.nanmedian(x)))\n","\n","# pos_inf_cols.remove('wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio')\n","# neg_inf_cols.remove('wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio')\n","\n","\n","# ### groupby stock id and fill positive infinity values with max of the stock id or 1e8\n","# for c in pos_inf_cols:\n","#     #train_feat_df[c] = train_feat_df.groupby('stock_id')[c].transform(lambda x: x.replace([np.inf], x.loc[~np.isinf(x)].max()))\n","#     train_feat_df[c].replace(np.inf, 1e8, inplace=True)\n","\n","# ### groupby stock id and fill negative infinity values with min of the stock id or -1e8\n","# for c in neg_inf_cols:\n","#     #train_feat_df[c] = train_feat_df.groupby('stock_id')[c].transform(lambda x: x.replace([-np.inf], x.loc[~np.isneginf(x)].min()))\n","#     train_feat_df[c].replace(-np.inf, -1e8, inplace=True)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mypN38N29YNX"},"outputs":[],"source":["# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# # with open('train_feat_df_full_before_transformation.pkl','wb') as fb:\n","# #       pickle.dump(train_feat_df, fb)\n","\n","# with open('train_feat_df_full_before_transformation.pkl','rb') as fb:\n","#      train_feat_df = pickle.load( fb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHzl28stLQH0"},"outputs":[],"source":["# os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_before_transformation.pkl','rb') as fb:\n","#     feat_df = pickle.load( fb)\n","\n","# feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9po04rvU9lu7"},"outputs":[],"source":["# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/liquidity_features')\n","\n","# with open('train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced.pkl','rb') as fb:\n","#      train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced = pickle.load( fb)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcJOgyLUK9PD"},"outputs":[],"source":["# for c in set(train_feat_df.columns):\n","#   if np.array_equal(train_feat_df[c], train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c]):\n","#     print(c, 'YES')\n","#   else:\n","#     print(c, 'NO')\n","#     diff = abs(train_feat_df[c] - train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c])\n","#     if max(diff) > 1e-6:\n","#       print(max(diff),'\\n')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Y042aBZwhj1"},"outputs":[],"source":["# train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced.drop(columns=['book_ewma_vol', 'log_target', 'log_target_standardized', 'trade_ewma_vol'], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKFaxGFoItrl"},"outputs":[],"source":["# for c in train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced.columns:\n","#     if  not (np.array_equal(train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c], train_feat_df[c])):\n","#         diff_df = train_feat_df_all_feat_large_null_neg_posinf_dropped_null_inf_replaced[c] - train_feat_df[c]\n","#         print(f'{c} # diff {sum(diff_df >= 1e-6)}')#, diff {diff_df[diff_df >= 1e-6]} scale {train_feat_df[diff_df >= 1e-6][c]}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENCW9Nmq2dOK"},"outputs":[],"source":["# import numpy as np\n","\n","\n","# ## Naming convention has t for transformation, e.g. tlog_1p for log transformation, texp for exp transformation\n","\n","# class FeatureTransformation():\n","#     def __init__(self, data):\n","#         self.data = data\n","\n","#     def log_3p(self, col_name):\n","#         \"\"\"log epsilon1 transformation\"\"\"\n","#         self.data[col_name] = np.log(self.data[col_name]+ 3)\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_3p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_3p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","#     def log_3p_test(self, x, mean, std):\n","#         \"\"\"log epsilon1 transformation for test data\"\"\"\n","#         x = np.log(x+ 3)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def log_1p(self, col_name):\n","#         \"\"\"log epsilon1 transformation\"\"\"\n","#         self.data[col_name] = np.log1p(self.data[col_name])\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","#     def log_1p_test(self, x, mean, std):\n","#         \"\"\"log epsilon1 transformation for test data\"\"\"\n","#         x = np.log1p(x)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def log_eps5e3(self, col_name):\n","#         \"\"\"log epsilon3 transformation\"\"\"\n","#         self.data[col_name] = np.log(self.data[col_name]+ 0.005)\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_eps523_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_eps523_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","#     def log_eps5e3_test(self, x, mean, std):\n","#         \"\"\"log epsilon3 transformation for test data\"\"\"\n","#         x = np.log(x+ 0.005)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def log_eps1e4(self, col_name):\n","#         \"\"\"log epsilon4 transformation\"\"\"\n","#         self.data[col_name] = np.log(self.data[col_name]+ 0.0001)\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","#     def log_eps1e4_test(self, x, mean, std):\n","#         \"\"\"log epsilon4 transformation for test data\"\"\"\n","#         x = np.log(x+ 0.0001)\n","#         x = (x - mean) / std\n","#         return x\n","\n","\n","#     def log(self, col_name):\n","#         \"\"\"log transformation\"\"\"\n","#         val = np.log(self.data[col_name])\n","#         val1 = (val - val.mean()) / val.std()\n","#         #return val1, 'tlog_' + col_name , val.mean(), val.std()\n","#         return val, 'tlog_' + col_name , val.mean(), val.std()\n","\n","#     def log_test(self, x, mean, std):\n","#         \"\"\"log transformation for test data\"\"\"\n","#         x = np.log(x)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def log_10p(self, col_name):\n","#         \"\"\"log epsilon2 transformation\"\"\"\n","#         self.data[col_name] = np.log(self.data[col_name]+ 10)\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_10p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_10p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","#     def log_10p_test(self, x, mean, std):\n","#         \"\"\"log epsilon2 transformation for test data\"\"\"\n","#         x = np.log(x+ 10)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def log_log1p(self, col_name):\n","#         \"\"\"log log epsilon1 transformation\"\"\"\n","#         self.data[col_name] = np.log(np.log1p(self.data[col_name]))\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_tlog1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_tlog1p_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","#     def log_log1p_test(self, x, mean, std):\n","#         \"\"\"log log epsilon1 transformation for test data\"\"\"\n","#         x = np.log(np.log1p(x))\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def log_log1p_eps1e4(self, col_name):\n","#         \"\"\"log log epsilon4 transformation\"\"\"\n","#         self.data[col_name] = np.log(np.log1p(self.data[col_name]+ 0.0001))\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_tlog1p_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name] , 'tlog_tlog1p_eps1e4_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","#     def log_log1p_eps1e4_test(self, x, mean, std):\n","#         \"\"\"log log epsilon4 transformation for test data\"\"\"\n","#         x = np.log(np.log1p(x+ 0.0001))\n","#         x = (x - mean) / std\n","#         return x\n","\n","\n","#     def log_lin100_1(self, col_name):\n","#         \"\"\"log of linear transformation\"\"\"\n","#         self.data[col_name] = np.log(self.data[col_name]*100 + 1)\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'tlog_tlinear_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'tlog_tlinear_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","#     def log_lin100_1_test(self, x, mean, std):\n","#         \"\"\"log of linear transformation for test data\"\"\"\n","#         x = np.log(x*100 + 1)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def standard_scaling(self, col_name):\n","#         \"\"\"standard scaling transformation\"\"\"\n","#         #self.data[col_name] = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return self.data[col_name], self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], self.data[col_name].mean(), self.data[col_name].std()\n","\n","#     def standard_scaling_test(self, x, mean, std):\n","#         \"\"\"standard scaling transformation for test data\"\"\"\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def exp(self, col_name):\n","#         \"\"\"exp transformation\"\"\"\n","#         self.data[col_name] = np.exp(self.data[col_name])\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","#     def exp_test(self, x, mean, std):\n","#         \"\"\"exp transformation for test data\"\"\"\n","#         x = np.exp(x)\n","#         x = (x - mean) / std\n","#         return x\n","\n","#     def exp_exp(self, col_name):\n","#         \"\"\"exp exp transformation\"\"\"\n","#         self.data[col_name] = np.exp(np.exp(self.data[col_name]))\n","#         val = (self.data[col_name] - self.data[col_name].mean()) / self.data[col_name].std()\n","#         #return val, 'texp_texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","#         return self.data[col_name], 'texp_texp_' + col_name , self.data[col_name].mean(), self.data[col_name].std()\n","\n","\n","#     def exp_exp_test(self, x, mean, std):\n","#         \"\"\"exp exp transformation for test data\"\"\"\n","#         x = np.exp(np.exp(x))\n","#         x = (x - mean) / std\n","#         return x\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bL0HhDaY2gMJ"},"outputs":[],"source":["# # import feature_transformation as feat_transformer\n","\n","# ft = FeatureTransformation(train_feat_df)\n","\n","# feat_normalization_mu_std_df = pd.DataFrame(index=[\"mean\", \"std\", \"transform\"])\n","\n","# # val, new_col_name, mean, std = ft.log(\"wap1_log_price_ret_vol\")\n","# # train_feat_df.rename(columns={'wap1_log_price_ret_vol':new_col_name}, inplace=True)\n","# # train_feat_df[new_col_name] = val\n","# # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df.drop(columns=['wap1_log_price_ret_vol'], inplace=True) ## we drop this because we have log_wap1_log_price_ret_vol already\n","\n","\n","# train_feat_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol\"], mean, std  = ft.standard_scaling(\"log_liq2_ret_*_wap_eqi_price1_ret_vol\")\n","# feat_normalization_mu_std_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\"], mean, std  = ft.standard_scaling(\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\")\n","# feat_normalization_mu_std_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_per_liq2_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_liq2_vol\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_per_liq2_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'], mean, std = ft.standard_scaling('log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio')\n","# feat_normalization_mu_std_df['log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio'] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_per_liq2_vol_15_ratio\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_liq2_vol_15_ratio\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_per_liq2_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\"], mean, std = ft.standard_scaling(\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\")\n","# feat_normalization_mu_std_df[\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# # train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\"], mean, std = ft.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\")\n","# # feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# # train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\"], mean, std = ft.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\")\n","# # feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\"], mean, std = ft.standard_scaling(\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\")\n","# feat_normalization_mu_std_df[\"log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\"],mean, std, = ft.standard_scaling(\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\")\n","# feat_normalization_mu_std_df[\"wap2_logprice_ret_changes_n_wap1_logprice_ret_constant_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_neg_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_pos_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_pos-neg_log_liq_ret_sqr_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_3p(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0\")\n","# train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:0':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_3p_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0\")\n","# train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\"],mean, std = ft.standard_scaling(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10\")\n","# train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:10':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10\")\n","# train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\"],mean, std, = ft.standard_scaling(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20\")\n","# train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20\")\n","# train_feat_df.rename(columns={'wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log(\"wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20\")\n","# train_feat_df.rename(columns={'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df[\"soft_stock_mean_tvpl2_:0\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_:0\")\n","# feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"soft_stock_mean_tvpl2_:10\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_:10\")\n","# feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"soft_stock_mean_tvpl2_:20\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_:20\")\n","# feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_:20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"soft_stock_mean_tvpl2_liqf\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_liqf\")\n","# feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"soft_stock_mean_tvpl2_liqf_volf10\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_liqf_volf10\")\n","# feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf_volf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"soft_stock_mean_tvpl2_liqf_volf20\"], mean, std = ft.standard_scaling(\"soft_stock_mean_tvpl2_liqf_volf20\")\n","# feat_normalization_mu_std_df[\"soft_stock_mean_tvpl2_liqf_volf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_15\"], mean, std = ft.standard_scaling(\"v1proj_25_15\")\n","# feat_normalization_mu_std_df[\"v1proj_25_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# # val, new_col_name, mean, std = ft.log(\"lsvol\")  ## This is causing lsvol to have nan values\n","# # train_feat_df.rename(columns={'lsvol':new_col_name}, inplace=True)\n","# # train_feat_df[new_col_name] = val\n","# # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df[\"lsvol\"], mean, std = ft.standard_scaling(\"lsvol\")\n","# feat_normalization_mu_std_df[\"lsvol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"liqvol1\"], mean, std = ft.standard_scaling(\"liqvol1\")\n","# feat_normalization_mu_std_df[\"liqvol1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"liqvol1_smean\"], mean, std = ft.standard_scaling(\"liqvol1_smean\")\n","# feat_normalization_mu_std_df[\"liqvol1_smean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"liqvol2\"], mean, std = ft.standard_scaling(\"liqvol2\")\n","# feat_normalization_mu_std_df[\"liqvol2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"liqvol1_15_15\"], mean, std = ft.standard_scaling(\"liqvol1_15_15\")\n","# feat_normalization_mu_std_df[\"liqvol1_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"trade_count\"], mean, std = ft.standard_scaling(\"trade_count\")\n","# feat_normalization_mu_std_df[\"trade_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"root_trade_count\"], mean, std = ft.standard_scaling(\"root_trade_count\")\n","# feat_normalization_mu_std_df[\"root_trade_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"root_trade_count_smean\"], mean, std = ft.standard_scaling(\"root_trade_count_smean\")\n","# feat_normalization_mu_std_df[\"root_trade_count_smean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"root_book_delta_count\"], mean, std = ft.standard_scaling(\"root_book_delta_count\")\n","# feat_normalization_mu_std_df[\"root_book_delta_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"root_trade_count_var\"], mean, std = ft.standard_scaling(\"root_trade_count_var\")\n","# feat_normalization_mu_std_df[\"root_trade_count_var\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"trade_count_15_15\"], mean, std = ft.standard_scaling(\"trade_count_15_15\")\n","# feat_normalization_mu_std_df[\"trade_count_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"root_trade_count_15_15\"], mean, std = ft.standard_scaling(\"root_trade_count_15_15\")\n","# feat_normalization_mu_std_df[\"root_trade_count_15_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_15\"], mean, std = ft.standard_scaling(\"v1proj_29_15\")\n","# feat_normalization_mu_std_df[\"v1proj_29_15\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_20\"], mean, std = ft.standard_scaling(\"v1proj_20\")\n","# feat_normalization_mu_std_df[\"v1proj_20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25\"], mean, std = ft.standard_scaling(\"v1proj_25\")\n","# feat_normalization_mu_std_df[\"v1proj_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29\"], mean, std = ft.standard_scaling(\"v1proj_29\")\n","# feat_normalization_mu_std_df[\"v1proj_29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_q1\"], mean, std = ft.standard_scaling(\"v1proj_29_q1\")\n","# feat_normalization_mu_std_df[\"v1proj_29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_q3\"], mean, std = ft.standard_scaling(\"v1proj_29_q3\")\n","# feat_normalization_mu_std_df[\"v1proj_29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_q1\"], mean, std = ft.standard_scaling(\"v1proj_25_q1\")\n","# feat_normalization_mu_std_df[\"v1proj_25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_q3\"], mean, std = ft.standard_scaling(\"v1proj_25_q3\")\n","# feat_normalization_mu_std_df[\"v1proj_25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_15_q1\"], mean, std = ft.standard_scaling(\"v1proj_29_15_q1\")\n","# feat_normalization_mu_std_df[\"v1proj_29_15_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_15_q3\"], mean, std = ft.standard_scaling(\"v1proj_29_15_q3\")\n","# feat_normalization_mu_std_df[\"v1proj_29_15_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_15_q1\"], mean, std = ft.standard_scaling(\"v1proj_25_15_q1\")\n","# feat_normalization_mu_std_df[\"v1proj_25_15_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_15_q3\"], mean, std = ft.standard_scaling(\"v1proj_25_15_q3\")\n","# feat_normalization_mu_std_df[\"v1proj_25_15_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_15_std\"], mean, std = ft.standard_scaling(\"v1proj_25_15_std\")\n","# feat_normalization_mu_std_df[\"v1proj_25_15_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# # train_feat_df.drop(columns=[\"v1proj_29_15_std\"], inplace=True) ## drop this column as it has very low variance\n","\n","# train_feat_df[\"v1proj_20_std\"], mean, std = ft.standard_scaling(\"v1proj_20_std\")\n","# feat_normalization_mu_std_df[\"v1proj_20_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_25_std\"], mean, std = ft.standard_scaling(\"v1proj_25_std\")\n","# feat_normalization_mu_std_df[\"v1proj_25_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# # train_feat_df[\"v1proj_29_std\"], mean, std = ft.standard_scaling(\"v1proj_29_std\")\n","# # feat_normalization_mu_std_df[\"v1proj_29_std\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1proj_29_q3q1\"], mean, std = ft.standard_scaling(\"v1proj_29_q3q1\")\n","# feat_normalization_mu_std_df[\"v1proj_29_q3q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_rmed2v1\"], mean, std = ft.standard_scaling(\"tvpl2_rmed2v1\")\n","# feat_normalization_mu_std_df[\"tvpl2_rmed2v1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_rmed2v1lf25\"], mean, std = ft.standard_scaling(\"tvpl2_rmed2v1lf25\")\n","# feat_normalization_mu_std_df[\"tvpl2_rmed2v1lf25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_rmed2v1lf29\"], mean, std = ft.standard_scaling(\"tvpl2_rmed2v1lf29\")\n","# feat_normalization_mu_std_df[\"tvpl2_rmed2v1lf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2\"], mean, std = ft.standard_scaling(\"tvpl2\")\n","# feat_normalization_mu_std_df[\"tvpl2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_liqf10\"], mean, std = ft.standard_scaling(\"tvpl2_liqf10\")\n","# feat_normalization_mu_std_df[\"tvpl2_liqf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_liqf20\"], mean, std = ft.standard_scaling(\"tvpl2_liqf20\")\n","# feat_normalization_mu_std_df[\"tvpl2_liqf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_liqf29\"], mean, std = ft.standard_scaling(\"tvpl2_liqf29\")\n","# feat_normalization_mu_std_df[\"tvpl2_liqf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_smean_vol\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol\")\n","# feat_normalization_mu_std_df[\"tvpl2_smean_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_smean_vol_liqf10\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol_liqf10\")\n","# feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_smean_vol_liqf20\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol_liqf20\")\n","# feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"tvpl2_smean_vol_liqf29\"], mean, std = ft.standard_scaling(\"tvpl2_smean_vol_liqf29\")\n","# feat_normalization_mu_std_df[\"tvpl2_smean_vol_liqf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1liq2projt5\"], mean, std = ft.standard_scaling(\"v1liq2projt5\")\n","# feat_normalization_mu_std_df[\"v1liq2projt5\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1liq2projt10\"], mean, std = ft.standard_scaling(\"v1liq2projt10\")\n","# feat_normalization_mu_std_df[\"v1liq2projt10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1liq2projt20\"], mean, std = ft.standard_scaling(\"v1liq2projt20\")\n","# feat_normalization_mu_std_df[\"v1liq2projt20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"liqt10rf29\"], mean, std = ft.standard_scaling(\"liqt10rf29\")\n","# feat_normalization_mu_std_df[\"liqt10rf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"liqt20rf29\"], mean, std = ft.standard_scaling(\"liqt20rf29\")\n","# feat_normalization_mu_std_df[\"liqt20rf29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1liq2sprojt10f25\"], mean, std = ft.standard_scaling(\"v1liq2sprojt10f25\")\n","# feat_normalization_mu_std_df[\"v1liq2sprojt10f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1liq2sprojt5f25\"], mean, std = ft.standard_scaling(\"v1liq2sprojt5f25\")\n","# feat_normalization_mu_std_df[\"v1liq2sprojt5f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt10f29\"], mean, std = ft.standard_scaling(\"v1spprojt10f29\")\n","# feat_normalization_mu_std_df[\"v1spprojt10f29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt15f25\"], mean, std = ft.standard_scaling(\"v1spprojt15f25\")\n","# feat_normalization_mu_std_df[\"v1spprojt15f25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt15f29\"], mean, std = ft.standard_scaling(\"v1spprojt15f29\")\n","# feat_normalization_mu_std_df[\"v1spprojt15f29\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt15f29_q1\"], mean, std = ft.standard_scaling(\"v1spprojt15f29_q1\")\n","# feat_normalization_mu_std_df[\"v1spprojt15f29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt15f29_q3\"], mean, std = ft.standard_scaling(\"v1spprojt15f29_q3\")\n","# feat_normalization_mu_std_df[\"v1spprojt15f29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt15f25_q1\"], mean, std = ft.standard_scaling(\"v1spprojt15f25_q1\")\n","# feat_normalization_mu_std_df[\"v1spprojt15f25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojt15f25_q3\"], mean, std = ft.standard_scaling(\"v1spprojt15f25_q3\")\n","# feat_normalization_mu_std_df[\"v1spprojt15f25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojtf29_q1\"], mean, std = ft.standard_scaling(\"v1spprojtf29_q1\")\n","# feat_normalization_mu_std_df[\"v1spprojtf29_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojtf29_q3\"], mean, std = ft.standard_scaling(\"v1spprojtf29_q3\")\n","# feat_normalization_mu_std_df[\"v1spprojtf29_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojtf25_q1\"], mean, std = ft.standard_scaling(\"v1spprojtf25_q1\")\n","# feat_normalization_mu_std_df[\"v1spprojtf25_q1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"v1spprojtf25_q3\"], mean, std = ft.standard_scaling(\"v1spprojtf25_q3\")\n","# feat_normalization_mu_std_df[\"v1spprojtf25_q3\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df.drop(columns=[\"wap1_log_price_ret_vol_from_0\"], inplace=True) ## drop this column as it has all 0 values\n","\n","# train_feat_df[\"wap1_log_price_ret_volstock_mean_from_0\"],mean, std, = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_0\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_0\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.exp(\"wap1_log_price_ret_vol_from_10\")\n","# train_feat_df.rename(columns={'wap1_log_price_ret_vol_from_10':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_volstock_mean_from_10\"],mean, std, = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_10\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_10\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.exp(\"wap1_log_price_ret_vol_from_20\")\n","# train_feat_df.rename(columns={'wap1_log_price_ret_vol_from_20':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","\n","\n","# train_feat_df[\"wap1_log_price_ret_volstock_mean_from_20\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_20\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_20\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_vol_from_25\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_vol_from_25\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_vol_from_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"wap1_log_price_ret_volstock_mean_from_25\"], mean, std = ft.standard_scaling(\"wap1_log_price_ret_volstock_mean_from_25\")\n","# feat_normalization_mu_std_df[\"wap1_log_price_ret_volstock_mean_from_25\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"vol1_mean\"], mean, std = ft.standard_scaling(\"vol1_mean\")\n","# feat_normalization_mu_std_df[\"vol1_mean\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df.drop(columns=[\"mean_half_delta\"], inplace=True)# drop this column as it has very few unique values\n","\n","# train_feat_df.drop(columns=[\"mean_half_delta_lsprd\"], inplace=True)# drop this column as it has very few unique values\n","\n","# train_feat_df[\"log_wap1_log_price_ret_vol\"], mean, std = ft.standard_scaling(\"log_wap1_log_price_ret_vol\")\n","# feat_normalization_mu_std_df[\"log_wap1_log_price_ret_vol\"] = [mean, std, \"standard_scaling_test\"]\n","\n","\n","# # train_feat_df.drop(columns=[\"log_target\"], inplace=True)# drop log_target as we create it here as tlog_target which is sames as log_target_standardize\n","# # train_feat_df.drop(columns=[\"log_target_standardized\"], inplace=True)#\n","# temp_target = train_feat_df[\"target\"]\n","# val , new_col_name, mean, std = ft.log(\"target\")\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","# train_feat_df[\"target\"] = temp_target\n","# del temp_target\n","\n","# train_feat_df[\"bid_lvl2_min_lvl1_size_feat\"], mean, std = ft.standard_scaling(\"bid_lvl2_min_lvl1_size_feat\")\n","# feat_normalization_mu_std_df[\"bid_lvl2_min_lvl1_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"ask_lvl2_min_lvl1_size_feat\"], mean, std = ft.standard_scaling(\"ask_lvl2_min_lvl1_size_feat\")\n","# feat_normalization_mu_std_df[\"ask_lvl2_min_lvl1_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"], mean, std = ft.standard_scaling(\"lvl2_minus_lvl1_bid_n_ask_size_feat\")\n","# feat_normalization_mu_std_df[\"lvl2_minus_lvl1_bid_n_ask_size_feat\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"sum_size\"], mean, std = ft.standard_scaling(\"sum_size\")\n","# feat_normalization_mu_std_df[\"sum_size\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"sum_order_count\"], mean, std = ft.standard_scaling(\"sum_order_count\")\n","# feat_normalization_mu_std_df[\"sum_order_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"sum_size_per_order_count\"], mean, std = ft.standard_scaling(\"sum_size_per_order_count\")\n","# feat_normalization_mu_std_df[\"sum_size_per_order_count\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_eps5e3(\"trade_price_n_wap1_dev\")\n","# train_feat_df.rename(columns={'trade_price_n_wap1_dev':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps5e3_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_eps5e3(\"trade_price_n_wap_eqi_price0_dev\")\n","# train_feat_df.rename(columns={'trade_price_n_wap_eqi_price0_dev':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps5e3_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"first_10_min_vol\")\n","# train_feat_df.rename(columns={'first_10_min_vol':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_eps1e4(\"trade_price_std\")\n","# train_feat_df.rename(columns={'trade_price_std':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps1e4_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_eps1e4(\"trade_price_real_vol\")\n","# train_feat_df.rename(columns={'trade_price_real_vol':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_eps1e4_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"trade_size_std\")\n","# train_feat_df.rename(columns={'trade_size_std':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"trade_size_mean\")\n","# train_feat_df.rename(columns={'trade_size_mean':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"trade_order_count_std\")\n","# train_feat_df.rename(columns={'trade_order_count_std':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"trade_order_count_mean\")\n","# train_feat_df.rename(columns={'trade_order_count_mean':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_4_clusters\")\n","# train_feat_df.rename(columns={'target_vol_sum_stats_4_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_10_clusters\")\n","# train_feat_df.rename(columns={'target_vol_sum_stats_10_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_16_clusters\")\n","# train_feat_df.rename(columns={'target_vol_sum_stats_16_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_sum_stats_30_clusters\")\n","# train_feat_df.rename(columns={'target_vol_sum_stats_30_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_2_clusters\")\n","# train_feat_df.rename(columns={'target_vol_robust_sum_stats_2_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_4_clusters\")\n","# train_feat_df.rename(columns={'target_vol_robust_sum_stats_4_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_14_clusters\")\n","# train_feat_df.rename(columns={'target_vol_robust_sum_stats_14_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_20_clusters\")\n","# train_feat_df.rename(columns={'target_vol_robust_sum_stats_20_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_32_clusters\")\n","# train_feat_df.rename(columns={'target_vol_robust_sum_stats_32_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p(\"target_vol_robust_sum_stats_60_clusters\")\n","# train_feat_df.rename(columns={'target_vol_robust_sum_stats_60_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_3_clusters\")\n","# train_feat_df.rename(columns={'target_vol_pcorr_3_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_49_clusters\")\n","# train_feat_df.rename(columns={'target_vol_pcorr_49_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_90_clusters\")\n","# train_feat_df.rename(columns={'target_vol_pcorr_90_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_10_clusters\")\n","# train_feat_df.rename(columns={'target_vol_pcorr_10_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log(\"target_vol_pcorr_26_clusters\")\n","# train_feat_df.rename(columns={'target_vol_pcorr_26_clusters':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df[\"min_bid_price1\"], mean, std = ft.standard_scaling(\"min_bid_price1\")\n","# feat_normalization_mu_std_df[\"min_bid_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"max_bid_price1\"], mean, std = ft.standard_scaling(\"max_bid_price1\")\n","# feat_normalization_mu_std_df[\"max_bid_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"min_ask_price1\"], mean, std = ft.standard_scaling(\"min_ask_price1\")\n","# feat_normalization_mu_std_df[\"min_ask_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"max_ask_price1\"], mean, std = ft.standard_scaling(\"max_ask_price1\")\n","# feat_normalization_mu_std_df[\"max_ask_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","\n","# train_feat_df.drop(columns=[\"min_bid_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# val, new_col_name, mean, std = ft.log(\"max_bid_size1\")\n","# train_feat_df.rename(columns={'max_bid_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df.drop(columns=[\"min_ask_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# val, new_col_name, mean, std = ft.log(\"max_ask_size1\")\n","# train_feat_df.rename(columns={'max_ask_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_log1p_eps1e4(\"range_ask_price1\")\n","# train_feat_df.rename(columns={'range_ask_price1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_log1p_eps1e4(\"range_bid_price1\")\n","# train_feat_df.rename(columns={'range_bid_price1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"range_ask_size1\")\n","# train_feat_df.rename(columns={'range_ask_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"range_bid_size1\")\n","# train_feat_df.rename(columns={'range_bid_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_log1p_eps1e4(\"sad_ask_price1\")\n","# train_feat_df.rename(columns={'sad_ask_price1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_log1p_eps1e4_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"sad_ask_size1\")\n","# train_feat_df.rename(columns={'sad_ask_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_bid_price1\")\n","# train_feat_df.rename(columns={'sad_bid_price1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_bid_size1\")\n","# train_feat_df.rename(columns={'sad_bid_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# train_feat_df[\"bs_bp_corr1\"], mean, std = ft.standard_scaling(\"bs_bp_corr1\")\n","# feat_normalization_mu_std_df[\"bs_bp_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bs_as_corr1\"], mean, std = ft.standard_scaling(\"bs_as_corr1\")\n","# feat_normalization_mu_std_df[\"bs_as_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bs_ap_corr1\"], mean, std = ft.standard_scaling(\"bs_ap_corr1\")\n","# feat_normalization_mu_std_df[\"bs_ap_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bp_as_corr1\"], mean, std = ft.standard_scaling(\"bp_as_corr1\")\n","# feat_normalization_mu_std_df[\"bp_as_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.exp_exp(\"bp_ap_corr1\")\n","# train_feat_df.rename(columns={'bp_ap_corr1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_exp_test\"]\n","\n","# train_feat_df[\"as_ap_corr1\"], mean, std = ft.standard_scaling(\"as_ap_corr1\")\n","# feat_normalization_mu_std_df[\"as_ap_corr1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"min_price1\"], mean, std = ft.standard_scaling(\"min_price1\")\n","# feat_normalization_mu_std_df[\"min_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"max_price1\"], mean, std = ft.standard_scaling(\"max_price1\")\n","# feat_normalization_mu_std_df[\"max_price1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df.drop(columns=[\"min_size1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# val, new_col_name, mean, std = ft.log(\"max_size1\")\n","# train_feat_df.rename(columns={'max_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","\n","# train_feat_df.drop(columns=[\"min_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# train_feat_df[\"max_order_count1\"], mean, std = ft.standard_scaling(\"max_order_count1\")\n","# feat_normalization_mu_std_df[\"max_order_count1\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"range_price1\")\n","# train_feat_df.rename(columns={'range_price1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"range_size1\")\n","# train_feat_df.rename(columns={'range_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","\n","# train_feat_df.drop(columns=[\"range_order_count1\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_price1\")\n","# train_feat_df.rename(columns={'sad_price1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_size1\")\n","# train_feat_df.rename(columns={'sad_size1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","\n","# val, new_col_name, mean, std = ft.log_1p(\"sad_order_count1\")\n","# train_feat_df.rename(columns={'sad_order_count1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","# val, new_col_name, mean, std = ft.exp(\"size_order_count_corr1\")\n","# train_feat_df.rename(columns={'size_order_count_corr1':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"exp_test\"]\n","\n","# # val, new_col_name, mean, std = ft.log(\"book_ewma_vol\")\n","# # train_feat_df.rename(columns={'book_ewma_vol':new_col_name}, inplace=True)\n","# # train_feat_df[new_col_name] = val\n","# # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# # val, new_col_name, mean, std = ft.log_1p(\"trade_ewma_vol\")\n","# # train_feat_df.rename(columns={'trade_ewma_vol':new_col_name}, inplace=True)\n","# # train_feat_df[new_col_name] = val\n","# # feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_1p_test\"]\n","\n","# train_feat_df[\"min_bid_price2\"], mean, std = ft.standard_scaling(\"min_bid_price2\")\n","# feat_normalization_mu_std_df[\"min_bid_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"max_bid_price2\"], mean, std = ft.standard_scaling(\"max_bid_price2\")\n","# feat_normalization_mu_std_df[\"max_bid_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"min_ask_price2\"], mean, std = ft.standard_scaling(\"min_ask_price2\")\n","# feat_normalization_mu_std_df[\"min_ask_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"max_ask_price2\"], mean, std = ft.standard_scaling(\"max_ask_price2\")\n","# feat_normalization_mu_std_df[\"max_ask_price2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df.drop(columns=[\"min_bid_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# val, new_col_name, mean, std = ft.log(\"max_bid_size2\")\n","# train_feat_df.rename(columns={'max_bid_size2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# train_feat_df.drop(columns=[\"min_ask_size2\"], inplace=True) ## drop this column as it mostly 0 values and highly kurtotic\n","\n","# val, new_col_name, mean, std = ft.log(\"max_ask_size2\")\n","# train_feat_df.rename(columns={'max_ask_size2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"range_ask_price2\")\n","# train_feat_df.rename(columns={'range_ask_price2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"range_bid_price2\")\n","# train_feat_df.rename(columns={'range_bid_price2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# val, new_col_name, mean, std = ft.log(\"range_ask_size2\")\n","# train_feat_df.rename(columns={'range_ask_size2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_ask_price2\")\n","# train_feat_df.rename(columns={'sad_ask_price2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_ask_size2\")\n","# train_feat_df.rename(columns={'sad_ask_size2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# val, new_col_name, mean, std = ft.log_lin100_1(\"sad_bid_price2\")\n","# train_feat_df.rename(columns={'sad_bid_price2':new_col_name}, inplace=True)\n","# train_feat_df[new_col_name] = val\n","# feat_normalization_mu_std_df[new_col_name] = [mean, std, \"log_lin100_1_test\"]\n","\n","# train_feat_df[\"bs_bp_corr2\"], mean, std = ft.standard_scaling(\"bs_bp_corr2\")\n","# feat_normalization_mu_std_df[\"bs_bp_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bs_as_corr2\"], mean, std = ft.standard_scaling(\"bs_as_corr2\")\n","# feat_normalization_mu_std_df[\"bs_as_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bs_ap_corr2\"], mean, std = ft.standard_scaling(\"bs_ap_corr2\")\n","# feat_normalization_mu_std_df[\"bs_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bp_as_corr2\"], mean, std = ft.standard_scaling(\"bp_as_corr2\")\n","# feat_normalization_mu_std_df[\"bp_as_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"bp_ap_corr2\"], mean, std = ft.standard_scaling(\"bp_ap_corr2\")\n","# feat_normalization_mu_std_df[\"bp_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"as_ap_corr2\"], mean, std = ft.standard_scaling(\"as_ap_corr2\")\n","# feat_normalization_mu_std_df[\"as_ap_corr2\"] = [mean, std, \"standard_scaling_test\"]\n","\n","# train_feat_df[\"sum_stats_4_clusters_labels\"] = train_feat_df[\"sum_stats_4_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"sum_stats_10_clusters_labels\"] = train_feat_df[\"sum_stats_10_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"sum_stats_16_clusters_labels\"] = train_feat_df[\"sum_stats_16_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"sum_stats_30_clusters_labels\"] = train_feat_df[\"sum_stats_30_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"pear_corr_3_clusters_labels\"] = train_feat_df[\"pear_corr_3_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"pear_corr_49_clusters_labels\"] = train_feat_df[\"pear_corr_49_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"pear_corr_90_clusters_labels\"] = train_feat_df[\"pear_corr_90_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"pear_corr_10_clusters_labels\"] = train_feat_df[\"pear_corr_10_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"pear_corr_26_clusters_labels\"] = train_feat_df[\"pear_corr_26_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"robust_sum_stats_2_clusters_labels\"] = train_feat_df[\"robust_sum_stats_2_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"robust_sum_stats_4_clusters_labels\"] = train_feat_df[\"robust_sum_stats_4_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"robust_sum_stats_14_clusters_labels\"] = train_feat_df[\"robust_sum_stats_14_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"robust_sum_stats_20_clusters_labels\"] = train_feat_df[\"robust_sum_stats_20_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"robust_sum_stats_32_clusters_labels\"] = train_feat_df[\"robust_sum_stats_32_clusters_labels\"].astype(\"category\")\n","\n","# train_feat_df[\"robust_sum_stats_60_clusters_labels\"] = train_feat_df[\"robust_sum_stats_60_clusters_labels\"].astype(\"category\")\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDK95AzvzZR3"},"outputs":[],"source":["train_feat_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_8pEPyX2jbs"},"outputs":[],"source":["# Check for NULL and INF values in train_feat_df\n","\n","nan_cols = []\n","pos_inf_cols = []\n","neg_inf_cols = []\n","\n","float_cols = [c for c in train_feat_df.columns if train_feat_df[c].dtype != \"category\"]\n","\n","for c in float_cols: ## exclude the caetgorical columns\n","    if np.isinf(train_feat_df[c]).any():\n","        print(\"pos INF: \",c,': ', np.isinf(train_feat_df[c]).sum())\n","        pos_inf_cols.append(c)\n","\n","    if np.isneginf(train_feat_df[c]).any():\n","        print(\"neg INF: \",c,': ', np.isneginf(train_feat_df[c]).sum())\n","        neg_inf_cols.append(c)\n","\n","    if np.isnan(train_feat_df[c]).any():\n","        print(\"NaN: \", c ,': ', np.isnan(train_feat_df[c]).sum())\n","        nan_cols.append(c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C81MPJ9r2lkZ"},"outputs":[],"source":["#### Check that no two columns have the same values\n","for c in train_feat_df.columns:\n","    ctr=0\n","    for c1 in train_feat_df.columns:\n","        if np.array_equal(train_feat_df[c], train_feat_df[c1]) and c!=c1:\n","            ctr+=1\n","            print(c, c1)\n","        # if (ctr>1):\n","        #     print(c, c1)\n","\n","# ### Drop the columns that have the same values\n","# train_feat_df.drop(columns=['exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2',\"exp_log_liq1_ret_*_wap_eqi_price1_ret_vol_2_15_ratio\", \"min_price\", \"max_price\" ], inplace=True)\n","# train_feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZZqqXH9z_9W"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission')\n","# with open('train_feat_df_full_after_transformation.pkl','wb') as fb:\n","#       pickle.dump(train_feat_df, fb)\n","\n","with open('train_feat_df_full_after_transformation.pkl','rb') as fb:\n","      train_feat_df = pickle.load(fb)"]},{"cell_type":"markdown","metadata":{"id":"g3B1D2EM1HVS"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"9cQUR4vE1O7B"},"source":["#### xgb_gpu_submission.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B501v0KR1LGq"},"outputs":[],"source":["import shap\n","import pickle\n","from datetime import datetime\n","import os\n","import gc\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.figure_factory as ff\n","import plotly.graph_objects as go\n","import seaborn as sns\n","\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from xgboost import plot_tree\n","from sklearn.utils import class_weight\n","from xgboost import plot_importance\n","\n","from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n","\n","import statsmodels.api as sm\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","\n","from statsmodels.genmod.generalized_linear_model import GLM\n","import scipy.stats as stats\n","\n","from sklearn.cluster import KMeans\n","import re\n","\n","import warnings\n","#warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import confusion_matrix\n","#from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","\n","import glob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYz6GaC72g3G"},"outputs":[],"source":["df_sub = train_feat_df.copy() ## df submission\n","del train_feat_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Je2_WYeP2NcX"},"outputs":[],"source":["df_sub.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UxIwoMmm1U24"},"outputs":[],"source":["df_sub['time_id'] = df_sub['time_id'].astype('category')\n","df_sub['stock_id'] = df_sub['stock_id'].astype('category')\n","\n","# ##### dropping categorical columns so that they can be selectively added later\n","# for c in df_sub.columns:\n","#   if 'labels' in c or 'clusters' in c or df_sub[c].dtype == 'category':\n","#     print(c)\n","#     df_sub.drop(columns=c, inplace=True)\n","\n","\n","df_sub.drop(columns=['tlog_target'], inplace=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jFA7K6Y24Hmb"},"outputs":[],"source":["# # ########## code to reorder time ids to correct sequence/order for training\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data')\n","\n","# import glob\n","\n","# import numpy as np\n","# import pandas as pd\n","# from joblib import Parallel, delayed\n","# from sklearn.manifold import TSNE\n","# from sklearn.preprocessing import minmax_scale\n","\n","\n","# def calc_price_from_tick(df):\n","#     tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n","#     return 0.01 / tick\n","\n","\n","# def calc_prices(r):\n","#     df = pd.read_parquet(r.book_path,\n","#                          columns=[\n","#                              'time_id',\n","#                              'ask_price1',\n","#                              'ask_price2',\n","#                              'bid_price1',\n","#                              'bid_price2'\n","#                          ])\n","#     df = df.groupby('time_id') \\\n","#         .apply(calc_price_from_tick).to_frame('price').reset_index()\n","#     df['stock_id'] = r.stock_id\n","#     return df\n","\n","\n","# paths = train_book_paths\n","\n","# df_files = pd.DataFrame(\n","#     {'book_path': paths}) \\\n","#     .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")',\n","#           engine='python')\n","\n","# # build price matrix using tick-size\n","# df_prices = pd.concat(\n","#     Parallel(n_jobs=4)(\n","#         delayed(calc_prices)(r) for _, r in df_files.iterrows()\n","#     )\n","# )\n","\n","\n","# df_prices = df_prices.pivot(columns='stock_id', values='price', index='time_id')\n","\n","# # t-SNE to recovering time-id order\n","# clf = TSNE(\n","#     n_components=1,\n","#     perplexity=400,\n","#     random_state=0,\n","#     n_iter=2000\n","# )\n","# compressed = clf.fit_transform(\n","#     pd.DataFrame(minmax_scale(df_prices.fillna(df_prices.mean())))\n","# )\n","\n","# order = np.argsort(compressed[:, 0])\n","# ordered = df_prices.reindex(order).reset_index(drop=True)\n","\n","# # correct direction of time-id order using known stock (id61 = AMZN)\n","# if ordered[61].iloc[0] > ordered[61].iloc[-1]:\n","#     ordered = ordered.reindex(ordered.index[::-1])\\\n","#         .reset_index(drop=True)\n","\n","\n","# correct_time_id_order = all_uniq_time_ids.reindex(order)[::-1].reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91Pwz4xG5BFp"},"outputs":[],"source":["os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/optimus_GPU_data/data/liquidity_features')\n","\n","with open('correct_time_id_order.pkl', 'rb') as f:\n","    correct_time_id_order = pickle.load(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZswjsI9A3SE2"},"outputs":[],"source":["# ########## code to reorder time ids to correct sequence/order for training\n","\n","#df_sub[\"seq_id\"] = -1 ## we exclude seq id because it cannot be generated for the submission data\n","\n","time_ids_reordered = correct_time_id_order['time_id'].values\n","\n","def my_reorder_stock_in_df(st_df, time_ids_reordered):\n","    common_values = [value for value in time_ids_reordered if value in st_df['time_id'].values]\n","    st_df = st_df.set_index('time_id')\n","    st_df = st_df.reindex(common_values)\n","    st_df = st_df.reset_index()\n","    #st_df[\"seq_id\"] = range(st_df.shape[0]) ## we exclude seq id because it cannot be generated for the submission data\n","    return st_df\n","\n","# Assuming you have a dataframe called 'df_sub' and an array of reordered time_ids called 'time_ids_reordered'\n","df_reordered = df_sub.groupby('stock_id').apply(my_reorder_stock_in_df, time_ids_reordered=time_ids_reordered).reset_index(drop=True)\n","del df_sub\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K6yzLIAM80no"},"source":["##### Manual Feature selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-imVyTWX83Vn"},"outputs":[],"source":["df2 = df_reordered.copy()\n","del df_reordered\n","\n","# ##### select columns to train\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aehoM7ma7_Xp"},"outputs":[],"source":["class train_validate_n_test(object):\n","\n","    def __init__(self,df) -> None:\n","\n","        self.time_id_order = df.loc[:3829,'time_id'].values # select ordered unique time_ids\n","        self.train_time_id_ind = int(len(self.time_id_order)*0.7)\n","        self.n_folds = 10\n","        folds = TimeSeriesSplit(n_splits=self.n_folds,)# max_train_size=None, gap=10)\n","        self.splits = folds.split( range( self.train_time_id_ind ) ) # split 70% train time_ids into n_fold splits\n","\n","        train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","\n","        test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        self.test_df = df[df['time_id'].isin(test_time_ids)]\n","        self.test_stock_id = self.test_df['stock_id']\n","        self.test_time_id = self.test_df['time_id']\n","        self.df = df\n","\n","        # feature_importances = pd.DataFrame()\n","        cols = list(df2.columns)\n","        #cols.remove('tlog_target')\n","        cols.remove('target')\n","        self.feat_cols_list =  cols #cat_feat_labels+float32_feat_labels+float64_feat_labels # int32_feat_labels+int64_feat_labels+float32_feat_labels+float64_feat_labels\n","        # feature_importances['feature'] = self.feat_cols_list\n","\n","        self.target_name = 'target' # _standardized' log target is easier to transform back than log_target_standardized\n","\n","        del df\n","\n","    # def onehotencode_cat_var(self,full_set):\n","    #     full_set = cat_feat_labels #full_set.astype({\"stn_id\":str,\"block_id\":str,\"ts_of_day\":str,\"hr_of_day\":str,\"day_of_wk\":str,\"day_of_mn\":str,\"wk_of_mon\":str })\n","    #     full_set = pd.get_dummies(full_set, prefix_sep=\"_\",columns =cat_feat_labels,drop_first=True)\n","    #     #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n","    #     return full_set\n","\n","    #### RMSPE cost function\n","    def rmspe(self,y_true, y_pred):\n","        return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","\n","\n","    # Custom RMSPE objective function\n","    def rmspe_objective(self,preds, dtrain):\n","        labels = dtrain.get_label()\n","        errors = (preds - labels) / labels\n","        gradient = 2 * errors / (1 + errors**2)\n","        hessian = 2 * (1 - errors**2) / (1 + errors**2)**2\n","        return gradient, hessian\n","\n","\n","    def xgb_RMSPE(self,preds, train_data):\n","        labels = train_data.get_label()\n","        return 'RMSPE', round(self.rmspe(y_true = labels, y_pred = preds),5)\n","\n","\n","    def nancorr(self,a, b):\n","        v = np.isfinite(a)*np.isfinite(b) > 0\n","        return np.corrcoef(a[v], b[v])[0,1]\n","\n","\n","    def xgb_train_validate(self,params_xgb,n_rounds,esr,trial):\n","        rmspe_val_score = []\n","        models= []\n","        test_y_preds = np.zeros(len(self.test_df))\n","        best_iterations = []\n","        learning_train_rmspe = []\n","        learning_val_rmspe = []\n","\n","        for fold_n, (train_index, valid_index) in enumerate(self.splits):\n","            print('Fold:',fold_n+1)\n","            # print('train_index',train_index)\n","            # print('valid_index',valid_index)\n","            train_time_ids = self.time_id_order[train_index]\n","            val_time_ids = self.time_id_order[valid_index]\n","            train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","            val_df = self.df[self.df['time_id'].isin(val_time_ids)]\n","\n","            X_train = train_df[self.feat_cols_list]\n","            y_train = train_df[self.target_name] # target\n","            X_valid = val_df[self.feat_cols_list]\n","            y_val = val_df[self.target_name] # target\n","\n","            v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n","            v1v = np.exp(  X_valid['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n","\n","            # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n","            # v1v = np.exp(np.exp(  X_valid['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n","            #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","\n","            w_train = y_train **-2 * v1tr**2\n","            w_val = y_val **-2 * v1v**2\n","\n","            print('Training....')\n","            dtrain = xgb.DMatrix(X_train, label=y_train/v1tr,weight=w_train,enable_categorical=True)\n","            dvalid = xgb.DMatrix(X_valid,   label=  y_val/v1v,weight=w_val,enable_categorical=True)\n","            watchlist  = [(dtrain,'train_loss_fold_'+str(fold_n+1)), (dvalid, 'val_loss_fold_'+str(fold_n+1))]\n","            evals_result = {}\n","            reg = xgb.train(params=params_xgb, dtrain=dtrain, num_boost_round=n_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE,  evals_result=evals_result,maximize=False,  early_stopping_rounds=esr,verbose_eval=False)\n","            learning_train_rmspe.append(evals_result['train_loss_fold_'+str(fold_n+1)])\n","            learning_val_rmspe.append(evals_result['val_loss_fold_'+str(fold_n+1)])\n","\n","            models.append(reg)\n","            best_iterations.append(reg.best_iteration)\n","\n","            p = reg.predict(dvalid)*v1v\n","            val_score =  np.mean( ((p-y_val)/y_val)**2 )**0.5\n","\n","            # full_score += y_val.shape[0]*score**2\n","\n","            print(f'fold: {fold_n+1}, val rmspe score is {val_score}')\n","            print('corr(p/v1v, y_val/v1v)',self.nancorr(       p/v1v ,        y_val/v1v ))\n","            print('log(corr( ))',self.nancorr(np.log(p/v1v), np.log(y_val/v1v)))\n","            print('corr(p, y_val)',self.nancorr(p, y_val))\n","            print('log(corr( ))',self.nancorr(np.log(p), np.log(y_val)))\n","\n","            #test_pred = reg.predict(self.test_df[self.feat_cols_list] )*v1ts ## this method is not suitable for Timeseries cross validation because initial splits are too far from test set.\n","            #test_y_preds += test_pred/self.n_folds\n","\n","            rmspe_val_score.append(val_score)\n","\n","        mean_rmspe_val_score = np.mean(rmspe_val_score)\n","        print(f'mean rmspe val score over {self.n_folds} splits is',mean_rmspe_val_score)\n","        #print(f'mean rmspe test score: ',  np.mean( ((test_y_preds-self.test_df[self.target_name])/self.test_df[self.target_name])**2 )**0.5  ) # target\n","\n","        # Plot learning curves\n","        fig,ax = plt.subplots(2,1,figsize=(10,6))\n","        for fold_n in range(len(rmspe_val_score)):\n","            ax[0].plot(learning_train_rmspe[fold_n]['RMSPE'], label=f'Fold {fold_n+1} Train RMSPE')\n","            ax[0].plot(learning_val_rmspe[fold_n]['RMSPE'],linestyle='dashed', label=f'Fold {fold_n+1} Validation RMSPE')\n","        last_fold = len(rmspe_val_score) - 1\n","        ax[1].plot(learning_val_rmspe[last_fold]['RMSPE'],linestyle='dashed', label=f'Fold {last_fold+1} Validation RMSPE')\n","        ax[1].set_xlabel('Boosting Round')\n","        ax[0].set_ylabel('RMSPE')\n","        ax[1].set_ylabel('RMSPE')\n","        ax[0].legend()\n","        ax[1].legend()\n","        ax[0].grid(True)\n","        ax[1].grid(True)\n","        fig.suptitle(f'Learning Curves, Trial: {trial.number}')\n","        fig.show()\n","\n","        del self.df, X_train, X_valid, y_train, y_val,train_df,val_df,dtrain,dvalid, v1tr, v1v\n","        gc.collect()\n","        return mean_rmspe_val_score,best_iterations[-1]\n","\n","\n","    def manual_shapley_addivity_check(self,model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name):\n","\n","        y_train_true = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","                #### ONLY for Explainer\n","        shap_pred = ( shap_values.base_values + shap_values.values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","                #### ONLY for TreeExplainer\n","        #shap_pred = ( model_base_value + shap_values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","        #print('shap_values.sum(axis=1)',shap_values.sum(axis=1))\n","        # print('shap_values.base_values',shap_values.base_values[0])\n","        # print('shap_values.values',shap_values.values[0].sum())\n","        #print('len(shap_values.values.sum(axis=1))',len(shap_values.values.sum(axis=1)))\n","\n","        model_shap_rmspe = self.rmspe(model_pred, shap_pred)\n","\n","        fig, ax = plt.subplots(2,1,figsize=(30,10))\n","        ax[0].plot(np.arange(0,len(y_train_true)),y_train_true,label='true rvol.',linestyle='dashed',c='g',marker='*',alpha=0.2)\n","        ax[0].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.6)\n","        ax[0].set_title(f'True Rvol. Vs. model predicted Rvol.' )\n","        ax[0].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[0].set_ylabel('rvol.')\n","        ax[0].legend()\n","        ax[0].grid(True)\n","\n","        ax[1].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.4)\n","        ax[1].plot(np.arange(0,len(shap_pred)),shap_pred,label='summed shap values prediction',linestyle='dashed',c='r',marker='*',alpha=0.4)\n","        ax[1].set_title(f'Check additivity of shap values, RMSPE:{model_shap_rmspe} between model and shap values prediction' )\n","        ax[1].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[1].set_ylabel('rvol.')\n","        ax[1].legend()\n","        ax[1].grid(True)\n","        fig.tight_layout()\n","        fig.show()\n","\n","\n","        del all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_PDP_n_Scatter(self,feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df):\n","        ####### compute partial dependence plot of most important features\n","\n","        ###### Partial dependence plot\n","        #fig,ax = plt.subplots()\n","        #shap.plots.partial_dependence(feature_name, model.predict, xgb.DMatrix(X_train,enable_categorical=True), model_expected_value=True, feature_expected_value=True)\n","        #fig.show()\n","\n","        ##### scatter plot\n","        print(f'\\n scatter plot of {feature_name} vs. shap values')\n","        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #fig,ax = plt.subplots()\n","        shap.plots.scatter(shap_values[:,feature_name])\n","        #fig.show()\n","\n","\n","        ##### scatter plot of feature vs. True target rvol. on trianing set\n","        fig,ax = plt.subplots()\n","        yval = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        xval = X[feature_name]\n","        ax.scatter(xval,yval)\n","        ax.plot([min(xval), max(xval)], [min(yval),max(yval)], color = 'red', linewidth = 1)\n","        ax.set_xlabel(feature_name)\n","        ax.set_ylabel('True target rvol.')\n","        ax.grid\n","        ax.set_title(f'scatter plot of {feature_name} Vs. True Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        ##### scatter plot of feature vs. predicted target rvol.on trianing set\n","        fig,ax = plt.subplots()\n","        yval1 = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        ax.scatter(xval,yval1)\n","        ax.plot([min(xval), max(xval)], [min(yval1),max(yval1)], color = 'red', linewidth = 1)\n","        ax.set_xlabel(feature_name)\n","        ax.set_ylabel('Predicted target rvol.')\n","        ax.grid\n","        ax.set_title(f'scatter plot of {feature_name} Vs. Predicted Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","    def compute_shapley_beeswarm(self,X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # shap.summary_plot(shap_values, X)\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.beeswarm(shap_values)\n","        #ax.set_title(f' stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","    def compute_shapley_barplot(self,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # plt.bar(shap_values.abs().sum(axis=1))\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values,)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_individual_stock_SHAP_values(self,final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        stock_id = stock_id\n","        view_time_ids_start = view_time_ids_start\n","        view_time_ids_end = view_time_ids_end\n","        X = X_train[X_train['stock_id'].isin([stock_id])].iloc[view_time_ids_start:view_time_ids_end]\n","\n","        ###### Explainer #######\n","        explainer = shap.Explainer(final_reg,X)\n","        shap_values = explainer(np.array(X),check_additivity=False)\n","        shap_values.feature_names = final_reg.feature_names\n","\n","        ###### TreeExplainer #######\n","        # explainer = shap.TreeExplainer(final_reg,feature_perturbation='interventional')\n","        # shap_values = explainer.shap_values(np.array(X),check_additivity=False)\n","        # shap_values.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","        # print(f'Model base value: {model_base_value} before scaling by v1tr')\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.manual_shapley_addivity_check(model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name )\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values.base_values = shap_values.base_values * all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values.values = np.multiply(shap_values.values.T ,stock_v1tr_df).T\n","        ###### check correctness of shap_values\n","        # sp = shap_values.base_values + shap_values.values.sum(axis=1)\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(len(sp)),sp)\n","        # model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # plt.plot(range(len(sp)), model_pred )\n","        # plt.show()\n","\n","        self.compute_shapley_beeswarm(X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        #shap_values = final_reg.predict(dtrain, pred_contribs=True)\n","        ### Calculate SHAP values for a specific instance (e.g., the first test instance)\n","        ### shap_values = explainer.shap_values(X_test.iloc[0])\n","\n","        self.compute_shapley_barplot(shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        self.compute_shapley_heatmap(shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df)\n","\n","        ####### INDIVIDUAL feature contributions ##############################\n","        ####### compute partial dependence plot of most important features\n","        self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n","\n","        #self.compute_shapley_decision(model_base_value,shap_values.data,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        ##### force plot has some error\n","        #self.compute_shapley_force(model_base_value,shap_values.data,X,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","\n","        del final_reg,X_train\n","        gc.collect()\n","        return\n","\n","\n","    def compute_overall_SHAP_values(self,final_reg,X_train,y_train,train_pred,v1tr):\n","\n","        print(f'\\nGround-Truth Rvol. grand average on train set: {y_train.values.mean()}')\n","        print(f'\\nModel Prediction Rvol. grand average on train set: {train_pred.values.mean()}')\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        X = X_train\n","\n","        ###### Explainer #######\n","        explainer = shap.Explainer(final_reg,X)\n","        shap_values_all = explainer(np.array(X),check_additivity=False)\n","        shap_values_all.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.overall_manual_shapley_addivity_check(train_pred,v1tr,shap_values_all)\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values_all.base_values = shap_values_all.base_values * v1tr.values  # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values_all.values = np.multiply(shap_values_all.values.T ,v1tr.values).T\n","\n","        ###### Beeswarm plot\n","        #### ONLY for Explainer\n","        print(f'\\n Overall Beeswarm plot for all stock ids and time ids')\n","        shap.plots.beeswarm(shap_values_all)\n","        #ax.set_title(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","\n","        ###### Bar plot MEAN Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        ###### Bar plot MAXIMUM Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        del final_reg,X_train,y_train,train_pred,v1tr\n","        gc.collect()\n","        return\n","\n","\n","    def overall_manual_shapley_addivity_check(self,train_pred,v1tr,shap_values_all):\n","\n","        #### ONLY for Explainer\n","        shap_pred_all = ( shap_values_all.base_values + shap_values_all.values.sum(axis=1) ) * v1tr #pd.DataFrame(all_stock_v1tr_df.values.ravel() , columns=['v1tr_all'])['v1tr_all'].values\n","\n","        model_shap_rmspe_all = self.rmspe(train_pred, shap_pred_all)\n","        print(f'\\n Check Additivity of shap values in all stock and time ids, model_shap_rmspe_all: {model_shap_rmspe_all}')\n","\n","        del train_pred,v1tr,shap_values_all\n","        gc.collect()\n","        return\n","\n","\n","\n","    def compute_shapley_heatmap(self,shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #print(' NOTE: Heatmap is sorted with f(X) from smallest values to biggest value !! (picture is wrong)')\n","        # fig,ax = plt.subplots(figsize=(13.5,2))\n","        # y_asc = np.sort( all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values )\n","        # ax.plot( range(len(y_asc)), y_asc, color='g')\n","        # ax.axhline(y_asc.mean(),color='r', linestyle='dashed')\n","        # ax.set_ylabel('Correct f(x) in Asc. order')\n","        # ax.set_yticks(np.arange(0,max(y_asc),0.002))\n","        # fig.show()\n","\n","        fig,ax = plt.subplots()\n","        # order = np.argsort(all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values)\n","        shap.plots.heatmap(shap_values,instance_order=shap_values.sum(1))\n","        #ax.title(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values,all_stock_train_pred_df\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_decision(self,model_base_value,shap_values,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nDECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.decision(model_base_value, shap_values,feature_names=feature_names)\n","        #ax.title(f'\\n DECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_force(self,model_base_value,shap_values,X,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # ### ONLY for Explainer\n","        # print(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig,ax = plt.subplots()\n","        # shap.plots.force(model_base_value,shap_values=shap_values[0],features=X[0],feature_names=feature_names, show=True) #matplotlib=True,\n","        # ax.title(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig.show()\n","\n","        # example_index = 0  # You can change this index to any other example\n","        # example = X[example_index]\n","        # # Explain the prediction of the example\n","        # shap.force_plot(explainer.expected_value, shap_values[example_index], example, feature_names=data.feature_names)\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","\n","\n","    def make_predictions(self,best_params,num_rounds ):\n","        train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","\n","        X_train = full_train_df[self.feat_cols_list]\n","        y_train = full_train_df[self.target_name] # target\n","        X_test = self.test_df[self.feat_cols_list]\n","        y_test = self.test_df[self.target_name] # target\n","\n","        # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        # v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train **-2 * v1tr**2\n","        w_test = y_test **-2 * v1ts**2\n","\n","        print('Final model')\n","        dtrain = xgb.DMatrix(X_train, label=y_train/v1tr,weight=w_train,enable_categorical=True )\n","        dtest = xgb.DMatrix(X_test, label=y_test/v1ts,weight=w_test,enable_categorical=True )\n","        watchlist  = [(dtrain,'train_loss')]\n","        evals_result = {}\n","        final_reg = xgb.train(params=best_params, dtrain=dtrain, num_boost_round=num_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE, evals_result=evals_result,maximize=False, verbose_eval=False)\n","        #test_error = evals_result['test_loss']\n","        train_pred = final_reg.predict(dtrain)*v1tr\n","        test_pred = final_reg.predict( dtest )*v1ts\n","\n","\n","        os.chdir('/content/drive/MyDrive/optiver_real_vol/temp_results')\n","        pickle.dump(final_reg, open(\"final_reg_tsfresh_feat_select.pkl\", 'wb' ) )\n","        pickle.dump(test_pred, open(\"test_pred_tsfresh_feat_select.pkl\", 'wb' ))\n","        pickle.dump(y_test,open(\"y_test_tsfresh_feat_select.pkl\", 'wb' ))\n","        pickle.dump(train_pred,open(\"train_pred_tsfresh_feat_select.pkl\", 'wb' ))\n","        pickle.dump(y_train,open(\"y_train_tsfresh_feat_select.pkl\", 'wb' ))\n","        pickle.dump(X_train,open(\"X_train_tsfresh_feat_select.pkl\",\"wb\"))\n","        pickle.dump(v1tr,open(\"v1tr_tsfresh_feat_select.pkl\",\"wb\"))\n","        pickle.dump(w_train,open(\"w_train_tsfresh_feat_select.pkl\",\"wb\"))\n","\n","        del full_train_df,X_train,X_test #,feat_names\n","        gc.collect()\n","\n","        return final_reg,test_pred, y_test,train_pred,y_train\n","\n","\n","    def compute_train_avg_target_rvol(self, unique_stock_ids, y_train):\n","        unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_target_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            train_target_df.loc[t_index, s] = y_train[st_index].values\n","        train_avg_target_rvol = train_target_df.ffill().bfill().mean(axis=1)\n","        return train_avg_target_rvol\n","\n","    def compute_test_avg_target_rvol(self, unique_stock_ids, y_test):\n","        unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        test_target_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            test_target_df.loc[t_index, s] = y_test[st_index].values\n","        test_avg_target_rvol = test_target_df.ffill().bfill().mean(axis=1)\n","        return test_avg_target_rvol\n","\n","\n","    def fraction_above_average(self,signal1, avg):\n","        # Count the fraction of times when signal1 is above signal2\n","        fraction_above_avg = (signal1 > avg).mean()\n","        return fraction_above_avg\n","\n","\n","    def compute_all_stock_v1tr_df(self, unique_stock_ids, v1tr):\n","        unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        all_stock_v1tr_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_v1tr_df.loc[t_index, s] = v1tr[st_index].values\n","        all_stock_v1tr_df = all_stock_v1tr_df.ffill().bfill()\n","        return all_stock_v1tr_df\n","\n","    def compute_all_stock_train_pred_df(self, unique_stock_ids, train_pred):\n","        unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        all_stock_train_pred_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_train_pred_df.loc[t_index, s] = train_pred[st_index].values\n","        all_stock_train_pred_df = all_stock_train_pred_df.ffill().bfill()\n","        return all_stock_train_pred_df\n","\n","    def compute_all_stock_test_pred_df(self, unique_stock_ids, test_pred):\n","        unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        all_stock_test_pred_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_test_pred_df.loc[t_index, s] = test_pred[st_index].values\n","        all_stock_test_pred_df = all_stock_test_pred_df.ffill().bfill()\n","        return all_stock_test_pred_df\n","\n","\n","    def compute_all_stock_y_train_df(self, unique_stock_ids, y_train):\n","        unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        all_stock_y_train_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_y_train_df.loc[t_index, s] = y_train[st_index].values\n","        all_stock_y_train_df = all_stock_y_train_df.ffill().bfill()\n","        return all_stock_y_train_df\n","\n","    def compute_all_stock_y_test_df(self, unique_stock_ids, y_test):\n","        unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        all_stock_y_test_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_y_test_df.loc[t_index, s] = y_test[st_index].values\n","        all_stock_y_test_df = all_stock_y_test_df.ffill().bfill()\n","        return all_stock_y_test_df\n","\n","\n","\n","    ######## Identify stocks belonging to clusters based on clusterings in dataset\n","    ######## find stock ids of clusters having same feature values\n","    ######## This is reverse-engineering cluster labels of already clustered stocks\n","    def calculate_cluster_fraction(self, column, n_clusters, stock_list):\n","        \"\"\" This function computes the fraction of stock ids in stock_list inside a cluster in the clustering feature.\n","        The fraction is between 0 - 1. 1 indicates all the stock ids in stock_list are in a particular cluster.\n","        \"\"\"\n","\n","        # self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        # self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","\n","        # unique_stock_ids = self.train_stock_id.unique()\n","        # time_id_order = df2.loc[:3829,'time_id'].values\n","        # train_time_id_ind = int(len(time_id_order)*0.7)\n","\n","        # train_time_ids = time_id_order[:train_time_id_ind]\n","        # train_stock_id = df2[df2['time_id'].isin(train_time_ids)]['stock_id']\n","        # train_time_id = df2[df2['time_id'].isin(train_time_ids)]['time_id']\n","\n","        unique_stock_ids = self.train_stock_id.unique()\n","        train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_col_df = self.df[self.df['time_id'].isin(train_time_ids)][column]\n","\n","        ## reshape the dataframe\n","        unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        all_stock_column_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_column_df.loc[t_index, s] = train_col_df[st_index].values\n","        all_stock_column_df = all_stock_column_df.ffill().bfill()\n","\n","        features = all_stock_column_df.T.to_numpy()\n","\n","        ## kmeans\n","        kmeans = KMeans(n_clusters=n_clusters,n_init=10)\n","        kmeans.fit(features)\n","        cluster_labels = kmeans.labels_\n","        cluster_labels\n","\n","        clusters_dict = {}\n","        unique_labels = np.unique(cluster_labels)\n","        for label in unique_labels:\n","            indices = np.where(cluster_labels == label)[0]\n","            stocks_in_cluster = unique_stock_ids[indices]\n","            clusters_dict[label] = stocks_in_cluster.tolist()\n","\n","        for c in clusters_dict.keys():\n","            cnt=0\n","            for s in stock_list:\n","                if s in clusters_dict[c]:\n","                    cnt+=1\n","            print(f'cluster: {c}, # stock ids in cluster: {cnt}, clustering fraction: {cnt/len(clusters_dict[c])}')\n","\n","        return\n","\n","\n","\n","    def check_stock_list_in_all_clustering_features(self, stock_list):\n","\n","        clustering_features_list = [    \"log_target_vol_corr_32_clusters_stnd\",\n","                                        \"log_target_vol_sum_stats_16_clusters_stnd\",\n","                                        \"sum_stats_4_clusters_labels\",\n","                                        \"sum_stats_10_clusters_labels\",\n","                                        \"sum_stats_16_clusters_labels\",\n","                                        \"sum_stats_30_clusters_labels\",\n","                                        \"pear_corr_32_clusters_labels\",\n","                                        \"pear_corr_4_clusters_labels\",\n","                                        \"pear_corr_49_clusters_labels\",\n","                                        \"pear_corr_90_clusters_labels\",]\n","\n","        print('stock_list: ' , stock_list)\n","        for feature in clustering_features_list:\n","            n_clusters = int(re.findall(r'\\d+', feature)[0])\n","            print('Feature: ', feature)\n","            print('Cluster Fractions: ')\n","            print(self.calculate_cluster_fraction( feature, n_clusters, stock_list))\n","\n","        return\n","\n","\n","    def compute_acf_pacf(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        plt.close('all')\n","        for s in unique_stock_ids[0:1]:#[0:40]:\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            plot_acf(stock_residual, lags=200,ax=ax[0])\n","            plot_pacf(stock_residual, lags=200,ax=ax[1])\n","            ax[0].set_title(f'Autocorrelation of stock {s} Residuals on train set')\n","            ax[1].set_title(f'Partial Autocorrelation of stock {s} Residuals on train set')\n","            ax[0].set_xticks(range(0,200,5))\n","            ax[1].set_xticks(range(0,200,5))\n","            ax[0].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_xlabel('lags')\n","            ax[0].set_ylabel('ACF')\n","            ax[1].set_ylabel('PACF')\n","            ax[0].grid(True)\n","            ax[1].grid(True)\n","            fig.show()\n","        return\n","\n","\n","\n","    def compute_IFFT(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","\n","        ##### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        ##### IFFT plot of reconstructed time series ######\n","        plt.close('all')\n","        for s in unique_stock_ids[100:]:#[40:112]:\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            x = stock_residual.values\n","            limit = 0.00001\n","\n","            n=len(x)\n","            fhat = np.fft.fft(x,n)\n","            PSD = fhat*np.conj(fhat) / n\n","            freq = (1/n)*np.arange(n)\n","            start=1 #ignore dc component\n","            L = np.arange(start,np.floor(n/2),dtype='int')\n","            # fig,ax = plt.subplots(figsize=(30,6))\n","            # #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            # ax.axhline(limit,  color='k', linestyle='-')\n","            # ax.plot(freq[L],PSD[L])\n","            # ax.set_xlabel('freq')\n","            # ax.set_ylabel('mag')\n","            # ax.set_title(f'mag plot of stock: {s} residual')\n","            # fig.show()\n","\n","            indices = PSD > limit\n","            num_freqs = len(np.where(indices>0)[0])\n","            print('# of frequencies in residual = ',num_freqs)\n","\n","            fhat = fhat*indices\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            ffilt = np.fft.ifft(fhat)\n","            ax[0].plot(np.arange(0,len(x)),ffilt.real,label='top '+str(num_freqs)+' frequencies in residual (train set)',c='g',alpha=1)\n","            ax[0].plot(np.arange(0,len(x)),x,label='original residual',c='r',alpha=0.2)\n","            ax[0].legend()\n","            ax[0].grid()\n","            ax[0].set_xlabel('time id')\n","            ax[0].set_ylabel('residual')\n","            ax[0].set_title(f'IFFT of stock: {s} residual')\n","\n","\n","            x1 = all_stock_y_train_df[s].values\n","            limit1 = 0.00001\n","            n1=len(x1)\n","            fhat1 = np.fft.fft(x1,n1)\n","            PSD1 = fhat1*np.conj(fhat1) / n1\n","            freq1 = (1/n1)*np.arange(n1)\n","            start1=1 #ignore dc component\n","            L1 = np.arange(start1,np.floor(n1/2),dtype='int')\n","            fig1,ax1 = plt.subplots(figsize=(30,6))\n","            #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            ax1.axhline(limit1,  color='k', linestyle='-')\n","            ax1.plot(freq1[L],PSD1[L])\n","            ax1.set_xlabel('freq')\n","            ax1.set_ylabel('mag')\n","            ax1.set_title(f'mag plot of stock: {s} rvol.')\n","            fig1.show()\n","\n","            indices1 = PSD1 > limit1\n","            num_freqs1 = len(np.where(indices1>0)[0])\n","            print('# of frequencies in rvol. = ',num_freqs1)\n","            fhat1 = fhat1*indices1\n","            ffilt1 = np.fft.ifft(fhat1)\n","            ax[1].plot(np.arange(0,len(x1)),ffilt1.real,label='top '+str(num_freqs1)+' frequencies in true rvol. (train set)',c='g',alpha=1)\n","            ax[1].plot(np.arange(0,len(x1)),x1,label='original true rvol.',c='r',alpha=0.2)\n","            ax[1].legend()\n","            ax[1].grid()\n","            ax[1].set_xlabel('time id')\n","            ax[1].set_ylabel('rvol.')\n","            ax[1].set_title(f'IFFT of stock: {s} rvol.')\n","            fig.show()\n","        return\n","\n","\n","\n","    def overall_stock_id_analysis(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals):\n","\n","        ###### Bar plot of RMSPE for all stocks in the training set\n","        fig, ax = plt.subplots(figsize=(40,10))\n","        rmspe_per_stock_train = []\n","        for s in unique_stock_ids:\n","            rmspe_per_stock_train.append( np.mean( ((all_stock_train_pred_df[s]-all_stock_y_train_df[s])/all_stock_y_train_df[s])**2 )**0.5  )\n","        all_stock_rmspe = pd.Series(rmspe_per_stock_train,index=unique_stock_ids)\n","        smallest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index.values[:10]\n","        largest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n","        ax.text(0,0.52,f'largest RMSPE stocks: {largest_10_rmspe_stocks}')\n","        ax.text(0,0.62,f'smallest RMSPE stocks: {smallest_10_rmspe_stocks}')\n","        ax.bar(unique_stock_ids, rmspe_per_stock_train)\n","        ax.set_xticks(unique_stock_ids)\n","        ax.set_yticks(np.arange(0, 1.1, 0.04))\n","        ax.grid()\n","        ax.set_title('RMSPE of Real. Vol. per stock on train set')\n","        ax.set_xlabel('Stock ID')\n","        ax.set_ylabel('RMSPE')\n","        fig.show()\n","        plt.close()\n","        ## check if the largest and smallest fall into a cluster of a clustering feature\n","        print('\\n 10_largest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = largest_10_rmspe_stocks)\n","        print('\\n 10_smallest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = smallest_10_rmspe_stocks)\n","\n","        ####### scatter plot of True Real. Vol. vs. Pred Real. Vol.\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(y_train, train_pred, c='b', )\n","        ax.plot(y_train, y_train, c='r',linestyle='solid' )\n","        ax.set_title('Scatter Plot of True vs Predicted Values on train set')\n","        ax.set_xlabel('True train rvol. Values')\n","        ax.set_ylabel('Predicted train rvol. Values')\n","        fig.show()\n","        plt.close()\n","\n","        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(y_train, train_residuals, c='c', )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        ax.axhline(y=np.mean(y_train), color='r', linestyle='-')\n","        ax.set_title(' True R.V. Vs. Residuals Values Plot on train set')\n","        ax.set_xlabel('True train Values')\n","        ax.set_ylabel('Train residuals')\n","        fig.show()\n","        plt.close()\n","\n","\n","        ####### scatter plot of Fitted rvol. Values Vs. train residuals Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(train_pred, train_residuals, c='m', )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        ax.axhline(y=np.mean(y_train), color='r', linestyle='-')\n","        ax.set_title(' fitted R.V. Vs. Residuals Values Plot on train set')\n","        ax.set_xlabel('fitted train Values')\n","        ax.set_ylabel('Train residuals')\n","        fig.show()\n","        plt.close()\n","\n","        ## Normal Q-Q Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        sm.qqplot(train_residuals, line='q', ax=ax)\n","        ax.set_title('QQ Plot of Residuals on train set')\n","        ax.set_xlabel('Theoretical Quantiles')\n","        ax.set_ylabel('Sample Quantiles')\n","        fig.show()\n","        plt.close()\n","\n","        ## y_train and train_pred Distributions Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( y_train,bins=1000, color='green', alpha=0.9, histtype='bar', rwidth=0.8)\n","        plt.hist( train_pred,bins=1000, color='red', alpha=0.3, ec='r')\n","        ax.set_title(f'Distribution of y_train (skew: {stats.skew(y_train)} , kurt:{stats.skew(y_train)}) and train_pred (skew: {stats.skew(train_pred)} , kurt:{stats.skew(train_pred)}) on train set')\n","        ax.set_xlabel(' y_train and train_pred')\n","        ax.set_ylabel('frequency')\n","        fig.legend(loc=\"upper left\")\n","        fig.show()\n","        plt.close()\n","\n","        ## Residuals Distribution Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( train_residuals,bins=1000)\n","        ax.set_title('Distribution of Residuals on train set')\n","        ax.set_xlabel('train Residuals')\n","        ax.set_ylabel('frequency')\n","        fig.show()\n","        plt.close()\n","\n","        del unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals\n","        gc.collect()\n","        return\n","\n","\n","    def individual_stock_id_analysis(self,picked_stock_id,unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,residuals,train_avg_target_rvol,set_name):\n","\n","        ## 1. scatter plot\n","        plt.figure(figsize=(10,10))\n","        plt.scatter(all_stock_y_train_df[picked_stock_id],all_stock_train_pred_df[picked_stock_id], c='blue',label=picked_stock_id, alpha=0.4)\n","        plt.plot(all_stock_y_train_df[picked_stock_id],all_stock_y_train_df[picked_stock_id],linestyle='solid', c='red',label=picked_stock_id, alpha=1 )\n","        plt.grid()\n","        plt.xlabel('y_train')\n","        plt.ylabel('train_pred')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s scatter plot of y_train vs. train_pred on train set\")\n","        #plt.show()\n","        plt.close()\n","        ## 2. Line plot of true vs average real. vol.\n","        fraction_above_avg = self.fraction_above_average(all_stock_y_train_df[picked_stock_id], train_avg_target_rvol)\n","        plt.figure(figsize=(30,5))\n","        plt.text(1,0.0275,f'fraction of times above mean = {fraction_above_avg}')\n","        plt.plot(range(len(all_stock_y_train_df[picked_stock_id])),all_stock_y_train_df[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.4 )\n","        plt.plot(range(len(train_avg_target_rvol)),train_avg_target_rvol,linestyle='solid', c='blue',label='train_avg_target_rvol', alpha=0.4 )\n","        plt.grid()\n","        plt.xlabel('time id')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s line plot of True y_train vs. train_avg_target_rvol on train set\")\n","        #plt.show()\n","        plt.close()\n","        ## 3. Line plot of pred vs true real. vol.\n","        plt.figure(figsize=(30,5))\n","        plt.plot(range(len(all_stock_y_train_df[picked_stock_id])),all_stock_y_train_df[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.7 )\n","        plt.plot(range(len(all_stock_train_pred_df[picked_stock_id])),all_stock_train_pred_df[picked_stock_id],linestyle='solid', c='red',label='Pred stock id: '+str(picked_stock_id), alpha=0.4 )\n","        plt.grid()\n","        plt.xlabel('time id')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s line plot of True y_train vs train_pred on train set\")\n","        #plt.show()\n","        plt.close()\n","\n","        ###### Autocorrelation Plot\n","        fig, ax = plt.subplots(figsize=(10,3))\n","        plot_acf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        ax.set_xlabel('Lag')\n","        ax.set_ylabel('Autocorrelation')\n","        ax.set_yticks(np.arange(-1, 1, 0.1))\n","        ax.grid()\n","        ax.set_title(f'Autocorrelation of {set_name} Residuals')\n","        fig.show()\n","\n","        ###### Partial Autocorrelation Plot\n","        fig, ax = plt.subplots(figsize=(10,3))\n","        plot_pacf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        ax.set_xlabel('Lag')\n","        ax.set_ylabel('Partial Autocorrelation')\n","        ax.set_yticks(np.arange(-1, 1, 0.1))\n","        ax.grid()\n","        plt.title(f'Partial Autocorrelation of {set_name} Residuals')\n","        plt.show()\n","\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        self.compute_acf_pacf(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n","\n","\n","        # #### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        # #### IFFT plot of reconstructed time series ######\n","        # self.compute_IFFT(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n","\n","        del picked_stock_id,unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,residuals\n","        gc.collect()\n","        return\n","\n","\n","    def overall_time_id_analysis(self, all_stock_train_pred_df,all_stock_y_train_df,train_avg_target_rvol):\n","\n","        ###### Bar plot of RMSPE for all time ids in the training set\n","        fig, ax = plt.subplots(2,1,figsize=(40,10))\n","        rmspe_per_time_id_train = []\n","        unique_time_ids = all_stock_train_pred_df.index\n","        for t in unique_time_ids:\n","            rmspe_per_time_id_train.append( np.mean( ((all_stock_train_pred_df.loc[t]-all_stock_y_train_df.loc[t])/all_stock_y_train_df.loc[t])**2 )**0.5  )\n","\n","        all_time_id_rmspe = pd.Series(rmspe_per_time_id_train,index=unique_time_ids)\n","        smallest_10_rmspe_time_ids = all_time_id_rmspe.sort_values(ascending=True).index.values[:10]\n","        largest_10_rmspe_time_ids = all_time_id_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n","        ax[0].text(0,0.2,f'10 smallest RMSPE time ids: {smallest_10_rmspe_time_ids}')\n","        ax[0].bar(smallest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[smallest_10_rmspe_time_ids])\n","        ax[0].set_yticks(np.arange(0, 0.4, 0.04))\n","        ax[0].set_ylabel('RMSPE')\n","        ax[0].set_title('10 smallest RMSPE time ids on train set')\n","        ax[0].grid()\n","        fig.show()\n","        ax[1].text(0,1.9,f'10 largest RMSPE time ids: {largest_10_rmspe_time_ids}')\n","        ax[1].bar(largest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[largest_10_rmspe_time_ids])\n","        ax[1].set_yticks(np.arange(0, 2.0, 0.08))\n","        ax[1].set_ylabel('RMSPE')\n","        ax[1].set_title('10 largest RMSPE time ids on train set')\n","        ax[1].grid()\n","        fig.show()\n","\n","\n","        ###### visualize the time ids with largest and smallest RMSPE on the average rvol. plot on training set\n","        plt.figure(figsize=(30,5))\n","        x_time_id_idx = range(len(train_avg_target_rvol))\n","        plt.plot(x_time_id_idx,train_avg_target_rvol,linestyle='solid', c='blue',label='train_avg_target_rvol', alpha=0.4 )\n","        large_idx = np.where(np.isin(unique_time_ids,largest_10_rmspe_time_ids))[0]\n","        red_colors = ['black','darkred','crimson','lightcoral','indianred','orchid','hotpink','palevioletred','violet','plum']\n","        for i,s in enumerate(large_idx):\n","            plt.axvline(x=s, ymin=0, ymax=1,color=red_colors[i],linestyle='-',label=str(i))\n","        small_idx = np.where(np.isin(unique_time_ids,smallest_10_rmspe_time_ids))[0]\n","        green_colors = ['gold','yellow','blue','darkgreen','lime','seagreen','mediumseagreen','springgreen','aquamarine','turquoise','lightgreen']\n","        for j,l in enumerate(small_idx):\n","            plt.axvline(x=l, ymin=0, ymax=1,color=green_colors[j],linestyle='-',label=str(j))\n","        plt.grid()\n","        plt.yticks(np.arange(0, 0.04, 0.01))\n","        plt.xlabel('sequential time id index')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.show()\n","\n","\n","        del all_stock_train_pred_df,all_stock_y_train_df,train_avg_target_rvol\n","        gc.collect()\n","        return\n","\n","\n","    def compute_model_bias_variance(self,y_test,y_train,X_train,best_mlxtend_xgb_params):\n","\n","        ## model bias and variance measurement\n","        # estimate bias and variance\n","        train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","\n","        X_train = full_train_df[self.feat_cols_list]\n","        y_train = full_train_df[self.target_name] #target\n","        X_test = self.test_df[self.feat_cols_list]\n","        y_test = self.test_df[self.target_name] #target\n","\n","        # Assuming best_mlxtend_xgb_params contains the hyperparameters\n","        max_depth, eta, subsample, colsample_bytree, gamma, reg_alpha, reg_lambda, min_child_weight, num_rounds = best_mlxtend_xgb_params\n","\n","        # Create XGBRegressor model\n","        xgb_model = XGBRegressor(\n","            max_depth=max_depth,\n","            learning_rate=eta,\n","            subsample=subsample,\n","            colsample_bytree=colsample_bytree,\n","            gamma=gamma,\n","            reg_alpha=reg_alpha,\n","            reg_lambda=reg_lambda,\n","            min_child_weight=min_child_weight,\n","            n_estimators=num_rounds,\n","            objective='reg:squarederror',\n","            tree_method = \"hist\",\n","            device = \"cuda\"\n","        )\n","\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train **-2 * v1tr**2\n","        w_test = y_test **-2 * v1ts**2\n","\n","        # Train XGBRegressor model\n","        xgb_model.fit(X_train.values, y_train.values/v1tr.values, sample_weight=w_train)\n","\n","        # Now you can use bias_variance_decomp\n","        mse, bias, var = bias_variance_decomp(xgb_model, X_train.values, y_train.values/v1tr.values, X_test.values, y_test.values/v1ts.values, loss='mse', num_rounds=30, random_seed=1)\n","        print('\\nMSE: %.3f' % mse)\n","        print('Bias: %.3f' % bias)\n","        print('Variance: %.3f' % var)\n","\n","        return\n","\n","\n","    def evaluate_predictions(self,final_reg,test_pred, y_test,train_pred,y_train,X_train,v1tr,w_train,best_mlxtend_xgb_params):\n","\n","        y_true = y_test\n","        y_pred = test_pred\n","        test_residuals = y_true - y_pred\n","        train_residuals = y_train - train_pred\n","        unique_stock_ids = self.train_stock_id.unique()\n","\n","        all_stock_train_pred_df = self.compute_all_stock_train_pred_df(unique_stock_ids, train_pred)\n","        all_stock_v1tr_df = self.compute_all_stock_v1tr_df(unique_stock_ids, v1tr)\n","        all_stock_test_pred_df = self.compute_all_stock_test_pred_df( unique_stock_ids, test_pred)\n","        all_stock_y_train_df = self.compute_all_stock_y_train_df(unique_stock_ids, y_train)\n","        all_stock_y_test_df = self.compute_all_stock_y_test_df( unique_stock_ids, y_test)\n","\n","        train_avg_target_rvol = self.compute_train_avg_target_rvol(unique_stock_ids, y_train)\n","\n","        print('\\n####################################### PREDICTION #################################################')\n","\n","        v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        print('corr(y_pred/v1ts, y_true/v1ts)',self.nancorr(       y_pred/v1ts ,        y_true/v1ts ))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred/v1ts), np.log(y_true/v1ts)))\n","        print('corr(y_pred, y_true)',self.nancorr(y_pred, y_true))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred), np.log(y_true)))\n","        print(f'RMSPE train score: ',  np.mean( ((train_pred-y_train)/y_train)**2 )**0.5  )\n","        print(f'RMSPE test score: ',  np.mean( ((y_pred-y_true)/y_true)**2 )**0.5  )\n","\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET #########################################################\n","        print('\\n####################################### TRAINING SET predictions #################################################')\n","        ###################################################################################################################\n","\n","        ################################################################################################\n","        ############################## OVERALL STOCK ANALYSIS START ######################################\n","\n","        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n","        #self.overall_stock_id_analysis(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals)\n","\n","        print('\\n############################## OVERALL STOCK ANALYSIS END #################################')\n","        ############################## OVERALL STOCK ANALYSIS END ######################################\n","        ################################################################################################\n","\n","\n","        ################################################################################################\n","        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n","\n","\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n","        ##### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n","        picked_stock_id = 0 #\n","        set_name = 'train'\n","        self.individual_stock_id_analysis(picked_stock_id,unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,train_residuals,train_avg_target_rvol,set_name)\n","\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n","        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS START ##################################\n","        ################################################################################################\n","        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n","\n","\n","        #self.overall_time_id_analysis(all_stock_train_pred_df,all_stock_y_train_df,train_avg_target_rvol)\n","\n","        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY START #########################################\n","        ###################################################################################################################\n","        print('\\n###################################### Feature importance & SHAPLEY START #########################################')\n","\n","        #self.compute_overall_SHAP_values(final_reg,X_train,y_train,train_pred,v1tr)\n","\n","        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n","        stock_id = 0\n","        view_time_ids_start = 0\n","        view_time_ids_end = 500\n","        #self.compute_individual_stock_SHAP_values(final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        print('\\n###################################### Feature importance & SHAPLEY END #########################################')\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY END #########################################\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE START ################################################\n","        ###################################################################################################################\n","\n","         #### Plot top 30 feature importances\n","        fig, ax = plt.subplots(figsize=(10, 10))\n","        xgb.plot_importance(final_reg, importance_type='gain', max_num_features=30, height=0.8, show_values=False)\n","        self.compute_model_bias_variance(y_test,y_train,X_train,best_mlxtend_xgb_params)\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE END #################################################\n","        ##################################################################################################################\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS START ########################################\n","        print('\\n####################################### TESTING SET predictions #################################################')\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","\n","\n","\n","        print('##################################################################################################')\n","\n","\n","        del X_train,y_train, all_stock_train_pred_df, all_stock_v1tr_df ,  all_stock_test_pred_df, all_stock_y_train_df,  all_stock_y_test_df\n","        del y_true, y_pred, test_residuals, train_residuals, unique_stock_ids\n","        gc.collect()\n","        return\n","\n","\n","\n","    def visualize_tree(self,):\n","        # feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n","        # feature_importances.to_csv('feature_importances.csv')\n","        # plt.figure(figsize=(16, 12))\n","        # sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(20), x='average', y='feature')\n","        # plt.title('20 TOP feature importance over {} folds average'.format(folds.n_splits));\n","\n","        # importances = pd.DataFrame({'Feature': model.feature_name(),\n","        #                             'Importance': sum( [model.feature_importance(importance_type='gain') for model in models] )})\n","        # importances2 = importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n","        # importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n","\n","        #TODO: #plot decision tree for interpretability\n","\n","        return\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icuqpFrM8XMB"},"outputs":[],"source":["num_trees = 3000\n","\n","def objective(trial):\n","\n","    #os.chdir(\"c:\\Work\\WORK_PACKAGE\\Demand_forecasting\\BLUESG_Demand_data\\Data-preprocessing_data_generation\")\n","    hand_picked_df = df2 #df2[hand_picked_cols]\n","    t_v_t = train_validate_n_test(hand_picked_df)\n","\n","    ######  SET Hyperparameter's range for tuning ######\n","    early_stopping_rounds = 25\n","    num_round= num_trees\n","    seed1=11\n","    missing_value = -np.inf   # Replace with a suitable value\n","\n","    # Hyperparameters and algorithm parameters are described here\n","    params = {'disable_default_eval_metric': 1,\n","              \"max_depth\": trial.suggest_int('max_depth', 2, 20),\n","            \"eta\": trial.suggest_float(name='eta', low=0.0001, high=1,log=True),\n","            \"subsample\" : round(trial.suggest_float(name='subsample', low=0.3, high=1.0,step=0.1),1),\n","            \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.05, high=0.8,step=0.05),1),\n","            'gamma': trial.suggest_int('gamma', 2, 10),\n","            'reg_alpha': trial.suggest_int('reg_alpha', 3, 10),\n","            'reg_lambda': trial.suggest_int('reg_lambda', 3, 10),\n","            'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n","            \"tree_method\": 'hist',\n","            \"device\": \"cuda\",\n","            \"seed\":seed1,\n","            #'missing': missing_value\n","            }\n","\n","\n","    ######  SET Hyperparameter's range for tuning ######\n","\n","    val_avg_error,best_iteration = t_v_t.xgb_train_validate(params,num_round,early_stopping_rounds,trial)\n","    print(f\"val_avg_error: {val_avg_error}, best_iteration: {best_iteration}\")\n","    trial.set_user_attr(\"best_iteration\", best_iteration)\n","\n","    del t_v_t\n","    gc.collect()\n","    return val_avg_error\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrMtHm-l8at3"},"outputs":[],"source":["\n","\n","#if __name__ == \"__main__\":\n","\n","#optuna.logging.set_verbosity(optuna.logging.WARNING)\n","# study_name= 'Correct_residual_autocorrrelation_HAR_n_target_lag_feat_n_target_pred'\n","\n","study = optuna.create_study(study_name ='Correct_residual_autocorrrelation_HAR_feat' ,direction=\"minimize\")\n","study.optimize(objective, timeout=12000, n_trials=2) # 50\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"Best number of iteration/boosting rounds: \",study.trials[trial.number].user_attrs['best_iteration'])\n","\n","print(\"Trial no.: \",trial.number)\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))\n","\n","#print(\"Best hyperparameters:\", study.best_params)\n","\n","fig = optuna.visualization.plot_parallel_coordinate(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_optimization_history(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_slice(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_param_importances(study)\n","fig.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoUkEVftLitf"},"outputs":[],"source":["print(\"Best trial:\")\n","bset_trial = study.best_trial\n","\n","print(\"Best number of iteration/boosting rounds: \",study.trials[trial.number].user_attrs['best_iteration'])\n","\n","print(\"Trial no.: \",bset_trial.number)\n","print(\"  Value: \", bset_trial.value)\n","\n","print(\"  Params: \")\n","for key, value in bset_trial.params.items():\n","    print(\"    {}: {}\".format(key, value))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8g-7q-l9RkS"},"outputs":[],"source":["# hand_picked_df = df2 #df2[hand_picked_cols]\n","\n","# num_rounds = 638 #838 #study.trials[trial.number].user_attrs['best_iteration']\n","# print('final best iteration: ',num_rounds )\n","# seed1 = 11\n","# missing_value = -np.inf  # Replace with a suitable value\n","\n","\n","# max_depth = 10\n","# eta =  0.02635275365033109\n","# subsample =  1\n","# colsample_bytree =  0.25\n","# gamma =  2\n","# reg_alpha =  4\n","# reg_lambda =  3\n","# min_child_weight =  4\n","\n","\n","# best_mlxtend_xgb_params = [max_depth,eta,subsample,colsample_bytree,gamma,reg_alpha,reg_lambda,min_child_weight,num_rounds]\n","\n","# best_params = { 'disable_default_eval_metric': 1,\n","#               \"max_depth\": max_depth,\n","#             \"eta\": eta,\n","#             \"subsample\" : subsample,\n","#             \"colsample_bytree\":  colsample_bytree,\n","#             'gamma':gamma,\n","#             'reg_alpha': reg_alpha,\n","#             'reg_lambda': reg_lambda,\n","#             'min_child_weight': min_child_weight,\n","#             \"tree_method\": 'hist',\n","#             \"device\": \"cuda\",\n","#             \"seed\":seed1,\n","#             #'missing': missing_value\n","#                }\n","\n","\n","\n","\n","\n","best_trial = study.best_trial\n","\n","hand_picked_df = df2 #df2[hand_picked_cols]\n","\n","num_rounds = study.best_trial.user_attrs['best_iteration']\n","print('final best iteration: ',num_rounds )\n","seed1 = 11\n","missing_value = -np.inf  # Replace with a suitable value\n","\n","\n","max_depth = best_trial.params['max_depth']\n","eta =  best_trial.params['eta']\n","subsample =  best_trial.params['subsample']\n","colsample_bytree =  best_trial.params['colsample_bytree']\n","gamma =  best_trial.params['gamma']\n","reg_alpha =  best_trial.params['reg_alpha']\n","reg_lambda = best_trial.params['reg_lambda']\n","min_child_weight = best_trial.params['min_child_weight']\n","\n","\n","best_mlxtend_xgb_params = [max_depth,eta,subsample,colsample_bytree,gamma,reg_alpha,reg_lambda,min_child_weight,num_rounds]\n","\n","best_params = { 'disable_default_eval_metric': 1,\n","              \"max_depth\": max_depth,\n","            \"eta\": eta,\n","            \"subsample\" : subsample,\n","            \"colsample_bytree\":  colsample_bytree,\n","            'gamma':gamma,\n","            'reg_alpha': reg_alpha,\n","            'reg_lambda': reg_lambda,\n","            'min_child_weight': min_child_weight,\n","            \"tree_method\": 'hist',\n","            \"device\": \"cuda\",\n","            \"seed\":seed1,\n","            #'missing': missing_value\n","               }\n","\n","\n","\n","\n","\n","\n","\n","t_v_t = train_validate_n_test(hand_picked_df)\n","final_reg,test_pred, y_test,train_pred,y_train = t_v_t.make_predictions(best_params,num_rounds)\n","\n","\n","os.chdir('/content/drive/MyDrive/optiver_real_vol/temp_results')\n","final_reg = pickle.load(open(\"final_reg_tsfresh_feat_select.pkl\", 'rb'))\n","test_pred = pickle.load(open(\"test_pred_tsfresh_feat_select.pkl\", 'rb'))\n","y_test = pickle.load(open(\"y_test_tsfresh_feat_select.pkl\", 'rb'))\n","train_pred =pickle.load(open(\"train_pred_tsfresh_feat_select.pkl\", 'rb'))\n","y_train = pickle.load(open(\"y_train_tsfresh_feat_select.pkl\", 'rb'))\n","X_train = pickle.load(open(\"X_train_tsfresh_feat_select.pkl\", 'rb'))\n","v1tr = pickle.load(open(\"v1tr_tsfresh_feat_select.pkl\", 'rb'))\n","w_train = pickle.load(open(\"w_train_tsfresh_feat_select.pkl\", 'rb'))\n","\n","t_v_t.evaluate_predictions(final_reg,test_pred, y_test,train_pred,y_train, X_train,v1tr,w_train,best_mlxtend_xgb_params)\n","\n","# ## save the best model with timestamp for future use.\n","# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","# filename = f'xgb_gpu_{timestamp}.pkl'\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Xgboost_gpu_models/xgb_gpu_model_registry')\n","# with open(filename, 'wb') as file:\n","#     pickle.dump(final_reg, file)\n","# print(f'Model saved to: {filename}')\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/liquidity_features')\n","\n","\n","# del study, trial,t_v_t,final_reg,test_pred, y_test,train_pred,y_train\n","# gc.collect()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyM8Di6xQ7SBSC1LLQBLpIvA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}