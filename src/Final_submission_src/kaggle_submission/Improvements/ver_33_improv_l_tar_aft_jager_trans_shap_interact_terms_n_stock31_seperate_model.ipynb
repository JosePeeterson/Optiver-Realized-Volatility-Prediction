{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0913de49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0913de49",
        "outputId": "cecc8c2c-0687-4e8f-8ad5-5ee88dca76ce"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "# os.chdir('/content/drive/MyDrive/optiver_real_vol')\n",
        "\n",
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Not connected to a GPU')\n",
        "# else:\n",
        "#   print(gpu_info)\n",
        "\n",
        "# from psutil import virtual_memory\n",
        "# ram_gb = virtual_memory().total / 1e9\n",
        "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "# if ram_gb < 20:\n",
        "#   print('Not using a high-RAM runtime')\n",
        "# else:\n",
        "#   print('You are using a high-RAM runtime!')\n",
        "\n",
        "\n",
        "# !pip install plotly_express\n",
        "# !pip install numba\n",
        "# !pip install optuna\n",
        "# !pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c4fec69",
      "metadata": {
        "id": "3c4fec69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import plotly.subplots as sub_plots\n",
        "import plotly.graph_objects as go\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import re\n",
        "\n",
        "import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from numba import jit, njit\n",
        "import numba as nb\n",
        "import plotly_express as px\n",
        "from itertools import combinations, permutations, product, combinations_with_replacement\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from scipy.signal import find_peaks\n",
        "import pickle\n",
        "from joblib import Parallel, delayed\n",
        "import seaborn as sns\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import r2_score\n",
        "import gc\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import scipy as sp\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.cluster import SpectralClustering, MiniBatchKMeans, MeanShift, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.stats import skew, kurtosis\n",
        "import shap\n",
        "from datetime import datetime\n",
        "import ipywidgets as widgets\n",
        "from matplotlib.patches import Rectangle\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from xgboost import plot_tree, plot_importance\n",
        "from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.genmod.generalized_linear_model import GLM\n",
        "import warnings\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import class_weight\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "from xgboost import XGBRegressor\n",
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "import re\n",
        "\n",
        "from matplotlib.pyplot import cm\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ab61745",
      "metadata": {
        "id": "4ab61745"
      },
      "source": [
        "## **Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Oj0U2qtRrYr3",
      "metadata": {
        "id": "Oj0U2qtRrYr3"
      },
      "outputs": [],
      "source": [
        "os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/partial_train_n_full_test')\n",
        "#os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "\n",
        "with open('train_feat_df_reordered.pkl','rb') as f:\n",
        "  train_feat_df_reordered = pickle.load(f)\n",
        "\n",
        "with open('test_feat_df.pkl','rb') as f:\n",
        "  test_feat_df = pickle.load(f)\n",
        "\n",
        "#os.chdir('/content/drive/MyDrive/optiver_real_vol/kaggle/input/optiver-realized-volatility-prediction')\n",
        "\n",
        "##### remove test from training data #####\n",
        "\n",
        "df_train_reordered = train_feat_df_reordered.copy()\n",
        "del train_feat_df_reordered\n",
        "\n",
        "df_test = test_feat_df.copy()\n",
        "del test_feat_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "Zn3_CAq_IBcg",
      "metadata": {
        "id": "Zn3_CAq_IBcg"
      },
      "outputs": [],
      "source": [
        "\n",
        "####################### Improvement 1 #######################\n",
        "## # drop the bottom 25 xgb features\n",
        "bottom_25_xgb_feat = ['tlog_tlog1p_eps1e4_range_bid_price1', 'bp_as_corr2', 'tlog_eps1e4_trade_price_std', 'v1liq2projt20', 'max_price1',\n",
        "                     'max_bid_price2', 'max_ask_price2', 'bs_ap_corr2', 'min_price1', 'ask_lvl2_min_lvl1_size_feat', 'tlog_1p_trade_order_count_std',\n",
        "                     'min_ask_price1', 'tlog_1p_trade_order_count_mean', 'min_bid_price2', 'lvl2_minus_lvl1_bid_n_ask_size_feat', 'bs_ap_corr1',\n",
        "                     'liqt20rf29', 'as_ap_corr1', 'bp_as_corr1', 'as_ap_corr2', 'bs_bp_corr1', 'max_bid_price1', 'min_bid_price1', 'max_ask_price1']\n",
        "df_train_reordered.drop(columns=bottom_25_xgb_feat, inplace=True)\n",
        "df_test = df_test.drop(columns=bottom_25_xgb_feat, inplace=False)\n",
        "\n",
        "\n",
        "####################### Improvement 2 #######################\n",
        "## # drop these clusterings as they are least important clustering type\n",
        "cluster_drop_cols = ['tlog_tlog1p_target_vol_sum_stats_4_clusters',\n",
        "'tlog_tlog1p_target_vol_sum_stats_10_clusters',\n",
        "'tlog_tlog1p_target_vol_sum_stats_16_clusters',\n",
        "'tlog_tlog1p_target_vol_sum_stats_30_clusters',\n",
        "'sum_stats_4_clusters_labels',\n",
        "'sum_stats_10_clusters_labels',\n",
        "'sum_stats_16_clusters_labels',\n",
        "'sum_stats_30_clusters_labels']\n",
        "df_train_reordered.drop(columns=cluster_drop_cols, inplace=True)\n",
        "df_test = df_test.drop(columns=cluster_drop_cols, inplace=False)\n",
        "\n",
        "\n",
        "####################### Improvement 3 #######################\n",
        "## # Add shap. interaction terms\n",
        "interaction_terms_list = [\n",
        "('tlog_tlinear_sad_ask_size2', 'vol1_mean'),\n",
        "('tlog_tlinear_sad_ask_size2', 'log_wap1_log_price_ret_vol'),\n",
        "('tlog_tlinear_sad_size1', 'tlog_eps523_trade_price_n_wap_eqi_price0_dev'),\n",
        "('tlog_tlinear_sad_size1', 'tlog_eps523_trade_price_n_wap1_dev'),\n",
        "('tlog_eps523_trade_price_n_wap_eqi_price0_dev','log_wap1_log_price_ret_vol'),\n",
        "('tlog_eps523_trade_price_n_wap_eqi_price0_dev', 'tlog_first_10_min_vol'),\n",
        "('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20',\n",
        " 'log_wap1_log_price_ret_vol'),\n",
        "('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20',\n",
        " 'tlog_first_10_min_vol'),\n",
        "('tvpl2_rmed2v1', 'tlog_first_10_min_vol'),\n",
        "('tvpl2_rmed2v1', 'log_wap1_log_price_ret_vol'),\n",
        "('wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0',\n",
        " 'log_wap1_log_price_ret_vol'),\n",
        "('wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0',\n",
        " 'tlog_first_10_min_vol'),\n",
        "('wap1_log_price_ret_volstock_mean_from_25', 'log_wap1_log_price_ret_vol'),\n",
        "('wap1_log_price_ret_volstock_mean_from_25', 'tlog_first_10_min_vol'),\n",
        "('v1spprojt15f25_q1', 'log_wap1_log_price_ret_vol'),\n",
        "('v1spprojt15f25_q1', 'tlog_first_10_min_vol'),\n",
        "('soft_stock_mean_tvpl2_:20', 'wap1_log_price_ret_volstock_mean_from_25'),\n",
        "('soft_stock_mean_tvpl2_:20', 'wap1_log_price_ret_volstock_mean_from_20'),\n",
        "('tlog_target_vol_pcorr_3_clusters', 'soft_stock_mean_tvpl2_:20'),\n",
        "('v1proj_29_15_q3', 'log_wap1_log_price_ret_vol'),\n",
        "('v1proj_29_15_q3', 'tlog_first_10_min_vol'),\n",
        "('root_trade_count_smean', 'tlog_first_10_min_vol'),\n",
        "('root_trade_count_smean', 'log_wap1_log_price_ret_vol'),\n",
        "('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n",
        " 'log_wap1_log_price_ret_vol'),\n",
        "('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n",
        " 'tlog_first_10_min_vol'),\n",
        "('wap1_log_price_ret_volstock_mean_from_20', 'log_wap1_log_price_ret_vol'),\n",
        "('wap1_log_price_ret_volstock_mean_from_20', 'tlog_first_10_min_vol')]\n",
        "\n",
        "\n",
        "## # create and add interaction terms to train and test set\n",
        "for pair in interaction_terms_list:\n",
        "    df_train_reordered[f'{pair[0]}_XXX_{pair[1]}'] = df_train_reordered[pair[0]] * df_train_reordered[pair[1]]\n",
        "    df_test[f'{pair[0]}_XXX_{pair[1]}'] = df_test[pair[0]] * df_test[pair[1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "98e5e260",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[(14, 0.01009101783901889),\n",
              " (119, 0.014168401580243904),\n",
              " (32, 0.03741445263985985),\n",
              " (35, 0.06193311142202087),\n",
              " (108, 0.06341247672773109),\n",
              " (123, 0.07127188083781733),\n",
              " (67, 0.08198003256470059),\n",
              " (109, 0.10404378818685812),\n",
              " (13, 0.11095662447992773),\n",
              " (51, 0.11314854112019265)]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "####################### find stocks' target similar to stock 31 target in terms of MSE and Spearman correlation #######################\n",
        "os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data')\n",
        "\n",
        "full_train = pd.read_csv('train.csv')\n",
        "full_train_31 = full_train[full_train['stock_id']==31]\n",
        "\n",
        "distances_dict = {}\n",
        "\n",
        "for stock in full_train['stock_id'].unique()[full_train['stock_id'].unique() != 31]:\n",
        "    full_train_stock = full_train[full_train['stock_id']==stock]\n",
        "    merged_df = pd.merge(full_train_31, full_train_stock, on='time_id', suffixes=('_31', f'_{stock}')).dropna()\n",
        "    MSE = np.mean((merged_df['target_31'] - merged_df[f'target_{stock}'])**2)\n",
        "    spearman_corr = 1 - spearmanr(merged_df['target_31'], merged_df[f'target_{stock}'])[0] # 1 - spearman correlation to get distance\n",
        "    distances_dict[stock] = (MSE, spearman_corr)\n",
        "\n",
        "\n",
        "mse_values = [value[0] for value in distances_dict.values()]\n",
        "spearman_values = [value[1] for value in distances_dict.values()]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# Reshape the data to fit the scaler\n",
        "mse_values = np.array(mse_values).reshape(-1, 1)\n",
        "spearman_values = np.array(spearman_values).reshape(-1, 1)\n",
        "# Fit and transform the data\n",
        "mse_values = scaler.fit_transform(mse_values).flatten()\n",
        "spearman_values = scaler.fit_transform(spearman_values).flatten()\n",
        "\n",
        "# Replace distances_dict values with standardized mse_values and spearman_values\n",
        "for i, stock_id in enumerate(distances_dict.keys()):\n",
        "    distances_dict[stock_id] = (mse_values[i], spearman_values[i])\n",
        "\n",
        "stock_ids = list(distances_dict.keys())\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.scatter(mse_values, spearman_values)\n",
        "for i, stock_id in enumerate(stock_ids):\n",
        "    plt.annotate(stock_id, (mse_values[i], spearman_values[i]))\n",
        "plt.xlabel('Min-max Normalized Mean Squared Error')\n",
        "plt.ylabel('Min-max Normalized (1 - Spearman Correlation)')\n",
        "plt.title('Stock Distance from Stock 31')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Calculate the magnitude (Euclidean distance) for each stock\n",
        "magnitudes = {stock: np.sqrt(mse**2 + spearman**2) for stock, (mse, spearman) in distances_dict.items()}\n",
        "# Sort the stocks by magnitude\n",
        "sorted_stocks = sorted(magnitudes.items(), key=lambda item: item[1])\n",
        "# Display the sorted stocks\n",
        "sorted_stocks[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "464500e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "####################### Improvement 4 #######################\n",
        "## # create seperate training and testing dataframes for stock id 31 using the 5 most similar stocks to stock 31\n",
        "####################### copy these similar stocks to a different dataframe #######################\n",
        "\n",
        "similar_stocks = [stock for stock, _ in sorted_stocks[:5]]  # 5 most similar stocks to stock 31\n",
        "\n",
        "df_train_reordered_for_stock_31 = df_train_reordered.copy()\n",
        "df_test_for_stock_31 = df_test.copy()\n",
        "\n",
        "df_train_reordered_for_stock_31 = df_train_reordered_for_stock_31[df_train_reordered_for_stock_31['stock_id'].isin(similar_stocks)]\n",
        "df_test_for_stock_31 = df_test_for_stock_31[df_test_for_stock_31['stock_id'].isin(similar_stocks)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b3f89aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "####################### Improvement 5 #######################\n",
        "## # drop stock id 31 from training set. \n",
        "df_train_reordered.drop(index=df_train_reordered[df_train_reordered['stock_id']==31].index, inplace=True)\n",
        "df_test.drop(index=df_test[df_test['stock_id']==31].index, inplace=True)\n",
        "os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/partial_train_n_full_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b29976a7",
      "metadata": {
        "id": "b29976a7"
      },
      "outputs": [],
      "source": [
        "class train_validate_n_test(object):\n",
        "\n",
        "    def __init__(self,df_train_reordered, df_test) -> None:\n",
        "\n",
        "        #self.time_id_order = df.loc[:3829,'time_id'].values # select ordered unique time_ids\n",
        "        #self.train_time_id_ind = int(len(self.time_id_order)*0.7)\n",
        "\n",
        "        largest_num_time_id_stock_idx = df_train_reordered.groupby('stock_id')['time_id'].apply(lambda x: x.nunique()).argmax()\n",
        "        largest_num_time_id_stock = df_train_reordered['stock_id'].unique()[largest_num_time_id_stock_idx]\n",
        "        self.time_id_order = df_train_reordered[df_train_reordered['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n",
        "        self.n_folds = 10\n",
        "        folds = TimeSeriesSplit(n_splits=self.n_folds,)# max_train_size=None, gap=10)\n",
        "        #self.splits = folds.split( range( self.train_time_id_ind ) ) # split 70% train time_ids into n_fold splits\n",
        "        nunique_train_time_ids = df_train_reordered['time_id'].nunique()\n",
        "        self.splits = folds.split( range( nunique_train_time_ids ) )\n",
        "\n",
        "        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        #self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n",
        "        #self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n",
        "        self.train_stock_id = df_train_reordered['stock_id']\n",
        "        self.train_time_id = df_train_reordered['time_id']\n",
        "\n",
        "        # test_time_ids = self.time_id_order[self.train_time_id_ind:]\n",
        "        # self.test_df = df[df['time_id'].isin(test_time_ids)]\n",
        "        self.test_time_id = df_test[df_test['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n",
        "        self.test_df = df_test\n",
        "        self.test_stock_id = self.test_df['stock_id']\n",
        "        self.test_time_id = self.test_df['time_id']\n",
        "\n",
        "        #self.df = df\n",
        "        self.df_train_reordered = df_train_reordered\n",
        "\n",
        "        # feature_importances = pd.DataFrame()\n",
        "        cols = list(df_train_reordered.columns)\n",
        "        cols.remove('tlog_target')\n",
        "        cols.remove('target')\n",
        "        cols.remove('time_id')\n",
        "        cols.remove('stock_id')\n",
        "        self.feat_cols_list =  cols #cat_feat_labels+float32_feat_labels+float64_feat_labels # int32_feat_labels+int64_feat_labels+float32_feat_labels+float64_feat_labels\n",
        "        # feature_importances['feature'] = self.feat_cols_list\n",
        "\n",
        "        self.target_name = 'target' # _standardized' log target is easier to transform back than log_target_standardized\n",
        "\n",
        "        self.target_shift = 3\n",
        "        #del df\n",
        "        del df_train_reordered, df_test\n",
        "        gc.collect()\n",
        "\n",
        "    # def onehotencode_cat_var(self,full_set):\n",
        "    #     full_set = cat_feat_labels #full_set.astype({\"stn_id\":str,\"block_id\":str,\"ts_of_day\":str,\"hr_of_day\":str,\"day_of_wk\":str,\"day_of_mn\":str,\"wk_of_mon\":str })\n",
        "    #     full_set = pd.get_dummies(full_set, prefix_sep=\"_\",columns =cat_feat_labels,drop_first=True)\n",
        "    #     #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n",
        "    #     return full_set\n",
        "\n",
        "    #### RMSPE cost function\n",
        "    def rmspe(self,y_true, y_pred):\n",
        "        y_true = np.exp(y_true - self.target_shift) # inverse 'tlog_target' transform\n",
        "        y_pred = np.exp(y_pred - self.target_shift) # inverse 'tlog_target' transform\n",
        "        return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "\n",
        "    # Custom RMSPE objective function\n",
        "    def rmspe_objective(self,preds, dtrain):\n",
        "        labels = dtrain.get_label()\n",
        "        preds = np.exp(preds - self.target_shift) # inverse 'tlog_target' transform\n",
        "        labels = np.exp(labels - self.target_shift) # inverse 'tlog_target' transform\n",
        "        errors = (preds - labels) / labels\n",
        "        gradient = 2 * errors / (1 + errors**2)\n",
        "        hessian = 2 * (1 - errors**2) / (1 + errors**2)**2\n",
        "        return gradient, hessian\n",
        "\n",
        "\n",
        "    def xgb_RMSPE(self,preds, train_data):\n",
        "        labels = train_data.get_label()\n",
        "        return 'RMSPE', round(self.rmspe(y_true = labels, y_pred = preds),5)\n",
        "\n",
        "\n",
        "    def nancorr(self,a, b):\n",
        "        v = np.isfinite(a)*np.isfinite(b) > 0\n",
        "        return np.corrcoef(a[v], b[v])[0,1]\n",
        "\n",
        "\n",
        "    def xgb_train_validate(self,params_xgb,n_rounds,esr,trial):\n",
        "        rmspe_val_score = []\n",
        "        #models= []\n",
        "        test_y_preds = np.zeros(len(self.test_df))\n",
        "        best_iterations = []\n",
        "        learning_train_rmspe = []\n",
        "        learning_val_rmspe = []\n",
        "\n",
        "        for fold_n, (train_index, valid_index) in enumerate(self.splits):\n",
        "            print('Fold:',fold_n+1)\n",
        "            # print('train_index',train_index)\n",
        "            # print('valid_index',valid_index)\n",
        "            train_time_ids = self.time_id_order[train_index]\n",
        "            val_time_ids = self.time_id_order[valid_index]\n",
        "            train_df = self.df_train_reordered[self.df_train_reordered['time_id'].isin(train_time_ids)]\n",
        "            val_df = self.df_train_reordered[self.df_train_reordered['time_id'].isin(val_time_ids)]\n",
        "\n",
        "            X_train = train_df[self.feat_cols_list]\n",
        "            y_train = train_df[self.target_name] # target\n",
        "            #train_weight = train_df['target']\n",
        "            X_valid = val_df[self.feat_cols_list]\n",
        "            y_val = val_df[self.target_name] # target\n",
        "            #val_weight = val_df['target']\n",
        "\n",
        "            v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n",
        "            #v1tr = np.array([1]*len(X_train['log_wap1_log_price_ret_vol']))\n",
        "            v1v = np.exp(  X_valid['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n",
        "            #v1v = np.array([1]*len(X_valid['log_wap1_log_price_ret_vol']))\n",
        "\n",
        "            # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n",
        "            # v1v = np.exp(np.exp(  X_valid['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n",
        "            #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n",
        "\n",
        "            w_train = y_train **-2 * v1tr**2\n",
        "            w_val = y_val **-2 * v1v**2\n",
        "\n",
        "            print('Training....')\n",
        "            dtrain = xgb.DMatrix(X_train, label=(np.log(y_train/v1tr) + self.target_shift),weight=w_train,enable_categorical=True)\n",
        "            dvalid = xgb.DMatrix(X_valid,   label= (np.log( y_val/v1v) + self.target_shift),weight=w_val,enable_categorical=True)\n",
        "            watchlist  = [(dtrain,'train_loss_fold_'+str(fold_n+1)), (dvalid, 'val_loss_fold_'+str(fold_n+1))]\n",
        "            evals_result = {}\n",
        "            reg = xgb.train(params=params_xgb, dtrain=dtrain, num_boost_round=n_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE,  evals_result=evals_result,maximize=False,  early_stopping_rounds=esr,verbose_eval=False)\n",
        "            learning_train_rmspe.append(evals_result['train_loss_fold_'+str(fold_n+1)])\n",
        "            learning_val_rmspe.append(evals_result['val_loss_fold_'+str(fold_n+1)])\n",
        "\n",
        "            #models.append(reg)\n",
        "            best_iterations.append(reg.best_iteration)\n",
        "\n",
        "            p = np.exp(reg.predict(dvalid) - self.target_shift) *v1v\n",
        "            p_inv = p # inverse 'tlog_target' transform\n",
        "            y_val_inv = y_val # np.exp(y_val - self.target_shift) # inverse 'tlog_target' transform\n",
        "            val_score = np.mean( ((p_inv-y_val_inv)/y_val_inv)**2 )**0.5\n",
        "\n",
        "            # full_score += y_val.shape[0]*score**2\n",
        "\n",
        "            print(f'fold: {fold_n+1}, val rmspe score is {val_score}')\n",
        "            print('corr(p/v1v, y_val/v1v)',self.nancorr(       p_inv/v1v ,        y_val_inv/v1v ))\n",
        "            print('log(corr( ))',self.nancorr(np.log(p_inv/v1v), np.log(y_val_inv/v1v)))\n",
        "            print('corr(p, y_val)',self.nancorr(p_inv, y_val_inv))\n",
        "            print('log(corr( ))',self.nancorr(np.log(p_inv), np.log(y_val_inv)))\n",
        "\n",
        "            #test_pred = reg.predict(self.test_df[self.feat_cols_list] )*v1ts ## this method is not suitable for Timeseries cross validation because initial splits are too far from test set.\n",
        "            #test_y_preds += test_pred/self.n_folds\n",
        "\n",
        "            rmspe_val_score.append(val_score)\n",
        "\n",
        "        mean_rmspe_val_score = np.mean(rmspe_val_score)\n",
        "        print(f'mean rmspe val score over {self.n_folds} splits is',mean_rmspe_val_score)\n",
        "        #print(f'mean rmspe test score: ',  np.mean( ((test_y_preds-self.test_df[self.target_name])/self.test_df[self.target_name])**2 )**0.5  ) # target\n",
        "\n",
        "        # Plot learning curves\n",
        "        fig,ax = plt.subplots(2,1,figsize=(10,6))\n",
        "        for fold_n in range(len(rmspe_val_score)):\n",
        "            ax[0].plot(learning_train_rmspe[fold_n]['RMSPE'], label=f'Fold {fold_n+1} Train RMSPE')\n",
        "            ax[0].plot(learning_val_rmspe[fold_n]['RMSPE'],linestyle='dashed', label=f'Fold {fold_n+1} Validation RMSPE')\n",
        "        last_fold = len(rmspe_val_score) - 1\n",
        "        ax[1].plot(learning_val_rmspe[last_fold]['RMSPE'],linestyle='dashed', label=f'Fold {last_fold+1} Validation RMSPE')\n",
        "        ax[1].set_xlabel('Boosting Round')\n",
        "        ax[0].set_ylabel('RMSPE')\n",
        "        ax[1].set_ylabel('RMSPE')\n",
        "        ax[0].legend()\n",
        "        ax[1].legend()\n",
        "        ax[0].grid(True)\n",
        "        ax[1].grid(True)\n",
        "        fig.suptitle(f'Learning Curves, Trial: {trial.number}')\n",
        "        fig.show()\n",
        "\n",
        "        del self.df_train_reordered, X_train, X_valid, y_train, y_val,train_df,val_df,dtrain,dvalid, v1tr, v1v\n",
        "        gc.collect()\n",
        "        return mean_rmspe_val_score,best_iterations[-1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def make_predictions(self,best_params,num_rounds ):\n",
        "        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        #full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n",
        "        full_train_df = self.df_train_reordered\n",
        "\n",
        "        X_train = full_train_df[self.feat_cols_list]\n",
        "        y_train = full_train_df[self.target_name] # target\n",
        "        y_train_inv = y_train  # no change\n",
        "        X_test = self.test_df[self.feat_cols_list]\n",
        "        #train_weight = full_train_df['target']\n",
        "        #y_test = self.test_df[self.target_name] # target\n",
        "        #test_weight = self.test_df['target']\n",
        "\n",
        "        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n",
        "        #v1tr = np.array([1]*len(X_train['log_wap1_log_price_ret_vol']))\n",
        "        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n",
        "        #v1ts = np.array([1]*len(X_test['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n",
        "        w_train = y_train **-2 * v1tr**2\n",
        "        #w_test = test_weight **-2 * v1ts**2\n",
        "\n",
        "        print('Final model')\n",
        "        dtrain = xgb.DMatrix(X_train, label=np.log(y_train/v1tr) + self.target_shift,weight=w_train,enable_categorical=True )\n",
        "        #dtest = xgb.DMatrix(X_test, label=y_test/v1ts,weight=w_test,enable_categorical=True )\n",
        "        dtest = xgb.DMatrix(X_test,enable_categorical=True )\n",
        "        watchlist  = [(dtrain,'train_loss')]\n",
        "        evals_result = {}\n",
        "        final_reg = xgb.train(params=best_params, dtrain=dtrain, num_boost_round=num_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE, evals_result=evals_result,maximize=False, verbose_eval=False)\n",
        "        #test_error = evals_result['test_loss']\n",
        "        train_pred = np.exp(final_reg.predict(dtrain) - self.target_shift)*v1tr\n",
        "        train_pred_inv = train_pred #np.exp(train_pred - self.target_shift) # inverse 'tlog_target' transform\n",
        "        test_pred = np.exp(final_reg.predict( dtest ) - self.target_shift)*v1ts\n",
        "        test_pred_inv = test_pred #np.exp(test_pred - self.target_shift) # inverse 'tlog_target' transform\n",
        "\n",
        "\n",
        "        del full_train_df#,X_train,X_test #,feat_names\n",
        "        gc.collect()\n",
        "\n",
        "        return final_reg,train_pred_inv,test_pred_inv,y_train_inv,X_train,X_test,v1tr,v1ts,w_train, self.target_name\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def local_manual_shapley_additivity_check(self,model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name):\n",
        "\n",
        "        y_train_true = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "        model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "                #### ONLY for Explainer\n",
        "        shap_pred = ( shap_values.base_values + shap_values.values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "\n",
        "                #### ONLY for TreeExplainer\n",
        "        #shap_pred = ( model_base_value + shap_values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "\n",
        "        #print('shap_values.sum(axis=1)',shap_values.sum(axis=1))\n",
        "        # print('shap_values.base_values',shap_values.base_values[0])\n",
        "        # print('shap_values.values',shap_values.values[0].sum())\n",
        "        #print('len(shap_values.values.sum(axis=1))',len(shap_values.values.sum(axis=1)))\n",
        "\n",
        "        model_shap_rmspe = self.rmspe(model_pred, shap_pred)\n",
        "\n",
        "        fig, ax = plt.subplots(2,1,figsize=(30,10))\n",
        "        ax[0].plot(np.arange(0,len(y_train_true)),y_train_true,label='true rvol.',linestyle='dashed',c='g',marker='*',alpha=0.2)\n",
        "        ax[0].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.6)\n",
        "        ax[0].set_title(f'True Rvol. Vs. model predicted Rvol.' )\n",
        "        ax[0].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n",
        "        ax[0].set_ylabel('rvol.')\n",
        "        ax[0].legend()\n",
        "        ax[0].grid(True)\n",
        "\n",
        "        ax[1].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.4)\n",
        "        ax[1].plot(np.arange(0,len(shap_pred)),shap_pred,label='summed shap values prediction',linestyle='dashed',c='r',marker='*',alpha=0.4)\n",
        "        ax[1].set_title(f'Check additivity of shap values, RMSPE:{model_shap_rmspe} between model and shap values prediction' )\n",
        "        ax[1].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n",
        "        ax[1].set_ylabel('rvol.')\n",
        "        ax[1].legend()\n",
        "        ax[1].grid(True)\n",
        "        fig.tight_layout()\n",
        "        fig.show()\n",
        "\n",
        "\n",
        "        del all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_shapley_PDP_n_Scatter(self,feature_name,shap_values,groundtruth,prediction,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n",
        "        ####### compute partial dependence plot of most important features\n",
        "\n",
        "        ###### Partial dependence plot\n",
        "        #fig,ax = plt.subplots()\n",
        "        #shap.plots.partial_dependence(feature_name, model.predict, xgb.DMatrix(X_train,enable_categorical=True), model_expected_value=True, feature_expected_value=True)\n",
        "        #fig.show()\n",
        "\n",
        "        ##### scatter plot\n",
        "        print(f'\\n scatter plot of {feature_name} vs. shap values')\n",
        "        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "\n",
        "        fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n",
        "        inds = shap.utils.potential_interactions(shap_values[:, feature_name], shap_values)\n",
        "        shap.plots.scatter(shap_values[:, feature_name], color=shap_values[:, inds[0]], title=f'scatter plot of {feature_name} Vs. Shap values on {set_name} set', ax=ax[0, 0])\n",
        "        shap.plots.scatter(shap_values[:, feature_name], color=shap_values[:, inds[1]], ax=ax[0, 1])\n",
        "        shap.plots.scatter(shap_values[:, feature_name], color=shap_values[:, inds[2]], ax=ax[1, 0])\n",
        "        plt.show()\n",
        "\n",
        "        error = groundtruth['tlog_target'] - prediction['tlog_target']\n",
        "        ##### scatter plot of feature shap values vs. error\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "        print(f'\\n scatter plot of {feature_name} shap values vs. error')\n",
        "        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        ax.scatter(error.values,shap_values[:, feature_name].values, alpha=0.4 )\n",
        "        ax.set_xlabel('error')\n",
        "        ax.set_ylabel(f'{feature_name} shap values')\n",
        "        ax.grid()\n",
        "        ax.set_title(f'scatter plot of {feature_name} shap values Vs. error on {set_name} set')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        ##### scatter plot of feature vs. True target rvol. on trianing set\n",
        "        # fig,ax = plt.subplots()\n",
        "        # yval = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "        # xval = X[feature_name]\n",
        "        # ax.scatter(xval,yval)\n",
        "        # ax.plot([min(xval), max(xval)], [min(yval),max(yval)], color = 'red', linewidth = 1)\n",
        "        # ax.set_xlabel(feature_name)\n",
        "        # ax.set_ylabel('True target rvol.')\n",
        "        # ax.grid\n",
        "        # ax.set_title(f'scatter plot of {feature_name} Vs. True Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n",
        "        # fig.show()\n",
        "\n",
        "\n",
        "        ##### scatter plot of feature vs. predicted target rvol.on trianing set\n",
        "        # fig,ax = plt.subplots()\n",
        "        # yval1 = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "        # ax.scatter(xval,yval1)\n",
        "        # ax.plot([min(xval), max(xval)], [min(yval1),max(yval1)], color = 'red', linewidth = 1)\n",
        "        # ax.set_xlabel(feature_name)\n",
        "        # ax.set_ylabel('Predicted target rvol.')\n",
        "        # ax.grid\n",
        "        # ax.set_title(f'scatter plot of {feature_name} Vs. Predicted Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n",
        "        # fig.show()\n",
        "\n",
        "\n",
        "        del shap_values\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def compute_shapley_beeswarm(self,shap_values,top_n_feat,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n",
        "\n",
        "        #### ONLY for TreeExplainer\n",
        "        # plt.figure()\n",
        "        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n",
        "        # shap.summary_plot(shap_values)\n",
        "        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        # plt.show()\n",
        "\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig,ax = plt.subplots()\n",
        "        ax.set_title(f'Beeswarm plot for {set_name} set showing top {top_n_feat} features')\n",
        "        shap.plots.beeswarm(shap_values, max_display=top_n_feat)\n",
        "        #ax.set_title(f' stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        plt.show()\n",
        "\n",
        "        del shap_values\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def compute_shapley_barplot(self,shap_values,top_n_feats,X,Y,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n",
        "\n",
        "        #### ONLY for TreeExplainer\n",
        "        # plt.figure()\n",
        "        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n",
        "        # plt.bar(shap_values.abs().sum(axis=1))\n",
        "        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        # plt.show()\n",
        "\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\nMEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig,ax = plt.subplots()\n",
        "        ax.set_title(f'MEAN ABSOLUTE of feature bar plot on {set_name} set showing top {top_n_feats} features')\n",
        "        #clustering = shap.utils.hclust(X,Y)\n",
        "        shap.plots.bar(shap_values, max_display=top_n_feats)#,clustering=clustering,clustering_cutoff=0.9)\n",
        "        #ax.title(f'MEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig.show()\n",
        "\n",
        "\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig,ax = plt.subplots()\n",
        "        #clustering = shap.utils.hclust(X,Y)\n",
        "        ax.set_title(f'MAXIMUM ABSOLUTE of feature bar plot on {set_name} set showing top {top_n_feats} features')\n",
        "        shap.plots.bar(shap_values.abs.max(0), max_display=top_n_feats)#,clustering=clustering,clustering_cutoff=0.9)\n",
        "        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig.show()\n",
        "\n",
        "        del shap_values, X,Y,stock_id,view_time_ids_start,view_time_ids_end,set_name,fig,ax,top_n_feats\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_individual_stock_SHAP_values(self,final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end):\n",
        "\n",
        "        # plot shapley feature importances for all samples\n",
        "        final_reg.set_param({\"device\": \"cuda\"})\n",
        "        shap.initjs()\n",
        "\n",
        "        stock_id = stock_id\n",
        "        view_time_ids_start = view_time_ids_start\n",
        "        view_time_ids_end = view_time_ids_end\n",
        "        X = X_train[X_train['stock_id'].isin([stock_id])].iloc[view_time_ids_start:view_time_ids_end]\n",
        "\n",
        "        ###### Explainer #######\n",
        "        #explainer = shap.Explainer(final_reg,X)\n",
        "        explainer = shap.TreeExplainer(final_reg, feature_perturbation=\"tree_path_dependent\")\n",
        "        shap_values = explainer(np.array(X),check_additivity=False)\n",
        "        shap_values.feature_names = final_reg.feature_names\n",
        "\n",
        "        ###### TreeExplainer #######\n",
        "        # explainer = shap.TreeExplainer(final_reg,feature_perturbation='interventional')\n",
        "        # shap_values = explainer.shap_values(np.array(X),check_additivity=False)\n",
        "        # shap_values.feature_names = final_reg.feature_names\n",
        "\n",
        "        model_base_value = explainer.expected_value\n",
        "        # print(f'Model base value: {model_base_value} before scaling by v1tr')\n",
        "\n",
        "        ####### GLOBAL ALL feature contributions ##############################\n",
        "        ###### Do manual additivity check because it fails\n",
        "        self.local_manual_shapley_additivity_check(model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name )\n",
        "\n",
        "        ####### Manually correct the shap values to accomodate v1tr scaling\n",
        "        shap_values.base_values = shap_values.base_values * all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n",
        "        stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n",
        "        shap_values.values = np.multiply(shap_values.values.T ,stock_v1tr_df).T\n",
        "        ###### check correctness of shap_values\n",
        "        # sp = shap_values.base_values + shap_values.values.sum(axis=1)\n",
        "        # plt.figure(figsize=(30,5))\n",
        "        # plt.plot(range(len(sp)),sp)\n",
        "        # model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n",
        "        # plt.plot(range(len(sp)), model_pred )\n",
        "        # plt.show()\n",
        "\n",
        "        self.compute_shapley_beeswarm(X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n",
        "\n",
        "        #shap_values = final_reg.predict(dtrain, pred_contribs=True)\n",
        "        ### Calculate SHAP values for a specific instance (e.g., the first test instance)\n",
        "        ### shap_values = explainer.shap_values(X_test.iloc[0])\n",
        "\n",
        "        self.compute_shapley_barplot(shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n",
        "\n",
        "        self.compute_shapley_heatmap(shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df)\n",
        "\n",
        "        ####### INDIVIDUAL feature contributions ##############################\n",
        "        ####### compute partial dependence plot of most important features\n",
        "        self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n",
        "\n",
        "        #self.compute_shapley_decision(model_base_value,shap_values.data,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n",
        "\n",
        "        ##### force plot has some error\n",
        "        #self.compute_shapley_force(model_base_value,shap_values.data,X,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n",
        "\n",
        "\n",
        "        del final_reg,X_train\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def identiy_largest_overall_n_under_n_over_prediction_errors(self,groundtruth,prediction,set_name):\n",
        "\n",
        "        # overall error\n",
        "        oveall_error = groundtruth['target'] - prediction['target']\n",
        "        # overall squared percentage error\n",
        "        ovearll_SPE = pd.DataFrame()\n",
        "        ovearll_SPE['target_spe'] = (oveall_error / groundtruth['target'] )**2\n",
        "        ovearll_SPE['stock_id'] = groundtruth['stock_id']\n",
        "        ovearll_SPE['time_id'] = groundtruth['time_id']\n",
        "\n",
        "        # sort the overall_SPE by descending order\n",
        "        top_n_instances = 50\n",
        "        sorted_ovearll_SPE = ovearll_SPE.sort_values('target_spe',ascending=False)[:top_n_instances].reset_index(drop=True)\n",
        "        # Plot the sorted_ovearll_SPE as a bar plot with dual x-axis for stock_id and time_id\n",
        "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "        # Plotting the bar graph for 'target_spe'\n",
        "        ax1.bar(sorted_ovearll_SPE.index, sorted_ovearll_SPE['target_spe'], color='b', alpha=0.6)\n",
        "        ax1.set_xlabel('Instance Index')\n",
        "        ax1.set_ylabel('Target Squared Percentage Error', color='b')\n",
        "        ax1.tick_params(axis='y', labelcolor='b')\n",
        "        # Creating a twin Axes sharing the x-axis\n",
        "        ax2 = ax1.twiny()\n",
        "        # Setting the x-ticks and labels for stock_id and time_id\n",
        "        ax2.set_xticks(sorted_ovearll_SPE.index)\n",
        "        ax2.set_xticklabels(sorted_ovearll_SPE['stock_id'], rotation=90, ha='center')\n",
        "        ax2.set_xlabel('Stock ID')\n",
        "        # Creating another twin Axes sharing the x-axis\n",
        "        ax3 = ax1.twiny()\n",
        "        # Offset the twin axis below the original x-axis\n",
        "        ax3.spines['top'].set_position(('outward', 40))\n",
        "        ax3.set_xticks(sorted_ovearll_SPE.index)\n",
        "        ax3.set_xticklabels(sorted_ovearll_SPE['time_id'], rotation=90, ha='center')\n",
        "        ax3.set_xlabel('Time ID')\n",
        "        plt.title(f'{top_n_instances} Largest Target SPE instances with Stock ID and Time ID on ' + set_name + ' Set')\n",
        "        plt.show()\n",
        "\n",
        "        overall_RMSPE = np.sqrt(np.mean(ovearll_SPE['target_spe']))\n",
        "        corrected_overall_SPE = ovearll_SPE.sort_values('target_spe',ascending=False)[top_n_instances:].reset_index(drop=True)\n",
        "        corrected_overall_RMSPE = np.sqrt(np.mean(corrected_overall_SPE['target_spe']))\n",
        "        print(f'\\nOverall RMSPE: {overall_RMSPE} on {set_name} set')\n",
        "        print(f'Corrected RMSPE: {corrected_overall_RMSPE} on {set_name} set')\n",
        "        print(f'Percentage improvment in RMSPE of ovearall error on {set_name} set after correcting the top {top_n_instances} instances:')\n",
        "        print(f'{(overall_RMSPE - corrected_overall_RMSPE )/overall_RMSPE*100}%')\n",
        "\n",
        "\n",
        "        # underprediction squared percentage error\n",
        "        up_SPE = ovearll_SPE[oveall_error > 0].reset_index(drop=True)\n",
        "        # sort the up_SPE by descending order\n",
        "        sorted_up_SPE = up_SPE.sort_values('target_spe',ascending=False)[:top_n_instances].reset_index(drop=True)\n",
        "        # Plot the sorted_up_SPE as a bar plot with dual x-axis for stock_id and time_id\n",
        "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "        # Plotting the bar graph for 'target_spe'\n",
        "        ax1.bar(sorted_up_SPE.index, sorted_up_SPE['target_spe'], color='b', alpha=0.6)\n",
        "        ax1.set_xlabel('Instance Index')\n",
        "        ax1.set_ylabel('Target Squared Percentage Error', color='b')\n",
        "        ax1.tick_params(axis='y', labelcolor='b')\n",
        "        # Creating a twin Axes sharing the x-axis\n",
        "        ax2 = ax1.twiny()\n",
        "        # Setting the x-ticks and labels for stock_id and time_id\n",
        "        ax2.set_xticks(sorted_up_SPE.index)\n",
        "        ax2.set_xticklabels(sorted_up_SPE['stock_id'], rotation=90, ha='center')\n",
        "        ax2.set_xlabel('Stock ID')\n",
        "        # Creating another twin Axes sharing the x-axis\n",
        "        ax3 = ax1.twiny()\n",
        "        # Offset the twin axis below the original x-axis\n",
        "        ax3.spines['top'].set_position(('outward', 40))\n",
        "        ax3.set_xticks(sorted_up_SPE.index)\n",
        "        ax3.set_xticklabels(sorted_up_SPE['time_id'], rotation=90, ha='center')\n",
        "        ax3.set_xlabel('Time ID')\n",
        "        plt.title(f'{top_n_instances} Largest target SPE Underprediction instances with Stock ID and Time ID on ' + set_name + ' Set')\n",
        "        plt.show()\n",
        "\n",
        "        up_RMSPE = np.sqrt(np.mean(up_SPE['target_spe']))\n",
        "        corrected_up_SPE = up_SPE.sort_values('target_spe',ascending=False)[top_n_instances:].reset_index(drop=True)\n",
        "        corrected_up_RMSPE = np.sqrt(np.mean(corrected_up_SPE['target_spe']))\n",
        "        print(f'\\nUnderprediction RMSPE: {up_RMSPE} on {set_name} set')\n",
        "        print(f'Corrected Underprediction RMSPE: {corrected_up_RMSPE} on {set_name} set')\n",
        "        print(f'Percentage improvment in RMSPE of underprediction error on {set_name} set after correcting the top {top_n_instances} instances:')\n",
        "        print(f'{(up_RMSPE - corrected_up_RMSPE )/up_RMSPE*100}%')\n",
        "\n",
        "\n",
        "        # overprediction squared percentage error\n",
        "        op_SPE = ovearll_SPE[oveall_error < 0].reset_index(drop=True)\n",
        "        # sort the op_SPE by descending order\n",
        "        sorted_op_SPE = op_SPE.sort_values('target_spe',ascending=False)[:top_n_instances].reset_index(drop=True)\n",
        "        # Plot the sorted_op_SPE as a bar plot with dual x-axis for stock_id and time_id\n",
        "        fig, ax1 = plt.subplots(figsize=(15, 8))\n",
        "        # Plotting the bar graph for 'target_spe'\n",
        "        ax1.bar(sorted_op_SPE.index, sorted_op_SPE['target_spe'], color='b', alpha=0.6)\n",
        "        ax1.set_xlabel('Instance Index')\n",
        "        ax1.set_ylabel('Target Squared Percentage Error', color='b')\n",
        "        ax1.tick_params(axis='y', labelcolor='b')\n",
        "        # Creating a twin Axes sharing the x-axis\n",
        "        ax2 = ax1.twiny()\n",
        "        # Setting the x-ticks and labels for stock_id and time_id\n",
        "        ax2.set_xticks(sorted_op_SPE.index)\n",
        "        ax2.set_xticklabels(sorted_op_SPE['stock_id'], rotation=90, ha='center')\n",
        "        ax2.set_xlabel('Stock ID')\n",
        "        # Creating another twin Axes sharing the x-axis\n",
        "        ax3 = ax1.twiny()\n",
        "        # Offset the twin axis below the original x-axis\n",
        "        ax3.spines['top'].set_position(('outward', 40))\n",
        "        ax3.set_xticks(sorted_op_SPE.index)\n",
        "        ax3.set_xticklabels(sorted_op_SPE['time_id'], rotation=90, ha='center')\n",
        "        ax3.set_xlabel('Time ID')\n",
        "        plt.title(f'{top_n_instances} Largest target SPE Overprediction instances with Stock ID and Time ID on ' + set_name + ' Set')\n",
        "        plt.show()\n",
        "\n",
        "        op_RMSPE = np.sqrt(np.mean(op_SPE['target_spe']))\n",
        "        corrected_op_SPE = op_SPE.sort_values('target_spe',ascending=False)[top_n_instances:].reset_index(drop=True)\n",
        "        corrected_op_RMSPE = np.sqrt(np.mean(corrected_op_SPE['target_spe']))\n",
        "        print(f'\\nOverprediction RMSPE: {op_RMSPE} on {set_name} set')\n",
        "        print(f'Corrected Overprediction RMSPE: {corrected_op_RMSPE} on {set_name} set')\n",
        "        print(f'Percentage improvment in RMSPE of overprediction error on {set_name} set after correcting the top {top_n_instances} instances:')\n",
        "        print(f'{(op_RMSPE - corrected_op_RMSPE )/op_RMSPE*100}%')\n",
        "\n",
        "\n",
        "        ovearll_error_idxs = []\n",
        "        up_error_idxs = []\n",
        "        op_error_idxs = []\n",
        "        for i in range(top_n_instances):\n",
        "            ovearll_error_idxs.append(groundtruth[(groundtruth['stock_id'] == sorted_ovearll_SPE['stock_id'][i]) & (groundtruth['time_id'] == sorted_ovearll_SPE['time_id'][i])].index[0])\n",
        "            up_error_idxs.append(groundtruth[(groundtruth['stock_id'] == sorted_up_SPE['stock_id'][i]) & (groundtruth['time_id'] == sorted_up_SPE['time_id'][i])].index[0])\n",
        "            op_error_idxs.append(groundtruth[(groundtruth['stock_id'] == sorted_op_SPE['stock_id'][i]) & (groundtruth['time_id'] == sorted_op_SPE['time_id'][i])].index[0])\n",
        "\n",
        "        return ovearll_error_idxs, up_error_idxs, op_error_idxs\n",
        "\n",
        "\n",
        "\n",
        "    def shapley_analysis_of_large_error_instances(self,ovearll_error_idxs, up_error_idxs, op_error_idxs,shap_values_all,set_name):\n",
        "\n",
        "        print('shap_values_all.feature_names',len(shap_values_all.feature_names))\n",
        "\n",
        "        # decision plot of the largest overall error instances\n",
        "        display_n_features = 40\n",
        "        fig, ax = plt.subplots(figsize=(50,10))\n",
        "        shap.decision_plot(np.mean(shap_values_all.base_values[ovearll_error_idxs]), shap_values_all.values[ovearll_error_idxs,:], \\\n",
        "                           feature_names=shap_values_all.feature_names, feature_order='importance', feature_display_range= slice(-1,-display_n_features,-1),\\\n",
        "                           title=f'Decision plot of the {display_n_features} largest overall error instances on {set_name} set \\n (ignore the WRONG expected value shown)')\n",
        "        plt.show()\n",
        "\n",
        "        # decision plot of the largest overprediction error instances\n",
        "        fig, ax = plt.subplots(figsize=(50,10))\n",
        "        shap.decision_plot(np.mean(shap_values_all.base_values[op_error_idxs]), shap_values_all.values[op_error_idxs,:], \\\n",
        "                            feature_names=shap_values_all.feature_names, feature_order='importance', feature_display_range= slice(-1,-30,-1),\\\n",
        "                            title=f'Decision plot of the {display_n_features} largest overprediction error instances on {set_name} set \\n (ignore the WRONG expected value shown)')\n",
        "        plt.show()\n",
        "\n",
        "        # decision plot of the largest underprediction error instances\n",
        "        fig, ax = plt.subplots(figsize=(50,10))\n",
        "        shap.decision_plot(np.mean(shap_values_all.base_values[up_error_idxs]), shap_values_all.values[up_error_idxs,:], \\\n",
        "                            feature_names=shap_values_all.feature_names, feature_order='importance', feature_display_range= slice(-1,-30,-1),\\\n",
        "                            title=f'Decision plot of the {display_n_features} largest underprediction error instances on {set_name} set \\n (ignore the WRONG expected value shown)')\n",
        "        plt.show()\n",
        "\n",
        "        del shap_values_all,ovearll_error_idxs, up_error_idxs, op_error_idxs,set_name\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def compute_global_SHAP_values(self,final_reg,X_train,y_train,train_pred,v1tr,set_name):\n",
        "\n",
        "        print(f'\\nGround-Truth Rvol. grand average on {set_name} set: {y_train[\"target\"].values.mean()}')\n",
        "        print(f'\\nModel Prediction Rvol. grand average on {set_name} set: {train_pred[\"target\"].values.mean()}')\n",
        "\n",
        "        # plot shapley feature importances for all samples\n",
        "        final_reg.set_param({\"device\": \"cuda\"})\n",
        "        shap.initjs()\n",
        "\n",
        "        X = X_train\n",
        "\n",
        "        ###### Explainer #######\n",
        "        #explainer = shap.Explainer(final_reg,X)\n",
        "        explainer = shap.TreeExplainer(final_reg, feature_perturbation=\"tree_path_dependent\")\n",
        "        shap_values_all = explainer(np.array(X),check_additivity=False)\n",
        "        shap_values_all.feature_names = final_reg.feature_names\n",
        "\n",
        "        model_base_value = explainer.expected_value\n",
        "\n",
        "        ####### GLOBAL ALL feature contributions ##############################\n",
        "        ###### Do manual additivity check because it fails\n",
        "        print(f'\\n -------------------- WARNING ------------------------')\n",
        "        print(f'shapley values are those of log transformed and shifted target so they will not add up to original target')\n",
        "        print(f'shapley values are those of log transformed and shifted target so they will not add up to original target')\n",
        "        print(f'\\n -------------------- WARNING ------------------------')\n",
        "        #self.global_manual_shapley_additivity_check(train_pred,v1tr,shap_values_all,set_name)\n",
        "\n",
        "        ####### Manually correct the shap values to accomodate v1tr scaling\n",
        "        # shap_values_all.base_values = np.exp(shap_values_all.base_values - self.target_shift) * v1tr['wap1_log_price_ret_vol'].values  # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n",
        "        # shap_values_all.values = np.multiply(np.exp(shap_values_all.values.T - self.target_shift),v1tr['wap1_log_price_ret_vol'].values).T\n",
        "\n",
        "        ###### Beeswarm plot\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\n Global Beeswarm plot for all stock ids and time ids')\n",
        "        top_n_feat = 25\n",
        "        self.compute_shapley_beeswarm(shap_values_all,top_n_feat,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name=set_name)\n",
        "\n",
        "        ###### Bar plot MEAN Absolute value of features\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\nMEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n",
        "        #clustering = shap.util.hclust(X,y)\n",
        "        shap.plots.bar(shap_values_all)# clustering=clustering)\n",
        "        #ax.title(f'MEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n",
        "\n",
        "        ###### Bar plot MAXIMUM Absolute value of features\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n",
        "        #clustering = shap.util.hclust(X,y)\n",
        "        shap.plots.bar(shap_values_all.abs.max(0), )#clustering=clustering)\n",
        "        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n",
        "\n",
        "        del final_reg,X_train,y_train,train_pred,v1tr\n",
        "        gc.collect()\n",
        "        return shap_values_all\n",
        "\n",
        "\n",
        "    def global_manual_shapley_additivity_check(self,train_pred,v1tr,shap_values_all, set_name):\n",
        "        #### ONLY for Explainer\n",
        "        shap_pred_all = ( shap_values_all.base_values + shap_values_all.values.sum(axis=1) ) * v1tr['wap1_log_price_ret_vol'] #pd.DataFrame(all_stock_v1tr_df.values.ravel() , columns=['v1tr_all'])['v1tr_all'].values\n",
        "        model_shap_rmspe_all = self.rmspe(train_pred['target'], shap_pred_all)\n",
        "\n",
        "        # # line plot of model prediction vs. shap values prediction\n",
        "        # fig, ax = plt.subplots(figsize=(30,10))\n",
        "        # ax.plot(np.arange(0,len(train_pred['target'])),train_pred['target'],label='prediction',linestyle='dashed',c='g',marker='*',alpha=0.2)\n",
        "        # ax.plot(np.arange(0,len(shap_pred_all)),shap_pred_all,label='shap_value',linestyle='dashed',c='b',marker='*',alpha=0.6)\n",
        "        # ax.set_title(f'Model prediction Vs. Shap_values for additivity check on {set_name} set' )\n",
        "        # ax.set_ylabel('rvol.')\n",
        "        # ax.legend()\n",
        "        # ax.grid(True)\n",
        "        # plt.show()\n",
        "        # plt.close()\n",
        "\n",
        "        # scatter plot of model prediction vs. shap values prediction\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        ax.scatter(train_pred['target'],shap_pred_all)\n",
        "        ax.plot([min(train_pred['target']), max(train_pred['target'])], [min(shap_pred_all),max(shap_pred_all)], color = 'red', linewidth = 1)\n",
        "        ax.set_xlabel('model prediction')\n",
        "        ax.set_ylabel('shap prediction')\n",
        "        ax.grid\n",
        "        ax.set_title(f'scatter plot of model prediction Vs. shap prediction on {set_name} set')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        print(f'\\n Check Additivity of shap values in all stock and time ids, model_shap_rmspe_all: {model_shap_rmspe_all} on {set_name} set')\n",
        "        del train_pred,v1tr,shap_values_all\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def analyze_global_SHAP_values(self,shap_values_all,groundtruth, prediction,set_name):\n",
        "\n",
        "        ###################################### All Error vs. feature Shapley values Analysis #######################################\n",
        "        ## filter out features that have high negative and positively shapley values and see if they are correlated with the error\n",
        "\n",
        "        top_largest_shap_val_feat = 100\n",
        "        error = groundtruth['target'] - prediction['target']\n",
        "        # select only the features that have high shapley values over all instances/samples\n",
        "        abs_summed_shap = np.abs(shap_values_all.values).sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(abs_summed_shap)[::-1]\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(abs_summed_shap)[::-1]]\n",
        "        top_n_feat = 50\n",
        "        print(f'\\nTop {top_n_feat} features based on summed absolute SHAP values over ALL instances/samples on {set_name} set')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        # from the features that have high shapley values over all instances/samples\n",
        "        # find out feature shapley values that are negatively correlated with the error\n",
        "        feat_error_corr_dict = {}\n",
        "        for feat in labels[:top_largest_shap_val_feat]:\n",
        "            # if self.nancorr(shap_values_all.values[:,shap_values_all.feature_names.index(feat)], error) < 0:\n",
        "            #     feat_error_corr_dict[feat] = self.nancorr(shap_values_all.values[:,shap_values_all.feature_names.index(feat)], error)\n",
        "            corr, _ = spearmanr(shap_values_all.values[:, shap_values_all.feature_names.index(feat)], error)\n",
        "            if corr < 0:\n",
        "                feat_error_corr_dict[feat] = corr\n",
        "\n",
        "        # sort the features based on the most negative correlation with the error first\n",
        "        sorted_feat_error_corr_dict = dict(sorted(feat_error_corr_dict.items(), key=lambda item: item[1]))\n",
        "\n",
        "        # scatter plot of features shapley values vs. error over all instances/samples for the top_n_feat features that have high negative correlation with the error\n",
        "        top_n_feat = 28\n",
        "        fig, ax = plt.subplots(int(top_n_feat/4), 4, figsize=(30, 30))\n",
        "        fig.suptitle(f\"{set_name} set\", fontsize=16)\n",
        "        for feat in list(sorted_feat_error_corr_dict.keys())[:top_n_feat]:\n",
        "            ax.flatten()[list(sorted_feat_error_corr_dict.keys()).index(feat)].scatter(shap_values_all.values[:,shap_values_all.feature_names.index(feat)], error, alpha=0.1)\n",
        "            ax.flatten()[list(sorted_feat_error_corr_dict.keys()).index(feat)].set_title(f\"{feat} shap values vs. error \\n, spear. corr: {sorted_feat_error_corr_dict[feat]}\")\n",
        "            ax.flatten()[list(sorted_feat_error_corr_dict.keys()).index(feat)].set_xlabel('error')\n",
        "        fig.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f'\\nTop {top_n_feat} features that have high negative correlation with the error on {set_name} set')\n",
        "        print(list(sorted_feat_error_corr_dict.keys())[:top_n_feat])\n",
        "\n",
        "\n",
        "        ###################################### Underprediction Error Shapley Analysis #######################################\n",
        "        ## filter out instance where underpreidction is happening, then identfy the features that are causing the underprediction\n",
        "        ## do not identfiy the features on the entire dataset.\n",
        "\n",
        "        top_n_feat = 50\n",
        "        underprediction = prediction['target'] < groundtruth['target']\n",
        "\n",
        "        abs_summed_shap = np.abs(shap_values_all.values[underprediction]).sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(abs_summed_shap)[::-1]\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(abs_summed_shap)[::-1]]\n",
        "        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on summed absolute SHAP values\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high absolute shapley values over all instances/samples on {set_name} set for UNDERprediction instances')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        simple_summed_shap = shap_values_all.values[underprediction].sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(simple_summed_shap)\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)]\n",
        "        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on simply summed SHAP values, Asc. order\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for UNDERprediction instances, Asc. order')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        simple_summed_shap = shap_values_all.values[underprediction].sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(simple_summed_shap)[::-1]\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)[::-1]]\n",
        "        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on simply summed SHAP values, Desc. order\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for UNDERprediction instances, Desc. order')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "\n",
        "        min_shap_values = shap_values_all.values[underprediction].min(axis=0)\n",
        "        sorted_min_shap = np.sort(min_shap_values)\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(min_shap_values)]\n",
        "        fig = px.bar(x=sorted_min_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on min SHAP values\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high min shapley values over all instances/samples on {set_name} set for UNDERprediction instances')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "\n",
        "        #Examine Individual Feature that lead to most negative shap values Contributions using shap dependence plot\n",
        "        #self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###################################### Overprediction Error Shapley Analysis #######################################\n",
        "\n",
        "        ## filter out instance where underpreidction is happening, then identfy the features that are causing the overprediction\n",
        "        ## do not identfiy the features on the entire dataset.\n",
        "        top_n_feat = 50\n",
        "        overprediction = prediction['target'] > groundtruth['target']\n",
        "\n",
        "        abs_summed_shap = np.abs(shap_values_all.values[overprediction]).sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(abs_summed_shap)[::-1]\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(abs_summed_shap)[::-1]]\n",
        "        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on summed absolute SHAP values\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high absolute shapley values over all instances/samples on {set_name} set for OVERprediction instances')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        simple_summed_shap = shap_values_all.values[overprediction].sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(simple_summed_shap)\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)]\n",
        "        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on simply summed SHAP values, Asc. order\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for OVERprediction instances, Asc. order')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        simple_summed_shap = shap_values_all.values[overprediction].sum(axis=0)\n",
        "        sorted_summed_shap = np.sort(simple_summed_shap)[::-1]\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)[::-1]]\n",
        "        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on simply summed SHAP values,  Desc. order\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for OVERprediction instances, Desc. order')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        max_shap_values = shap_values_all.values[overprediction].max(axis=0)\n",
        "        sorted_max_shap = np.sort(max_shap_values)[::-1]\n",
        "        labels = np.array(shap_values_all.feature_names)[np.argsort(max_shap_values)[::-1]]\n",
        "        fig = px.bar(x=sorted_max_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on max SHAP values\", height=800)\n",
        "        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n",
        "        fig.show()\n",
        "        print(f'\\nTop {top_n_feat} features that have high max shapley values over all instances/samples on {set_name} set for OVERprediction instances')\n",
        "        print(list(labels[:top_n_feat]))\n",
        "\n",
        "        #Examine Individual Feature that lead to most negative shap values Contributions using shap dependence plot\n",
        "        #self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n",
        "\n",
        "        print(f\"\\n\\nFraction of underprediction instances: {sum(underprediction) / len(prediction)} on {set_name} set\")\n",
        "        print(f\"Underprediction RMSPE: {self.rmspe(groundtruth['target'][underprediction], prediction['target'][underprediction])} on {set_name} set\")\n",
        "        print(f\"Fraction of overprediction instances: {sum(overprediction) / len(prediction)} on {set_name} set\")\n",
        "        print(f\"Overprediction RMSPE: {self.rmspe(groundtruth['target'][overprediction], prediction['target'][overprediction])} on {set_name} set\")\n",
        "        print(f\"OVERALL RMSPE: {self.rmspe(groundtruth['target'], prediction['target'])} on {set_name} set\")\n",
        "\n",
        "\n",
        "\n",
        "        del shap_values_all,groundtruth, prediction, error, underprediction, overprediction, abs_summed_shap, sorted_summed_shap,\\\n",
        "            labels, feat_error_corr_dict, sorted_feat_error_corr_dict, min_shap_values, max_shap_values, sorted_min_shap, sorted_max_shap\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def compute_shapley_heatmap(self,shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df):\n",
        "\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        #print(' NOTE: Heatmap is sorted with f(X) from smallest values to biggest value !! (picture is wrong)')\n",
        "        # fig,ax = plt.subplots(figsize=(13.5,2))\n",
        "        # y_asc = np.sort( all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values )\n",
        "        # ax.plot( range(len(y_asc)), y_asc, color='g')\n",
        "        # ax.axhline(y_asc.mean(),color='r', linestyle='dashed')\n",
        "        # ax.set_ylabel('Correct f(x) in Asc. order')\n",
        "        # ax.set_yticks(np.arange(0,max(y_asc),0.002))\n",
        "        # fig.show()\n",
        "\n",
        "        fig,ax = plt.subplots()\n",
        "        # order = np.argsort(all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values)\n",
        "        shap.plots.heatmap(shap_values,instance_order=shap_values.sum(1))\n",
        "        #ax.title(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig.show()\n",
        "\n",
        "        del shap_values,all_stock_train_pred_df\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_shapley_decision(self,model_base_value,shap_values,feature_names,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n",
        "\n",
        "\n",
        "\n",
        "        # Create decision plot\n",
        "        shap.decision_plot(model_base_value, shap_values, X_test, feature_names=feature_names)\n",
        "\n",
        "\n",
        "        #### ONLY for Explainer\n",
        "        print(f'\\nDECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig,ax = plt.subplots()\n",
        "        shap.plots.decision(model_base_value,shap_values=shap_values.values,features=shap_values.data,feature_names=feature_names, show=True) #matplotlib=True,\n",
        "        ax.set_title(f'\\n DECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        fig.show()\n",
        "\n",
        "        del shap_values\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_shapley_force(self,model_base_value,shap_values,X,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n",
        "\n",
        "        # ### ONLY for Explainer\n",
        "        # print(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        # fig,ax = plt.subplots()\n",
        "        # shap.plots.force(model_base_value,shap_values=shap_values[0],features=X[0],feature_names=feature_names, show=True) #matplotlib=True,\n",
        "        # ax.title(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n",
        "        # fig.show()\n",
        "\n",
        "        # example_index = 0  # You can change this index to any other example\n",
        "        # example = X[example_index]\n",
        "        # # Explain the prediction of the example\n",
        "        # shap.force_plot(explainer.expected_value, shap_values[example_index], example, feature_names=data.feature_names)\n",
        "\n",
        "        del shap_values\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    # def compute_train_avg_target_rvol(self, unique_stock_ids, y_train):\n",
        "    #     # unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "    #     unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "    #     train_target_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n",
        "    #     for s in unique_stock_ids:\n",
        "    #         st_index = self.train_stock_id == s\n",
        "    #         t_index = self.train_time_id[st_index]\n",
        "    #         train_target_df.loc[t_index, s] = y_train[st_index].values\n",
        "    #     train_avg_target_rvol = train_target_df.ffill().bfill().mean(axis=1)\n",
        "    #     return train_avg_target_rvol\n",
        "\n",
        "    # def compute_test_avg_target_rvol(self, unique_stock_ids, y_test):\n",
        "    #     #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n",
        "    #     unique_test_time_ids = self.test_time_id\n",
        "    #     test_target_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n",
        "    #     for s in unique_stock_ids:\n",
        "    #         st_index = self.test_stock_id == s\n",
        "    #         t_index = self.test_time_id[st_index]\n",
        "    #         test_target_df.loc[t_index, s] = y_test[st_index].values\n",
        "    #     test_avg_target_rvol = test_target_df.ffill().bfill().mean(axis=1)\n",
        "    #     return test_avg_target_rvol\n",
        "\n",
        "\n",
        "    def fraction_above_average(self,signal1, avg):\n",
        "        # Count the fraction of times when signal1 is above signal2\n",
        "        fraction_above_avg = (signal1 > avg).mean()\n",
        "        return fraction_above_avg\n",
        "\n",
        "\n",
        "    def compute_all_stock_v1tr_df(self, unique_stock_ids, v1tr):\n",
        "        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "        all_stock_v1tr_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n",
        "        for s in unique_stock_ids:\n",
        "            st_index = self.train_stock_id == s\n",
        "            t_index = self.train_time_id[st_index]\n",
        "            all_stock_v1tr_df.loc[t_index, s] = v1tr[st_index].values\n",
        "        all_stock_v1tr_df = all_stock_v1tr_df.ffill().bfill()\n",
        "        return all_stock_v1tr_df\n",
        "\n",
        "    def compute_all_stock_train_pred_df(self, unique_stock_ids, train_pred):\n",
        "        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "        all_stock_train_pred_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n",
        "        for s in unique_stock_ids:\n",
        "            st_index = self.train_stock_id == s\n",
        "            t_index = self.train_time_id[st_index]\n",
        "            all_stock_train_pred_df.loc[t_index, s] = train_pred[st_index].values\n",
        "        all_stock_train_pred_df = all_stock_train_pred_df.ffill().bfill()\n",
        "        return all_stock_train_pred_df\n",
        "\n",
        "    def compute_all_stock_test_pred_df(self, unique_stock_ids, test_pred):\n",
        "        # unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n",
        "        unique_test_time_ids = self.test_time_id\n",
        "        all_stock_test_pred_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n",
        "        for s in unique_stock_ids:\n",
        "            st_index = self.test_stock_id == s\n",
        "            t_index = self.test_time_id[st_index]\n",
        "            all_stock_test_pred_df.loc[t_index, s] = test_pred[st_index].values\n",
        "        all_stock_test_pred_df = all_stock_test_pred_df.ffill().bfill()\n",
        "        return all_stock_test_pred_df\n",
        "\n",
        "\n",
        "    def compute_all_stock_y_train_df(self, unique_stock_ids, y_train):\n",
        "        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "        all_stock_y_train_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n",
        "        for s in unique_stock_ids:\n",
        "            st_index = self.train_stock_id == s\n",
        "            t_index = self.train_time_id[st_index]\n",
        "            all_stock_y_train_df.loc[t_index, s] = y_train[st_index].values\n",
        "        all_stock_y_train_df = all_stock_y_train_df.ffill().bfill()\n",
        "        return all_stock_y_train_df\n",
        "\n",
        "    def compute_all_stock_y_test_df(self, unique_stock_ids, y_test):\n",
        "        #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n",
        "        unique_test_time_ids = self.test_time_id\n",
        "        all_stock_y_test_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n",
        "        for s in unique_stock_ids:\n",
        "            st_index = self.test_stock_id == s\n",
        "            t_index = self.test_time_id[st_index]\n",
        "            all_stock_y_test_df.loc[t_index, s] = y_test[st_index].values\n",
        "        all_stock_y_test_df = all_stock_y_test_df.ffill().bfill()\n",
        "        return all_stock_y_test_df\n",
        "\n",
        "\n",
        "\n",
        "    ######## Identify stocks belonging to clusters based on clusterings in dataset\n",
        "    ######## find stock ids of clusters having same feature values\n",
        "    ######## This is reverse-engineering cluster labels of already clustered stocks\n",
        "    def calculate_cluster_fraction(self, column, n_clusters, stock_list):\n",
        "        \"\"\" This function computes the fraction of stock ids in stock_list inside a cluster in the clustering feature.\n",
        "        The fraction is between 0 - 1. 1 indicates all the stock ids in stock_list are in a particular cluster.\n",
        "        \"\"\"\n",
        "\n",
        "        # self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n",
        "        # self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n",
        "\n",
        "        # unique_stock_ids = self.train_stock_id.unique()\n",
        "        # time_id_order = df2.loc[:3829,'time_id'].values\n",
        "        # train_time_id_ind = int(len(time_id_order)*0.7)\n",
        "\n",
        "        # train_time_ids = time_id_order[:train_time_id_ind]\n",
        "        # train_stock_id = df2[df2['time_id'].isin(train_time_ids)]['stock_id']\n",
        "        # train_time_id = df2[df2['time_id'].isin(train_time_ids)]['time_id']\n",
        "\n",
        "        unique_stock_ids = self.train_stock_id.unique()\n",
        "        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "\n",
        "        train_col_df = self.df[self.df['time_id'].isin(train_time_ids)][column]\n",
        "\n",
        "        ## reshape the dataframe\n",
        "        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "        all_stock_column_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n",
        "        for s in unique_stock_ids:\n",
        "            st_index = self.train_stock_id == s\n",
        "            t_index = self.train_time_id[st_index]\n",
        "            all_stock_column_df.loc[t_index, s] = train_col_df[st_index].values\n",
        "        all_stock_column_df = all_stock_column_df.ffill().bfill()\n",
        "\n",
        "        features = all_stock_column_df.T.to_numpy()\n",
        "\n",
        "        ## kmeans\n",
        "        kmeans = KMeans(n_clusters=n_clusters,n_init=10)\n",
        "        kmeans.fit(features)\n",
        "        cluster_labels = kmeans.labels_\n",
        "        cluster_labels\n",
        "\n",
        "        clusters_dict = {}\n",
        "        unique_labels = np.unique(cluster_labels)\n",
        "        for label in unique_labels:\n",
        "            indices = np.where(cluster_labels == label)[0]\n",
        "            stocks_in_cluster = unique_stock_ids[indices]\n",
        "            clusters_dict[label] = stocks_in_cluster.tolist()\n",
        "\n",
        "        for c in clusters_dict.keys():\n",
        "            cnt=0\n",
        "            for s in stock_list:\n",
        "                if s in clusters_dict[c]:\n",
        "                    cnt+=1\n",
        "            print(f'cluster: {c}, # stock ids in cluster: {cnt}, clustering fraction: {cnt/len(clusters_dict[c])}')\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def check_stock_list_in_all_clustering_features(self, stock_list):\n",
        "\n",
        "        clustering_features_list = [    \"log_target_vol_corr_32_clusters_stnd\",\n",
        "                                        \"log_target_vol_sum_stats_16_clusters_stnd\",\n",
        "                                        \"sum_stats_4_clusters_labels\",\n",
        "                                        \"sum_stats_10_clusters_labels\",\n",
        "                                        \"sum_stats_16_clusters_labels\",\n",
        "                                        \"sum_stats_30_clusters_labels\",\n",
        "                                        \"pear_corr_32_clusters_labels\",\n",
        "                                        \"pear_corr_4_clusters_labels\",\n",
        "                                        \"pear_corr_49_clusters_labels\",\n",
        "                                        \"pear_corr_90_clusters_labels\",]\n",
        "\n",
        "        print('stock_list: ' , stock_list)\n",
        "        for feature in clustering_features_list:\n",
        "            n_clusters = int(re.findall(r'\\d+', feature)[0])\n",
        "            print('Feature: ', feature)\n",
        "            print('Cluster Fractions: ')\n",
        "            print(self.calculate_cluster_fraction( feature, n_clusters, stock_list))\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def compute_acf_pacf(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n",
        "        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n",
        "        plt.close('all')\n",
        "        for s in unique_stock_ids[0:1]:#[0:40]:\n",
        "            fig,ax = plt.subplots(2,1,figsize=(30,6))\n",
        "            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n",
        "            plot_acf(stock_residual, lags=200,ax=ax[0])\n",
        "            plot_pacf(stock_residual, lags=200,ax=ax[1])\n",
        "            ax[0].set_title(f'Autocorrelation of stock {s} Residuals on train set')\n",
        "            ax[1].set_title(f'Partial Autocorrelation of stock {s} Residuals on train set')\n",
        "            ax[0].set_xticks(range(0,200,5))\n",
        "            ax[1].set_xticks(range(0,200,5))\n",
        "            ax[0].set_yticks(np.arange(-1, 1, 0.1))\n",
        "            ax[1].set_yticks(np.arange(-1, 1, 0.1))\n",
        "            ax[1].set_xlabel('lags')\n",
        "            ax[0].set_ylabel('ACF')\n",
        "            ax[1].set_ylabel('PACF')\n",
        "            ax[0].grid(True)\n",
        "            ax[1].grid(True)\n",
        "            fig.show()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def compute_IFFT(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n",
        "\n",
        "        ##### FAST FOURIER TRANSFORM plot of EVERY individual stock\n",
        "        ##### IFFT plot of reconstructed time series ######\n",
        "        plt.close('all')\n",
        "        for s in unique_stock_ids[100:]:#[40:112]:\n",
        "            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n",
        "            x = stock_residual.values\n",
        "            limit = 0.00001\n",
        "\n",
        "            n=len(x)\n",
        "            fhat = np.fft.fft(x,n)\n",
        "            PSD = fhat*np.conj(fhat) / n\n",
        "            freq = (1/n)*np.arange(n)\n",
        "            start=1 #ignore dc component\n",
        "            L = np.arange(start,np.floor(n/2),dtype='int')\n",
        "            # fig,ax = plt.subplots(figsize=(30,6))\n",
        "            # #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n",
        "            # ax.axhline(limit,  color='k', linestyle='-')\n",
        "            # ax.plot(freq[L],PSD[L])\n",
        "            # ax.set_xlabel('freq')\n",
        "            # ax.set_ylabel('mag')\n",
        "            # ax.set_title(f'mag plot of stock: {s} residual')\n",
        "            # fig.show()\n",
        "\n",
        "            indices = PSD > limit\n",
        "            num_freqs = len(np.where(indices>0)[0])\n",
        "            print('# of frequencies in residual = ',num_freqs)\n",
        "\n",
        "            fhat = fhat*indices\n",
        "            fig,ax = plt.subplots(2,1,figsize=(30,6))\n",
        "            ffilt = np.fft.ifft(fhat)\n",
        "            ax[0].plot(np.arange(0,len(x)),ffilt.real,label='top '+str(num_freqs)+' frequencies in residual (train set)',c='g',alpha=1)\n",
        "            ax[0].plot(np.arange(0,len(x)),x,label='original residual',c='r',alpha=0.2)\n",
        "            ax[0].legend()\n",
        "            ax[0].grid()\n",
        "            ax[0].set_xlabel('time id')\n",
        "            ax[0].set_ylabel('residual')\n",
        "            ax[0].set_title(f'IFFT of stock: {s} residual')\n",
        "\n",
        "\n",
        "            x1 = all_stock_y_train_df[s].values\n",
        "            limit1 = 0.00001\n",
        "            n1=len(x1)\n",
        "            fhat1 = np.fft.fft(x1,n1)\n",
        "            PSD1 = fhat1*np.conj(fhat1) / n1\n",
        "            freq1 = (1/n1)*np.arange(n1)\n",
        "            start1=1 #ignore dc component\n",
        "            L1 = np.arange(start1,np.floor(n1/2),dtype='int')\n",
        "            fig1,ax1 = plt.subplots(figsize=(30,6))\n",
        "            #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n",
        "            ax1.axhline(limit1,  color='k', linestyle='-')\n",
        "            ax1.plot(freq1[L],PSD1[L])\n",
        "            ax1.set_xlabel('freq')\n",
        "            ax1.set_ylabel('mag')\n",
        "            ax1.set_title(f'mag plot of stock: {s} rvol.')\n",
        "            fig1.show()\n",
        "\n",
        "            indices1 = PSD1 > limit1\n",
        "            num_freqs1 = len(np.where(indices1>0)[0])\n",
        "            print('# of frequencies in rvol. = ',num_freqs1)\n",
        "            fhat1 = fhat1*indices1\n",
        "            ffilt1 = np.fft.ifft(fhat1)\n",
        "            ax[1].plot(np.arange(0,len(x1)),ffilt1.real,label='top '+str(num_freqs1)+' frequencies in true rvol. (train set)',c='g',alpha=1)\n",
        "            ax[1].plot(np.arange(0,len(x1)),x1,label='original true rvol.',c='r',alpha=0.2)\n",
        "            ax[1].legend()\n",
        "            ax[1].grid()\n",
        "            ax[1].set_xlabel('time id')\n",
        "            ax[1].set_ylabel('rvol.')\n",
        "            ax[1].set_title(f'IFFT of stock: {s} rvol.')\n",
        "            fig.show()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_total_gap(self,data):\n",
        "        # 1. Order the ground truth in ascending order\n",
        "        sorted_data = np.sort(data)\n",
        "\n",
        "        # 2. Take up to the 75th percentile (remove upper outliers)\n",
        "        percentile_75 = np.percentile(sorted_data, 75)\n",
        "        filtered_data = sorted_data[sorted_data <= percentile_75]\n",
        "\n",
        "        # 3. Take first differences\n",
        "        first_differences = np.diff(filtered_data)\n",
        "\n",
        "        # 4. Order the first differences in descending order\n",
        "        sorted_differences = np.sort(first_differences)[::-1]  # Sort in descending order\n",
        "\n",
        "        # 5. Take the sum of the first differences up to the 50th percentile\n",
        "        length = len(sorted_differences)\n",
        "        cutoff_index = int(0.5 * length)  # 50th percentile index\n",
        "        total_gap = np.sum(sorted_differences[:cutoff_index])\n",
        "\n",
        "        return total_gap\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def overall_stock_id_analysis(self,unique_stock_ids,all_stock_pred_df,all_stock_y_GroundTruth_df,residuals,train_flag):\n",
        "\n",
        "        if train_flag:\n",
        "            set_name = 'train'\n",
        "        else:\n",
        "            set_name = 'test'\n",
        "\n",
        "        all_stock_pred_pivot = all_stock_pred_df.pivot(index='time_id', columns='stock_id', values='target')\n",
        "        if train_flag:\n",
        "            all_stock_pred_pivot = all_stock_pred_pivot.reindex(self.time_id_order)\n",
        "\n",
        "        all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='time_id', columns='stock_id', values='target')\n",
        "        if train_flag:\n",
        "            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_pivot.reindex(self.time_id_order)\n",
        "\n",
        "\n",
        "        rmspe_per_stock = []\n",
        "        total_gap_per_stock = []\n",
        "        for s in unique_stock_ids:\n",
        "            rmspe_per_stock.append( np.nanmean( ((all_stock_pred_pivot[s]-all_stock_y_GroundTruth_pivot[s])/all_stock_y_GroundTruth_pivot[s])**2 )**0.5  )\n",
        "            total_gap_per_stock.append( self.calculate_total_gap(all_stock_y_GroundTruth_pivot[s].values) )\n",
        "\n",
        "\n",
        "        ###### Bar plot of RMSPE for all stocks in the training set\n",
        "        all_stock_rmspe = pd.Series(rmspe_per_stock,index=unique_stock_ids)\n",
        "        smallest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index.values[:10]\n",
        "        largest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n",
        "        fig, ax = plt.subplots(figsize=(40,10))\n",
        "        ax.text(0,0.52,f'10 largest RMSPE stocks: {largest_10_rmspe_stocks}')\n",
        "        ax.text(0,0.62,f'10 smallest RMSPE stocks: {smallest_10_rmspe_stocks}')\n",
        "        ax.bar(unique_stock_ids, rmspe_per_stock)\n",
        "        ax.set_xticks(unique_stock_ids)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        ax.set_yticks(np.arange(0, max(rmspe_per_stock) + 0.01, 0.04))\n",
        "        ax.grid()\n",
        "        ax.set_title(f'RMSPE of Real. Vol. for each stock on {set_name} set')\n",
        "        ax.set_xlabel('Stock ID')\n",
        "        ax.set_ylabel('RMSPE')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        ## check if the largest and smallest fall into a cluster of a clustering feature\n",
        "        print('\\n 10_largest_rmspe_stocks in clustering feature')\n",
        "        #self.check_stock_list_in_all_clustering_features(stock_list = largest_10_rmspe_stocks)\n",
        "        print('\\n 10_smallest_rmspe_stocks in clustering feature')\n",
        "        #self.check_stock_list_in_all_clustering_features(stock_list = smallest_10_rmspe_stocks)\n",
        "        print(f'30_largest_rmspe_stocks in {set_name} set: ',all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:30])\n",
        "\n",
        "\n",
        "        ####### plot total_gap in descending order for all stocks  ######\n",
        "        all_stock_total_gap = pd.Series(total_gap_per_stock,index=unique_stock_ids)\n",
        "        smallest_10_total_gap_stocks = all_stock_total_gap.sort_values(ascending=True).index.values[:10]\n",
        "        largest_10_total_gap_stocks = all_stock_total_gap.sort_values(ascending=True).index[::-1].values[:10]\n",
        "        fig, ax = plt.subplots(figsize=(40,10))\n",
        "        ax.text(4, max(total_gap_per_stock),f'10 largest total gap stocks: {largest_10_total_gap_stocks}')\n",
        "        ax.text(4, max(total_gap_per_stock)-0.003,f'10 smallest total gap stocks: {smallest_10_total_gap_stocks}')\n",
        "        sorted_indices = np.argsort(total_gap_per_stock)[::-1]\n",
        "        sorted_unique_stock_ids = unique_stock_ids[sorted_indices]\n",
        "        sorted_total_gap_per_stock = np.array(total_gap_per_stock)[sorted_indices]\n",
        "        ax.bar(range(len(sorted_unique_stock_ids)), sorted_total_gap_per_stock,tick_label=sorted_unique_stock_ids)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "        ax.set_yticks(np.arange(0, max(sorted_total_gap_per_stock) + 0.001, 0.001))\n",
        "        ax.grid()\n",
        "        ax.set_title(f'Total Gap of Real. Vol. for each stock on GroundTruth {set_name} set')\n",
        "        ax.set_xlabel('Stock ID')\n",
        "        ax.set_ylabel('Total Gap')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        print(f'30_largest_total_gap_stocks in {set_name} set: ',all_stock_total_gap.sort_values(ascending=True).index[::-1].values[:30])\n",
        "\n",
        "\n",
        "        groundtruth = all_stock_y_GroundTruth_df['target'].values\n",
        "        prediction = all_stock_pred_df['target'].values\n",
        "        residuals = residuals['target'].values # copy is made, its not modified inplace\n",
        "\n",
        "        ####### scatter plot of True Real. Vol. vs. Pred Real. Vol.\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        ax.scatter(groundtruth, prediction, c='b', alpha=0.1)\n",
        "        ax.plot(groundtruth, groundtruth, c='r',linestyle='solid' )\n",
        "        ax.set_title(f'Scatter Plot of True vs Predicted Values on {set_name} set')\n",
        "        ax.set_xlabel(f'True {set_name} rvol. Values')\n",
        "        ax.set_ylabel(f'Predicted {set_name} rvol. Values')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## y_train and train_pred Distributions Histogram:\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        plt.hist( groundtruth,bins=1000, color='green', alpha=0.9, histtype='bar', rwidth=0.8, label='GroundTruth')\n",
        "        plt.hist( prediction,bins=1000, color='red', alpha=0.3, ec='r', label='Prediction')\n",
        "        ax.set_title(f'Distribution of GroundTruth (skew: {stats.skew(groundtruth)} , kurt:{stats.kurtosis(groundtruth)}) and Prediction (skew: {stats.skew(prediction)} , kurt:{stats.kurtosis(prediction)}) on {set_name} set')\n",
        "        ax.set_xlabel(f' GroundTruth and Prediction on {set_name} set')\n",
        "        ax.set_ylabel('frequency')\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        ax.scatter(groundtruth, residuals, c='c',alpha=0.1 )\n",
        "        ax.axhline(y=0, color='g', linestyle='-')\n",
        "        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n",
        "        ax.set_title(f' True R.V. Vs. Residuals Values Plot on {set_name} set')\n",
        "        ax.set_xlabel(f'True {set_name} Values')\n",
        "        ax.set_ylabel(f'{set_name} residuals')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        ####### scatter plot of Fitted rvol. Values Vs. train residuals Plot:\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        ax.scatter(prediction, residuals, c='m',alpha=0.1 )\n",
        "        ax.axhline(y=0, color='g', linestyle='-')\n",
        "        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n",
        "        ax.set_title(f' fitted R.V. Vs. Residuals Values Plot on {set_name} set')\n",
        "        ax.set_xlabel(f'fitted {set_name} Values')\n",
        "        ax.set_ylabel(f'{set_name} residuals')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## Normal Q-Q Plot:\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        sm.qqplot(residuals, line='q', ax=ax)\n",
        "        ax.set_title(f'QQ Plot of Residuals on {set_name} set')\n",
        "        ax.set_xlabel('Theoretical Quantiles')\n",
        "        ax.set_ylabel('Sample Quantiles')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## Residuals Distribution Histogram:\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        plt.hist( residuals,bins=1000)\n",
        "        ax.set_title(f'Distribution of Residuals on {set_name} set')\n",
        "        ax.set_xlabel(f'{set_name} Residuals')\n",
        "        ax.set_ylabel('frequency')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        del all_stock_pred_df,all_stock_y_GroundTruth_df,residuals,all_stock_y_GroundTruth_pivot,all_stock_pred_pivot,groundtruth,prediction\n",
        "        gc.collect()\n",
        "        return pd.DataFrame({f'{set_name}_rmspe_per_stock':rmspe_per_stock}, index=unique_stock_ids)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def individual_stock_id_analysis(self,picked_stock_id,unique_stock_ids,all_stock_y_GroundTruth_df,all_stock_pred_df,residuals,avg_target_rvol,train_flag):\n",
        "\n",
        "        if train_flag:\n",
        "            set_name = 'train'\n",
        "        else:\n",
        "            set_name = 'test'\n",
        "\n",
        "        all_stock_pred_pivot = all_stock_pred_df.pivot(index='time_id', columns='stock_id', values='target')\n",
        "        if train_flag:\n",
        "            all_stock_pred_pivot = all_stock_pred_pivot.reindex(self.time_id_order)\n",
        "\n",
        "        all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='time_id', columns='stock_id', values='target')\n",
        "        if train_flag:\n",
        "            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_pivot.reindex(self.time_id_order)\n",
        "\n",
        "        groundtruth = all_stock_y_GroundTruth_pivot[picked_stock_id].values\n",
        "        prediction = all_stock_pred_pivot[picked_stock_id].values\n",
        "        picked_st_residuals = groundtruth - prediction\n",
        "\n",
        "        ## 1. scatter plot\n",
        "        plt.figure(figsize=(10,10))\n",
        "        plt.scatter(all_stock_y_GroundTruth_pivot[picked_stock_id],all_stock_pred_pivot[picked_stock_id], c='blue',label=picked_stock_id, alpha=0.4)\n",
        "        plt.plot(all_stock_y_GroundTruth_pivot[picked_stock_id],all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='red',label=picked_stock_id, alpha=1 )\n",
        "        plt.grid()\n",
        "        plt.xlabel(f'GroundTruth')\n",
        "        plt.ylabel(f'Prediction')\n",
        "        plt.legend()\n",
        "        plt.title(f\"stock {picked_stock_id}'s scatter plot of y_{set_name} vs. {set_name}_pred on {set_name} set\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## 2. Line plot of true vs average real. vol.\n",
        "        fraction_above_avg = self.fraction_above_average(all_stock_y_GroundTruth_pivot[picked_stock_id], avg_target_rvol)\n",
        "        plt.figure(figsize=(30,5))\n",
        "        plt.text(1,all_stock_y_GroundTruth_pivot[picked_stock_id].max(),f\"fraction of times this stock's values are above all stocks' avg_target_rvol = {fraction_above_avg}\")\n",
        "        plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.4 )\n",
        "        #plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),[avg_target_rvol]*len(all_stock_y_GroundTruth_pivot[picked_stock_id]),linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n",
        "        plt.plot(range(len(avg_target_rvol)),avg_target_rvol,linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n",
        "        plt.grid()\n",
        "        plt.xlabel('index')\n",
        "        plt.ylabel('train rvol.')\n",
        "        plt.legend()\n",
        "        plt.title(f\"stock {picked_stock_id}'s line plot of True y_{set_name} vs. {set_name}_avg_target_rvol on {set_name} set\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # ## 3. Line plot of pred vs true real. vol.\n",
        "        # plt.figure(figsize=(30,5))\n",
        "        # plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.7 )\n",
        "        # plt.plot(range(len(all_stock_pred_pivot[picked_stock_id])),all_stock_pred_pivot[picked_stock_id],linestyle='solid', c='red',label='Pred stock id: '+str(picked_stock_id), alpha=0.4 )\n",
        "        # plt.grid()\n",
        "        # plt.xlabel('index')\n",
        "        # plt.ylabel(f'{set_name} rvol.')\n",
        "        # plt.legend()\n",
        "        # plt.title(f\"stock {picked_stock_id}'s line plot of True y_{set_name} vs {set_name}_pred on {set_name} set\")\n",
        "        # plt.show()\n",
        "        # plt.close()\n",
        "\n",
        "        ## y_train and train_pred Distributions Histogram:\n",
        "        fig, ax = plt.subplots(2, 1, figsize=(30, 10))\n",
        "        max_val = max(groundtruth.max(), prediction.max())\n",
        "        ax[0].hist(groundtruth, bins=1000, color='green', alpha=1, histtype='bar', rwidth=0.8, label='GroundTruth')\n",
        "        ax[0].set_xlim(-0.001, max_val)\n",
        "        ax[0].set_title(f\"stock {picked_stock_id}'s Distribution of GroundTruth (skew: {stats.skew(groundtruth)} , kurt:{stats.kurtosis(groundtruth)}) and Prediction (skew: {stats.skew(prediction)} , kurt:{stats.kurtosis(prediction)}) on {set_name} set\")\n",
        "        ax[0].set_xlabel(f' GroundTruth on {set_name} set')\n",
        "        ax[0].set_ylabel('frequency')\n",
        "        ax[1].hist(prediction, bins=1000, color='red', alpha=1, ec='r', label='Prediction')\n",
        "        ax[1].set_xlim(-0.001, max_val)\n",
        "        ax[1].set_xlabel(f' Prediction on {set_name} set')\n",
        "        ax[1].set_ylabel('frequency')\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n",
        "        fig, ax = plt.subplots(2,1,figsize=(30,10))\n",
        "        max_val = max(groundtruth.max(), prediction.max())\n",
        "        ax[0].scatter(groundtruth, picked_st_residuals, c='c',alpha=0.1 )\n",
        "        ax[0].set_xlim(-0.001, max_val)\n",
        "        ax[0].axhline(y=0, color='g', linestyle='-')\n",
        "        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n",
        "        ax[0].set_title(f\"stock {picked_stock_id}'s True R.V. Vs. Residuals Values Plot on {set_name} set\")\n",
        "        ax[0].set_xlabel(f'True {set_name} Values')\n",
        "        ax[0].set_ylabel(f'{set_name} picked_st_residuals')\n",
        "        ax[1].scatter(prediction, picked_st_residuals, c='m',alpha=0.1 )\n",
        "        ax[1].set_xlim(-0.001, max_val)\n",
        "        ax[1].axhline(y=0, color='g', linestyle='-')\n",
        "        ax[1].set_title(f\"stock {picked_stock_id}'s fitted R.V. Vs. Residuals Values Plot on {set_name} set\")\n",
        "        ax[1].set_xlabel(f'fitted {set_name} Values')\n",
        "        ax[1].set_ylabel(f'{set_name} picked_st_residuals')\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## Normal Q-Q Plot:\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        sm.qqplot(picked_st_residuals, line='q', ax=ax)\n",
        "        ax.set_title(f\"stock {picked_stock_id}'s QQ Plot of Residuals on {set_name} set\")\n",
        "        ax.set_xlabel('Theoretical Quantiles')\n",
        "        ax.set_ylabel('Sample Quantiles')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## Residuals Distribution Histogram:\n",
        "        fig, ax = plt.subplots(figsize=(30,10))\n",
        "        plt.hist( picked_st_residuals,bins=1000)\n",
        "        ax.set_title(f\"stock {picked_stock_id}'s Distribution of Residuals on {set_name} set\")\n",
        "        ax.set_xlabel(f'{set_name} Residuals')\n",
        "        ax.set_ylabel('frequency')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        ## box plot of residuals, groundtruth and prediction\n",
        "        fig, ax = plt.subplots(figsize=(5,5))\n",
        "        ax.boxplot([picked_st_residuals,groundtruth,prediction],labels=['residuals','groundtruth','prediction'])\n",
        "        ax.set_title(f\" stock {picked_stock_id}'s Box Plot of Residuals, GroundTruth and Prediction on {set_name} set\")\n",
        "        ax.set_ylabel('value')\n",
        "        #ax.set_yticks(np.arange(-0.01, 0.025, 0.001))\n",
        "        ax.grid()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # ###### Autocorrelation Plot\n",
        "        # fig, ax = plt.subplots(figsize=(10,3))\n",
        "        # plot_acf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n",
        "        # ax.set_xlabel('Lag')\n",
        "        # ax.set_ylabel('Autocorrelation')\n",
        "        # ax.set_yticks(np.arange(-1, 1, 0.1))\n",
        "        # ax.grid()\n",
        "        # ax.set_title(f'Autocorrelation of {set_name} Residuals')\n",
        "        # fig.show()\n",
        "\n",
        "        # ###### Partial Autocorrelation Plot\n",
        "        # fig, ax = plt.subplots(figsize=(10,3))\n",
        "        # plot_pacf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n",
        "        # ax.set_xlabel('Lag')\n",
        "        # ax.set_ylabel('Partial Autocorrelation')\n",
        "        # ax.set_yticks(np.arange(-1, 1, 0.1))\n",
        "        # ax.grid()\n",
        "        # plt.title(f'Partial Autocorrelation of {set_name} Residuals')\n",
        "        # plt.show()\n",
        "\n",
        "        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n",
        "        #self.compute_acf_pacf(unique_stock_ids,all_stock_pred_df,all_stock_y_GroundTruth_df)\n",
        "\n",
        "\n",
        "        # #### FAST FOURIER TRANSFORM plot of EVERY individual stock\n",
        "        # #### IFFT plot of reconstructed time series ######\n",
        "        # self.compute_IFFT(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n",
        "\n",
        "        del picked_stock_id,unique_stock_ids,all_stock_y_GroundTruth_df,all_stock_pred_df,residuals\n",
        "        gc.collect()\n",
        "        return groundtruth\n",
        "\n",
        "\n",
        "\n",
        "    def overall_time_id_analysis(self, all_stock_y_GroundTruth_df,all_stock_pred_df,avg_target_rvol,train_flag):\n",
        "\n",
        "        # Precompute variables\n",
        "        set_name = 'train' if train_flag else 'test'\n",
        "\n",
        "        # Pivot with 'stock_id' as index and 'time_id' as columns\n",
        "        if train_flag:\n",
        "            all_stock_pred_pivot = all_stock_pred_df.pivot(index='stock_id', columns='time_id', values='target').reindex(columns=self.time_id_order)\n",
        "            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='stock_id', columns='time_id', values='target').reindex(columns=self.time_id_order)\n",
        "        else:\n",
        "            all_stock_pred_pivot = all_stock_pred_df.pivot(index='stock_id', columns='time_id', values='target')\n",
        "            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='stock_id', columns='time_id', values='target')\n",
        "\n",
        "        unique_time_ids = all_stock_y_GroundTruth_pivot.columns.values\n",
        "        # Vectorized RMSPE calculation along the 'stock_id' axis\n",
        "        rmspe_per_time_id = np.sqrt(\n",
        "            np.mean(\n",
        "                ((all_stock_pred_pivot - all_stock_y_GroundTruth_pivot) / all_stock_y_GroundTruth_pivot) ** 2, axis=0\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # print('all_stock_pred_pivot.index',all_stock_pred_pivot)\n",
        "        # display_n_time_ids = 10\n",
        "        # sorted_rmspe_per_time_id = rmspe_per_time_id.sort_values(ascending=False).head(display_n_time_ids)\n",
        "        # for t in sorted_rmspe_per_time_id.index:\n",
        "        #     plt.figure(figsize=(30,5))\n",
        "        #     plt.plot(all_stock_pred_pivot.index,all_stock_pred_pivot[t],linestyle='solid', c='red',label='pred', alpha=0.4 )\n",
        "        #     plt.plot(all_stock_pred_pivot.index,all_stock_y_GroundTruth_pivot[t],linestyle='solid', c='green',label='true', alpha=0.4 )\n",
        "        #     plt.title(f'time index {t}, rmspe {np.sqrt(np.mean(((all_stock_pred_pivot[t]-all_stock_y_GroundTruth_pivot[t])/all_stock_y_GroundTruth_pivot[t])**2))}')\n",
        "        #     plt.xticks(all_stock_pred_pivot.index,rotation=90)\n",
        "        #     plt.grid()\n",
        "        #     plt.show()\n",
        "        #     plt.close()\n",
        "\n",
        "        # plot the RMSPE for all time ids\n",
        "        display_n_time_ids = 200\n",
        "        sorted_rmspe_per_time_id = rmspe_per_time_id.sort_values(ascending=False).head(display_n_time_ids)\n",
        "        fig, ax = plt.subplots(figsize=(40, 10))\n",
        "        ax.bar(range(len(sorted_rmspe_per_time_id)), sorted_rmspe_per_time_id, tick_label=sorted_rmspe_per_time_id.index)\n",
        "        ax.set_xticks(range(len(sorted_rmspe_per_time_id)))\n",
        "        ax.tick_params(axis='x', rotation=90)\n",
        "        ax.set_yticks(np.arange(0, max(sorted_rmspe_per_time_id) + 0.01, 0.04))\n",
        "        ax.grid()\n",
        "        ax.set_title(f'RMSPE of Real. Vol. for each time id on {set_name} set')\n",
        "        ax.set_xlabel('Time ID')\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        all_time_id_rmspe = rmspe_per_time_id\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(40, 10))\n",
        "        largest_20_rmspe_time_ids = all_time_id_rmspe.nlargest(20).index.values\n",
        "        # Precompute the maximum RMSPE value for the largest 20 time IDs\n",
        "        max_large_val = all_time_id_rmspe.loc[largest_20_rmspe_time_ids].max()\n",
        "        # Create the second bar plot for the largest 20 RMSPE time IDs\n",
        "        ax.text(5, max_large_val + 0.001, f'20 largest RMSPE time ids: {largest_20_rmspe_time_ids}', fontsize=12)\n",
        "        # Convert time ids to string once and plot the data\n",
        "        ax.bar(largest_20_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[largest_20_rmspe_time_ids])\n",
        "        # Set y-ticks efficiently, using the precomputed max value\n",
        "        ax.set_yticks(np.arange(0, max_large_val + 0.001, 0.08))\n",
        "        ax.set_ylabel('RMSPE')\n",
        "        ax.set_title(f'20 largest RMSPE time ids on {set_name} set')\n",
        "        ax.grid()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        all_time_id_rmspe = rmspe_per_time_id\n",
        "        # Use `nsmallest` and `nlargest` to directly get the top 10 smallest and largest RMSPE values without full sorting\n",
        "        smallest_10_rmspe_time_ids = all_time_id_rmspe.nsmallest(10).index.values\n",
        "        # Precompute max value for y-ticks range in a single operation\n",
        "        max_small_val = all_time_id_rmspe.loc[smallest_10_rmspe_time_ids].max()\n",
        "        # Create the bar plot for RMSPE with precomputed values\n",
        "        fig, ax = plt.subplots(figsize=(40, 10))\n",
        "        # Set text once, and avoid converting indices to strings multiple times\n",
        "        ax.text(5, max_small_val + 0.001, f'10 smallest RMSPE time ids: {smallest_10_rmspe_time_ids}', fontsize=12)\n",
        "        # Convert time ids to string once and avoid recalculating max_small_val in yticks\n",
        "        ax.bar(smallest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[smallest_10_rmspe_time_ids])\n",
        "        # Set y-ticks with precomputed values (adjusting the range once)\n",
        "        ax.set_yticks(np.arange(0, max_small_val + 0.001, 0.04))\n",
        "        ax.set_ylabel('RMSPE')\n",
        "        ax.set_title(f'10 smallest RMSPE time ids on {set_name} set')\n",
        "        ax.grid()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "        ###### visualize the time ids with largest and smallest RMSPE on the average rvol. plot on training set\n",
        "        plt.figure(figsize=(30,5))\n",
        "        plt.plot(range(len(avg_target_rvol)),avg_target_rvol,linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n",
        "        large_idx = np.where(np.isin(unique_time_ids,largest_20_rmspe_time_ids))[0]\n",
        "        red_colors = colors = ['black', 'maroon', 'darkred', 'firebrick', 'crimson', 'indianred', \\\n",
        "          'tomato', 'lightcoral', 'salmon', 'hotpink', 'palevioletred', \\\n",
        "          'mediumvioletred', 'orchid', 'fuchsia', 'magenta', 'violet', \\\n",
        "          'plum', 'mediumorchid', 'lavender', 'thistle']\n",
        "        for i,s in enumerate(large_idx):\n",
        "            plt.axvline(x=s, ymin=0, ymax=1,color=red_colors[i],linestyle='-',label=str(i))\n",
        "        small_idx = np.where(np.isin(unique_time_ids,smallest_10_rmspe_time_ids))[0]\n",
        "        green_colors = ['gold', 'yellow', 'lightyellow', 'khaki', 'greenyellow', \\\n",
        "                'chartreuse', 'lime', 'lawngreen', 'darkgreen', 'forestgreen', \\\n",
        "                'seagreen', 'mediumseagreen', 'springgreen', 'aquamarine', \\\n",
        "                'turquoise', 'lightgreen', 'mediumspringgreen', 'cyan', 'skyblue', 'deepskyblue']\n",
        "        for j,l in enumerate(small_idx):\n",
        "            plt.axvline(x=l, ymin=0, ymax=1,color=green_colors[j],linestyle='-',label=str(j))\n",
        "        plt.grid()\n",
        "        plt.yticks(np.arange(0, 0.04, 0.01))\n",
        "        plt.xlabel('sequential time id index')\n",
        "        plt.ylabel(f'{set_name} rvol.')\n",
        "        plt.title(f'Average {set_name} rvol. Darker reddish lines for 20 largest RMSPE time ids and greenish lines for 10 smallest RMSPE time ids')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        del all_stock_y_GroundTruth_df,all_stock_pred_df,avg_target_rvol\n",
        "        gc.collect()\n",
        "        return all_stock_y_GroundTruth_pivot[largest_20_rmspe_time_ids],  all_stock_pred_pivot[largest_20_rmspe_time_ids]\n",
        "\n",
        "\n",
        "\n",
        "    def compute_model_bias_variance(self,y_test,y_train,X_train,best_mlxtend_xgb_params):\n",
        "\n",
        "        ## model bias and variance measurement\n",
        "        # estimate bias and variance\n",
        "        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n",
        "        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n",
        "\n",
        "        #full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n",
        "\n",
        "        #X_train = full_train_df[self.feat_cols_list]\n",
        "        X_train = X_train[self.feat_cols_list]\n",
        "        #y_train = full_train_df[self.target_name] #target\n",
        "        y_train = y_train[self.target_name]\n",
        "        #train_weight = y_train['target']\n",
        "        X_test = self.test_df[self.feat_cols_list]\n",
        "        #y_test = self.test_df[self.target_name] #target\n",
        "\n",
        "\n",
        "        # Assuming best_mlxtend_xgb_params contains the hyperparameters\n",
        "        max_depth, eta, subsample, colsample_bytree, gamma, reg_alpha, reg_lambda, min_child_weight, num_rounds = best_mlxtend_xgb_params\n",
        "\n",
        "        # Create XGBRegressor model\n",
        "        xgb_model = XGBRegressor(\n",
        "            max_depth=max_depth,\n",
        "            learning_rate=eta,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            gamma=gamma,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            min_child_weight=min_child_weight,\n",
        "            n_estimators=num_rounds,\n",
        "            objective='reg:squarederror',\n",
        "            tree_method = \"hist\",\n",
        "            device = \"cuda\"\n",
        "        )\n",
        "\n",
        "        #v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n",
        "        v1tr = np.array([1]*len(self.train_df['log_wap1_log_price_ret_vol'])) # remove\n",
        "        #v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n",
        "        v1ts = np.array([1]*len( self.test_df['log_wap1_log_price_ret_vol'])) # remove\n",
        "\n",
        "        w_train = y_train **-2 * v1tr**2\n",
        "        #w_test = y_test **-2 * v1ts**2\n",
        "\n",
        "        # Train XGBRegressor model\n",
        "        xgb_model.fit(X_train.values, (np.log(y_train.values/v1tr.values) + self.target_shift), sample_weight=w_train)\n",
        "\n",
        "        # Now you can use bias_variance_decomp\n",
        "        mse, bias, var = bias_variance_decomp(xgb_model, X_train.values, (np.log(y_train.values/v1tr.values) + self.target_shift), X_test.values, (np.log(y_test['target'].values/v1ts.values) + self.target_shift), loss='mse', num_rounds=30, random_seed=1)\n",
        "        print('\\nMSE: %.3f' % mse)\n",
        "        print('Bias: %.3f' % bias)\n",
        "        print('Variance: %.3f' % var)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    ############################################################################################################\n",
        "    \"\"\"  TRAINING and TESTING SET COMPARISON functions START\"\"\"\n",
        "    ############################################################################################################\n",
        "\n",
        "    def train_n_test_set_st_RMSPE_comparison(self,train_rmspe_per_stock,test_rmspe_per_stock):\n",
        "        # Combine train and test RMSPE into a single DataFrame for easier plotting\n",
        "        rmspe_df = pd.DataFrame({\n",
        "            'Stock ID': train_rmspe_per_stock.index,\n",
        "            'Train RMSPE': train_rmspe_per_stock['train_rmspe_per_stock'],\n",
        "            'Test RMSPE': test_rmspe_per_stock['test_rmspe_per_stock']\n",
        "        })\n",
        "        fig = px.line(rmspe_df, x='Stock ID', y=['Train RMSPE', 'Test RMSPE'], \\\n",
        "                      title=f'Train vs Test RMSPE per Stock,\\n Corrrelation between train and test RMSPE: {rmspe_df[\"Train RMSPE\"].corr(rmspe_df[\"Test RMSPE\"])}')\n",
        "        fig.update_layout(xaxis=dict(tickvals=rmspe_df['Stock ID'], tickangle=90), yaxis_title='RMSPE', xaxis_title='Stock ID')\n",
        "        fig.show()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def train_n_test_set_picked_st_distribution_comparison(self,train_groundtruth, test_groundtruth , picked_stock_id):\n",
        "\n",
        "        # perform Kolmogorov-Smirnov test\n",
        "        ks_stat, ks_pval = stats.ks_2samp(train_groundtruth, test_groundtruth)\n",
        "        print(f\"Kolmogorov-Smirnov test: KS Statistic: {ks_stat}, P-Value: {ks_pval}\")\n",
        "        # Check if the distributions are the same\n",
        "        alpha = 0.05  # Significance level\n",
        "        if ks_pval < alpha:\n",
        "            print(\"The null hypothesis is rejected. The test set does not come from the same distribution as the train set.\")\n",
        "            ks_test_stats = f\"K-S TEST: Train and Test sets are DIFFERENT\"\n",
        "        else:\n",
        "            print(\"The null hypothesis is accepted. The test set may come from the same distribution as the train set.\")\n",
        "            ks_test_stats = f\"K-S TEST: Train and Test sets are SAME\"\n",
        "\n",
        "        # violin plot train_groundtruth and test_groundtruth on the same plot\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Violin(x=['Train']*len(train_groundtruth), y=train_groundtruth, name='Train', box_visible=True, meanline_visible=True))\n",
        "        fig.add_trace(go.Violin(x=['Test']*len(test_groundtruth), y=test_groundtruth, name='Test', box_visible=True, meanline_visible=True))\n",
        "        train_stats = f\"Train - Mean: {np.mean(train_groundtruth):.4f}, Median: {np.median(train_groundtruth):.4f}, Std: {np.std(train_groundtruth):.4f}, Skew: {stats.skew(train_groundtruth):.4f}, Kurtosis: {stats.kurtosis(train_groundtruth):.4f}, 75th Percentile: {np.percentile(train_groundtruth, 75):.4f}\"\n",
        "        test_stats = f\"Test - Mean: {np.mean(test_groundtruth):.4f}, Median: {np.median(test_groundtruth):.4f}, Std: {np.std(test_groundtruth):.4f}, Skew: {stats.skew(test_groundtruth):.4f}, Kurtosis: {stats.kurtosis(test_groundtruth):.4f}, 75th Percentile: {np.percentile(test_groundtruth, 75):.4f}\"\n",
        "        fig.update_layout(\n",
        "            title=f\"Stock {picked_stock_id}'s Train vs Test GroundTruth Distribution<br>{train_stats}<br>{test_stats}<br>   {ks_test_stats}\",\n",
        "            yaxis=dict(tickmode='linear', tick0=0, dtick=0.002)  # Increase y tick resolution\n",
        "        )\n",
        "        fig.show()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def each_time_id_RMSPE_across_stocks_comparison(self,gt_time_id_df, pred_time_id_df,set_name,y_train_df):\n",
        "\n",
        "        y_train_pivot = y_train_df.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "        for time_id in gt_time_id_df.columns.values:\n",
        "            gt_df = pd.DataFrame({'gt':gt_time_id_df[time_id],'pred':pred_time_id_df[time_id]}, index=gt_time_id_df.index)\n",
        "            plt.figure(figsize=(40, 10))\n",
        "            plt.title(f'Time ID {time_id} GroundTruth vs Prediction on {set_name} set')\n",
        "            plt.plot(gt_df.index, gt_df['gt'].values, label='GroundTruth', color='blue')\n",
        "            plt.plot(gt_df.index, gt_df['pred'].values, label='Prediction', color='red')\n",
        "            # ## Add box plot of y_train_pivot for each stock\n",
        "            plt.boxplot([y_train_pivot[stock_id].dropna().values for stock_id in gt_df.index], positions=gt_df.index)\n",
        "            plt.xticks(ticks=gt_df.index, labels=gt_df.index, rotation=90)\n",
        "            plt.yticks(np.arange(0, 0.04, 0.002))\n",
        "            plt.ylim(0, 0.04)\n",
        "            plt.grid()\n",
        "            plt.legend()\n",
        "            plt.xlabel('Stock ID')\n",
        "            plt.ylabel('Real. Vol.')\n",
        "            plt.show()\n",
        "\n",
        "        del gt_time_id_df, pred_time_id_df, y_train_df,y_train_pivot\n",
        "        return\n",
        "\n",
        "\n",
        "    def val_set_n_test_set_representation_plot(self,mean_val_set_rmspe_error, test_set_rmspe_error, train_set_rmspe_error):\n",
        "        ## check if average walk-forward validation set error and test set error are positively correlated.\n",
        "        val_test_corr = np.corrcoef(mean_val_set_rmspe_error,test_set_rmspe_error)[0,1]\n",
        "        fig, ax1 = plt.subplots()\n",
        "        # Original scatter plot\n",
        "        ax1.scatter(test_set_rmspe_error, mean_val_set_rmspe_error, alpha=0.4, color='blue', label='Mean Validation RMSPE')\n",
        "        ax1.set_xlabel('Test Set RMSPE Error')\n",
        "        ax1.set_ylabel('Mean Validation Set RMSPE Error', color='blue')\n",
        "        ax1.tick_params(axis='y', labelcolor='blue')\n",
        "        # Create twin axis\n",
        "        ax2 = ax1.twinx()\n",
        "        # Add train_set_rmspe_error to the twin axis\n",
        "        ax2.scatter(test_set_rmspe_error, train_set_rmspe_error, alpha=0.4, color='red', label='Train RMSPE')\n",
        "        ax2.set_ylabel('Train Set RMSPE Error', color='red')\n",
        "        ax2.tick_params(axis='y', labelcolor='red')\n",
        "        # Add correlation to the title\n",
        "        train_test_corr = np.corrcoef(test_set_rmspe_error, train_set_rmspe_error)[0, 1]\n",
        "        plt.title(f'Test Set RMSPE Error vs. Mean Validation/Train Set RMSPE Error \\n : val_test_corr: {val_test_corr:.3f} \\n train_test_corr {train_test_corr:.3f}')\n",
        "        # Add legend\n",
        "        plt.legend(loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    ############################################################################################################\n",
        "    \"\"\"  TRAINING and TESTING SET COMPARISON functions END\"\"\"\n",
        "    ############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_predictions(self,final_reg,test_pred, y_test,train_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,w_test,best_mlxtend_xgb_params,mean_val_set_rmspe_error):\n",
        "\n",
        "        y_true = y_test\n",
        "        y_pred = test_pred\n",
        "        test_residuals = pd.DataFrame(y_true['target'] - y_pred['target'])\n",
        "        test_residuals[['stock_id','time_id']] = y_true[['stock_id','time_id']]\n",
        "        train_residuals = pd.DataFrame(y_train['target'] - train_pred['target'])\n",
        "        train_residuals[['stock_id','time_id']] = y_train[['stock_id','time_id']]\n",
        "        unique_stock_ids = self.train_stock_id.unique()\n",
        "\n",
        "        #all_stock_train_pred_df = self.compute_all_stock_train_pred_df(unique_stock_ids, train_pred)\n",
        "        all_stock_train_pred_df = train_pred\n",
        "        #all_stock_v1tr_df = self.compute_all_stock_v1tr_df(unique_stock_ids, v1tr)\n",
        "        all_stock_v1tr_df = v1tr\n",
        "        #all_stock_y_train_df = self.compute_all_stock_y_train_df(unique_stock_ids, y_train)\n",
        "        all_stock_y_train_df = y_train\n",
        "        #train_avg_target_rvol = self.compute_train_avg_target_rvol(unique_stock_ids, y_train)\n",
        "        train_avg_target_rvol = y_train.groupby('time_id')['target'].mean().reindex(self.time_id_order)\n",
        "        test_avg_target_rvol = y_test.groupby('time_id')['target'].mean() #.reindex(self.time_id_order)\n",
        "        # train_avg_target_rvol = y_train.groupby('time_id')['target'].median().reindex(self.time_id_order) ## median\n",
        "        # test_avg_target_rvol = y_test.groupby('time_id')['target'].median() #.reindex(self.time_id_order) ## median\n",
        "\n",
        "        #all_stock_test_pred_df = self.compute_all_stock_test_pred_df( unique_stock_ids, test_pred)\n",
        "        all_stock_test_pred_df = test_pred\n",
        "        #all_stock_y_test_df = self.compute_all_stock_y_test_df( unique_stock_ids, y_test)\n",
        "        all_stock_y_test_df = y_test\n",
        "        all_stock_v1ts_df = v1ts\n",
        "\n",
        "        train_set_rmspe_error = [0.190656,0.190647,0.195972,0.192530]\n",
        "        test_set_rmspe_error = [0.230367,0.227526,0.232057,0.266616]\n",
        "\n",
        "        print('\\n####################################### PREDICTION #################################################')\n",
        "\n",
        "        #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n",
        "        #v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n",
        "        print('corr(y_pred/v1ts, y_true/v1ts)',self.nancorr(       y_pred['target'].values/v1ts['wap1_log_price_ret_vol'] ,        y_true['target'].values/v1ts['wap1_log_price_ret_vol'] ))\n",
        "        print('log(corr( ))',self.nancorr(np.log(y_pred['target'].values/v1ts['wap1_log_price_ret_vol']), np.log(y_true['target'].values/v1ts['wap1_log_price_ret_vol'])))\n",
        "        print('corr(y_pred, y_true)',self.nancorr(y_pred['target'].values, y_true['target'].values))\n",
        "        print('log(corr( ))',self.nancorr(np.log(y_pred['target'].values), np.log(y_true['target'].values)))\n",
        "        train_score = np.mean( ((train_pred['target'].values-y_train['target'].values)/y_train['target'].values)**2 )**0.5\n",
        "        print(f'RMSPE train score: ',  train_score)\n",
        "        train_set_rmspe_error.append(train_score)\n",
        "        test_score = np.mean( ((y_pred['target'].values-y_true['target'].values)/y_true['target'].values)**2 )**0.5\n",
        "        print(f'RMSPE test score: ',  test_score  )\n",
        "        test_set_rmspe_error.append(test_score)\n",
        "\n",
        "        ############################ SET PARAMETERS HERE ##############################\n",
        "        ##### individual stock id analysis parameters START #####\n",
        "        high_rmspe_stocks = ['31','37', '18','33', '112', '88','60','110','27', '9','16','30',  '103',  '5', '58', '89', '66', '40',  '0', '4', '75',  '90', '98']\n",
        "        start_index = 9\n",
        "        end_index = 10\n",
        "        good_pdp_feature_name = ['tlog_target_vol_pcorr_3_clusters', 'v1proj_29', 'vol1_mean', 'wap1_log_price_ret_volstock_mean_from_25'][3]\n",
        "        bad_up_pdp_feature_name = ['wap1_log_price_ret_volstock_mean_from_10',\n",
        "                                'v1spprojt10f29','pear_corr_90_clusters_labels',\\\n",
        "                                'robust_sum_stats_60_clusters_labels','pear_corr_3_clusters_labels',\\\n",
        "                                'tlog_tlog1p_target_vol_robust_sum_stats_2_clusters',][2]\n",
        "        bad_op_pdp_feature_name = ['root_book_delta_count',\\\n",
        "                                'soft_stock_mean_tvpl2_:10','soft_stock_mean_tvpl2_:20' ,\\\n",
        "                                'root_trade_count_smean'][0]\n",
        "        decision_plot_feature_names = ['wap1_log_price_ret_volstock_mean_from_25', 'v1spprojt15f25', 'v1spprojt15f25_q1', 'soft_stock_mean_tvpl2_:20', 'tlog_target_vol_pcorr_3_clusters', 'pear_corr_90_clusters_labels', 'root_trade_count_smean', 'robust_sum_stats_60_clusters_labels', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20', 'v1proj_29_15_q3', 'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock', 'wap1_log_price_ret_volstock_mean_from_20', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0', 'vol1_mean', 'texp_wap1_log_price_ret_vol_from_20', 'v1spprojt15f29_q3', 'liqvol1_smean', 'soft_stock_mean_tvpl2_liqf_volf20', 'tvpl2', 'v1proj_29_q3', 'v1spprojt15f29', 'tvpl2_rmed2v1', 'tlog_eps523_trade_price_n_wap_eqi_price0_dev', 'root_trade_count', 'tvpl2_smean_vol', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10', 'v1liq2sprojt5f25', 'wap1_log_price_ret_vol_from_25', 'tlog_target_vol_pcorr_10_clusters', 'v1proj_25_15_std', 'v1proj_29', 'tlog_wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20', 'tlog_tlog1p_target_vol_robust_sum_stats_2_clusters', 'pear_corr_3_clusters_labels', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20', 'v1liq2sprojt10f25', 'root_book_delta_count', 'wap1_log_price_ret_volstock_mean_from_10', 'wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock', 'pear_corr_10_clusters_labels', 'wap1_log_price_ret_volstock_mean_from_0', 'texp_wap1_log_price_ret_vol_from_10', 'v1spprojt10f29', 'wap1_log_price_ret_per_liq2_vol_15_ratio', 'log_wap1_log_price_ret_vol', 'v1proj_25', 'v1proj_25_std', 'tvpl2_rmed2v1lf25', 'tlog_tlog1p_target_vol_robust_sum_stats_4_clusters', 'tlog_eps523_trade_price_n_wap1_dev']\n",
        "        ##### individual stock id analysis parameters END #####\n",
        "\n",
        "        ############################ SET PARAMETERS HERE ##############################\n",
        "\n",
        "        \"\"\" TRAINING SET PREDICTIONS START \"\"\"\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ############################################ TRAINING SET #########################################################\n",
        "        print('\\n########################################################################################################################')\n",
        "        print('\\n####################################### TRAINING SET predictions START #################################################')\n",
        "        print('\\n########################################################################################################################')\n",
        "        ###################################################################################################################\n",
        "\n",
        "        ################################################################################################\n",
        "        ############################## OVERALL STOCK ANALYSIS START ######################################\n",
        "        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n",
        "        # train_rmspe_per_stock = self.overall_stock_id_analysis(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals,train_flag=True)\n",
        "        print('\\n####################################### OVERALL STOCK ANALYSIS END ######################################')\n",
        "        ############################## OVERALL STOCK ANALYSIS END ######################################\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "        ################################################################################################\n",
        "        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n",
        "        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n",
        "        #### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n",
        "        # for picked_stock_id in high_rmspe_stocks[start_index:end_index]:\n",
        "        #     train_groundtruth = self.individual_stock_id_analysis(int(picked_stock_id),unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,train_residuals,train_avg_target_rvol,train_flag=True)\n",
        "        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n",
        "        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################################################################################################\n",
        "        ############################## OVERALL TIME ID ANALYSIS START ##################################\n",
        "        ################################################################################################\n",
        "        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n",
        "        #train_gt_time_id_df, train_pred_time_id_df = self.overall_time_id_analysis(all_stock_y_train_df,all_stock_train_pred_df,train_avg_target_rvol, train_flag=True)\n",
        "        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n",
        "        ################################################################################################\n",
        "        ############################## OVERALL TIME ID ANALYSIS END #################################\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ###################################### Feature importance & SHAPLEY START #########################################\n",
        "        ###################################################################################################################\n",
        "        print('\\n###################################### Feature importance & SHAPLEY START #########################################')\n",
        "        print('\\n#---------------------------------------------compute_global_SHAP_values----------------------------------------------------------------#')\n",
        "        #train_shap_values_all = self.compute_global_SHAP_values(final_reg,X_train,y_train,train_pred,v1tr,set_name='train')\n",
        "        #os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n",
        "        #os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "        #with open('train_shap_values_all_log_target_after_jager_transform_improv_shap_interact.pkl', 'wb') as file:\n",
        "        #    pickle.dump(train_shap_values_all, file)\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "\n",
        "        print('\\n#---------------------------------------------analyze_global_SHAP_values----------------------------------------------------------------#')\n",
        "        #os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n",
        "        # os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "        # with open('train_shap_values_all_log_target_after_jager_transform_improv.pkl', 'rb') as file:\n",
        "        #     train_shap_values_all = pickle.load( file)\n",
        "        #self.analyze_global_SHAP_values(train_shap_values_all,all_stock_y_train_df,all_stock_train_pred_df,set_name='train')\n",
        "        #top_n_feats = 50\n",
        "        #self.compute_shapley_barplot(train_shap_values_all,top_n_feats,X_train,y_train['target'],stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='train')\n",
        "        #self.compute_shapley_PDP_n_Scatter(bad_op_pdp_feature_name,train_shap_values_all,all_stock_y_train_df,all_stock_train_pred_df,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='train')\n",
        "        #self.compute_shapley_decision(train_shap_values_all.base_values,train_shap_values_all,decision_plot_feature_names,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='train')\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "\n",
        "        print('\\n#---------------------------------------------compute_individual_stock_SHAP_values----------------------------------------------------------------#')\n",
        "        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n",
        "        stock_id = 0\n",
        "        view_time_ids_start = 0\n",
        "        view_time_ids_end = 500\n",
        "        #self.compute_individual_stock_SHAP_values(final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "\n",
        "        print('\\n#---------------------------------------------compute_individual_instance_SHAP_values----------------------------------------------------------------#')\n",
        "        #ovearll_error_idxs, up_error_idxs, op_error_idxs = self.identiy_largest_overall_n_under_n_over_prediction_errors(all_stock_y_train_df,all_stock_train_pred_df,set_name='train')\n",
        "        #self.shapley_analysis_of_large_error_instances(ovearll_error_idxs, up_error_idxs, op_error_idxs,train_shap_values_all,set_name='train')\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "        print('\\n###################################### Feature importance & SHAPLEY END #########################################')\n",
        "        ###################################################################################################################\n",
        "        ###################################### Feature importance & SHAPLEY END ###########################################\n",
        "        ###################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ###################################### MODEL BIAS VARINANCE START ################################################\n",
        "        ###################################################################################################################\n",
        "\n",
        "         #### Plot top 30 feature importances\n",
        "        # fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        # xgb.plot_importance(final_reg, importance_type='gain', max_num_features=30, height=0.8, show_values=False)\n",
        "        # self.compute_model_bias_variance(y_test,y_train,X_train,best_mlxtend_xgb_params)\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ###################################### MODEL BIAS VARINANCE END #################################################\n",
        "        ##################################################################################################################\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ############################################ TRAINING SET PREDICTIONS END ##########################################\n",
        "        ###################################################################################################################\n",
        "        print('\\n####################################### TRAINING SET predictions END #################################################\\n')\n",
        "        print('\\n######################################################################################################################\\n')\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\" TESTING SET PREDICTIONS START \"\"\"\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ############################################ TESTING SET PREDICTIONS START ########################################\n",
        "        print('\\n#######################################################################################################################')\n",
        "        print('\\n####################################### TESTING SET predictions START #################################################')\n",
        "        print('\\n#######################################################################################################################')\n",
        "        ###################################################################################################################\n",
        "\n",
        "        ################################################################################################\n",
        "        ############################## OVERALL STOCK ANALYSIS START ######################################\n",
        "        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n",
        "        # test_rmspe_per_stock = self.overall_stock_id_analysis(unique_stock_ids,all_stock_test_pred_df,all_stock_y_test_df,test_residuals,train_flag=False)\n",
        "        print('\\n####################################### OVERALL STOCK ANALYSIS END ######################################')\n",
        "        ############################## OVERALL STOCK ANALYSIS END ######################################\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ################################################################################################\n",
        "        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n",
        "        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n",
        "        ##### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n",
        "        # for picked_stock_id in high_rmspe_stocks[start_index:end_index]:\n",
        "        #     test_groundtruth = self.individual_stock_id_analysis(int(picked_stock_id),unique_stock_ids,all_stock_y_test_df,all_stock_test_pred_df,test_residuals,test_avg_target_rvol,train_flag=False)\n",
        "        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n",
        "        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "        ################################################################################################\n",
        "        ############################## OVERALL TIME ID ANALYSIS START ##################################\n",
        "        ################################################################################################\n",
        "        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n",
        "        #test_gt_time_id_df, test_pred_time_id_df = self.overall_time_id_analysis(all_stock_y_test_df,all_stock_test_pred_df,test_avg_target_rvol, train_flag=False)\n",
        "        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n",
        "        ################################################################################################\n",
        "        ############################## OVERALL TIME ID ANALYSIS END #################################\n",
        "        ################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ###################################### TEST Feature importance & SHAPLEY START #########################################\n",
        "        ###################################################################################################################\n",
        "\n",
        "        print('\\n###################################### TEST Feature importance & SHAPLEY START #########################################')\n",
        "        print('\\n#---------------------------------------------compute_global_SHAP_values----------------------------------------------------------------#')\n",
        "        # test_shap_values_all = self.compute_global_SHAP_values(final_reg,X_test,y_test,test_pred,v1ts,set_name='test')\n",
        "        # os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n",
        "        # #os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "        # with open('test_shap_values_all_log_target_after_jager_transform_improv_shap_interact.pkl', 'wb') as file:\n",
        "        #     pickle.dump(test_shap_values_all, file)\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "\n",
        "        print('\\n#---------------------------------------------analyze_global_SHAP_values----------------------------------------------------------------#')\n",
        "        #os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n",
        "        # os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "        # with open('test_shap_values_all_log_target_after_jager_transform_improv.pkl', 'rb') as file:\n",
        "        #     test_shap_values_all = pickle.load( file)\n",
        "        #self.analyze_global_SHAP_values(test_shap_values_all,all_stock_y_test_df,all_stock_test_pred_df,set_name='test')\n",
        "        #self.compute_shapley_barplot(test_shap_values_all,top_n_feats,X_test,y_test['target'],stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='test')\n",
        "        #self.compute_shapley_PDP_n_Scatter(bad_op_pdp_feature_name,test_shap_values_all,all_stock_y_test_df,all_stock_test_pred_df,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='test')\n",
        "        #self.compute_shapley_decision(test_shap_values_all.base_values,test_shap_values_all,decision_plot_feature_names,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='test')\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "\n",
        "        print('\\n#---------------------------------------------compute_individual_stock_SHAP_values-------------------------------------------------------#')\n",
        "        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n",
        "        stock_id = 0\n",
        "        view_time_ids_start = 0\n",
        "        view_time_ids_end = 500\n",
        "        #self.compute_individual_stock_SHAP_values(final_reg,X_test,all_stock_test_pred_df,all_stock_v1tr_df,all_stock_y_test_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "\n",
        "\n",
        "        print('\\n#---------------------------------------------compute_individual_instance_SHAP_values-------------------------------------------------------#')\n",
        "        #ovearll_error_idxs, up_error_idxs, op_error_idxs = self.identiy_largest_overall_n_under_n_over_prediction_errors(all_stock_y_test_df,all_stock_test_pred_df,set_name='test')\n",
        "        #self.shapley_analysis_of_large_error_instances(ovearll_error_idxs, up_error_idxs, op_error_idxs,test_shap_values_all,set_name='test')\n",
        "        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n",
        "        print('\\n###################################### TEST Feature importance & SHAPLEY END #########################################')\n",
        "        ###################################################################################################################\n",
        "        ###################################### Feature importance & SHAPLEY END #########################################\n",
        "        ###################################################################################################################\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ############################################ TESTING SET PREDICTIONS END ##########################################\n",
        "        ###################################################################################################################\n",
        "        print('##################################################################################################')\n",
        "        print('\\n####################################### TESTING SET predictions END #################################################\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\" TRAINING AND TESTING SET PERFORMANCE COMPARISON START \"\"\"\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ############################################ TRAINING AND TESTING SET PERFORMANCE COMPARISON START ##########################################\n",
        "        ###################################################################################################################\n",
        "        print('\\n####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON START #################################################')\n",
        "        #self.train_n_test_set_st_RMSPE_comparison(train_rmspe_per_stock,test_rmspe_per_stock)\n",
        "        print('##################################################################################################')\n",
        "        print('\\n####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON END #################################################\\n')\n",
        "\n",
        "        ###################################################################################################################\n",
        "        print('\\n####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON START #################################################')\n",
        "        #self.train_n_test_set_picked_st_distribution_comparison(train_groundtruth, test_groundtruth,high_rmspe_stocks[start_index:end_index])\n",
        "        print('##################################################################################################')\n",
        "        print('\\n####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON END #################################################\\n')\n",
        "        ###################################################################################################################\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        print('##################################################################################################')\n",
        "        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on train set START ########################')\n",
        "        #self.each_time_id_RMSPE_across_stocks_comparison(train_gt_time_id_df, train_pred_time_id_df,set_name='train',y_train_df = all_stock_y_train_df)\n",
        "        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on train set END ########################')\n",
        "        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on test set START ##########################\\n')\n",
        "        #self.each_time_id_RMSPE_across_stocks_comparison(test_gt_time_id_df, test_pred_time_id_df,set_name='test',y_train_df = all_stock_y_train_df)\n",
        "        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on test set END ##########################\\n')\n",
        "        print('##################################################################################################')\n",
        "        ###################################################################################################################\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        print('\\n####################################### Representativeness of validation set in test set START #################################################')\n",
        "        ###################################################################################################################\n",
        "        self.val_set_n_test_set_representation_plot(mean_val_set_rmspe_error, test_set_rmspe_error, train_set_rmspe_error)\n",
        "        ###################################################################################################################\n",
        "        print('\\n####################################### Representativeness of validation set in test set END #################################################')\n",
        "        ###################################################################################################################\n",
        "\n",
        "\n",
        "        ###################################################################################################################\n",
        "        ############################################ TRAINING AND TESTING SET COMPARISON END ##########################################\n",
        "        ###################################################################################################################\n",
        "\n",
        "\n",
        "        del X_train,y_train, all_stock_train_pred_df, all_stock_v1tr_df ,  all_stock_test_pred_df, all_stock_y_train_df,  all_stock_y_test_df\n",
        "        del y_true, y_pred, test_residuals, train_residuals, unique_stock_ids, all_stock_v1ts_df, train_avg_target_rvol, test_avg_target_rvol\n",
        "        gc.collect()\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def visualize_tree(self,):\n",
        "        # feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n",
        "        # feature_importances.to_csv('feature_importances.csv')\n",
        "        # plt.figure(figsize=(16, 12))\n",
        "        # sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(20), x='average', y='feature')\n",
        "        # plt.title('20 TOP feature importance over {} folds average'.format(folds.n_splits));\n",
        "\n",
        "        # importances = pd.DataFrame({'Feature': model.feature_name(),\n",
        "        #                             'Importance': sum( [model.feature_importance(importance_type='gain') for model in models] )})\n",
        "        # importances2 = importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n",
        "        # importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n",
        "\n",
        "        #TODO: #plot decision tree for interpretability\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4f6992",
      "metadata": {},
      "source": [
        "#### ------------------------- STOCK ID 31 Training and Prediction START -------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "81137779",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def objective_st31(trial):\n",
        "\n",
        "    t_v_t = train_validate_n_test(df_train_reordered_for_stock_31, df_test_for_stock_31)\n",
        "\n",
        "    ######  SET Hyperparameter's range for tuning ######\n",
        "    # Hyperparameters and algorithm parameters are described here\n",
        "    seed1=11\n",
        "    missing_value = -np.inf   # Replace with a suitable value\n",
        "    early_stopping_rounds = 25\n",
        "    num_round= 1000 # num_trees\n",
        "    params = {'disable_default_eval_metric': 1,\n",
        "              \"max_depth\": trial.suggest_int('max_depth', 5, 30),\n",
        "            \"eta\": trial.suggest_float(name='eta', low=0.00001, high=0.2,log=True),\n",
        "            \"subsample\" : round(trial.suggest_float(name='subsample', low=0.6, high=1.0,step=0.05),1),\n",
        "            \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.5, high=1,step=0.05),1),\n",
        "            'gamma': trial.suggest_int('gamma', 1, 10),\n",
        "            'reg_alpha': trial.suggest_int('reg_alpha', 1, 14),\n",
        "            'reg_lambda': trial.suggest_int('reg_lambda', 1, 14),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "            \"tree_method\": 'hist',\n",
        "            \"device\": \"cuda\",\n",
        "            \"seed\":seed1,\n",
        "            #'missing': missing_value\n",
        "            }\n",
        "\n",
        "    # params = {'disable_default_eval_metric': 1,\n",
        "    #           \"max_depth\": trial.suggest_int('max_depth', 25, 35),\n",
        "    #         \"eta\": trial.suggest_float(name='eta', low=0.0001, high=0.01,log=True),\n",
        "    #         \"subsample\" : round(trial.suggest_float(name='subsample', low=0.8, high=1.0,step=0.05),1),\n",
        "    #         \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.5, high=0.8,step=0.05),1),\n",
        "    #         'gamma': trial.suggest_int('gamma', 0, 5),\n",
        "    #         'reg_alpha': trial.suggest_int('reg_alpha', 0, 5),\n",
        "    #         'reg_lambda': trial.suggest_int('reg_lambda', 0, 5),\n",
        "    #         'min_child_weight': trial.suggest_int('min_child_weight', 2, 6),\n",
        "    #         \"tree_method\": 'hist',\n",
        "    #         \"device\": \"cuda\",\n",
        "    #         \"seed\":seed1,\n",
        "    #         #'missing': missing_value\n",
        "    #         }\n",
        "    ######  SET Hyperparameter's range for tuning ######\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    val_avg_error,best_iteration = t_v_t.xgb_train_validate(params,num_round,early_stopping_rounds,trial)\n",
        "    print(f\"val_avg_error: {val_avg_error}, best_iteration: {best_iteration}\")\n",
        "    trial.set_user_attr(\"best_iteration\", best_iteration)\n",
        "\n",
        "    del t_v_t\n",
        "    gc.collect()\n",
        "    return val_avg_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ab4299db",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-10-16 18:48:06,143] A new study created in memory with name: stock 31 model\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.9578689282652775\n",
            "corr(p/v1v, y_val/v1v) 0.006714736120036688\n",
            "log(corr( )) 0.0017762285138822281\n",
            "corr(p, y_val) 0.9190363537283903\n",
            "log(corr( )) 0.916558588709664\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.9572380174547609\n",
            "corr(p/v1v, y_val/v1v) 0.025674446936390084\n",
            "log(corr( )) 0.003166379469646316\n",
            "corr(p, y_val) 0.8024682582686788\n",
            "log(corr( )) 0.8664031415806589\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.9565284303028173\n",
            "corr(p/v1v, y_val/v1v) 0.015013788000564746\n",
            "log(corr( )) -0.012954185048949405\n",
            "corr(p, y_val) 0.754104314798804\n",
            "log(corr( )) 0.8295666611834406\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.9545496260654782\n",
            "corr(p/v1v, y_val/v1v) -0.022433160017750687\n",
            "log(corr( )) -0.001482638216578241\n",
            "corr(p, y_val) 0.8488965612243135\n",
            "log(corr( )) 0.8581094734274959\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.9554777238940974\n",
            "corr(p/v1v, y_val/v1v) -0.011354768993293941\n",
            "log(corr( )) -0.017305302486119157\n",
            "corr(p, y_val) 0.8363359188216537\n",
            "log(corr( )) 0.8662771466959337\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.9544026487794861\n",
            "corr(p/v1v, y_val/v1v) 0.3395821217272403\n",
            "log(corr( )) 0.3939314115877242\n",
            "corr(p, y_val) 0.8783119473139747\n",
            "log(corr( )) 0.8683391037696551\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.953319004132172\n",
            "corr(p/v1v, y_val/v1v) 0.33909714479739295\n",
            "log(corr( )) 0.41323410969759905\n",
            "corr(p, y_val) 0.8588621305954867\n",
            "log(corr( )) 0.8626083969780336\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.9527400140454437\n",
            "corr(p/v1v, y_val/v1v) 0.3122465222431129\n",
            "log(corr( )) 0.3607422159020542\n",
            "corr(p, y_val) 0.8919206605554174\n",
            "log(corr( )) 0.8932279783287778\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.9518355522462327\n",
            "corr(p/v1v, y_val/v1v) 0.23171415448645843\n",
            "log(corr( )) 0.3868862692635532\n",
            "corr(p, y_val) 0.7295808966368424\n",
            "log(corr( )) 0.802705629477591\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 18:51:09,887] Trial 0 finished with value: 0.9547191090116363 and parameters: {'max_depth': 12, 'eta': 2.9205922055886686e-05, 'subsample': 0.9, 'colsample_bytree': 0.65, 'gamma': 5, 'reg_alpha': 1, 'reg_lambda': 9, 'min_child_weight': 5}. Best is trial 0 with value: 0.9547191090116363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.9532311449305957\n",
            "corr(p/v1v, y_val/v1v) 0.29675025452044307\n",
            "log(corr( )) 0.34781381946264456\n",
            "corr(p, y_val) 0.8671039865335619\n",
            "log(corr( )) 0.8560641874123727\n",
            "mean rmspe val score over 10 splits is 0.9547191090116363\n",
            "val_avg_error: 0.9547191090116363, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.9701379477150147\n",
            "corr(p/v1v, y_val/v1v) -0.00503794773876263\n",
            "log(corr( )) 0.005064692785469142\n",
            "corr(p, y_val) 0.91903635372839\n",
            "log(corr( )) 0.916558588709664\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.9709891448155645\n",
            "corr(p/v1v, y_val/v1v) 0.04302298621149746\n",
            "log(corr( )) 0.012548313473332573\n",
            "corr(p, y_val) 0.8024682582686787\n",
            "log(corr( )) 0.8664031415806589\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.9710629066388905\n",
            "corr(p/v1v, y_val/v1v) 0.3732931563570557\n",
            "log(corr( )) 0.49457636408035327\n",
            "corr(p, y_val) 0.7553260746454469\n",
            "log(corr( )) 0.8300714934365069\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.9700766810030778\n",
            "corr(p/v1v, y_val/v1v) 0.32641712324353717\n",
            "log(corr( )) 0.3897055670308904\n",
            "corr(p, y_val) 0.8547659984327597\n",
            "log(corr( )) 0.8599793228232314\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.9708570165864333\n",
            "corr(p/v1v, y_val/v1v) 0.35090519695425243\n",
            "log(corr( )) 0.4308180875312797\n",
            "corr(p, y_val) 0.8437218513112318\n",
            "log(corr( )) 0.868927560767587\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.9702316467303587\n",
            "corr(p/v1v, y_val/v1v) 0.3476056339539972\n",
            "log(corr( )) 0.40300146168319867\n",
            "corr(p, y_val) 0.8829724549751797\n",
            "log(corr( )) 0.869573801757659\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.9696218916817407\n",
            "corr(p/v1v, y_val/v1v) 0.34296695648738246\n",
            "log(corr( )) 0.4172012735135186\n",
            "corr(p, y_val) 0.8652191769062506\n",
            "log(corr( )) 0.864212153043014\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.9693034622806463\n",
            "corr(p/v1v, y_val/v1v) 0.3229903714293759\n",
            "log(corr( )) 0.37375152484333124\n",
            "corr(p, y_val) 0.8957498576766778\n",
            "log(corr( )) 0.8941568730260422\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.9687496397220177\n",
            "corr(p/v1v, y_val/v1v) 0.23179836194645728\n",
            "log(corr( )) 0.38698193239636985\n",
            "corr(p, y_val) 0.7330323888868141\n",
            "log(corr( )) 0.8031700909069821\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 18:54:20,469] Trial 1 finished with value: 0.9700670724464233 and parameters: {'max_depth': 8, 'eta': 1.1197124159494646e-05, 'subsample': 0.75, 'colsample_bytree': 0.8, 'gamma': 1, 'reg_alpha': 11, 'reg_lambda': 2, 'min_child_weight': 9}. Best is trial 0 with value: 0.9547191090116363.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.9696403872904893\n",
            "corr(p/v1v, y_val/v1v) 0.2985968324258916\n",
            "log(corr( )) 0.34989649121933747\n",
            "corr(p, y_val) 0.8688194307255392\n",
            "log(corr( )) 0.8571778883935406\n",
            "mean rmspe val score over 10 splits is 0.9700670724464233\n",
            "val_avg_error: 0.9700670724464233, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.9279840108832994\n",
            "corr(p/v1v, y_val/v1v) -0.015891268895858963\n",
            "log(corr( )) -0.0009461973373641775\n",
            "corr(p, y_val) 0.9190363537283903\n",
            "log(corr( )) 0.9165585887096643\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.9257935542338465\n",
            "corr(p/v1v, y_val/v1v) 0.016234452000108195\n",
            "log(corr( )) 0.013205642949363796\n",
            "corr(p, y_val) 0.8024682582686788\n",
            "log(corr( )) 0.8664031415806587\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.9240237471731597\n",
            "corr(p/v1v, y_val/v1v) 0.016687138446818676\n",
            "log(corr( )) 0.021771919308480676\n",
            "corr(p, y_val) 0.7541043147988041\n",
            "log(corr( )) 0.8295666611834406\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.9202723685998013\n",
            "corr(p/v1v, y_val/v1v) 0.32736403722211166\n",
            "log(corr( )) 0.39225138018911915\n",
            "corr(p, y_val) 0.8530403028058123\n",
            "log(corr( )) 0.8594410614771881\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.9218206538135258\n",
            "corr(p/v1v, y_val/v1v) 0.35026644800196505\n",
            "log(corr( )) 0.4303442636525827\n",
            "corr(p, y_val) 0.841944002292902\n",
            "log(corr( )) 0.8683248576899222\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.9196712586162277\n",
            "corr(p/v1v, y_val/v1v) 0.3446063598298308\n",
            "log(corr( )) 0.3990726764344979\n",
            "corr(p, y_val) 0.8816490924757285\n",
            "log(corr( )) 0.8692517333432915\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.9177531680008907\n",
            "corr(p/v1v, y_val/v1v) 0.34062425066967555\n",
            "log(corr( )) 0.4147676740368164\n",
            "corr(p, y_val) 0.8638524283329108\n",
            "log(corr( )) 0.8638903710351541\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.9167274016465232\n",
            "corr(p/v1v, y_val/v1v) 0.31869197149243816\n",
            "log(corr( )) 0.3682972711702941\n",
            "corr(p, y_val) 0.894621542907262\n",
            "log(corr( )) 0.8939529943498066\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.9151815746548613\n",
            "corr(p/v1v, y_val/v1v) 0.23173973713394883\n",
            "log(corr( )) 0.3869358605458754\n",
            "corr(p, y_val) 0.7322388841942541\n",
            "log(corr( )) 0.8031053061884749\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 18:57:12,882] Trial 2 finished with value: 0.9206827351268616 and parameters: {'max_depth': 12, 'eta': 6.937113599688224e-05, 'subsample': 0.75, 'colsample_bytree': 0.55, 'gamma': 1, 'reg_alpha': 9, 'reg_lambda': 10, 'min_child_weight': 10}. Best is trial 2 with value: 0.9206827351268616.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.917599613646481\n",
            "corr(p/v1v, y_val/v1v) 0.29892409349966464\n",
            "log(corr( )) 0.35042425804247856\n",
            "corr(p, y_val) 0.8687206652952618\n",
            "log(corr( )) 0.8569665622993485\n",
            "mean rmspe val score over 10 splits is 0.9206827351268616\n",
            "val_avg_error: 0.9206827351268616, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.23148210195888333\n",
            "corr(p/v1v, y_val/v1v) 0.34776069821736466\n",
            "log(corr( )) 0.37602725369146617\n",
            "corr(p, y_val) 0.918254801189247\n",
            "log(corr( )) 0.9124898532506107\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.248522273963643\n",
            "corr(p/v1v, y_val/v1v) 0.3815364189094895\n",
            "log(corr( )) 0.4646065681042868\n",
            "corr(p, y_val) 0.8397782072883898\n",
            "log(corr( )) 0.8796659841822673\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.20336212793492806\n",
            "corr(p/v1v, y_val/v1v) 0.43241258835496876\n",
            "log(corr( )) 0.5508203796353927\n",
            "corr(p, y_val) 0.8131535174300889\n",
            "log(corr( )) 0.8537907929191048\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18879408230070052\n",
            "corr(p/v1v, y_val/v1v) 0.4199906579342063\n",
            "log(corr( )) 0.48350544315023763\n",
            "corr(p, y_val) 0.8756988150883288\n",
            "log(corr( )) 0.8699986761716767\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.18354261719045778\n",
            "corr(p/v1v, y_val/v1v) 0.48868569872018924\n",
            "log(corr( )) 0.5615039715715046\n",
            "corr(p, y_val) 0.8657079972950363\n",
            "log(corr( )) 0.8837563887270963\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18596689886557385\n",
            "corr(p/v1v, y_val/v1v) 0.48944316301162566\n",
            "log(corr( )) 0.5368245142898852\n",
            "corr(p, y_val) 0.897302587224196\n",
            "log(corr( )) 0.8785480468358815\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19344251048291183\n",
            "corr(p/v1v, y_val/v1v) 0.48601058125926205\n",
            "log(corr( )) 0.5449379151557172\n",
            "corr(p, y_val) 0.8860103582341774\n",
            "log(corr( )) 0.8742193761497101\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18920177018818357\n",
            "corr(p/v1v, y_val/v1v) 0.4667862543364996\n",
            "log(corr( )) 0.503825394372202\n",
            "corr(p, y_val) 0.9046539480821589\n",
            "log(corr( )) 0.9016285665543421\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20418368509129087\n",
            "corr(p/v1v, y_val/v1v) 0.29548312794200043\n",
            "log(corr( )) 0.4799475378598035\n",
            "corr(p, y_val) 0.7473496682486277\n",
            "log(corr( )) 0.8088582550612293\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 18:58:36,080] Trial 3 finished with value: 0.20107875258723729 and parameters: {'max_depth': 13, 'eta': 0.003195558019482054, 'subsample': 0.85, 'colsample_bytree': 0.7, 'gamma': 9, 'reg_alpha': 13, 'reg_lambda': 1, 'min_child_weight': 5}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.18228945789579998\n",
            "corr(p/v1v, y_val/v1v) 0.4680430414785605\n",
            "log(corr( )) 0.5069903756585178\n",
            "corr(p, y_val) 0.882006712852272\n",
            "log(corr( )) 0.8715846530139374\n",
            "mean rmspe val score over 10 splits is 0.20107875258723729\n",
            "val_avg_error: 0.20107875258723729, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.5405713852664269\n",
            "corr(p/v1v, y_val/v1v) 0.29374003856385694\n",
            "log(corr( )) 0.32251004115458604\n",
            "corr(p, y_val) 0.9159266493196292\n",
            "log(corr( )) 0.9098954227906102\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.5517705763642077\n",
            "corr(p/v1v, y_val/v1v) 0.3647076050075514\n",
            "log(corr( )) 0.4514990912970562\n",
            "corr(p, y_val) 0.8360558876384964\n",
            "log(corr( )) 0.8780088961973304\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.5299774680852949\n",
            "corr(p/v1v, y_val/v1v) 0.3933629724453333\n",
            "log(corr( )) 0.5213598776404699\n",
            "corr(p, y_val) 0.8061134255208909\n",
            "log(corr( )) 0.8489422768686828\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.5090309248200849\n",
            "corr(p/v1v, y_val/v1v) 0.36036262249627443\n",
            "log(corr( )) 0.43379090338191895\n",
            "corr(p, y_val) 0.872027530988652\n",
            "log(corr( )) 0.8654230911011587\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.5204246589692154\n",
            "corr(p/v1v, y_val/v1v) 0.4289025069773261\n",
            "log(corr( )) 0.5093735356878342\n",
            "corr(p, y_val) 0.8599917989897209\n",
            "log(corr( )) 0.8778634810019705\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.5080825220196641\n",
            "corr(p/v1v, y_val/v1v) 0.437136126751469\n",
            "log(corr( )) 0.4896965709951224\n",
            "corr(p, y_val) 0.8930261602172143\n",
            "log(corr( )) 0.8746137792586923\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.49690992701203673\n",
            "corr(p/v1v, y_val/v1v) 0.438563637147758\n",
            "log(corr( )) 0.5056238194136489\n",
            "corr(p, y_val) 0.8816749137307779\n",
            "log(corr( )) 0.870366653025123\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.4874807532477617\n",
            "corr(p/v1v, y_val/v1v) 0.4228956470482628\n",
            "log(corr( )) 0.4674999182081635\n",
            "corr(p, y_val) 0.9028855305338915\n",
            "log(corr( )) 0.8987104919869903\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.487001083768128\n",
            "corr(p/v1v, y_val/v1v) 0.286585829157595\n",
            "log(corr( )) 0.4679127576334213\n",
            "corr(p, y_val) 0.7461608685467747\n",
            "log(corr( )) 0.8078342249959294\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:01:30,982] Trial 4 finished with value: 0.5134336087154295 and parameters: {'max_depth': 7, 'eta': 0.0007848889521379181, 'subsample': 0.75, 'colsample_bytree': 0.7, 'gamma': 9, 'reg_alpha': 9, 'reg_lambda': 10, 'min_child_weight': 6}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.5030867876014755\n",
            "corr(p/v1v, y_val/v1v) 0.44506827229760176\n",
            "log(corr( )) 0.48874458105170915\n",
            "corr(p, y_val) 0.8807963231625187\n",
            "log(corr( )) 0.8686482608320211\n",
            "mean rmspe val score over 10 splits is 0.5134336087154295\n",
            "val_avg_error: 0.5134336087154295, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.46983297267806456\n",
            "corr(p/v1v, y_val/v1v) 0.3376083838676021\n",
            "log(corr( )) 0.3629083151739089\n",
            "corr(p, y_val) 0.9143876267806357\n",
            "log(corr( )) 0.9087008753304628\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.49615960903355494\n",
            "corr(p/v1v, y_val/v1v) 0.38881204054828034\n",
            "log(corr( )) 0.47392561728938515\n",
            "corr(p, y_val) 0.8406856478183503\n",
            "log(corr( )) 0.880448472321399\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.4589954747109698\n",
            "corr(p/v1v, y_val/v1v) 0.4393091625030952\n",
            "log(corr( )) 0.5564268290498461\n",
            "corr(p, y_val) 0.8153369863260691\n",
            "log(corr( )) 0.8547604275066707\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.43307132750954086\n",
            "corr(p/v1v, y_val/v1v) 0.4319974637161531\n",
            "log(corr( )) 0.49635772456629823\n",
            "corr(p, y_val) 0.8773585485815226\n",
            "log(corr( )) 0.8714007760162148\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.44539769658616934\n",
            "corr(p/v1v, y_val/v1v) 0.4919768365436501\n",
            "log(corr( )) 0.5637280690471711\n",
            "corr(p, y_val) 0.8651867402042341\n",
            "log(corr( )) 0.8840542784826856\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.4280077540078436\n",
            "corr(p/v1v, y_val/v1v) 0.48467294229792257\n",
            "log(corr( )) 0.531365953867832\n",
            "corr(p, y_val) 0.8955560298074569\n",
            "log(corr( )) 0.8782443232022448\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.41359664300144067\n",
            "corr(p/v1v, y_val/v1v) 0.48497562191781796\n",
            "log(corr( )) 0.5438704880507466\n",
            "corr(p, y_val) 0.8860212392759611\n",
            "log(corr( )) 0.8743172599704958\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.4007261660938624\n",
            "corr(p/v1v, y_val/v1v) 0.47042361928278814\n",
            "log(corr( )) 0.5077182006969886\n",
            "corr(p, y_val) 0.9057658508673077\n",
            "log(corr( )) 0.9021410736034952\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.4041075421876937\n",
            "corr(p/v1v, y_val/v1v) 0.297630655391804\n",
            "log(corr( )) 0.4831674457281472\n",
            "corr(p, y_val) 0.7496836699633043\n",
            "log(corr( )) 0.809546520706262\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:04:53,076] Trial 5 finished with value: 0.4372542962570578 and parameters: {'max_depth': 19, 'eta': 0.0010151768863163174, 'subsample': 0.95, 'colsample_bytree': 0.7, 'gamma': 3, 'reg_alpha': 6, 'reg_lambda': 11, 'min_child_weight': 9}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.42264777676143767\n",
            "corr(p/v1v, y_val/v1v) 0.4702529812409089\n",
            "log(corr( )) 0.5099693833309292\n",
            "corr(p, y_val) 0.8840440438699663\n",
            "log(corr( )) 0.8718800149142732\n",
            "mean rmspe val score over 10 splits is 0.4372542962570578\n",
            "val_avg_error: 0.4372542962570578, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.8326705127342307\n",
            "corr(p/v1v, y_val/v1v) 0.2705607449381856\n",
            "log(corr( )) 0.2972656088879209\n",
            "corr(p, y_val) 0.918145423695225\n",
            "log(corr( )) 0.9146056900929248\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.8327402642046947\n",
            "corr(p/v1v, y_val/v1v) 0.28186020133941947\n",
            "log(corr( )) 0.35579802247738895\n",
            "corr(p, y_val) 0.8178762453874515\n",
            "log(corr( )) 0.8716643226498583\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.8299476581778789\n",
            "corr(p/v1v, y_val/v1v) 0.3803836738729015\n",
            "log(corr( )) 0.5059441155474077\n",
            "corr(p, y_val) 0.7856345700516493\n",
            "log(corr( )) 0.8416294649794955\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.8222561930075667\n",
            "corr(p/v1v, y_val/v1v) 0.3417771148026643\n",
            "log(corr( )) 0.4106762214648264\n",
            "corr(p, y_val) 0.8657628370794692\n",
            "log(corr( )) 0.8631730990523458\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.8264019070527174\n",
            "corr(p/v1v, y_val/v1v) 0.38246725719962227\n",
            "log(corr( )) 0.46591827300268057\n",
            "corr(p, y_val) 0.8539255927169551\n",
            "log(corr( )) 0.8732787300485817\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.8218280172807231\n",
            "corr(p/v1v, y_val/v1v) 0.3748903681936337\n",
            "log(corr( )) 0.43255776229114906\n",
            "corr(p, y_val) 0.8893381053942381\n",
            "log(corr( )) 0.8714752560779221\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.8180986478062003\n",
            "corr(p/v1v, y_val/v1v) 0.35446342026446115\n",
            "log(corr( )) 0.43078714139375546\n",
            "corr(p, y_val) 0.8745424083629176\n",
            "log(corr( )) 0.8657426900598028\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.8161648520251362\n",
            "corr(p/v1v, y_val/v1v) 0.3163397884886139\n",
            "log(corr( )) 0.3682626416266733\n",
            "corr(p, y_val) 0.8986553838522251\n",
            "log(corr( )) 0.8940913061825273\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.8135363012535999\n",
            "corr(p/v1v, y_val/v1v) 0.24087379953169258\n",
            "log(corr( )) 0.4025758201437177\n",
            "corr(p, y_val) 0.740938279147957\n",
            "log(corr( )) 0.8037574488195607\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:07:57,666] Trial 6 finished with value: 0.8232585390531441 and parameters: {'max_depth': 20, 'eta': 0.00019142048540526035, 'subsample': 0.65, 'colsample_bytree': 0.65, 'gamma': 2, 'reg_alpha': 12, 'reg_lambda': 5, 'min_child_weight': 7}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.8189410369886925\n",
            "corr(p/v1v, y_val/v1v) 0.34627513005536154\n",
            "log(corr( )) 0.3988890152256732\n",
            "corr(p, y_val) 0.8732513285059686\n",
            "log(corr( )) 0.8599645147337178\n",
            "mean rmspe val score over 10 splits is 0.8232585390531441\n",
            "val_avg_error: 0.8232585390531441, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.8919387334698202\n",
            "corr(p/v1v, y_val/v1v) 0.007595437166941309\n",
            "log(corr( )) 0.007509489839274157\n",
            "corr(p, y_val) 0.9190363537283902\n",
            "log(corr( )) 0.9165585887096646\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.8909067290213791\n",
            "corr(p/v1v, y_val/v1v) 0.2767125123305986\n",
            "log(corr( )) 0.3472434218102845\n",
            "corr(p, y_val) 0.8045580649980599\n",
            "log(corr( )) 0.8671649980892704\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.8889069403551031\n",
            "corr(p/v1v, y_val/v1v) 0.3727097045210848\n",
            "log(corr( )) 0.4946930563347339\n",
            "corr(p, y_val) 0.7658616173832677\n",
            "log(corr( )) 0.8343204956781726\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.8836275166918354\n",
            "corr(p/v1v, y_val/v1v) 0.3306955130574997\n",
            "log(corr( )) 0.39615982748848144\n",
            "corr(p, y_val) 0.8592038828860786\n",
            "log(corr( )) 0.8613069889179942\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.8861191730561695\n",
            "corr(p/v1v, y_val/v1v) 0.3575868016663969\n",
            "log(corr( )) 0.4384885936013594\n",
            "corr(p, y_val) 0.8475770140718444\n",
            "log(corr( )) 0.8703753057389195\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.8829939103800123\n",
            "corr(p/v1v, y_val/v1v) 0.3578696809537958\n",
            "log(corr( )) 0.41362847375544404\n",
            "corr(p, y_val) 0.8851462403924708\n",
            "log(corr( )) 0.8703078974289309\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.8803825486451822\n",
            "corr(p/v1v, y_val/v1v) 0.3484001419659019\n",
            "log(corr( )) 0.42335528997715605\n",
            "corr(p, y_val) 0.8686128548833147\n",
            "log(corr( )) 0.8649722034833474\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.8790040171984952\n",
            "corr(p/v1v, y_val/v1v) 0.3174014665522466\n",
            "log(corr( )) 0.3678373411000034\n",
            "corr(p, y_val) 0.8968516369192493\n",
            "log(corr( )) 0.8942615660454849\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.8769834919113861\n",
            "corr(p/v1v, y_val/v1v) 0.2333932980936155\n",
            "log(corr( )) 0.3888837871794607\n",
            "corr(p, y_val) 0.7347569809605075\n",
            "log(corr( )) 0.803320995778757\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:10:56,891] Trial 7 finished with value: 0.8841335933094163 and parameters: {'max_depth': 14, 'eta': 0.00011296780923775426, 'subsample': 0.65, 'colsample_bytree': 0.9, 'gamma': 9, 'reg_alpha': 13, 'reg_lambda': 7, 'min_child_weight': 10}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.8804728723647793\n",
            "corr(p/v1v, y_val/v1v) 0.30624606394047066\n",
            "log(corr( )) 0.3584119176738912\n",
            "corr(p, y_val) 0.869783657838849\n",
            "log(corr( )) 0.857744146198546\n",
            "mean rmspe val score over 10 splits is 0.8841335933094163\n",
            "val_avg_error: 0.8841335933094163, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.23582166620945602\n",
            "corr(p/v1v, y_val/v1v) 0.36935071811638975\n",
            "log(corr( )) 0.392937035295892\n",
            "corr(p, y_val) 0.9131078604440392\n",
            "log(corr( )) 0.911135843653656\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.2639221074899662\n",
            "corr(p/v1v, y_val/v1v) 0.3878744624133072\n",
            "log(corr( )) 0.4743151929683754\n",
            "corr(p, y_val) 0.8454276322173107\n",
            "log(corr( )) 0.8811297721284458\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.2053773453689068\n",
            "corr(p/v1v, y_val/v1v) 0.45843774450481406\n",
            "log(corr( )) 0.5673984105939822\n",
            "corr(p, y_val) 0.8200183531468271\n",
            "log(corr( )) 0.8583334490693276\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18682642834152982\n",
            "corr(p/v1v, y_val/v1v) 0.45249260809835695\n",
            "log(corr( )) 0.5134658776040227\n",
            "corr(p, y_val) 0.8790919255077466\n",
            "log(corr( )) 0.8737831288269515\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.18402362357165739\n",
            "corr(p/v1v, y_val/v1v) 0.5196231211576052\n",
            "log(corr( )) 0.5875265978090578\n",
            "corr(p, y_val) 0.8694552947609071\n",
            "log(corr( )) 0.8880567393058698\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18416432421350687\n",
            "corr(p/v1v, y_val/v1v) 0.5063868285166483\n",
            "log(corr( )) 0.5501154903470967\n",
            "corr(p, y_val) 0.8977594330961757\n",
            "log(corr( )) 0.8807947462528238\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.1912084135468937\n",
            "corr(p/v1v, y_val/v1v) 0.49894877862344605\n",
            "log(corr( )) 0.5553400732983496\n",
            "corr(p, y_val) 0.8883686559919228\n",
            "log(corr( )) 0.8758649089632967\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.1865427521076212\n",
            "corr(p/v1v, y_val/v1v) 0.48582894256388126\n",
            "log(corr( )) 0.5211928049650398\n",
            "corr(p, y_val) 0.9065361872783032\n",
            "log(corr( )) 0.9041773319981655\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.2025624342412253\n",
            "corr(p/v1v, y_val/v1v) 0.29847459973652546\n",
            "log(corr( )) 0.48463313535560976\n",
            "corr(p, y_val) 0.7486914785074296\n",
            "log(corr( )) 0.8093237090108399\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:14:08,420] Trial 8 finished with value: 0.20211211498698622 and parameters: {'max_depth': 13, 'eta': 0.003134820615942899, 'subsample': 0.65, 'colsample_bytree': 0.7, 'gamma': 5, 'reg_alpha': 10, 'reg_lambda': 6, 'min_child_weight': 1}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.18067205477909878\n",
            "corr(p/v1v, y_val/v1v) 0.48196376781518707\n",
            "log(corr( )) 0.5199601434147595\n",
            "corr(p, y_val) 0.8866682215130077\n",
            "log(corr( )) 0.8730925361746992\n",
            "mean rmspe val score over 10 splits is 0.20211211498698622\n",
            "val_avg_error: 0.20211211498698622, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.9604974948050148\n",
            "corr(p/v1v, y_val/v1v) 0.0016756626971982781\n",
            "log(corr( )) -0.006134570887240229\n",
            "corr(p, y_val) 0.9190363537283902\n",
            "log(corr( )) 0.9165585887096646\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.9594180328697376\n",
            "corr(p/v1v, y_val/v1v) -0.028241212873667466\n",
            "log(corr( )) -0.007789548140499049\n",
            "corr(p, y_val) 0.8024682582686788\n",
            "log(corr( )) 0.8664031415806589\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.9585677730625674\n",
            "corr(p/v1v, y_val/v1v) 0.12012223063140606\n",
            "log(corr( )) 0.170602824932628\n",
            "corr(p, y_val) 0.7556977265550657\n",
            "log(corr( )) 0.829845810203056\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.9565678623899754\n",
            "corr(p/v1v, y_val/v1v) 0.10890326927023648\n",
            "log(corr( )) 0.16210874312174658\n",
            "corr(p, y_val) 0.8495876878629954\n",
            "log(corr( )) 0.858253787289887\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.957378972540095\n",
            "corr(p/v1v, y_val/v1v) 0.034540643594926514\n",
            "log(corr( )) 0.03778536538830025\n",
            "corr(p, y_val) 0.836188021348049\n",
            "log(corr( )) 0.8662223915771918\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.95630504747861\n",
            "corr(p/v1v, y_val/v1v) 0.0872806773219008\n",
            "log(corr( )) 0.10813088130021126\n",
            "corr(p, y_val) 0.8777517138839315\n",
            "log(corr( )) 0.8681919244858495\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.9552252511283725\n",
            "corr(p/v1v, y_val/v1v) 0.09671862161718575\n",
            "log(corr( )) 0.13131919113536733\n",
            "corr(p, y_val) 0.8566589444666992\n",
            "log(corr( )) 0.8619813742361618\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.9546341632113045\n",
            "corr(p/v1v, y_val/v1v) 0.08501009733292987\n",
            "log(corr( )) 0.1063692001768629\n",
            "corr(p, y_val) 0.889720625967823\n",
            "log(corr( )) 0.8925057517250159\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.9537185814379899\n",
            "corr(p/v1v, y_val/v1v) 0.20928973188202563\n",
            "log(corr( )) 0.36789849197020535\n",
            "corr(p, y_val) 0.7269685911880471\n",
            "log(corr( )) 0.8020877520536082\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:16:58,212] Trial 9 finished with value: 0.956739165035177 and parameters: {'max_depth': 7, 'eta': 2.737441027057912e-05, 'subsample': 0.75, 'colsample_bytree': 1.0, 'gamma': 10, 'reg_alpha': 5, 'reg_lambda': 10, 'min_child_weight': 1}. Best is trial 3 with value: 0.20107875258723729.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.9550784714281018\n",
            "corr(p/v1v, y_val/v1v) 0.29179901048147766\n",
            "log(corr( )) 0.3488344255303803\n",
            "corr(p, y_val) 0.8654039446606944\n",
            "log(corr( )) 0.8548950316379266\n",
            "mean rmspe val score over 10 splits is 0.956739165035177\n",
            "val_avg_error: 0.956739165035177, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.23306063722444495\n",
            "corr(p/v1v, y_val/v1v) 0.3340254709568056\n",
            "log(corr( )) 0.3494749782323929\n",
            "corr(p, y_val) 0.9021101673525973\n",
            "log(corr( )) 0.9043013462924648\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.22719721567228535\n",
            "corr(p/v1v, y_val/v1v) 0.38519115585056246\n",
            "log(corr( )) 0.4675948332953514\n",
            "corr(p, y_val) 0.8424810206234352\n",
            "log(corr( )) 0.8810905876512699\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18862727867216603\n",
            "corr(p/v1v, y_val/v1v) 0.48018976307328054\n",
            "log(corr( )) 0.5829017196740117\n",
            "corr(p, y_val) 0.8209613061790504\n",
            "log(corr( )) 0.8594038779393349\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18608034014534341\n",
            "corr(p/v1v, y_val/v1v) 0.45828880834341046\n",
            "log(corr( )) 0.5106090060085003\n",
            "corr(p, y_val) 0.8766890753482065\n",
            "log(corr( )) 0.8737651075228483\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16822153306558038\n",
            "corr(p/v1v, y_val/v1v) 0.5247662850162071\n",
            "log(corr( )) 0.5915378699602033\n",
            "corr(p, y_val) 0.8704806524884263\n",
            "log(corr( )) 0.8886298409378014\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18474704527173846\n",
            "corr(p/v1v, y_val/v1v) 0.4973106883025477\n",
            "log(corr( )) 0.5396386759992066\n",
            "corr(p, y_val) 0.8961221238726014\n",
            "log(corr( )) 0.8800686310919428\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.20076289720278304\n",
            "corr(p/v1v, y_val/v1v) 0.5008148727967754\n",
            "log(corr( )) 0.5547565215123631\n",
            "corr(p, y_val) 0.8863351300987001\n",
            "log(corr( )) 0.8757035489514938\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.20315788453506026\n",
            "corr(p/v1v, y_val/v1v) 0.4875407743771696\n",
            "log(corr( )) 0.5231894696671833\n",
            "corr(p, y_val) 0.9074815647765143\n",
            "log(corr( )) 0.9049827990759552\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.2215592776405132\n",
            "corr(p/v1v, y_val/v1v) 0.29474576004139047\n",
            "log(corr( )) 0.4756629910512135\n",
            "corr(p, y_val) 0.7417548092840308\n",
            "log(corr( )) 0.8084071367514185\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:17:17,910] Trial 10 finished with value: 0.1995673103675705 and parameters: {'max_depth': 28, 'eta': 0.0753943152953198, 'subsample': 0.85, 'colsample_bytree': 0.5, 'gamma': 7, 'reg_alpha': 14, 'reg_lambda': 14, 'min_child_weight': 4}. Best is trial 10 with value: 0.1995673103675705.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.18225899424578978\n",
            "corr(p/v1v, y_val/v1v) 0.48930560503697546\n",
            "log(corr( )) 0.5253360596509633\n",
            "corr(p, y_val) 0.8847855127986938\n",
            "log(corr( )) 0.8748289284528559\n",
            "mean rmspe val score over 10 splits is 0.1995673103675705\n",
            "val_avg_error: 0.1995673103675705, best_iteration: 37\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 1.5670859532202495\n",
            "corr(p/v1v, y_val/v1v) -0.03620783561239317\n",
            "log(corr( )) -0.0010897953802334265\n",
            "corr(p, y_val) 0.9190363537283902\n",
            "log(corr( )) 0.916558588709664\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 6.81434730345696\n",
            "corr(p/v1v, y_val/v1v) nan\n",
            "log(corr( )) -2.486175902723684e-16\n",
            "corr(p, y_val) 0.8024682582686785\n",
            "log(corr( )) 0.8664031415806588\n",
            "Fold: 3\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 3, val rmspe score is 13.466185277306309\n",
            "corr(p/v1v, y_val/v1v) 0.005610141205824561\n",
            "log(corr( )) -0.003641779794624291\n",
            "corr(p, y_val) 0.754104314798804\n",
            "log(corr( )) 0.8295666611834405\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 21.4675858490514\n",
            "corr(p/v1v, y_val/v1v) 0.03799288488422296\n",
            "log(corr( )) 0.03590706450185605\n",
            "corr(p, y_val) 0.8488965612243131\n",
            "log(corr( )) 0.858109473427496\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 25.76189298657387\n",
            "corr(p/v1v, y_val/v1v) 0.07633410341657244\n",
            "log(corr( )) nan\n",
            "corr(p, y_val) 0.8363359188216537\n",
            "log(corr( )) 0.8662771466959337\n",
            "Fold: 6\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 6, val rmspe score is 31.885216376440162\n",
            "corr(p/v1v, y_val/v1v) -0.04795892093475293\n",
            "log(corr( )) -0.0579279135718787\n",
            "corr(p, y_val) 0.8776235599047215\n",
            "log(corr( )) 0.8681410269846421\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 37.37416958497987\n",
            "corr(p/v1v, y_val/v1v) 0.008764053225182707\n",
            "log(corr( )) -0.0014723675635393558\n",
            "corr(p, y_val) 0.8564721154611634\n",
            "log(corr( )) 0.8619598374275838\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 40.475146539450655\n",
            "corr(p/v1v, y_val/v1v) 0.02540720722669871\n",
            "log(corr( )) nan\n",
            "corr(p, y_val) 0.8894627209465874\n",
            "log(corr( )) 0.8924562343747472\n",
            "Fold: 9\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 9, val rmspe score is 44.11665672870059\n",
            "corr(p/v1v, y_val/v1v) -0.045151293816417776\n",
            "log(corr( )) -0.010365055999254077\n",
            "corr(p, y_val) 0.7264404285996237\n",
            "log(corr( )) 0.8020093579316736\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/home/optimusprime/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:17:23,657] Trial 11 finished with value: 26.673911059855733 and parameters: {'max_depth': 29, 'eta': 0.16636055856301235, 'subsample': 0.85, 'colsample_bytree': 0.5, 'gamma': 7, 'reg_alpha': 14, 'reg_lambda': 14, 'min_child_weight': 4}. Best is trial 10 with value: 0.1995673103675705.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 43.810823999377234\n",
            "corr(p/v1v, y_val/v1v) -0.0010056749887523933\n",
            "log(corr( )) nan\n",
            "corr(p, y_val) 0.8640652200892429\n",
            "log(corr( )) 0.8543218325241455\n",
            "mean rmspe val score over 10 splits is 26.673911059855733\n",
            "val_avg_error: 26.673911059855733, best_iteration: 0\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22432759466790722\n",
            "corr(p/v1v, y_val/v1v) 0.3764029218650609\n",
            "log(corr( )) 0.4031462416558886\n",
            "corr(p, y_val) 0.9194834119666552\n",
            "log(corr( )) 0.9144664491819426\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.228889303111307\n",
            "corr(p/v1v, y_val/v1v) 0.388460679599354\n",
            "log(corr( )) 0.46946284149589157\n",
            "corr(p, y_val) 0.8411831281584072\n",
            "log(corr( )) 0.8805910621662745\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.1863982930761243\n",
            "corr(p/v1v, y_val/v1v) 0.45751227737628036\n",
            "log(corr( )) 0.5569130810398681\n",
            "corr(p, y_val) 0.8297452263791467\n",
            "log(corr( )) 0.8603738977836132\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.20092850250527003\n",
            "corr(p/v1v, y_val/v1v) 0.38422852809815317\n",
            "log(corr( )) 0.42918879470955\n",
            "corr(p, y_val) 0.8352844799865322\n",
            "log(corr( )) 0.8451247344659797\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.18030861259257155\n",
            "corr(p/v1v, y_val/v1v) 0.42617943831297644\n",
            "log(corr( )) 0.4909946188579071\n",
            "corr(p, y_val) 0.8354652757097034\n",
            "log(corr( )) 0.8670012444987527\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.1957456784895242\n",
            "corr(p/v1v, y_val/v1v) 0.42198508748172475\n",
            "log(corr( )) 0.4607558620236497\n",
            "corr(p, y_val) 0.8562863293412447\n",
            "log(corr( )) 0.8650945586095352\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.20176487093229337\n",
            "corr(p/v1v, y_val/v1v) 0.4443662477598241\n",
            "log(corr( )) 0.4910437079478221\n",
            "corr(p, y_val) 0.859081685877316\n",
            "log(corr( )) 0.8640549616183277\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.19983791336854276\n",
            "corr(p/v1v, y_val/v1v) 0.46074637753684317\n",
            "log(corr( )) 0.4915311775182526\n",
            "corr(p, y_val) 0.8888132366642837\n",
            "log(corr( )) 0.8965418753851427\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.2127268960392778\n",
            "corr(p/v1v, y_val/v1v) 0.2822617408199735\n",
            "log(corr( )) 0.46354425866886273\n",
            "corr(p, y_val) 0.7333482433609818\n",
            "log(corr( )) 0.7989369043007486\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:17:54,687] Trial 12 finished with value: 0.20143811152054775 and parameters: {'max_depth': 30, 'eta': 0.0410383760908011, 'subsample': 1.0, 'colsample_bytree': 0.55, 'gamma': 7, 'reg_alpha': 14, 'reg_lambda': 1, 'min_child_weight': 3}. Best is trial 10 with value: 0.1995673103675705.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.18345345042265884\n",
            "corr(p/v1v, y_val/v1v) 0.46017955059442495\n",
            "log(corr( )) 0.4964205448015291\n",
            "corr(p, y_val) 0.87938885662867\n",
            "log(corr( )) 0.8659784157393869\n",
            "mean rmspe val score over 10 splits is 0.20143811152054775\n",
            "val_avg_error: 0.20143811152054775, best_iteration: 85\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22426732441168828\n",
            "corr(p/v1v, y_val/v1v) 0.3742830667533218\n",
            "log(corr( )) 0.40147843502912994\n",
            "corr(p, y_val) 0.917868012838086\n",
            "log(corr( )) 0.914079381489116\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.22814505490270603\n",
            "corr(p/v1v, y_val/v1v) 0.3986240177684245\n",
            "log(corr( )) 0.48472940062620945\n",
            "corr(p, y_val) 0.8445849977853513\n",
            "log(corr( )) 0.8819729407891391\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18878020189176412\n",
            "corr(p/v1v, y_val/v1v) 0.46918026374372934\n",
            "log(corr( )) 0.5748036095416741\n",
            "corr(p, y_val) 0.8188697873711741\n",
            "log(corr( )) 0.8584032781345962\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18318967535344635\n",
            "corr(p/v1v, y_val/v1v) 0.4555075721522562\n",
            "log(corr( )) 0.5136991763774762\n",
            "corr(p, y_val) 0.878656327006377\n",
            "log(corr( )) 0.8738140913880127\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16862885516371373\n",
            "corr(p/v1v, y_val/v1v) 0.5268406217759519\n",
            "log(corr( )) 0.5908013564535586\n",
            "corr(p, y_val) 0.8697364538004495\n",
            "log(corr( )) 0.8888504772044774\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18404610753017492\n",
            "corr(p/v1v, y_val/v1v) 0.4947224128498474\n",
            "log(corr( )) 0.5348336545338102\n",
            "corr(p, y_val) 0.8944164376300666\n",
            "log(corr( )) 0.8792434857801877\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19346038256866746\n",
            "corr(p/v1v, y_val/v1v) 0.49827073076816214\n",
            "log(corr( )) 0.5540526863701029\n",
            "corr(p, y_val) 0.8878556935408726\n",
            "log(corr( )) 0.8758884773255655\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18897146901138995\n",
            "corr(p/v1v, y_val/v1v) 0.48338685792348296\n",
            "log(corr( )) 0.5195606635735628\n",
            "corr(p, y_val) 0.9074663035654608\n",
            "log(corr( )) 0.9036956747810042\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20488352895872552\n",
            "corr(p/v1v, y_val/v1v) 0.2973026122006312\n",
            "log(corr( )) 0.48276390089730103\n",
            "corr(p, y_val) 0.7474618123529124\n",
            "log(corr( )) 0.8090269358172499\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:19:17,198] Trial 13 finished with value: 0.19441023954780545 and parameters: {'max_depth': 23, 'eta': 0.013153169007659789, 'subsample': 0.85, 'colsample_bytree': 0.8, 'gamma': 7, 'reg_alpha': 12, 'reg_lambda': 4, 'min_child_weight': 3}. Best is trial 13 with value: 0.19441023954780545.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17972979568577804\n",
            "corr(p/v1v, y_val/v1v) 0.48249861214325457\n",
            "log(corr( )) 0.5196815791172127\n",
            "corr(p, y_val) 0.8839556401851689\n",
            "log(corr( )) 0.8732290367889739\n",
            "mean rmspe val score over 10 splits is 0.19441023954780545\n",
            "val_avg_error: 0.19441023954780545, best_iteration: 286\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22205224892822062\n",
            "corr(p/v1v, y_val/v1v) 0.3902419568227747\n",
            "log(corr( )) 0.4153298929874302\n",
            "corr(p, y_val) 0.9188294863240741\n",
            "log(corr( )) 0.9159618101761421\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23197114204097136\n",
            "corr(p/v1v, y_val/v1v) 0.38613934102649655\n",
            "log(corr( )) 0.4674624810608043\n",
            "corr(p, y_val) 0.8398754390618427\n",
            "log(corr( )) 0.8800219307125402\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18943101826000466\n",
            "corr(p/v1v, y_val/v1v) 0.4892230143514819\n",
            "log(corr( )) 0.5892664967161618\n",
            "corr(p, y_val) 0.8215165192768307\n",
            "log(corr( )) 0.8597397093252911\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18444238456722126\n",
            "corr(p/v1v, y_val/v1v) 0.4552423276607645\n",
            "log(corr( )) 0.5069836702761455\n",
            "corr(p, y_val) 0.8767723820932122\n",
            "log(corr( )) 0.8735467048563313\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16920908725073125\n",
            "corr(p/v1v, y_val/v1v) 0.5262776085199381\n",
            "log(corr( )) 0.5871651698941154\n",
            "corr(p, y_val) 0.8688697018744929\n",
            "log(corr( )) 0.8886766455009768\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18416706218810608\n",
            "corr(p/v1v, y_val/v1v) 0.5032343488762364\n",
            "log(corr( )) 0.5404509335540094\n",
            "corr(p, y_val) 0.8947330750113828\n",
            "log(corr( )) 0.8808040546711445\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19316445270625612\n",
            "corr(p/v1v, y_val/v1v) 0.5042214715076285\n",
            "log(corr( )) 0.5579316648303022\n",
            "corr(p, y_val) 0.8877293541528241\n",
            "log(corr( )) 0.8766024521374124\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18750264053955398\n",
            "corr(p/v1v, y_val/v1v) 0.5012412712809678\n",
            "log(corr( )) 0.5361152703354825\n",
            "corr(p, y_val) 0.9107392459264378\n",
            "log(corr( )) 0.9057192744712972\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20472152754118708\n",
            "corr(p/v1v, y_val/v1v) 0.3008425024492482\n",
            "log(corr( )) 0.48763836825560236\n",
            "corr(p, y_val) 0.7517734217401412\n",
            "log(corr( )) 0.8099040586626755\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:20:15,644] Trial 14 finished with value: 0.19463659266466593 and parameters: {'max_depth': 25, 'eta': 0.017599632288342115, 'subsample': 0.85, 'colsample_bytree': 0.8500000000000001, 'gamma': 7, 'reg_alpha': 7, 'reg_lambda': 4, 'min_child_weight': 3}. Best is trial 13 with value: 0.19441023954780545.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17970436262440684\n",
            "corr(p/v1v, y_val/v1v) 0.4828246965735996\n",
            "log(corr( )) 0.5190300382004236\n",
            "corr(p, y_val) 0.8831694738623677\n",
            "log(corr( )) 0.8731763648526297\n",
            "mean rmspe val score over 10 splits is 0.19463659266466593\n",
            "val_avg_error: 0.19463659266466593, best_iteration: 205\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22076545773544393\n",
            "corr(p/v1v, y_val/v1v) 0.4056860280270735\n",
            "log(corr( )) 0.43240678120402043\n",
            "corr(p, y_val) 0.920723622855343\n",
            "log(corr( )) 0.9172217090273387\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23434133104118457\n",
            "corr(p/v1v, y_val/v1v) 0.3786384667269435\n",
            "log(corr( )) 0.45283695762582293\n",
            "corr(p, y_val) 0.8363385996646232\n",
            "log(corr( )) 0.8788339383084645\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18778225284281716\n",
            "corr(p/v1v, y_val/v1v) 0.5004640026296984\n",
            "log(corr( )) 0.5953471722721044\n",
            "corr(p, y_val) 0.8274437840678908\n",
            "log(corr( )) 0.8628876186765646\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18259279432629158\n",
            "corr(p/v1v, y_val/v1v) 0.46778867273511315\n",
            "log(corr( )) 0.5205405014096594\n",
            "corr(p, y_val) 0.8788825405325013\n",
            "log(corr( )) 0.8751468329732518\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16761665940731146\n",
            "corr(p/v1v, y_val/v1v) 0.5371365598467502\n",
            "log(corr( )) 0.5966691266438375\n",
            "corr(p, y_val) 0.8712182558586102\n",
            "log(corr( )) 0.890831257929461\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18154403600844005\n",
            "corr(p/v1v, y_val/v1v) 0.5116063097031669\n",
            "log(corr( )) 0.5503179003710389\n",
            "corr(p, y_val) 0.8965417996923303\n",
            "log(corr( )) 0.8819067872444877\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19154321548197964\n",
            "corr(p/v1v, y_val/v1v) 0.5111030761916006\n",
            "log(corr( )) 0.5631676554358349\n",
            "corr(p, y_val) 0.8882888284077901\n",
            "log(corr( )) 0.8773768612651021\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18726201785688884\n",
            "corr(p/v1v, y_val/v1v) 0.5013426389782827\n",
            "log(corr( )) 0.5344732921566717\n",
            "corr(p, y_val) 0.9094956413168933\n",
            "log(corr( )) 0.9064962196923014\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20437775589365023\n",
            "corr(p/v1v, y_val/v1v) 0.29350109804280644\n",
            "log(corr( )) 0.47830923477335774\n",
            "corr(p, y_val) 0.7422891352354936\n",
            "log(corr( )) 0.8052895393332425\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:21:26,405] Trial 15 finished with value: 0.19419560585392748 and parameters: {'max_depth': 24, 'eta': 0.015469798571247579, 'subsample': 0.95, 'colsample_bytree': 0.8500000000000001, 'gamma': 6, 'reg_alpha': 7, 'reg_lambda': 4, 'min_child_weight': 2}. Best is trial 15 with value: 0.19419560585392748.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.1841305379452674\n",
            "corr(p/v1v, y_val/v1v) 0.46609114711985583\n",
            "log(corr( )) 0.49313406251934633\n",
            "corr(p, y_val) 0.8651470994537537\n",
            "log(corr( )) 0.8693749893782928\n",
            "mean rmspe val score over 10 splits is 0.19419560585392748\n",
            "val_avg_error: 0.19419560585392748, best_iteration: 234\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.2206514312377806\n",
            "corr(p/v1v, y_val/v1v) 0.44016497029097995\n",
            "log(corr( )) 0.4569385617794345\n",
            "corr(p, y_val) 0.9219103112339465\n",
            "log(corr( )) 0.9195447110057273\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23849610990208372\n",
            "corr(p/v1v, y_val/v1v) 0.38093003237394857\n",
            "log(corr( )) 0.4557671654388456\n",
            "corr(p, y_val) 0.8430991335909244\n",
            "log(corr( )) 0.879259901168675\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.1860155448059834\n",
            "corr(p/v1v, y_val/v1v) 0.5203934017617762\n",
            "log(corr( )) 0.6084874441270942\n",
            "corr(p, y_val) 0.8344243797701515\n",
            "log(corr( )) 0.8668980847875771\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18178647521100477\n",
            "corr(p/v1v, y_val/v1v) 0.47060100616501027\n",
            "log(corr( )) 0.5195613191285109\n",
            "corr(p, y_val) 0.87899013113861\n",
            "log(corr( )) 0.8758298607869724\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.17035524564580282\n",
            "corr(p/v1v, y_val/v1v) 0.4971065354461257\n",
            "log(corr( )) 0.5528655922348066\n",
            "corr(p, y_val) 0.8629362174910009\n",
            "log(corr( )) 0.8855148758360176\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18307141426680465\n",
            "corr(p/v1v, y_val/v1v) 0.4907692571666127\n",
            "log(corr( )) 0.5243154686812901\n",
            "corr(p, y_val) 0.8838231128921609\n",
            "log(corr( )) 0.8799486240980242\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.1913819489928018\n",
            "corr(p/v1v, y_val/v1v) 0.5073017480831236\n",
            "log(corr( )) 0.5524585166992209\n",
            "corr(p, y_val) 0.8802774695265174\n",
            "log(corr( )) 0.8773647285394122\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18912828694613953\n",
            "corr(p/v1v, y_val/v1v) 0.5039517756587842\n",
            "log(corr( )) 0.5243374779285117\n",
            "corr(p, y_val) 0.8955855410685414\n",
            "log(corr( )) 0.9053073856053873\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20204815274849516\n",
            "corr(p/v1v, y_val/v1v) 0.2984888953991105\n",
            "log(corr( )) 0.4876340720356111\n",
            "corr(p, y_val) 0.7449858957687387\n",
            "log(corr( )) 0.8071491318911522\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:22:34,377] Trial 16 finished with value: 0.1945170277405493 and parameters: {'max_depth': 23, 'eta': 0.0167598881390714, 'subsample': 1.0, 'colsample_bytree': 0.95, 'gamma': 4, 'reg_alpha': 4, 'reg_lambda': 3, 'min_child_weight': 2}. Best is trial 15 with value: 0.19419560585392748.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.18223566764859647\n",
            "corr(p/v1v, y_val/v1v) 0.4797130343177927\n",
            "log(corr( )) 0.5060818027158611\n",
            "corr(p, y_val) 0.8664291575648049\n",
            "log(corr( )) 0.871871171370499\n",
            "mean rmspe val score over 10 splits is 0.1945170277405493\n",
            "val_avg_error: 0.1945170277405493, best_iteration: 214\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22238954828122054\n",
            "corr(p/v1v, y_val/v1v) 0.39132951456853915\n",
            "log(corr( )) 0.4168763592884973\n",
            "corr(p, y_val) 0.9229058810339166\n",
            "log(corr( )) 0.9163184851493269\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23011976260454717\n",
            "corr(p/v1v, y_val/v1v) 0.39653314908425247\n",
            "log(corr( )) 0.4771624837079302\n",
            "corr(p, y_val) 0.8412120402397408\n",
            "log(corr( )) 0.8808166654826315\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.1881262025990481\n",
            "corr(p/v1v, y_val/v1v) 0.48758094966719157\n",
            "log(corr( )) 0.5884462847438646\n",
            "corr(p, y_val) 0.823743313511196\n",
            "log(corr( )) 0.8606535821946466\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18468451681152\n",
            "corr(p/v1v, y_val/v1v) 0.45666143885445865\n",
            "log(corr( )) 0.5080660332936148\n",
            "corr(p, y_val) 0.8775693794866619\n",
            "log(corr( )) 0.8736474561085671\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16933811727862044\n",
            "corr(p/v1v, y_val/v1v) 0.5292701886835449\n",
            "log(corr( )) 0.590359557095622\n",
            "corr(p, y_val) 0.8700880971370509\n",
            "log(corr( )) 0.8892145772680604\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18274715766734445\n",
            "corr(p/v1v, y_val/v1v) 0.5043180293695836\n",
            "log(corr( )) 0.543208937109543\n",
            "corr(p, y_val) 0.8962282190004724\n",
            "log(corr( )) 0.8809932234083644\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19183683716989164\n",
            "corr(p/v1v, y_val/v1v) 0.5083690189598329\n",
            "log(corr( )) 0.560720337023931\n",
            "corr(p, y_val) 0.8874986477525665\n",
            "log(corr( )) 0.8768283353356507\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18638645477187019\n",
            "corr(p/v1v, y_val/v1v) 0.5011202740425832\n",
            "log(corr( )) 0.5318338221679878\n",
            "corr(p, y_val) 0.9073091902995001\n",
            "log(corr( )) 0.9060321413839166\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.2022658269564406\n",
            "corr(p/v1v, y_val/v1v) 0.29963730890782586\n",
            "log(corr( )) 0.48722551916253615\n",
            "corr(p, y_val) 0.7502728739784078\n",
            "log(corr( )) 0.8097107843964342\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:23:38,006] Trial 17 finished with value: 0.19367609714896872 and parameters: {'max_depth': 24, 'eta': 0.008282553329960017, 'subsample': 0.95, 'colsample_bytree': 0.8, 'gamma': 8, 'reg_alpha': 3, 'reg_lambda': 4, 'min_child_weight': 2}. Best is trial 17 with value: 0.19367609714896872.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.178866547349184\n",
            "corr(p/v1v, y_val/v1v) 0.4863744731318216\n",
            "log(corr( )) 0.523032043352957\n",
            "corr(p, y_val) 0.8871725172027952\n",
            "log(corr( )) 0.8732259253103749\n",
            "mean rmspe val score over 10 splits is 0.19367609714896872\n",
            "val_avg_error: 0.19367609714896872, best_iteration: 450\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22018555137142282\n",
            "corr(p/v1v, y_val/v1v) 0.4203810129477594\n",
            "log(corr( )) 0.4413709280709867\n",
            "corr(p, y_val) 0.9216098609026131\n",
            "log(corr( )) 0.9184109477295105\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.2337518181082983\n",
            "corr(p/v1v, y_val/v1v) 0.4007632515328395\n",
            "log(corr( )) 0.47979271861919487\n",
            "corr(p, y_val) 0.8410107087023713\n",
            "log(corr( )) 0.8815545698829315\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.1878013239442163\n",
            "corr(p/v1v, y_val/v1v) 0.5116011477123688\n",
            "log(corr( )) 0.602111865239015\n",
            "corr(p, y_val) 0.8291871016358329\n",
            "log(corr( )) 0.8642461080964793\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.1824509141408537\n",
            "corr(p/v1v, y_val/v1v) 0.46677380331053475\n",
            "log(corr( )) 0.5185306435219856\n",
            "corr(p, y_val) 0.879191726680439\n",
            "log(corr( )) 0.8752743518575307\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16675535319832563\n",
            "corr(p/v1v, y_val/v1v) 0.5447364901033801\n",
            "log(corr( )) 0.6037918140297664\n",
            "corr(p, y_val) 0.8724921559895712\n",
            "log(corr( )) 0.8921190232749338\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.17912014626286038\n",
            "corr(p/v1v, y_val/v1v) 0.5221383916382665\n",
            "log(corr( )) 0.5623173963151952\n",
            "corr(p, y_val) 0.899146030573792\n",
            "log(corr( )) 0.8831170738747631\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.18994450565117674\n",
            "corr(p/v1v, y_val/v1v) 0.5205983595717645\n",
            "log(corr( )) 0.5703288541154062\n",
            "corr(p, y_val) 0.8888403043664804\n",
            "log(corr( )) 0.8782860680346086\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.1846333227081461\n",
            "corr(p/v1v, y_val/v1v) 0.5108860710080785\n",
            "log(corr( )) 0.5406763060120492\n",
            "corr(p, y_val) 0.9102153240681893\n",
            "log(corr( )) 0.9076161721337387\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20074661478184916\n",
            "corr(p/v1v, y_val/v1v) 0.30202096992895733\n",
            "log(corr( )) 0.4915391998055732\n",
            "corr(p, y_val) 0.7539144000105038\n",
            "log(corr( )) 0.8108369540635187\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:25:41,643] Trial 18 finished with value: 0.19225416158318057 and parameters: {'max_depth': 25, 'eta': 0.006781966321902254, 'subsample': 0.95, 'colsample_bytree': 0.8500000000000001, 'gamma': 6, 'reg_alpha': 2, 'reg_lambda': 7, 'min_child_weight': 2}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17715206566465658\n",
            "corr(p/v1v, y_val/v1v) 0.5016750244546683\n",
            "log(corr( )) 0.5369473340076928\n",
            "corr(p, y_val) 0.8871897010054456\n",
            "log(corr( )) 0.8752876648192678\n",
            "mean rmspe val score over 10 splits is 0.19225416158318057\n",
            "val_avg_error: 0.19225416158318057, best_iteration: 558\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22443382943853699\n",
            "corr(p/v1v, y_val/v1v) 0.38669632460586256\n",
            "log(corr( )) 0.4081362683841795\n",
            "corr(p, y_val) 0.9192772243573837\n",
            "log(corr( )) 0.9144800847494962\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.2366926622924518\n",
            "corr(p/v1v, y_val/v1v) 0.38597810744615924\n",
            "log(corr( )) 0.460045286976191\n",
            "corr(p, y_val) 0.8349230180193555\n",
            "log(corr( )) 0.8792170190829264\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.19053485645531573\n",
            "corr(p/v1v, y_val/v1v) 0.49219989030327144\n",
            "log(corr( )) 0.5935238119209759\n",
            "corr(p, y_val) 0.8233380220019404\n",
            "log(corr( )) 0.8605635219723166\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18302257490294738\n",
            "corr(p/v1v, y_val/v1v) 0.465321283348083\n",
            "log(corr( )) 0.5199212913827188\n",
            "corr(p, y_val) 0.8789029502557032\n",
            "log(corr( )) 0.8749833465532074\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.17034177205164056\n",
            "corr(p/v1v, y_val/v1v) 0.5290718217797111\n",
            "log(corr( )) 0.5908433771032551\n",
            "corr(p, y_val) 0.8699581296249291\n",
            "log(corr( )) 0.8892633656685355\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18138249356053776\n",
            "corr(p/v1v, y_val/v1v) 0.5104981255248426\n",
            "log(corr( )) 0.5503549331536272\n",
            "corr(p, y_val) 0.8968828547649227\n",
            "log(corr( )) 0.881787365024681\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19083873775216836\n",
            "corr(p/v1v, y_val/v1v) 0.5123596812246127\n",
            "log(corr( )) 0.5649655448645484\n",
            "corr(p, y_val) 0.8882930891932354\n",
            "log(corr( )) 0.8773680233335487\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.1850877846606853\n",
            "corr(p/v1v, y_val/v1v) 0.5052682921272333\n",
            "log(corr( )) 0.5362599392573613\n",
            "corr(p, y_val) 0.9093382765859356\n",
            "log(corr( )) 0.9066725696687808\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20224270581368572\n",
            "corr(p/v1v, y_val/v1v) 0.2993129781146767\n",
            "log(corr( )) 0.4853716666578667\n",
            "corr(p, y_val) 0.7504558683961592\n",
            "log(corr( )) 0.8100398975232683\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:28:21,092] Trial 19 finished with value: 0.1942813706313667 and parameters: {'max_depth': 26, 'eta': 0.004754532595351697, 'subsample': 0.95, 'colsample_bytree': 0.9, 'gamma': 8, 'reg_alpha': 1, 'reg_lambda': 8, 'min_child_weight': 7}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.1782362893856973\n",
            "corr(p/v1v, y_val/v1v) 0.49341586460053766\n",
            "log(corr( )) 0.529808477177166\n",
            "corr(p, y_val) 0.8872665572230548\n",
            "log(corr( )) 0.874382854841152\n",
            "mean rmspe val score over 10 splits is 0.1942813706313667\n",
            "val_avg_error: 0.1942813706313667, best_iteration: 784\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.6576207075686704\n",
            "corr(p/v1v, y_val/v1v) 0.347752079438583\n",
            "log(corr( )) 0.3748908772185329\n",
            "corr(p, y_val) 0.9184511295221315\n",
            "log(corr( )) 0.9117630741055558\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.6783885481223775\n",
            "corr(p/v1v, y_val/v1v) 0.3905969517814752\n",
            "log(corr( )) 0.47823153544432356\n",
            "corr(p, y_val) 0.845042315109747\n",
            "log(corr( )) 0.8813013900426953\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.6537952358269022\n",
            "corr(p/v1v, y_val/v1v) 0.4313504803642382\n",
            "log(corr( )) 0.550741543683118\n",
            "corr(p, y_val) 0.8132715021833835\n",
            "log(corr( )) 0.853559539277667\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.6370783954595132\n",
            "corr(p/v1v, y_val/v1v) 0.42114247823324985\n",
            "log(corr( )) 0.48717830631730297\n",
            "corr(p, y_val) 0.8763704684347366\n",
            "log(corr( )) 0.8702903360207878\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.6458259498028394\n",
            "corr(p/v1v, y_val/v1v) 0.4815831664830733\n",
            "log(corr( )) 0.5558271969234138\n",
            "corr(p, y_val) 0.8645513082775164\n",
            "log(corr( )) 0.8828975702429465\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.6341444714726758\n",
            "corr(p/v1v, y_val/v1v) 0.4732438618056368\n",
            "log(corr( )) 0.5212291355098332\n",
            "corr(p, y_val) 0.894709029472698\n",
            "log(corr( )) 0.8772574844825527\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.6244312783753554\n",
            "corr(p/v1v, y_val/v1v) 0.4733599584361479\n",
            "log(corr( )) 0.5339478649672608\n",
            "corr(p, y_val) 0.8846925556867417\n",
            "log(corr( )) 0.8732556990253303\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.6164660996535896\n",
            "corr(p/v1v, y_val/v1v) 0.46174365994707034\n",
            "log(corr( )) 0.4999120650197389\n",
            "corr(p, y_val) 0.9044844302022994\n",
            "log(corr( )) 0.9013447785176291\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.6170417573208211\n",
            "corr(p/v1v, y_val/v1v) 0.29452271023213156\n",
            "log(corr( )) 0.4786326565880663\n",
            "corr(p, y_val) 0.7484315359144528\n",
            "log(corr( )) 0.80914407292504\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:154: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
            "  fig,ax = plt.subplots(2,1,figsize=(10,6))\n",
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:31:56,315] Trial 20 finished with value: 0.6395768284213632 and parameters: {'max_depth': 21, 'eta': 0.0004954086167614811, 'subsample': 0.9, 'colsample_bytree': 0.8, 'gamma': 4, 'reg_alpha': 3, 'reg_lambda': 6, 'min_child_weight': 2}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.630975840610889\n",
            "corr(p/v1v, y_val/v1v) 0.46450287566267745\n",
            "log(corr( )) 0.5046776890670553\n",
            "corr(p, y_val) 0.8824987247087315\n",
            "log(corr( )) 0.8711917691994484\n",
            "mean rmspe val score over 10 splits is 0.6395768284213632\n",
            "val_avg_error: 0.6395768284213632, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22301567857389065\n",
            "corr(p/v1v, y_val/v1v) 0.40547054565555146\n",
            "log(corr( )) 0.4240974141657689\n",
            "corr(p, y_val) 0.9216920016927099\n",
            "log(corr( )) 0.916276115467957\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23234848438276431\n",
            "corr(p/v1v, y_val/v1v) 0.41450029295061597\n",
            "log(corr( )) 0.49729818535580056\n",
            "corr(p, y_val) 0.8462379894974751\n",
            "log(corr( )) 0.8832577843373914\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18788156639127102\n",
            "corr(p/v1v, y_val/v1v) 0.5068372587568897\n",
            "log(corr( )) 0.5991661529798905\n",
            "corr(p, y_val) 0.8278580148978102\n",
            "log(corr( )) 0.8635373190659676\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18318623647280688\n",
            "corr(p/v1v, y_val/v1v) 0.46495416969014275\n",
            "log(corr( )) 0.5140702644112697\n",
            "corr(p, y_val) 0.8783089585731154\n",
            "log(corr( )) 0.8747381682091826\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16712355657263656\n",
            "corr(p/v1v, y_val/v1v) 0.5421426907009408\n",
            "log(corr( )) 0.6011118068880905\n",
            "corr(p, y_val) 0.8723799121328947\n",
            "log(corr( )) 0.891747745802377\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18027688509115247\n",
            "corr(p/v1v, y_val/v1v) 0.5138385654432319\n",
            "log(corr( )) 0.5524432845911219\n",
            "corr(p, y_val) 0.8971854666607808\n",
            "log(corr( )) 0.8824640617466218\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.1902320677386765\n",
            "corr(p/v1v, y_val/v1v) 0.5168799840869612\n",
            "log(corr( )) 0.5675146853214932\n",
            "corr(p, y_val) 0.8886750172232685\n",
            "log(corr( )) 0.8779088373758286\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18504179188887393\n",
            "corr(p/v1v, y_val/v1v) 0.5084674718689369\n",
            "log(corr( )) 0.5381991297945445\n",
            "corr(p, y_val) 0.9087615441182523\n",
            "log(corr( )) 0.9074200091188713\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.2011022358036217\n",
            "corr(p/v1v, y_val/v1v) 0.3024044702923706\n",
            "log(corr( )) 0.4908079793411153\n",
            "corr(p, y_val) 0.7526846416076337\n",
            "log(corr( )) 0.8105433253270946\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:34:06,721] Trial 21 finished with value: 0.19282046493510951 and parameters: {'max_depth': 17, 'eta': 0.007563302901563003, 'subsample': 0.95, 'colsample_bytree': 0.8500000000000001, 'gamma': 6, 'reg_alpha': 3, 'reg_lambda': 4, 'min_child_weight': 2}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17799614643540115\n",
            "corr(p/v1v, y_val/v1v) 0.49821290606316926\n",
            "log(corr( )) 0.5318933751867211\n",
            "corr(p, y_val) 0.8839690069396139\n",
            "log(corr( )) 0.8748661509786724\n",
            "mean rmspe val score over 10 splits is 0.19282046493510951\n",
            "val_avg_error: 0.19282046493510951, best_iteration: 483\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22404737296516847\n",
            "corr(p/v1v, y_val/v1v) 0.4038519199694913\n",
            "log(corr( )) 0.4152294549492873\n",
            "corr(p, y_val) 0.9153752273241764\n",
            "log(corr( )) 0.9151374477787282\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23260829578806044\n",
            "corr(p/v1v, y_val/v1v) 0.38879264390176166\n",
            "log(corr( )) 0.46726892821638305\n",
            "corr(p, y_val) 0.8416063561366405\n",
            "log(corr( )) 0.8808259703315106\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18754317562334702\n",
            "corr(p/v1v, y_val/v1v) 0.49983881464905633\n",
            "log(corr( )) 0.5960196236058471\n",
            "corr(p, y_val) 0.827695213354939\n",
            "log(corr( )) 0.8630552412629946\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18317933212801074\n",
            "corr(p/v1v, y_val/v1v) 0.4664560512135754\n",
            "log(corr( )) 0.516113663011079\n",
            "corr(p, y_val) 0.8785441422748334\n",
            "log(corr( )) 0.8750103699555092\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16893664966574726\n",
            "corr(p/v1v, y_val/v1v) 0.5306372091665646\n",
            "log(corr( )) 0.5891143001317164\n",
            "corr(p, y_val) 0.8694828785333466\n",
            "log(corr( )) 0.8895957511323961\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.18061425734126227\n",
            "corr(p/v1v, y_val/v1v) 0.516671253967936\n",
            "log(corr( )) 0.5552520596805776\n",
            "corr(p, y_val) 0.8970884514193435\n",
            "log(corr( )) 0.8825133643379622\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.1902570120899813\n",
            "corr(p/v1v, y_val/v1v) 0.5181944507554961\n",
            "log(corr( )) 0.5681611452703799\n",
            "corr(p, y_val) 0.8876898481382178\n",
            "log(corr( )) 0.8780665759761417\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18501190881782537\n",
            "corr(p/v1v, y_val/v1v) 0.5107689264994801\n",
            "log(corr( )) 0.5392816241699927\n",
            "corr(p, y_val) 0.908333007812914\n",
            "log(corr( )) 0.9072664134377815\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.2018780445220951\n",
            "corr(p/v1v, y_val/v1v) 0.3033470371157259\n",
            "log(corr( )) 0.4909024823251547\n",
            "corr(p, y_val) 0.7492992239525176\n",
            "log(corr( )) 0.8107954102805005\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:36:09,575] Trial 22 finished with value: 0.19319446936138363 and parameters: {'max_depth': 17, 'eta': 0.008164580489768255, 'subsample': 1.0, 'colsample_bytree': 0.9, 'gamma': 6, 'reg_alpha': 3, 'reg_lambda': 6, 'min_child_weight': 1}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17786864467233843\n",
            "corr(p/v1v, y_val/v1v) 0.4969711977690729\n",
            "log(corr( )) 0.5311963110011304\n",
            "corr(p, y_val) 0.8852778302068258\n",
            "log(corr( )) 0.8747477258797748\n",
            "mean rmspe val score over 10 splits is 0.19319446936138363\n",
            "val_avg_error: 0.19319446936138363, best_iteration: 466\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.2972370333737854\n",
            "corr(p/v1v, y_val/v1v) 0.3955543438518414\n",
            "log(corr( )) 0.4155035069887059\n",
            "corr(p, y_val) 0.9190095224571381\n",
            "log(corr( )) 0.9158587987777689\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.34533186502405994\n",
            "corr(p/v1v, y_val/v1v) 0.3887537151677014\n",
            "log(corr( )) 0.4681132260033668\n",
            "corr(p, y_val) 0.8411646802762516\n",
            "log(corr( )) 0.8806052227553429\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.294185804622113\n",
            "corr(p/v1v, y_val/v1v) 0.48413555191350555\n",
            "log(corr( )) 0.5853214506851068\n",
            "corr(p, y_val) 0.8215497317259068\n",
            "log(corr( )) 0.8599809108238651\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.26213166817701106\n",
            "corr(p/v1v, y_val/v1v) 0.46445159419756143\n",
            "log(corr( )) 0.5200066197602317\n",
            "corr(p, y_val) 0.8793839490273593\n",
            "log(corr( )) 0.8749863100926543\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.2717028363513995\n",
            "corr(p/v1v, y_val/v1v) 0.5243098590115268\n",
            "log(corr( )) 0.5880727318077568\n",
            "corr(p, y_val) 0.8699883225461678\n",
            "log(corr( )) 0.8887636965876751\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.25349555308459903\n",
            "corr(p/v1v, y_val/v1v) 0.5132727509276259\n",
            "log(corr( )) 0.554887940236344\n",
            "corr(p, y_val) 0.8981307453186979\n",
            "log(corr( )) 0.8821081747491653\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.24159164917516784\n",
            "corr(p/v1v, y_val/v1v) 0.5130548433014465\n",
            "log(corr( )) 0.5639669792633195\n",
            "corr(p, y_val) 0.8869012645741275\n",
            "log(corr( )) 0.8774075011717272\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.22445637533310767\n",
            "corr(p/v1v, y_val/v1v) 0.5090343477336514\n",
            "log(corr( )) 0.5388758522277619\n",
            "corr(p, y_val) 0.9089306584437626\n",
            "log(corr( )) 0.9069932598181457\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.23840073590671537\n",
            "corr(p/v1v, y_val/v1v) 0.3021977208145085\n",
            "log(corr( )) 0.4889697228663994\n",
            "corr(p, y_val) 0.7491215533208694\n",
            "log(corr( )) 0.8106446430651503\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:39:27,278] Trial 23 finished with value: 0.2678078497318921 and parameters: {'max_depth': 16, 'eta': 0.001840603298275159, 'subsample': 1.0, 'colsample_bytree': 1.0, 'gamma': 6, 'reg_alpha': 2, 'reg_lambda': 7, 'min_child_weight': 1}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.2495449762709622\n",
            "corr(p/v1v, y_val/v1v) 0.4923764981450834\n",
            "log(corr( )) 0.5270742515504178\n",
            "corr(p, y_val) 0.8845637169913143\n",
            "log(corr( )) 0.8742295795628513\n",
            "mean rmspe val score over 10 splits is 0.2678078497318921\n",
            "val_avg_error: 0.2678078497318921, best_iteration: 999\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.2174263994516955\n",
            "corr(p/v1v, y_val/v1v) 0.44703376643447074\n",
            "log(corr( )) 0.4630062175338343\n",
            "corr(p, y_val) 0.9180132772853519\n",
            "log(corr( )) 0.920365129407528\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.2457427268931186\n",
            "corr(p/v1v, y_val/v1v) 0.3537986073035934\n",
            "log(corr( )) 0.417651132152963\n",
            "corr(p, y_val) 0.8303558317218552\n",
            "log(corr( )) 0.8725754666827377\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18620049325735613\n",
            "corr(p/v1v, y_val/v1v) 0.5217471465122956\n",
            "log(corr( )) 0.607628828544992\n",
            "corr(p, y_val) 0.8351079728312384\n",
            "log(corr( )) 0.8669004671685089\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18071948614487846\n",
            "corr(p/v1v, y_val/v1v) 0.4824229338303113\n",
            "log(corr( )) 0.5332980923381521\n",
            "corr(p, y_val) 0.8809011537336597\n",
            "log(corr( )) 0.8777727220593075\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16489280815725055\n",
            "corr(p/v1v, y_val/v1v) 0.5480326213422124\n",
            "log(corr( )) 0.6063731286937075\n",
            "corr(p, y_val) 0.8752005576361543\n",
            "log(corr( )) 0.8940575089409123\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.1770250054565237\n",
            "corr(p/v1v, y_val/v1v) 0.5316398853373943\n",
            "log(corr( )) 0.5715905098244503\n",
            "corr(p, y_val) 0.8991858095618899\n",
            "log(corr( )) 0.8852125492826156\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.19423019619071816\n",
            "corr(p/v1v, y_val/v1v) 0.5206794900595523\n",
            "log(corr( )) 0.568155758635255\n",
            "corr(p, y_val) 0.8873293523027478\n",
            "log(corr( )) 0.8781342684562734\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.19485576343655384\n",
            "corr(p/v1v, y_val/v1v) 0.5112623299556235\n",
            "log(corr( )) 0.5428899714719414\n",
            "corr(p, y_val) 0.9113166579763521\n",
            "log(corr( )) 0.9084978781968043\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20962682120362763\n",
            "corr(p/v1v, y_val/v1v) 0.3032582796236539\n",
            "log(corr( )) 0.4922345863224535\n",
            "corr(p, y_val) 0.7482587114108463\n",
            "log(corr( )) 0.8107375402652891\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:40:00,103] Trial 24 finished with value: 0.19468674991714108 and parameters: {'max_depth': 16, 'eta': 0.04200488837039658, 'subsample': 0.9, 'colsample_bytree': 0.9, 'gamma': 4, 'reg_alpha': 4, 'reg_lambda': 6, 'min_child_weight': 1}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17614779897968802\n",
            "corr(p/v1v, y_val/v1v) 0.5133012339248922\n",
            "log(corr( )) 0.5479970619685586\n",
            "corr(p, y_val) 0.8897217297415925\n",
            "log(corr( )) 0.8774315007989609\n",
            "mean rmspe val score over 10 splits is 0.19468674991714108\n",
            "val_avg_error: 0.19468674991714108, best_iteration: 91\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.22328789894694898\n",
            "corr(p/v1v, y_val/v1v) 0.4138829877814235\n",
            "log(corr( )) 0.42818522684207666\n",
            "corr(p, y_val) 0.9199016745843823\n",
            "log(corr( )) 0.9163797497378114\n",
            "Fold: 2\n",
            "Training....\n",
            "fold: 2, val rmspe score is 0.23557090088477284\n",
            "corr(p/v1v, y_val/v1v) 0.3908137821545299\n",
            "log(corr( )) 0.464799996731034\n",
            "corr(p, y_val) 0.8356272593181612\n",
            "log(corr( )) 0.8797734777294429\n",
            "Fold: 3\n",
            "Training....\n",
            "fold: 3, val rmspe score is 0.18745751611838413\n",
            "corr(p/v1v, y_val/v1v) 0.5154929851899747\n",
            "log(corr( )) 0.6063299148754018\n",
            "corr(p, y_val) 0.8316766622388498\n",
            "log(corr( )) 0.865382564038787\n",
            "Fold: 4\n",
            "Training....\n",
            "fold: 4, val rmspe score is 0.18156436382189642\n",
            "corr(p/v1v, y_val/v1v) 0.47340883763950115\n",
            "log(corr( )) 0.524271016779013\n",
            "corr(p, y_val) 0.8801377721916548\n",
            "log(corr( )) 0.8764057630486769\n",
            "Fold: 5\n",
            "Training....\n",
            "fold: 5, val rmspe score is 0.16706599889281934\n",
            "corr(p/v1v, y_val/v1v) 0.5438094684750272\n",
            "log(corr( )) 0.6023423133645891\n",
            "corr(p, y_val) 0.8722601106399657\n",
            "log(corr( )) 0.8919192839927143\n",
            "Fold: 6\n",
            "Training....\n",
            "fold: 6, val rmspe score is 0.1784888770148517\n",
            "corr(p/v1v, y_val/v1v) 0.5246894523117013\n",
            "log(corr( )) 0.564265044164471\n",
            "corr(p, y_val) 0.8987210827346847\n",
            "log(corr( )) 0.8835803357399682\n",
            "Fold: 7\n",
            "Training....\n",
            "fold: 7, val rmspe score is 0.18903779985064217\n",
            "corr(p/v1v, y_val/v1v) 0.5244840485943685\n",
            "log(corr( )) 0.5734720371213413\n",
            "corr(p, y_val) 0.889530106776536\n",
            "log(corr( )) 0.8789253870873956\n",
            "Fold: 8\n",
            "Training....\n",
            "fold: 8, val rmspe score is 0.18382143226269645\n",
            "corr(p/v1v, y_val/v1v) 0.5167653686048559\n",
            "log(corr( )) 0.5449447868953533\n",
            "corr(p, y_val) 0.9105654240566514\n",
            "log(corr( )) 0.9082295572071328\n",
            "Fold: 9\n",
            "Training....\n",
            "fold: 9, val rmspe score is 0.20037183544369158\n",
            "corr(p/v1v, y_val/v1v) 0.3040182446054891\n",
            "log(corr( )) 0.4934648636455676\n",
            "corr(p, y_val) 0.7517322893642009\n",
            "log(corr( )) 0.8114315781189705\n",
            "Fold: 10\n",
            "Training....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_100141/4016801012.py:168: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  fig.show()\n",
            "[I 2024-10-16 19:42:26,131] Trial 25 finished with value: 0.19235965244120784 and parameters: {'max_depth': 18, 'eta': 0.006231304919528031, 'subsample': 1.0, 'colsample_bytree': 0.95, 'gamma': 5, 'reg_alpha': 2, 'reg_lambda': 8, 'min_child_weight': 4}. Best is trial 18 with value: 0.19225416158318057.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold: 10, val rmspe score is 0.17692990117537488\n",
            "corr(p/v1v, y_val/v1v) 0.504390474340713\n",
            "log(corr( )) 0.5386411362199692\n",
            "corr(p, y_val) 0.8860429436506083\n",
            "log(corr( )) 0.8760346210358662\n",
            "mean rmspe val score over 10 splits is 0.19235965244120784\n",
            "val_avg_error: 0.19235965244120784, best_iteration: 609\n",
            "Fold: 1\n",
            "Training....\n",
            "fold: 1, val rmspe score is 0.29463510557362393\n",
            "corr(p/v1v, y_val/v1v) 0.3902190244766071\n",
            "log(corr( )) 0.41232649482040334\n",
            "corr(p, y_val) 0.9188353099952775\n",
            "log(corr( )) 0.9143721051560952\n",
            "Fold: 2\n",
            "Training....\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "\n",
        "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "# study_name= 'Correct_residual_autocorrrelation_HAR_n_target_lag_feat_n_target_pred'\n",
        "\n",
        "mean_val_set_rmspe_error = [ 0.226999,0.226498,0.226883,0.239605]\n",
        "\n",
        "study = optuna.create_study(study_name ='stock 31 model' ,direction=\"minimize\")\n",
        "study.optimize(objective_st31, n_trials=50) # 75\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"Best number of iteration/boosting rounds: \",study.trials[trial.number].user_attrs['best_iteration'])\n",
        "\n",
        "print(\"Trial no.: \",trial.number)\n",
        "print(\"  Value: \", trial.value)\n",
        "mean_val_set_rmspe_error.append(trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "fig = optuna.visualization.plot_parallel_coordinate(study)\n",
        "fig.show()\n",
        "\n",
        "fig = optuna.visualization.plot_optimization_history(study)\n",
        "fig.show()\n",
        "\n",
        "fig = optuna.visualization.plot_slice(study)\n",
        "fig.show()\n",
        "\n",
        "fig = optuna.visualization.plot_param_importances(study)\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c9afa016",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final best iteration:  999\n",
            "self.time_id_order [ 4294 31984 31570 ...  7107 16367 26678]\n",
            "Final model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_95412/1988183853.py:83: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "############ Best parameters Manual Start ############\n",
        "# num_rounds = 1135 #838 #study.trials[trial.number].user_attrs['best_iteration']\n",
        "# max_depth = 17\n",
        "# eta =  0.012302593098587278\n",
        "# subsample =  0.75\n",
        "# colsample_bytree =  0.8\n",
        "# gamma = 0\n",
        "# reg_alpha =  11\n",
        "# reg_lambda = 8\n",
        "# min_child_weight =  6\n",
        "############ Best parameters Manual End ############\n",
        "\n",
        "############ Best parameters Automatic Start ############\n",
        "best_trial = study.best_trial\n",
        "num_rounds = study.best_trial.user_attrs['best_iteration']\n",
        "print('final best iteration: ',num_rounds )\n",
        "seed1 = 11\n",
        "missing_value = -np.inf  # Replace with a suitable value\n",
        "max_depth = best_trial.params['max_depth']\n",
        "eta =  best_trial.params['eta']\n",
        "subsample =  best_trial.params['subsample']\n",
        "colsample_bytree =  best_trial.params['colsample_bytree']\n",
        "gamma =  best_trial.params['gamma']\n",
        "reg_alpha =  best_trial.params['reg_alpha']\n",
        "reg_lambda = best_trial.params['reg_lambda']\n",
        "min_child_weight = best_trial.params['min_child_weight']\n",
        "############ Best parameters Automatic End ############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_mlxtend_xgb_params = [max_depth,eta,subsample,colsample_bytree,gamma,reg_alpha,reg_lambda,min_child_weight,num_rounds]\n",
        "\n",
        "best_params = { 'disable_default_eval_metric': 1,\n",
        "              \"max_depth\": max_depth,\n",
        "            \"eta\": eta,\n",
        "            \"subsample\" : subsample,\n",
        "            \"colsample_bytree\":  colsample_bytree,\n",
        "            'gamma':gamma,\n",
        "            'reg_alpha': reg_alpha,\n",
        "            'reg_lambda': reg_lambda,\n",
        "            'min_child_weight': min_child_weight,\n",
        "            \"tree_method\": 'hist',\n",
        "            \"device\": \"cuda\",\n",
        "            \"seed\":seed1,\n",
        "            #'missing': missing_value\n",
        "               }\n",
        "\n",
        "t_v_t = train_validate_n_test(df_train_reordered_for_stock_31, df_test_for_stock_31)\n",
        "final_reg,train_pred,test_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,target_name = t_v_t.make_predictions(best_params,num_rounds)\n",
        "#target_name = 'target'\n",
        "\n",
        "train_pred = pd.DataFrame(train_pred).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n",
        "train_pred['time_id'] = df_train_reordered['time_id']\n",
        "train_pred['stock_id'] = df_train_reordered['stock_id']\n",
        "\n",
        "y_train = pd.DataFrame(y_train).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n",
        "y_train['time_id'] = df_train_reordered['time_id']\n",
        "y_train['stock_id'] = df_train_reordered['stock_id']\n",
        "\n",
        "v1tr = pd.DataFrame(v1tr)\n",
        "v1tr['time_id'] = df_train_reordered['time_id']\n",
        "v1tr['stock_id'] = df_train_reordered['stock_id']\n",
        "v1tr['wap1_log_price_ret_vol'] = v1tr['log_wap1_log_price_ret_vol']\n",
        "v1tr.drop(columns=['log_wap1_log_price_ret_vol'], inplace=True)\n",
        "\n",
        "test_pred = pd.DataFrame(test_pred).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n",
        "test_pred['time_id'] = df_test['time_id'].astype(int)\n",
        "test_pred['stock_id'] = df_test['stock_id'].astype(int)\n",
        "\n",
        "## # Merge the DataFrames on 'time_id' and 'stock_id' columns\n",
        "os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data')\n",
        "#os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "\n",
        "v1ts = pd.DataFrame(v1ts)\n",
        "v1ts['time_id'] = df_test['time_id'].astype(int)\n",
        "v1ts['stock_id'] = df_test['stock_id'].astype(int)\n",
        "v1ts['wap1_log_price_ret_vol'] = v1ts['log_wap1_log_price_ret_vol']\n",
        "v1ts.drop(columns=['log_wap1_log_price_ret_vol'], inplace=True)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "X_test['time_id'] = df_test['time_id']\n",
        "X_test['stock_id'] = df_test['stock_id']\n",
        "y_test_df = pd.merge(X_test[['time_id', 'stock_id']], train[['time_id', 'stock_id', 'target']], on=['time_id', 'stock_id'], how='left')\n",
        "y_test_df['time_id'] = df_test['time_id'].astype(int)\n",
        "y_test_df['stock_id'] = df_test['stock_id'].astype(int)\n",
        "w_test = y_test_df['target'] **-2 * v1ts['wap1_log_price_ret_vol']**2\n",
        "X_test.drop(columns=['time_id'], inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99d1ebf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "t_v_t.evaluate_predictions(final_reg,test_pred, y_test_df,train_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,w_test,best_mlxtend_xgb_params,mean_val_set_rmspe_error)\n",
        "\n",
        "train_pred_stock_31 = train_pred[train_pred['stock_id'] == 31].copy()\n",
        "test_pred_stock_31 = test_pred[test_pred['stock_id'] == 31].copy()\n",
        "\n",
        "del t_v_t,final_reg,train_pred,y_test_df,test_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,w_test, df_train_reordered_for_stock_31, df_test_for_stock_31,best_mlxtend_xgb_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0e4150",
      "metadata": {},
      "source": [
        "#### ------------------------- STOCK ID 31 Training and Prediction END -------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b32a683",
      "metadata": {},
      "source": [
        "#### ------------------------- All remaining STOCK IDs Training and Prediction START -------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8e78ed9d",
      "metadata": {
        "id": "8e78ed9d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def objective(trial):\n",
        "\n",
        "\n",
        "    t_v_t = train_validate_n_test(df_train_reordered, df_test)\n",
        "\n",
        "    ######  SET Hyperparameter's range for tuning ######\n",
        "    # Hyperparameters and algorithm parameters are described here\n",
        "    seed1=11\n",
        "    missing_value = -np.inf   # Replace with a suitable value\n",
        "    early_stopping_rounds = 50\n",
        "    num_round= 6000 # num_trees\n",
        "    # params = {'disable_default_eval_metric': 1,\n",
        "    #           \"max_depth\": trial.suggest_int('max_depth', 15, 50),\n",
        "    #         \"eta\": trial.suggest_float(name='eta', low=0.00001, high=0.2,log=True),\n",
        "    #         \"subsample\" : round(trial.suggest_float(name='subsample', low=0.6, high=1.0,step=0.05),1),\n",
        "    #         \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.5, high=1,step=0.05),1),\n",
        "    #         'gamma': trial.suggest_int('gamma', 1, 10),\n",
        "    #         'reg_alpha': trial.suggest_int('reg_alpha', 1, 14),\n",
        "    #         'reg_lambda': trial.suggest_int('reg_lambda', 1, 14),\n",
        "    #         'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "    #         \"tree_method\": 'hist',\n",
        "    #         \"device\": \"cuda\",\n",
        "    #         \"seed\":seed1,\n",
        "    #         #'missing': missing_value\n",
        "    #         }\n",
        "\n",
        "    params = {'disable_default_eval_metric': 1,\n",
        "              \"max_depth\": trial.suggest_int('max_depth', 25, 35),\n",
        "            \"eta\": trial.suggest_float(name='eta', low=0.0001, high=0.01,log=True),\n",
        "            \"subsample\" : round(trial.suggest_float(name='subsample', low=0.8, high=1.0,step=0.05),1),\n",
        "            \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.5, high=0.8,step=0.05),1),\n",
        "            'gamma': trial.suggest_int('gamma', 0, 5),\n",
        "            'reg_alpha': trial.suggest_int('reg_alpha', 0, 5),\n",
        "            'reg_lambda': trial.suggest_int('reg_lambda', 0, 5),\n",
        "            'min_child_weight': trial.suggest_int('min_child_weight', 2, 6),\n",
        "            \"tree_method\": 'hist',\n",
        "            \"device\": \"cuda\",\n",
        "            \"seed\":seed1,\n",
        "            #'missing': missing_value\n",
        "            }\n",
        "    ######  SET Hyperparameter's range for tuning ######\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    val_avg_error,best_iteration = t_v_t.xgb_train_validate(params,num_round,early_stopping_rounds,trial)\n",
        "    print(f\"val_avg_error: {val_avg_error}, best_iteration: {best_iteration}\")\n",
        "    trial.set_user_attr(\"best_iteration\", best_iteration)\n",
        "\n",
        "    del t_v_t\n",
        "    gc.collect()\n",
        "    return val_avg_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59525d4a",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "59525d4a",
        "outputId": "e31d2b33-9eab-4b2f-e472-3969e5bfb825"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "\n",
        "#optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "# study_name= 'Correct_residual_autocorrrelation_HAR_n_target_lag_feat_n_target_pred'\n",
        "\n",
        "mean_val_set_rmspe_error = [ 0.226999,0.226498,0.226883,0.239605]\n",
        "\n",
        "study = optuna.create_study(study_name ='reduce gaps (variance reduction) in-between target data points, log of jager transform' ,direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=120) # 75\n",
        "\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"Best number of iteration/boosting rounds: \",study.trials[trial.number].user_attrs['best_iteration'])\n",
        "\n",
        "print(\"Trial no.: \",trial.number)\n",
        "print(\"  Value: \", trial.value)\n",
        "mean_val_set_rmspe_error.append(trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "#print(\"Best hyperparameters:\", study.best_params)\n",
        "\n",
        "fig = optuna.visualization.plot_parallel_coordinate(study)\n",
        "fig.show()\n",
        "\n",
        "fig = optuna.visualization.plot_optimization_history(study)\n",
        "fig.show()\n",
        "\n",
        "fig = optuna.visualization.plot_slice(study)\n",
        "fig.show()\n",
        "\n",
        "fig = optuna.visualization.plot_param_importances(study)\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "6d620dba",
      "metadata": {
        "id": "6d620dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final best iteration:  58\n"
          ]
        }
      ],
      "source": [
        "\n",
        "############ Best parameters Manual Start ############\n",
        "# num_rounds = 1135 #838 #study.trials[trial.number].user_attrs['best_iteration']\n",
        "# max_depth = 17\n",
        "# eta =  0.012302593098587278\n",
        "# subsample =  0.75\n",
        "# colsample_bytree =  0.8\n",
        "# gamma = 0\n",
        "# reg_alpha =  11\n",
        "# reg_lambda = 8\n",
        "# min_child_weight =  6\n",
        "############ Best parameters Manual End ############\n",
        "\n",
        "############ Best parameters Automatic Start ############\n",
        "best_trial = study.best_trial\n",
        "num_rounds = study.best_trial.user_attrs['best_iteration']\n",
        "print('final best iteration: ',num_rounds )\n",
        "seed1 = 11\n",
        "missing_value = -np.inf  # Replace with a suitable value\n",
        "max_depth = best_trial.params['max_depth']\n",
        "eta =  best_trial.params['eta']\n",
        "subsample =  best_trial.params['subsample']\n",
        "colsample_bytree =  best_trial.params['colsample_bytree']\n",
        "gamma =  best_trial.params['gamma']\n",
        "reg_alpha =  best_trial.params['reg_alpha']\n",
        "reg_lambda = best_trial.params['reg_lambda']\n",
        "min_child_weight = best_trial.params['min_child_weight']\n",
        "############ Best parameters Automatic End ############\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_mlxtend_xgb_params = [max_depth,eta,subsample,colsample_bytree,gamma,reg_alpha,reg_lambda,min_child_weight,num_rounds]\n",
        "\n",
        "best_params = { 'disable_default_eval_metric': 1,\n",
        "              \"max_depth\": max_depth,\n",
        "            \"eta\": eta,\n",
        "            \"subsample\" : subsample,\n",
        "            \"colsample_bytree\":  colsample_bytree,\n",
        "            'gamma':gamma,\n",
        "            'reg_alpha': reg_alpha,\n",
        "            'reg_lambda': reg_lambda,\n",
        "            'min_child_weight': min_child_weight,\n",
        "            \"tree_method\": 'hist',\n",
        "            \"device\": \"cuda\",\n",
        "            \"seed\":seed1,\n",
        "            #'missing': missing_value\n",
        "               }\n",
        "\n",
        "t_v_t = train_validate_n_test(df_train_reordered, df_test)\n",
        "final_reg,train_pred,test_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,target_name = t_v_t.make_predictions(best_params,num_rounds)\n",
        "#target_name = 'target'\n",
        "\n",
        "train_pred = pd.DataFrame(train_pred).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n",
        "train_pred['time_id'] = df_train_reordered['time_id']\n",
        "train_pred['stock_id'] = df_train_reordered['stock_id']\n",
        "\n",
        "y_train = pd.DataFrame(y_train).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n",
        "y_train['time_id'] = df_train_reordered['time_id']\n",
        "y_train['stock_id'] = df_train_reordered['stock_id']\n",
        "\n",
        "v1tr = pd.DataFrame(v1tr)\n",
        "v1tr['time_id'] = df_train_reordered['time_id']\n",
        "v1tr['stock_id'] = df_train_reordered['stock_id']\n",
        "v1tr['wap1_log_price_ret_vol'] = v1tr['log_wap1_log_price_ret_vol']\n",
        "v1tr.drop(columns=['log_wap1_log_price_ret_vol'], inplace=True)\n",
        "\n",
        "test_pred = pd.DataFrame(test_pred).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n",
        "test_pred['time_id'] = df_test['time_id'].astype(int)\n",
        "test_pred['stock_id'] = df_test['stock_id'].astype(int)\n",
        "\n",
        "## # Merge the DataFrames on 'time_id' and 'stock_id' columns\n",
        "os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data')\n",
        "#os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n",
        "\n",
        "v1ts = pd.DataFrame(v1ts)\n",
        "v1ts['time_id'] = df_test['time_id'].astype(int)\n",
        "v1ts['stock_id'] = df_test['stock_id'].astype(int)\n",
        "v1ts['wap1_log_price_ret_vol'] = v1ts['log_wap1_log_price_ret_vol']\n",
        "v1ts.drop(columns=['log_wap1_log_price_ret_vol'], inplace=True)\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "X_test['time_id'] = df_test['time_id']\n",
        "X_test['stock_id'] = df_test['stock_id']\n",
        "y_test_df = pd.merge(X_test[['time_id', 'stock_id']], train[['time_id', 'stock_id', 'target']], on=['time_id', 'stock_id'], how='left')\n",
        "y_test_df['time_id'] = df_test['time_id'].astype(int)\n",
        "y_test_df['stock_id'] = df_test['stock_id'].astype(int)\n",
        "w_test = y_test_df['target'] **-2 * v1ts['wap1_log_price_ret_vol']**2\n",
        "X_test.drop(columns=['time_id'], inplace=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "a082387f",
      "metadata": {
        "id": "a082387f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "####################################### PREDICTION #################################################\n",
            "corr(y_pred/v1ts, y_true/v1ts) 0.35941009887008885\n",
            "log(corr( )) 0.43013790919554173\n",
            "corr(y_pred, y_true) 0.889578570144278\n",
            "log(corr( )) 0.9062178761199308\n",
            "RMSPE train score:  0.9694494421640716\n",
            "RMSPE test score:  0.9694846658083726\n",
            "\n",
            "########################################################################################################################\n",
            "\n",
            "####################################### TRAINING SET predictions START #################################################\n",
            "\n",
            "########################################################################################################################\n",
            "\n",
            "####################################### OVERALL STOCK ANALYSIS START ######################################\n",
            "\n",
            "####################################### OVERALL STOCK ANALYSIS END ######################################\n",
            "\n",
            "############################## INDIVIDUAL STOCK ANALYSIS START #################################\n",
            "\n",
            "############################## INDIVIDUAL STOCK ANALYSIS END #################################\n",
            "\n",
            "############################## OVERALL TIME ANALYSIS START #################################\n",
            "\n",
            "############################## OVERALL TIME ANALYSIS END #################################\n",
            "\n",
            "###################################### Feature importance & SHAPLEY START #########################################\n",
            "\n",
            "#---------------------------------------------compute_global_SHAP_values----------------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "#---------------------------------------------analyze_global_SHAP_values----------------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "#---------------------------------------------compute_individual_stock_SHAP_values----------------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "#---------------------------------------------compute_individual_instance_SHAP_values----------------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "###################################### Feature importance & SHAPLEY END #########################################\n",
            "\n",
            "####################################### TRAINING SET predictions END #################################################\n",
            "\n",
            "\n",
            "######################################################################################################################\n",
            "\n",
            "\n",
            "#######################################################################################################################\n",
            "\n",
            "####################################### TESTING SET predictions START #################################################\n",
            "\n",
            "#######################################################################################################################\n",
            "\n",
            "####################################### OVERALL STOCK ANALYSIS START ######################################\n",
            "\n",
            "####################################### OVERALL STOCK ANALYSIS END ######################################\n",
            "\n",
            "############################## INDIVIDUAL STOCK ANALYSIS START #################################\n",
            "\n",
            "############################## INDIVIDUAL STOCK ANALYSIS END #################################\n",
            "\n",
            "############################## OVERALL TIME ANALYSIS START #################################\n",
            "\n",
            "############################## OVERALL TIME ANALYSIS END #################################\n",
            "\n",
            "###################################### TEST Feature importance & SHAPLEY START #########################################\n",
            "\n",
            "#---------------------------------------------compute_global_SHAP_values----------------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "#---------------------------------------------analyze_global_SHAP_values----------------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "#---------------------------------------------compute_individual_stock_SHAP_values-------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "#---------------------------------------------compute_individual_instance_SHAP_values-------------------------------------------------------#\n",
            "\n",
            "#-----------------------------------------------------------------------------------------------------------------#\n",
            "\n",
            "###################################### TEST Feature importance & SHAPLEY END #########################################\n",
            "##################################################################################################\n",
            "\n",
            "####################################### TESTING SET predictions END #################################################\n",
            "\n",
            "\n",
            "####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON START #################################################\n",
            "##################################################################################################\n",
            "\n",
            "####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON END #################################################\n",
            "\n",
            "\n",
            "####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON START #################################################\n",
            "##################################################################################################\n",
            "\n",
            "####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON END #################################################\n",
            "\n",
            "##################################################################################################\n",
            "\n",
            "################## time_id RMSPE COMPARISON between groundtruth and prediction on train set START ########################\n",
            "\n",
            "################## time_id RMSPE COMPARISON between groundtruth and prediction on train set END ########################\n",
            "\n",
            "################## time_id RMSPE COMPARISON between groundtruth and prediction on test set START ##########################\n",
            "\n",
            "\n",
            "################## time_id RMSPE COMPARISON between groundtruth and prediction on test set END ##########################\n",
            "\n",
            "##################################################################################################\n",
            "\n",
            "####################################### Representativeness of validation set in test set START #################################################\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "####################################### Representativeness of validation set in test set END #################################################\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'train_shap_values_all' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_shap_values_all, test_shap_values_all \u001b[38;5;241m=\u001b[39m \u001b[43mt_v_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv1tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv1ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mw_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbest_mlxtend_xgb_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmean_val_set_rmspe_error\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[51], line 2109\u001b[0m, in \u001b[0;36mtrain_validate_n_test.evaluate_predictions\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m y_true, y_pred, test_residuals, train_residuals, unique_stock_ids, all_stock_v1ts_df, train_avg_target_rvol, test_avg_target_rvol\n\u001b[1;32m   2108\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m-> 2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_shap_values_all\u001b[49m, test_shap_values_all\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_shap_values_all' is not defined"
          ]
        }
      ],
      "source": [
        "t_v_t.evaluate_predictions(final_reg,test_pred, y_test_df,train_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,w_test,best_mlxtend_xgb_params,mean_val_set_rmspe_error)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eHR-7FXYlRUx",
      "metadata": {
        "id": "eHR-7FXYlRUx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 2344753,
          "sourceId": 27233,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "optiver_linux",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 15331.990222,
      "end_time": "2024-09-16T06:04:13.265071",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-09-16T01:48:41.274849",
      "version": "2.5.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
