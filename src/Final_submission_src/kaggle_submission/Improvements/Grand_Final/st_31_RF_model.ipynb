{"cells":[{"cell_type":"code","execution_count":1,"id":"0913de49","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11867,"status":"ok","timestamp":1728510593987,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"0913de49","outputId":"a92c3193-f425-406f-c0e9-155954d28015"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","# import os\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol')\n","\n","# gpu_info = !nvidia-smi\n","# gpu_info = '\\n'.join(gpu_info)\n","# if gpu_info.find('failed') >= 0:\n","#   print('Not connected to a GPU')\n","# else:\n","#   print(gpu_info)\n","\n","# from psutil import virtual_memory\n","# ram_gb = virtual_memory().total / 1e9\n","# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","# if ram_gb < 20:\n","#   print('Not using a high-RAM runtime')\n","# else:\n","#   print('You are using a high-RAM runtime!')\n","\n","\n","# !pip install plotly_express\n","# !pip install numba\n","# !pip install optuna\n","# !pip install shap"]},{"cell_type":"code","execution_count":null,"id":"a9f9fc51","metadata":{},"outputs":[],"source":["import cuml\n","from cuml.ensemble import RandomForestRegressor as cuRF\n","from cuml.preprocessing import OneHotEncoder as cuOneHotEncoder\n","import cudf\n","import numpy as np\n","import gc\n","import matplotlib.pyplot as plt\n","import os\n","import pickle\n","from scipy.stats import spearmanr\n","from sklearn.preprocessing import MinMaxScaler\n","from cuml.preprocessing import LabelEncoder\n","import pandas as pd\n","from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","import optuna\n","from optuna.trial import TrialState\n","from joblib import Parallel, delayed\n","from sklearn.preprocessing import OneHotEncoder\n","import plotly_express as px\n","import plotly.subplots as sub_plots\n","import plotly.graph_objects as go\n","import shap\n","import scipy.stats as stats\n","import statsmodels.api as sm\n","import cupy as cp\n","\n","import warnings\n","\n","# Suppress specific warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"cuml.internals.api_decorators\")\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"To use pickling first train using float32 data to fit the estimator\")\n"]},{"cell_type":"code","execution_count":3,"id":"3c4fec69","metadata":{"id":"3c4fec69"},"outputs":[],"source":["# import os\n","# import glob\n","# import pandas as pd\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","# import pandas as pd\n","# import numpy as np\n","# import glob\n","# import os\n","# import matplotlib.pyplot as plt\n","# import statsmodels.api as sm\n","# import plotly.subplots as sub_plots\n","# import plotly.graph_objects as go\n","# import statsmodels.api as sm\n","# import scipy.stats as stats\n","\n","\n","# from sklearn.cluster import KMeans\n","# import re\n","\n","# import warnings\n","# #warnings.filterwarnings(\"ignore\")\n","# from sklearn.metrics import confusion_matrix\n","# #from sklearn.metrics import plot_confusion_matrix\n","# from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","# from sklearn.utils import class_weight\n","# import optuna\n","# from optuna.trial import TrialState\n","\n","\n","# from mlxtend.evaluate import bias_variance_decomp\n","# from sklearn.ensemble import RandomForestRegressor\n","# from sklearn.compose import ColumnTransformer\n","# from sklearn.pipeline import Pipeline\n","\n","# import glob\n","# import pandas as pd\n","# import numpy as np\n","# import glob\n","# import os\n","# from numba import jit, njit\n","# import numba as nb\n","# import plotly_express as px\n","# from itertools import combinations, permutations, product, combinations_with_replacement\n","# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","# from scipy.signal import find_peaks\n","# import pickle\n","# from joblib import Parallel, delayed\n","# import seaborn as sns\n","# from sklearn import model_selection\n","# from sklearn.metrics import r2_score\n","# import gc\n","# from sklearn.decomposition import PCA\n","# from sklearn.cluster import KMeans, AgglomerativeClustering\n","# from sklearn.mixture import GaussianMixture\n","# import scipy as sp\n","# from sklearn.metrics import silhouette_samples, silhouette_score\n","# from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","# from sklearn.metrics import silhouette_samples, silhouette_score\n","# from sklearn.cluster import SpectralClustering, MiniBatchKMeans, MeanShift, AgglomerativeClustering\n","# from sklearn.mixture import GaussianMixture\n","# from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","# from scipy.spatial.distance import squareform\n","# from scipy.stats import skew, kurtosis\n","# import shap\n","# from datetime import datetime\n","# import ipywidgets as widgets\n","# from matplotlib.patches import Rectangle\n","\n","# from sklearn.preprocessing import OneHotEncoder\n","\n","# from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n","# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","# from statsmodels.genmod.generalized_linear_model import GLM\n","# import warnings\n","# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","# from sklearn.utils import class_weight\n","# import optuna\n","# from optuna.trial import TrialState\n","# from mlxtend.evaluate import bias_variance_decomp\n","# import re\n","\n","# from matplotlib.pyplot import cm\n","\n","\n","# from sklearn.manifold import TSNE\n","# from sklearn.preprocessing import minmax_scale\n","\n","# from scipy.stats import spearmanr"]},{"cell_type":"markdown","id":"4ab61745","metadata":{"id":"4ab61745"},"source":["## **Model Training**"]},{"cell_type":"code","execution_count":4,"id":"Oj0U2qtRrYr3","metadata":{"id":"Oj0U2qtRrYr3"},"outputs":[],"source":["os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/partial_train_n_full_test')\n","#os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n","\n","with open('train_feat_df_reordered.pkl','rb') as f:\n","  train_feat_df_reordered = pickle.load(f)\n","\n","with open('test_feat_df.pkl','rb') as f:\n","  test_feat_df = pickle.load(f)\n","\n","#os.chdir('/content/drive/MyDrive/optiver_real_vol/kaggle/input/optiver-realized-volatility-prediction')\n","\n","##### remove test from training data #####\n","\n","df_train_reordered = train_feat_df_reordered.copy()\n","del train_feat_df_reordered\n","\n","df_test = test_feat_df.copy()\n","del test_feat_df\n","\n","## keep same data types\n","df_test['tlog_eps523_trade_price_n_wap1_dev'] = df_test['tlog_eps523_trade_price_n_wap1_dev'].astype('float64')\n","df_test['tlog_eps523_trade_price_n_wap_eqi_price0_dev'] = df_test['tlog_eps523_trade_price_n_wap_eqi_price0_dev'].astype('float64')\n","\n","for c in df_train_reordered.columns:\n","    if 'float' in c:\n","        df_train_reordered[c] = df_train_reordered[c].astype('float32')\n","\n","for c in df_test.columns:\n","    if 'float' in c:\n","        df_test[c] = df_test[c].astype('float32')"]},{"cell_type":"code","execution_count":5,"id":"8cd53f74","metadata":{},"outputs":[],"source":["\n","# ####################### Improvement 1 #######################\n","# ## # drop the bottom 25 xgb features\n","# bottom_25_xgb_feat = ['tlog_tlog1p_eps1e4_range_bid_price1', 'bp_as_corr2', 'tlog_eps1e4_trade_price_std', 'v1liq2projt20', 'max_price1',\n","#                      'max_bid_price2', 'max_ask_price2', 'bs_ap_corr2', 'min_price1', 'ask_lvl2_min_lvl1_size_feat', 'tlog_1p_trade_order_count_std',\n","#                      'min_ask_price1', 'tlog_1p_trade_order_count_mean', 'min_bid_price2', 'lvl2_minus_lvl1_bid_n_ask_size_feat', 'bs_ap_corr1',\n","#                      'liqt20rf29', 'as_ap_corr1', 'bp_as_corr1', 'as_ap_corr2', 'bs_bp_corr1', 'max_bid_price1', 'min_bid_price1', 'max_ask_price1']\n","# df_train_reordered.drop(columns=bottom_25_xgb_feat, inplace=True)\n","# df_test = df_test.drop(columns=bottom_25_xgb_feat, inplace=False)\n","\n","\n","# ####################### Improvement 2 #######################\n","# ## # drop these clusterings as they are least important clustering type\n","# cluster_drop_cols = ['tlog_tlog1p_target_vol_sum_stats_4_clusters',\n","# 'tlog_tlog1p_target_vol_sum_stats_10_clusters',\n","# 'tlog_tlog1p_target_vol_sum_stats_16_clusters',\n","# 'tlog_tlog1p_target_vol_sum_stats_30_clusters',\n","# 'sum_stats_4_clusters_labels',\n","# 'sum_stats_10_clusters_labels',\n","# 'sum_stats_16_clusters_labels',\n","# 'sum_stats_30_clusters_labels']\n","# df_train_reordered.drop(columns=cluster_drop_cols, inplace=True)\n","# df_test = df_test.drop(columns=cluster_drop_cols, inplace=False)\n","\n","\n","# ####################### Improvement 3 #######################\n","# ## # Add shap. interaction terms\n","# interaction_terms_list = [\n","# ('tlog_tlinear_sad_ask_size2', 'vol1_mean'),\n","# ('tlog_tlinear_sad_ask_size2', 'log_wap1_log_price_ret_vol'),\n","# ('tlog_tlinear_sad_size1', 'tlog_eps523_trade_price_n_wap_eqi_price0_dev'),\n","# ('tlog_tlinear_sad_size1', 'tlog_eps523_trade_price_n_wap1_dev'),\n","# ('tlog_eps523_trade_price_n_wap_eqi_price0_dev','log_wap1_log_price_ret_vol'),\n","# ('tlog_eps523_trade_price_n_wap_eqi_price0_dev', 'tlog_first_10_min_vol'),\n","# ('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20',\n","#  'log_wap1_log_price_ret_vol'),\n","# ('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20',\n","#  'tlog_first_10_min_vol'),\n","# ('tvpl2_rmed2v1', 'tlog_first_10_min_vol'),\n","# ('tvpl2_rmed2v1', 'log_wap1_log_price_ret_vol'),\n","# ('wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0',\n","#  'log_wap1_log_price_ret_vol'),\n","# ('wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0',\n","#  'tlog_first_10_min_vol'),\n","# ('wap1_log_price_ret_volstock_mean_from_25', 'log_wap1_log_price_ret_vol'),\n","# ('wap1_log_price_ret_volstock_mean_from_25', 'tlog_first_10_min_vol'),\n","# ('v1spprojt15f25_q1', 'log_wap1_log_price_ret_vol'),\n","# ('v1spprojt15f25_q1', 'tlog_first_10_min_vol'),\n","# ('soft_stock_mean_tvpl2_:20', 'wap1_log_price_ret_volstock_mean_from_25'),\n","# ('soft_stock_mean_tvpl2_:20', 'wap1_log_price_ret_volstock_mean_from_20'),\n","# ('tlog_target_vol_pcorr_3_clusters', 'soft_stock_mean_tvpl2_:20'),\n","# ('v1proj_29_15_q3', 'log_wap1_log_price_ret_vol'),\n","# ('v1proj_29_15_q3', 'tlog_first_10_min_vol'),\n","# ('root_trade_count_smean', 'tlog_first_10_min_vol'),\n","# ('root_trade_count_smean', 'log_wap1_log_price_ret_vol'),\n","# ('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n","#  'log_wap1_log_price_ret_vol'),\n","# ('tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n","#  'tlog_first_10_min_vol'),\n","# ('wap1_log_price_ret_volstock_mean_from_20', 'log_wap1_log_price_ret_vol'),\n","# ('wap1_log_price_ret_volstock_mean_from_20', 'tlog_first_10_min_vol')]\n","\n","\n","# ## # create and add interaction terms to train and test set\n","# for pair in interaction_terms_list:\n","#     df_train_reordered[f'{pair[0]}_XXX_{pair[1]}'] = df_train_reordered[pair[0]] * df_train_reordered[pair[1]]\n","#     df_test[f'{pair[0]}_XXX_{pair[1]}'] = df_test[pair[0]] * df_test[pair[1]]"]},{"cell_type":"code","execution_count":null,"id":"6fa44f84","metadata":{},"outputs":[],"source":["####################### find stocks' target similar to stock 31 target in terms of MSE and Spearman correlation #######################\n","os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data')\n","\n","full_train = pd.read_csv('train.csv')\n","full_train_31 = full_train[full_train['stock_id']==31]\n","\n","distances_dict = {}\n","\n","for stock in full_train['stock_id'].unique()[full_train['stock_id'].unique() != 31]:\n","    full_train_stock = full_train[full_train['stock_id']==stock]\n","    merged_df = pd.merge(full_train_31, full_train_stock, on='time_id', suffixes=('_31', f'_{stock}')).dropna()\n","    MSE = np.mean((merged_df['target_31'] - merged_df[f'target_{stock}'])**2)\n","    spearman_corr = 1 - spearmanr(merged_df['target_31'], merged_df[f'target_{stock}'])[0] # 1 - spearman correlation to get distance\n","    distances_dict[stock] = (MSE, spearman_corr)\n","\n","\n","mse_values = [value[0] for value in distances_dict.values()]\n","spearman_values = [value[1] for value in distances_dict.values()]\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","# Reshape the data to fit the scaler\n","mse_values = np.array(mse_values).reshape(-1, 1)\n","spearman_values = np.array(spearman_values).reshape(-1, 1)\n","# Fit and transform the data\n","mse_values = scaler.fit_transform(mse_values).flatten()\n","spearman_values = scaler.fit_transform(spearman_values).flatten()\n","\n","# Replace distances_dict values with standardized mse_values and spearman_values\n","for i, stock_id in enumerate(distances_dict.keys()):\n","    distances_dict[stock_id] = (mse_values[i], spearman_values[i])\n","\n","stock_ids = list(distances_dict.keys())\n","plt.figure(figsize=(20, 10))\n","plt.scatter(mse_values, spearman_values)\n","for i, stock_id in enumerate(stock_ids):\n","    plt.annotate(stock_id, (mse_values[i], spearman_values[i]))\n","plt.xlabel('Min-max Normalized Mean Squared Error')\n","plt.ylabel('Min-max Normalized (1 - Spearman Correlation)')\n","plt.title('Stock Distance from Stock 31')\n","plt.grid(True)\n","plt.show()\n","\n","\n","# Calculate the magnitude (Euclidean distance) for each stock\n","magnitudes = {stock: np.sqrt(mse**2 + spearman**2) for stock, (mse, spearman) in distances_dict.items()}\n","# Sort the stocks by magnitude\n","sorted_stocks = sorted(magnitudes.items(), key=lambda item: item[1])\n","# Display the sorted stocks\n","sorted_stocks[:10]"]},{"cell_type":"code","execution_count":7,"id":"655ae9e8","metadata":{},"outputs":[],"source":["####################### Improvement 4 #######################\n","## # create seperate training and testing dataframes for stock id 31 using the 5 most similar stocks to stock 31\n","####################### copy these similar stocks to a different dataframe #######################\n","\n","similar_stocks = [] #[stock for stock, _ in sorted_stocks[:5]]  # 5 most similar stocks to stock 31\n","similar_stocks.append(31)  # add stock 31 to the list\n","similar_stocks = sorted(similar_stocks) # sort the list\n","\n","df_train_reordered_for_stock_31 = df_train_reordered.copy()\n","df_test_for_stock_31 = df_test.copy()\n","\n","df_train_reordered_for_stock_31 = df_train_reordered_for_stock_31[df_train_reordered_for_stock_31['stock_id'].isin(similar_stocks)]\n","df_test_for_stock_31 = df_test_for_stock_31[df_test_for_stock_31['stock_id'].isin(similar_stocks)]"]},{"cell_type":"code","execution_count":8,"id":"a166a498","metadata":{},"outputs":[],"source":["####################### Improvement 5 #######################\n","## # drop stock id 31 from training set. \n","# df_train_reordered.drop(index=df_train_reordered[df_train_reordered['stock_id']==31].index, inplace=True)\n","# df_test.drop(index=df_test[df_test['stock_id']==31].index, inplace=True)\n","# os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/partial_train_n_full_test')\n","del df_train_reordered, df_test"]},{"cell_type":"code","execution_count":null,"id":"c23fe7a7","metadata":{},"outputs":[],"source":["## check transformed target distribution\n","target_trans = (df_train_reordered_for_stock_31['target']/np.exp(df_train_reordered_for_stock_31['log_wap1_log_price_ret_vol']))\n","target_trans = np.log(target_trans) + 2\n","target_trans.plot(kind='hist', bins=1000, figsize=(5, 5))\n","plt.show()\n","print('skew',target_trans.skew())\n","print('kurt', target_trans.kurtosis())\n"]},{"cell_type":"code","execution_count":10,"id":"b29976a7","metadata":{"id":"b29976a7"},"outputs":[],"source":["\n","class train_validate_n_test(object):\n","\n","    def __init__(self,df_train_reordered, df_test) -> None:\n","\n","        #self.time_id_order = df.loc[:3829,'time_id'].values # select ordered unique time_ids\n","        #self.train_time_id_ind = int(len(self.time_id_order)*0.7)\n","\n","        largest_num_time_id_stock_idx = df_train_reordered.groupby('stock_id')['time_id'].apply(lambda x: x.nunique()).argmax()\n","        largest_num_time_id_stock = df_train_reordered['stock_id'].unique()[largest_num_time_id_stock_idx]\n","        self.time_id_order = df_train_reordered[df_train_reordered['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n","        self.n_folds = 10\n","        folds = TimeSeriesSplit(n_splits=self.n_folds,)# max_train_size=None, gap=10)\n","        #self.splits = folds.split( range( self.train_time_id_ind ) ) # split 70% train time_ids into n_fold splits\n","        nunique_train_time_ids = df_train_reordered['time_id'].nunique()\n","        self.splits = folds.split( range( nunique_train_time_ids ) )\n","\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        #self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        #self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","        self.train_stock_id = df_train_reordered['stock_id']\n","        self.train_time_id = df_train_reordered['time_id']\n","\n","        # test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        # self.test_df = df[df['time_id'].isin(test_time_ids)]\n","        self.test_time_id = df_test[df_test['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n","        self.test_df = df_test.dropna()\n","        self.test_stock_id = self.test_df['stock_id']\n","        self.test_time_id = self.test_df['time_id']\n","\n","        #self.df = df\n","        self.df_train_reordered = df_train_reordered.dropna()\n","\n","        # feature_importances = pd.DataFrame()\n","        cols = list(df_train_reordered.columns)\n","        cols.remove('tlog_target')\n","        cols.remove('target')\n","        cols.remove('time_id')\n","        self.feat_cols_list =  cols #cat_feat_labels+float32_feat_labels+float64_feat_labels # int32_feat_labels+int64_feat_labels+float32_feat_labels+float64_feat_labels\n","        # feature_importances['feature'] = self.feat_cols_list\n","\n","        self.target_name = 'target' # _standardized' log target is easier to transform back than log_target_standardized\n","        self.target_shift = 2\n","\n","        #del df\n","        del df_train_reordered,df_test\n","        gc.collect()\n","\n","    # def onehotencode_cat_var(self,full_set):\n","    #     full_set = cat_feat_labels #full_set.astype({\"stn_id\":str,\"block_id\":str,\"ts_of_day\":str,\"hr_of_day\":str,\"day_of_wk\":str,\"day_of_mn\":str,\"wk_of_mon\":str })\n","    #     full_set = pd.get_dummies(full_set, prefix_sep=\"_\",columns =cat_feat_labels,drop_first=True)\n","    #     #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n","    #     return full_set\n","\n","    #### RMSPE cost function\n","    def rmspe(self,y_true, y_pred):\n","        return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","\n","\n","    # Custom RMSPE objective function\n","    def rmspe_objective(self,preds, dtrain):\n","        labels = dtrain.get_label()\n","        errors = (preds - labels) / labels\n","        gradient = 2 * errors / (1 + errors**2)\n","        hessian = 2 * (1 - errors**2) / (1 + errors**2)**2\n","        return gradient, hessian\n","\n","\n","    def RF_RMSPE(self,preds, train_data):\n","        labels = train_data.get_label()\n","        return 'RMSPE', round(self.rmspe(y_true = labels, y_pred = preds),5)\n","\n","\n","    def nancorr(self,a, b):\n","        v = np.isfinite(a)*np.isfinite(b) > 0\n","        return np.corrcoef(a[v], b[v])[0,1]\n","\n","\n","    def RF_train_validate(self, params, trial):\n","        rmspe_val_score = []\n","\n","        for fold_n, (train_index, valid_index) in enumerate(self.splits):\n","            #print('Fold:', fold_n + 1)\n","            train_time_ids = self.time_id_order[train_index]\n","            val_time_ids = self.time_id_order[valid_index]\n","\n","            # Convert to cuDF for GPU-accelerated processing\n","            train_df = cudf.DataFrame(self.df_train_reordered[self.df_train_reordered['time_id'].isin(train_time_ids)])\n","            val_df = cudf.DataFrame(self.df_train_reordered[self.df_train_reordered['time_id'].isin(val_time_ids)])\n","\n","            X_train = train_df[self.feat_cols_list]\n","            y_train = train_df[self.target_name]\n","            X_valid = val_df[self.feat_cols_list]\n","            y_val = val_df[self.target_name]\n","\n","            X_train['stock_id'] = X_train['stock_id'].astype('category')\n","\n","            v1tr = np.exp(X_train['log_wap1_log_price_ret_vol'])\n","            v1v = np.exp(X_valid['log_wap1_log_price_ret_vol'])\n","\n","            # Calculate weights and adjust the target\n","            w_train = y_train ** -2 * v1tr ** 2\n","            #weighted_y_train = (y_train / w_train).fillna(0)  # Adjust target values based on weights\n","\n","            # Identify numeric and categorical features\n","            numeric_features = X_train.select_dtypes(include=['float64', 'int64', 'float32']).columns.tolist()\n","            categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n","\n","            # Encode categorical features using the same encoders\n","            encoded_categorical_dfs_train = []\n","            encoded_categorical_dfs_valid = []\n","            \n","            for cat_feature in categorical_features:\n","                encoder = LabelEncoder()\n","                encoded_feature_train = encoder.fit_transform(X_train[cat_feature])\n","                encoded_feature_valid = encoder.transform(X_valid[cat_feature])  # Use the same encoder\n","\n","                encoded_categorical_dfs_train.append(encoded_feature_train)\n","                encoded_categorical_dfs_valid.append(encoded_feature_valid)\n","\n","            # Concatenate numeric features and encoded categorical features for training\n","            transformed_X_train = cudf.concat([X_train[numeric_features]] + encoded_categorical_dfs_train, axis=1)\n","            transformed_X_train.columns = numeric_features + [f\"{cat_feature}_encoded\" for cat_feature in categorical_features]\n","\n","            # Concatenate numeric features and encoded categorical features for validation\n","            transformed_X_valid = cudf.concat([X_valid[numeric_features]] + encoded_categorical_dfs_valid, axis=1)\n","            transformed_X_valid.columns = numeric_features + [f\"{cat_feature}_encoded\" for cat_feature in categorical_features]\n","\n","            # Initialize and train the Random Forest Regressor\n","            model = cuRF(\n","                n_estimators=params['n_estimators'],\n","                max_depth=params['max_depth'],\n","                min_samples_split=params['min_samples_split'],\n","                min_samples_leaf=params['min_samples_leaf'],\n","                max_features=params['max_features'],\n","                max_leaves=params['max_leaves'],\n","                random_state=42,\n","            )\n","\n","            # Fit the model\n","            model.fit(transformed_X_train, (np.log(y_train / v1tr) + self.target_shift))\n","\n","            # Make predictions\n","            y_val_pred = np.exp(model.predict(transformed_X_valid) - self.target_shift) * v1v\n","\n","            # Calculate RMSPE for validation set\n","            rmspe_val = self.rmspe(y_val.to_pandas(), y_val_pred.to_pandas())  # convert back to pandas for compatibility\n","            rmspe_val_score.append(rmspe_val)\n","\n","            #print(f'Fold: {fold_n + 1}, Validation RMSPE: {rmspe_val}')\n","\n","        mean_rmspe_val_score = np.mean(rmspe_val_score)\n","        print(f'RMSPE validation score over {self.n_folds} splits is {rmspe_val_score}')\n","        print(f'Mean RMSPE validation score over {self.n_folds} splits is {mean_rmspe_val_score}')\n","\n","        # # Plot the validation RMSPE scores\n","        # plt.figure(figsize=(10, 6))\n","        # plt.plot(range(1, len(rmspe_val_score) + 1), rmspe_val_score, marker='o', label='Validation RMSPE')\n","        # plt.title(f'Validation RMSPE Scores Across Folds (Trial: {trial.number})')\n","        # plt.xlabel('Fold Number')\n","        # plt.ylabel('RMSPE')\n","        # plt.xticks(range(1, len(rmspe_val_score) + 1))  # Set x-ticks to correspond to fold numbers\n","        # plt.grid(True)\n","        # plt.legend()\n","        # plt.show()\n","\n","        del self.df_train_reordered, X_train, transformed_X_train, X_valid, y_train, y_val, train_df, val_df, v1tr, v1v\n","        gc.collect()\n","\n","        return mean_rmspe_val_score\n","\n","\n","\n","    def make_predictions(self, best_params):\n","\n","        # Full training dataframe in cuDF for GPU processing\n","        full_train_df = cudf.DataFrame(self.df_train_reordered)\n","\n","        X_train = full_train_df[self.feat_cols_list]\n","        y_train = full_train_df[self.target_name]  # target\n","        X_test = cudf.DataFrame(self.test_df[self.feat_cols_list])  # Convert test set to cuDF\n","\n","        X_train['stock_id'] = X_train['stock_id'].astype('category')\n","\n","        # Double exponential to nullify log\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol'])  \n","        v1ts = np.exp(X_test['log_wap1_log_price_ret_vol'])  \n","\n","        w_train = y_train ** -2 * v1tr ** 2\n","        \n","        # Identify numeric and categorical features\n","        numeric_features = X_train.select_dtypes(include=['float64', 'int64', 'float32']).columns.tolist()\n","        categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n","\n","        # Encode categorical features using the same encoders\n","        encoded_categorical_train = []\n","        encoded_categorical_test = []\n","\n","        for cat_feature in categorical_features:\n","            encoder = LabelEncoder()\n","            encoded_feature_train = encoder.fit_transform(X_train[cat_feature])\n","            encoded_feature_test = encoder.transform(X_test[cat_feature])  # Use the same encoder\n","\n","            encoded_categorical_train.append(encoded_feature_train)\n","            encoded_categorical_test.append(encoded_feature_test)\n","\n","        # Concatenate numeric features and encoded categorical features for training\n","        transformed_X_train = cudf.concat([X_train[numeric_features]] + encoded_categorical_train, axis=1)\n","        transformed_X_train.columns = numeric_features + [f\"{cat_feature}_encoded\" for cat_feature in categorical_features]\n","\n","        # Concatenate numeric features and encoded categorical features for test\n","        transformed_X_test = cudf.concat([X_test[numeric_features]] + encoded_categorical_test, axis=1)\n","        transformed_X_test.columns = numeric_features + [f\"{cat_feature}_encoded\" for cat_feature in categorical_features]\n","\n","        print('Final model')\n","        \n","        # Initialize and train the Random Forest Regressor\n","        model = cuRF(\n","            n_estimators=best_params['n_estimators'],\n","            max_depth=best_params['max_depth'],\n","            min_samples_split=best_params['min_samples_split'],\n","            min_samples_leaf=best_params['min_samples_leaf'],\n","            max_features=best_params['max_features'],\n","            max_leaves=best_params['max_leaves'],\n","            random_state=42,\n","        )\n","\n","        # Train the model directly (no weights applied)\n","        raw_train_gn = np.log(y_train / v1tr) + self.target_shift\n","        cuml_model = model.fit(transformed_X_train, raw_train_gn)\n","        \n","        # Make predictions\n","        raw_train_pred = model.predict(transformed_X_train)\n","        raw_test_pred = model.predict(transformed_X_test)\n","        train_pred = np.exp(raw_train_pred - self.target_shift) * v1tr\n","        test_pred = np.exp(raw_test_pred - self.target_shift) * v1ts\n","\n","        del full_train_df  # Clean up\n","        gc.collect()\n","\n","        return cuml_model, train_pred, test_pred, y_train.to_pandas(), X_train.to_pandas(), X_test.to_pandas(), v1tr, v1ts,w_train, self.target_name, raw_train_pred, raw_test_pred,raw_train_gn,self.target_shift\n","\n","\n","\n","    def local_manual_shapley_additivity_check(self,model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name):\n","\n","        y_train_true = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","                #### ONLY for Explainer\n","        shap_pred = ( shap_values.base_values + shap_values.values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","                #### ONLY for TreeExplainer\n","        #shap_pred = ( model_base_value + shap_values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","        #print('shap_values.sum(axis=1)',shap_values.sum(axis=1))\n","        # print('shap_values.base_values',shap_values.base_values[0])\n","        # print('shap_values.values',shap_values.values[0].sum())\n","        #print('len(shap_values.values.sum(axis=1))',len(shap_values.values.sum(axis=1)))\n","\n","        model_shap_rmspe = self.rmspe(model_pred, shap_pred)\n","\n","        fig, ax = plt.subplots(2,1,figsize=(30,10))\n","        ax[0].plot(np.arange(0,len(y_train_true)),y_train_true,label='true rvol.',linestyle='dashed',c='g',marker='*',alpha=0.2)\n","        ax[0].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.6)\n","        ax[0].set_title(f'True Rvol. Vs. model predicted Rvol.' )\n","        ax[0].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[0].set_ylabel('rvol.')\n","        ax[0].legend()\n","        ax[0].grid(True)\n","\n","        ax[1].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.4)\n","        ax[1].plot(np.arange(0,len(shap_pred)),shap_pred,label='summed shap values prediction',linestyle='dashed',c='r',marker='*',alpha=0.4)\n","        ax[1].set_title(f'Check additivity of shap values, RMSPE:{model_shap_rmspe} between model and shap values prediction' )\n","        ax[1].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[1].set_ylabel('rvol.')\n","        ax[1].legend()\n","        ax[1].grid(True)\n","        fig.tight_layout()\n","        fig.show()\n","\n","\n","        del all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_PDP_n_Scatter(self,feature_name,shap_values,groundtruth,prediction,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n","        ####### compute partial dependence plot of most important features\n","\n","        ###### Partial dependence plot\n","        #fig,ax = plt.subplots()\n","        #shap.plots.partial_dependence(feature_name, model.predict, RF.DMatrix(X_train,enable_categorical=True), model_expected_value=True, feature_expected_value=True)\n","        #fig.show()\n","\n","        ##### scatter plot\n","        print(f'\\n scatter plot of {feature_name} vs. shap values')\n","        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","\n","        fig, ax = plt.subplots(2, 2, figsize=(20, 20))\n","        inds = shap.utils.potential_interactions(shap_values[:, feature_name], shap_values)\n","        shap.plots.scatter(shap_values[:, feature_name], color=shap_values[:, inds[0]], title=f'scatter plot of {feature_name} Vs. Shap values on {set_name} set', ax=ax[0, 0])\n","        shap.plots.scatter(shap_values[:, feature_name], color=shap_values[:, inds[1]], ax=ax[0, 1])\n","        shap.plots.scatter(shap_values[:, feature_name], color=shap_values[:, inds[2]], ax=ax[1, 0])\n","        plt.show()\n","\n","        error = groundtruth['target'] - prediction['target']\n","        ##### scatter plot of feature shap values vs. error\n","        fig, ax = plt.subplots(figsize=(5, 5))\n","        print(f'\\n scatter plot of {feature_name} shap values vs. error')\n","        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        ax.scatter(error.values,shap_values[:, feature_name].values, alpha=0.4 )\n","        ax.set_xlabel('error')\n","        ax.set_ylabel(f'{feature_name} shap values')\n","        ax.grid()\n","        ax.set_title(f'scatter plot of {feature_name} shap values Vs. error on {set_name} set')\n","        plt.tight_layout()\n","        plt.show()\n","\n","\n","        ##### scatter plot of feature vs. True target rvol. on trianing set\n","        # fig,ax = plt.subplots()\n","        # yval = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # xval = X[feature_name]\n","        # ax.scatter(xval,yval)\n","        # ax.plot([min(xval), max(xval)], [min(yval),max(yval)], color = 'red', linewidth = 1)\n","        # ax.set_xlabel(feature_name)\n","        # ax.set_ylabel('True target rvol.')\n","        # ax.grid\n","        # ax.set_title(f'scatter plot of {feature_name} Vs. True Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        # fig.show()\n","\n","\n","        ##### scatter plot of feature vs. predicted target rvol.on trianing set\n","        # fig,ax = plt.subplots()\n","        # yval1 = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # ax.scatter(xval,yval1)\n","        # ax.plot([min(xval), max(xval)], [min(yval1),max(yval1)], color = 'red', linewidth = 1)\n","        # ax.set_xlabel(feature_name)\n","        # ax.set_ylabel('Predicted target rvol.')\n","        # ax.grid\n","        # ax.set_title(f'scatter plot of {feature_name} Vs. Predicted Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        # fig.show()\n","\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","\n","    def compute_shapley_beeswarm(self,shap_values,top_n_feat,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # shap.summary_plot(shap_values)\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        ax.set_title(f'Beeswarm plot for {set_name} set showing top {top_n_feat} features')\n","        shap.plots.beeswarm(shap_values, max_display=top_n_feat)\n","        #ax.set_title(f' stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        plt.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","\n","    def compute_shapley_barplot(self,shap_values,top_n_feats,X,Y,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # plt.bar(shap_values.abs().sum(axis=1))\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        ax.set_title(f'MEAN ABSOLUTE of feature bar plot on {set_name} set showing top {top_n_feats} features')\n","        #clustering = shap.utils.hclust(X,Y)\n","        shap.plots.bar(shap_values, max_display=top_n_feats)#,clustering=clustering,clustering_cutoff=0.9)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.utils.hclust(X,Y)\n","        ax.set_title(f'MAXIMUM ABSOLUTE of feature bar plot on {set_name} set showing top {top_n_feats} features')\n","        shap.plots.bar(shap_values.abs.max(0), max_display=top_n_feats)#,clustering=clustering,clustering_cutoff=0.9)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values, X,Y,stock_id,view_time_ids_start,view_time_ids_end,set_name,fig,ax,top_n_feats\n","        gc.collect()\n","        return\n","\n","\n","    def compute_individual_stock_SHAP_values(self,final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        stock_id = stock_id\n","        view_time_ids_start = view_time_ids_start\n","        view_time_ids_end = view_time_ids_end\n","        X = X_train[X_train['stock_id'].isin([stock_id])].iloc[view_time_ids_start:view_time_ids_end]\n","\n","        ###### Explainer #######\n","        #explainer = shap.Explainer(final_reg,X)\n","        explainer = shap.TreeExplainer(final_reg, feature_perturbation=\"tree_path_dependent\")\n","        shap_values = explainer(np.array(X),check_additivity=False)\n","        shap_values.feature_names = final_reg.feature_names\n","\n","        ###### TreeExplainer #######\n","        # explainer = shap.TreeExplainer(final_reg,feature_perturbation='interventional')\n","        # shap_values = explainer.shap_values(np.array(X),check_additivity=False)\n","        # shap_values.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","        # print(f'Model base value: {model_base_value} before scaling by v1tr')\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.local_manual_shapley_additivity_check(model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name )\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values.base_values = shap_values.base_values * all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values.values = np.multiply(shap_values.values.T ,stock_v1tr_df).T\n","        ###### check correctness of shap_values\n","        # sp = shap_values.base_values + shap_values.values.sum(axis=1)\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(len(sp)),sp)\n","        # model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # plt.plot(range(len(sp)), model_pred )\n","        # plt.show()\n","\n","        self.compute_shapley_beeswarm(X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        #shap_values = final_reg.predict(dtrain, pred_contribs=True)\n","        ### Calculate SHAP values for a specific instance (e.g., the first test instance)\n","        ### shap_values = explainer.shap_values(X_test.iloc[0])\n","\n","        self.compute_shapley_barplot(shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        self.compute_shapley_heatmap(shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df)\n","\n","        ####### INDIVIDUAL feature contributions ##############################\n","        ####### compute partial dependence plot of most important features\n","        self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n","\n","        #self.compute_shapley_decision(model_base_value,shap_values.data,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        ##### force plot has some error\n","        #self.compute_shapley_force(model_base_value,shap_values.data,X,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","\n","        del final_reg,X_train\n","        gc.collect()\n","        return\n","\n","\n","\n","    def identiy_largest_overall_n_under_n_over_prediction_errors(self,groundtruth,prediction,set_name):\n","\n","        # overall error\n","        oveall_error = groundtruth['target'] - prediction['target']\n","        # overall squared percentage error\n","        ovearll_SPE = pd.DataFrame()\n","        ovearll_SPE['target_spe'] = (oveall_error / groundtruth['target'] )**2\n","        ovearll_SPE['stock_id'] = groundtruth['stock_id']\n","        ovearll_SPE['time_id'] = groundtruth['time_id']\n","\n","        # sort the overall_SPE by descending order\n","        top_n_instances = 50\n","        sorted_ovearll_SPE = ovearll_SPE.sort_values('target_spe',ascending=False)[:top_n_instances].reset_index(drop=True)\n","        # Plot the sorted_ovearll_SPE as a bar plot with dual x-axis for stock_id and time_id\n","        fig, ax1 = plt.subplots(figsize=(15, 8))\n","        # Plotting the bar graph for 'target_spe'\n","        ax1.bar(sorted_ovearll_SPE.index, sorted_ovearll_SPE['target_spe'], color='b', alpha=0.6)\n","        ax1.set_xlabel('Instance Index')\n","        ax1.set_ylabel('Target Squared Percentage Error', color='b')\n","        ax1.tick_params(axis='y', labelcolor='b')\n","        # Creating a twin Axes sharing the x-axis\n","        ax2 = ax1.twiny()\n","        # Setting the x-ticks and labels for stock_id and time_id\n","        ax2.set_xticks(sorted_ovearll_SPE.index)\n","        ax2.set_xticklabels(sorted_ovearll_SPE['stock_id'], rotation=90, ha='center')\n","        ax2.set_xlabel('Stock ID')\n","        # Creating another twin Axes sharing the x-axis\n","        ax3 = ax1.twiny()\n","        # Offset the twin axis below the original x-axis\n","        ax3.spines['top'].set_position(('outward', 40))\n","        ax3.set_xticks(sorted_ovearll_SPE.index)\n","        ax3.set_xticklabels(sorted_ovearll_SPE['time_id'], rotation=90, ha='center')\n","        ax3.set_xlabel('Time ID')\n","        plt.title(f'{top_n_instances} Largest Target SPE instances with Stock ID and Time ID on ' + set_name + ' Set')\n","        plt.show()\n","\n","        overall_RMSPE = np.sqrt(np.mean(ovearll_SPE['target_spe']))\n","        corrected_overall_SPE = ovearll_SPE.sort_values('target_spe',ascending=False)[top_n_instances:].reset_index(drop=True)\n","        corrected_overall_RMSPE = np.sqrt(np.mean(corrected_overall_SPE['target_spe']))\n","        print(f'\\nOverall RMSPE: {overall_RMSPE} on {set_name} set')\n","        print(f'Corrected RMSPE: {corrected_overall_RMSPE} on {set_name} set')\n","        print(f'Percentage improvment in RMSPE of ovearall error on {set_name} set after correcting the top {top_n_instances} instances:')\n","        print(f'{(overall_RMSPE - corrected_overall_RMSPE )/overall_RMSPE*100}%')\n","\n","\n","        # underprediction squared percentage error\n","        up_SPE = ovearll_SPE[oveall_error > 0].reset_index(drop=True)\n","        # sort the up_SPE by descending order\n","        sorted_up_SPE = up_SPE.sort_values('target_spe',ascending=False)[:top_n_instances].reset_index(drop=True)\n","        # Plot the sorted_up_SPE as a bar plot with dual x-axis for stock_id and time_id\n","        fig, ax1 = plt.subplots(figsize=(15, 8))\n","        # Plotting the bar graph for 'target_spe'\n","        ax1.bar(sorted_up_SPE.index, sorted_up_SPE['target_spe'], color='b', alpha=0.6)\n","        ax1.set_xlabel('Instance Index')\n","        ax1.set_ylabel('Target Squared Percentage Error', color='b')\n","        ax1.tick_params(axis='y', labelcolor='b')\n","        # Creating a twin Axes sharing the x-axis\n","        ax2 = ax1.twiny()\n","        # Setting the x-ticks and labels for stock_id and time_id\n","        ax2.set_xticks(sorted_up_SPE.index)\n","        ax2.set_xticklabels(sorted_up_SPE['stock_id'], rotation=90, ha='center')\n","        ax2.set_xlabel('Stock ID')\n","        # Creating another twin Axes sharing the x-axis\n","        ax3 = ax1.twiny()\n","        # Offset the twin axis below the original x-axis\n","        ax3.spines['top'].set_position(('outward', 40))\n","        ax3.set_xticks(sorted_up_SPE.index)\n","        ax3.set_xticklabels(sorted_up_SPE['time_id'], rotation=90, ha='center')\n","        ax3.set_xlabel('Time ID')\n","        plt.title(f'{top_n_instances} Largest target SPE Underprediction instances with Stock ID and Time ID on ' + set_name + ' Set')\n","        plt.show()\n","\n","        up_RMSPE = np.sqrt(np.mean(up_SPE['target_spe']))\n","        corrected_up_SPE = up_SPE.sort_values('target_spe',ascending=False)[top_n_instances:].reset_index(drop=True)\n","        corrected_up_RMSPE = np.sqrt(np.mean(corrected_up_SPE['target_spe']))\n","        print(f'\\nUnderprediction RMSPE: {up_RMSPE} on {set_name} set')\n","        print(f'Corrected Underprediction RMSPE: {corrected_up_RMSPE} on {set_name} set')\n","        print(f'Percentage improvment in RMSPE of underprediction error on {set_name} set after correcting the top {top_n_instances} instances:')\n","        print(f'{(up_RMSPE - corrected_up_RMSPE )/up_RMSPE*100}%')\n","\n","\n","        # overprediction squared percentage error\n","        op_SPE = ovearll_SPE[oveall_error < 0].reset_index(drop=True)\n","        # sort the op_SPE by descending order\n","        sorted_op_SPE = op_SPE.sort_values('target_spe',ascending=False)[:top_n_instances].reset_index(drop=True)\n","        # Plot the sorted_op_SPE as a bar plot with dual x-axis for stock_id and time_id\n","        fig, ax1 = plt.subplots(figsize=(15, 8))\n","        # Plotting the bar graph for 'target_spe'\n","        ax1.bar(sorted_op_SPE.index, sorted_op_SPE['target_spe'], color='b', alpha=0.6)\n","        ax1.set_xlabel('Instance Index')\n","        ax1.set_ylabel('Target Squared Percentage Error', color='b')\n","        ax1.tick_params(axis='y', labelcolor='b')\n","        # Creating a twin Axes sharing the x-axis\n","        ax2 = ax1.twiny()\n","        # Setting the x-ticks and labels for stock_id and time_id\n","        ax2.set_xticks(sorted_op_SPE.index)\n","        ax2.set_xticklabels(sorted_op_SPE['stock_id'], rotation=90, ha='center')\n","        ax2.set_xlabel('Stock ID')\n","        # Creating another twin Axes sharing the x-axis\n","        ax3 = ax1.twiny()\n","        # Offset the twin axis below the original x-axis\n","        ax3.spines['top'].set_position(('outward', 40))\n","        ax3.set_xticks(sorted_op_SPE.index)\n","        ax3.set_xticklabels(sorted_op_SPE['time_id'], rotation=90, ha='center')\n","        ax3.set_xlabel('Time ID')\n","        plt.title(f'{top_n_instances} Largest target SPE Overprediction instances with Stock ID and Time ID on ' + set_name + ' Set')\n","        plt.show()\n","\n","        op_RMSPE = np.sqrt(np.mean(op_SPE['target_spe']))\n","        corrected_op_SPE = op_SPE.sort_values('target_spe',ascending=False)[top_n_instances:].reset_index(drop=True)\n","        corrected_op_RMSPE = np.sqrt(np.mean(corrected_op_SPE['target_spe']))\n","        print(f'\\nOverprediction RMSPE: {op_RMSPE} on {set_name} set')\n","        print(f'Corrected Overprediction RMSPE: {corrected_op_RMSPE} on {set_name} set')\n","        print(f'Percentage improvment in RMSPE of overprediction error on {set_name} set after correcting the top {top_n_instances} instances:')\n","        print(f'{(op_RMSPE - corrected_op_RMSPE )/op_RMSPE*100}%')\n","\n","\n","        ovearll_error_idxs = []\n","        up_error_idxs = []\n","        op_error_idxs = []\n","        for i in range(top_n_instances):\n","            ovearll_error_idxs.append(groundtruth[(groundtruth['stock_id'] == sorted_ovearll_SPE['stock_id'][i]) & (groundtruth['time_id'] == sorted_ovearll_SPE['time_id'][i])].index[0])\n","            up_error_idxs.append(groundtruth[(groundtruth['stock_id'] == sorted_up_SPE['stock_id'][i]) & (groundtruth['time_id'] == sorted_up_SPE['time_id'][i])].index[0])\n","            op_error_idxs.append(groundtruth[(groundtruth['stock_id'] == sorted_op_SPE['stock_id'][i]) & (groundtruth['time_id'] == sorted_op_SPE['time_id'][i])].index[0])\n","\n","        return ovearll_error_idxs, up_error_idxs, op_error_idxs\n","\n","\n","\n","    def shapley_analysis_of_large_error_instances(self,ovearll_error_idxs, up_error_idxs, op_error_idxs,shap_values_all,set_name):\n","\n","        print('shap_values_all.feature_names',len(shap_values_all.feature_names))\n","\n","        # decision plot of the largest overall error instances\n","        display_n_features = 40\n","        fig, ax = plt.subplots(figsize=(50,10))\n","        shap.decision_plot(np.mean(shap_values_all.base_values[ovearll_error_idxs]), shap_values_all.values[ovearll_error_idxs,:], \\\n","                           feature_names=shap_values_all.feature_names, feature_order='importance', feature_display_range= slice(-1,-display_n_features,-1),\\\n","                           title=f'Decision plot of the {display_n_features} largest overall error instances on {set_name} set \\n (ignore the WRONG expected value shown)')\n","        plt.show()\n","\n","        # decision plot of the largest overprediction error instances\n","        fig, ax = plt.subplots(figsize=(50,10))\n","        shap.decision_plot(np.mean(shap_values_all.base_values[op_error_idxs]), shap_values_all.values[op_error_idxs,:], \\\n","                            feature_names=shap_values_all.feature_names, feature_order='importance', feature_display_range= slice(-1,-30,-1),\\\n","                            title=f'Decision plot of the {display_n_features} largest overprediction error instances on {set_name} set \\n (ignore the WRONG expected value shown)')\n","        plt.show()\n","\n","        # decision plot of the largest underprediction error instances\n","        fig, ax = plt.subplots(figsize=(50,10))\n","        shap.decision_plot(np.mean(shap_values_all.base_values[up_error_idxs]), shap_values_all.values[up_error_idxs,:], \\\n","                            feature_names=shap_values_all.feature_names, feature_order='importance', feature_display_range= slice(-1,-30,-1),\\\n","                            title=f'Decision plot of the {display_n_features} largest underprediction error instances on {set_name} set \\n (ignore the WRONG expected value shown)')\n","        plt.show()\n","\n","        del shap_values_all,ovearll_error_idxs, up_error_idxs, op_error_idxs,set_name\n","        return\n","\n","\n","\n","    def compute_global_SHAP_values(self,final_reg,X_train,y_train,train_pred,v1tr,set_name):\n","\n","        print(f'\\nGround-Truth Rvol. grand average on {set_name} set: {y_train[\"target\"].values.mean()}')\n","        print(f'\\nModel Prediction Rvol. grand average on {set_name} set: {train_pred[\"target\"].values.mean()}')\n","\n","        # final_reg.set_param({\"device\": \"cuda\"})\n","        # shap.initjs()\n","        # X = X_train\n","        # ###### Explainer #######\n","        # #explainer = shap.Explainer(final_reg,X)\n","        # explainer = shap.TreeExplainer(final_reg, feature_perturbation=\"tree_path_dependent\")\n","        # shap_values_all = explainer(np.array(X),check_additivity=False)\n","        # shap_values_all.feature_names = final_reg.feature_names\n","        # model_base_value = explainer.expected_value\n","\n","\n","        ###### cuML Explainer #######\n","        shap.initjs()\n","        X = X_train\n","        explainer = cuml.explainer.TreeExplainer(model=final_reg)\n","        shap_values_all = cp.asnumpy(explainer.shap_values(X))\n","        model_base_value = cp.asnumpy(explainer.expected_value)\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.global_manual_shapley_additivity_check(train_pred,v1tr,shap_values_all,model_base_value,set_name)\n","\n","        # ####### Manually correct the shap values to accomodate v1tr scaling\n","        # shap_values_all.base_values = shap_values_all.base_values * v1tr['wap1_log_price_ret_vol'].values  # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        # shap_values_all.values = np.multiply(shap_values_all.values.T ,v1tr['wap1_log_price_ret_vol'].values).T\n","\n","        ###### Beeswarm plot\n","        #### ONLY for Explainer\n","        print(f'\\n Global Beeswarm plot for all stock ids and time ids')\n","        top_n_feat = 25\n","        self.compute_shapley_beeswarm(shap_values_all,top_n_feat,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name=set_name)\n","\n","        ###### Bar plot MEAN Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        ###### Bar plot MAXIMUM Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        del final_reg,X_train,y_train,train_pred,v1tr,X\n","        gc.collect()\n","        return shap_values_all\n","\n","\n","    def global_manual_shapley_additivity_check(self,train_pred,v1tr,shap_values_all,model_base_value, set_name):\n","        #### ONLY for Explainer\n","        # shap_pred_all = ( shap_values_all.base_values + shap_values_all.values.sum(axis=1) ) * v1tr['wap1_log_price_ret_vol'] #pd.DataFrame(all_stock_v1tr_df.values.ravel() , columns=['v1tr_all'])['v1tr_all'].values\n","        # model_shap_rmspe_all = self.rmspe(train_pred['target'], shap_pred_all)\n","\n","        shap_pred_all = np.sum(shap_values_all,axis=1) + model_base_value\n","        model_shap_rmspe_all = self.rmspe(train_pred['target'], shap_pred_all)\n","\n","        # # line plot of model prediction vs. shap values prediction\n","        # fig, ax = plt.subplots(figsize=(30,10))\n","        # ax.plot(np.arange(0,len(train_pred['target'])),train_pred['target'],label='prediction',linestyle='dashed',c='g',marker='*',alpha=0.2)\n","        # ax.plot(np.arange(0,len(shap_pred_all)),shap_pred_all,label='shap_value',linestyle='dashed',c='b',marker='*',alpha=0.6)\n","        # ax.set_title(f'Model prediction Vs. Shap_values for additivity check on {set_name} set' )\n","        # ax.set_ylabel('rvol.')\n","        # ax.legend()\n","        # ax.grid(True)\n","        # plt.show()\n","        # plt.close()\n","\n","        # scatter plot of model prediction vs. shap values prediction\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(train_pred['target'],shap_pred_all, alpha=0.4)\n","        ax.plot([min(train_pred['target']), max(train_pred['target'])], [min(shap_pred_all),max(shap_pred_all)], color = 'red', linewidth = 1)\n","        ax.set_xlabel('model prediction')\n","        ax.set_ylabel('shap prediction')\n","        ax.grid\n","        ax.set_title(f'scatter plot of model prediction Vs. shap prediction on {set_name} set')\n","        plt.show()\n","        plt.close()\n","\n","        print(f'\\n Check Additivity of shap values in all stock and time ids, model_shap_rmspe_all: {model_shap_rmspe_all} on {set_name} set')\n","        del train_pred,v1tr,shap_values_all\n","        gc.collect()\n","        return\n","\n","\n","\n","    def analyze_global_SHAP_values(self,shap_values_all,groundtruth, prediction,set_name):\n","\n","        ###################################### All Error vs. feature Shapley values Analysis #######################################\n","        ## filter out features that have high negative and positively shapley values and see if they are correlated with the error\n","\n","        top_largest_shap_val_feat = 100\n","        error = groundtruth['target'] - prediction['target']\n","        # select only the features that have high shapley values over all instances/samples\n","        abs_summed_shap = np.abs(shap_values_all.values).sum(axis=0)\n","        sorted_summed_shap = np.sort(abs_summed_shap)[::-1]\n","        labels = np.array(shap_values_all.feature_names)[np.argsort(abs_summed_shap)[::-1]]\n","        top_n_feat = 50\n","        print(f'\\nTop {top_n_feat} features based on summed absolute SHAP values over ALL instances/samples on {set_name} set')\n","        print(list(labels[:top_n_feat]))\n","        bot_n_feat = 25\n","        print(f'Bottom {bot_n_feat} features based on summed absolute SHAP values over ALL instances/samples on {set_name} set')\n","        print(list(labels[-bot_n_feat:]))\n","\n","        # from the features that have high shapley values over all instances/samples\n","        # find out feature shapley values that are negatively correlated with the error\n","        feat_error_corr_dict = {}\n","        for feat in labels[:top_largest_shap_val_feat]:\n","            # if self.nancorr(shap_values_all.values[:,shap_values_all.feature_names.index(feat)], error) < 0:\n","            #     feat_error_corr_dict[feat] = self.nancorr(shap_values_all.values[:,shap_values_all.feature_names.index(feat)], error)\n","            corr, _ = spearmanr(shap_values_all.values[:, shap_values_all.feature_names.index(feat)], error)\n","            if corr < 0:\n","                feat_error_corr_dict[feat] = corr\n","\n","        # sort the features based on the most negative correlation with the error first\n","        sorted_feat_error_corr_dict = dict(sorted(feat_error_corr_dict.items(), key=lambda item: item[1]))\n","\n","        # scatter plot of features shapley values vs. error over all instances/samples for the top_n_feat features that have high negative correlation with the error\n","        top_n_feat = 28\n","        fig, ax = plt.subplots(int(top_n_feat/4), 4, figsize=(30, 30))\n","        fig.suptitle(f\"{set_name} set\", fontsize=16)\n","        for feat in list(sorted_feat_error_corr_dict.keys())[:top_n_feat]:\n","            ax.flatten()[list(sorted_feat_error_corr_dict.keys()).index(feat)].scatter(shap_values_all.values[:,shap_values_all.feature_names.index(feat)], error, alpha=0.1)\n","            ax.flatten()[list(sorted_feat_error_corr_dict.keys()).index(feat)].set_title(f\"{feat} shap values vs. error \\n, spear. corr: {sorted_feat_error_corr_dict[feat]}\")\n","            ax.flatten()[list(sorted_feat_error_corr_dict.keys()).index(feat)].set_xlabel('error')\n","        fig.tight_layout()\n","        plt.show()\n","\n","        print(f'\\nTop {top_n_feat} features that have high negative correlation with the error on {set_name} set')\n","        print(list(sorted_feat_error_corr_dict.keys())[:top_n_feat])\n","\n","\n","        ###################################### Underprediction Error Shapley Analysis #######################################\n","        ## filter out instance where underpreidction is happening, then identfy the features that are causing the underprediction\n","        ## do not identfiy the features on the entire dataset.\n","\n","        top_n_feat = 50\n","        underprediction = prediction['target'] < groundtruth['target']\n","        bad_feat_up_idx = (prediction['target'] < groundtruth['target']) & (prediction['target'] > shap_values_all.base_values)\n","        good_feat_up_idx = (prediction['target'] < groundtruth['target']) & (prediction['target'] < shap_values_all.base_values)\n","\n","        # abs_summed_shap = np.abs(shap_values_all.values[underprediction]).sum(axis=0)\n","        # sorted_summed_shap = np.sort(abs_summed_shap)[::-1]\n","        # labels = np.array(shap_values_all.feature_names)[np.argsort(abs_summed_shap)[::-1]]\n","        # fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on summed absolute SHAP values\", height=800)\n","        # fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        # fig.show()\n","        # print(f'\\nTop {top_n_feat} features that have high absolute shapley values over all instances/samples on {set_name} set for UNDERprediction instances')\n","        # print(list(labels[:top_n_feat]))\n","\n","        #simple_summed_shap = shap_values_all.values[underprediction].sum(axis=0)\n","        simple_summed_shap = shap_values_all.values[bad_feat_up_idx].sum(axis=0)        \n","        sorted_summed_shap = np.sort(simple_summed_shap)\n","        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)]\n","        #fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on simply summed SHAP values, Asc. order\", height=800)\n","        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances with prediction > base value, Truly BAD Feature importance based on simply summed SHAP values, Asc. order\", height=800)\n","        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        fig.show()\n","        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for UNDERprediction instances, Asc. order')\n","        print(list(labels[:top_n_feat]))\n","\n","        #simple_summed_shap = shap_values_all.values[underprediction].sum(axis=0)\n","        simple_summed_shap = shap_values_all.values[good_feat_up_idx].sum(axis=0)\n","        sorted_summed_shap = np.sort(simple_summed_shap)[::-1]\n","        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)[::-1]]\n","        # fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on simply summed SHAP values, Desc. order\", height=800)\n","        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances with prediction < base value, Truly GOOD Feature importance based on simply summed SHAP values, Desc. order\", height=800)\n","        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        fig.show()\n","        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for UNDERprediction instances, Desc. order')\n","        print(list(labels[:top_n_feat]))\n","\n","\n","        # min_shap_values = shap_values_all.values[underprediction].min(axis=0)\n","        # sorted_min_shap = np.sort(min_shap_values)\n","        # labels = np.array(shap_values_all.feature_names)[np.argsort(min_shap_values)]\n","        # fig = px.bar(x=sorted_min_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's UNDERprediction instances, Feature importance based on min SHAP values\", height=800)\n","        # fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        # fig.show()\n","        # print(f'\\nTop {top_n_feat} features that have high min shapley values over all instances/samples on {set_name} set for UNDERprediction instances')\n","        # print(list(labels[:top_n_feat]))\n","\n","\n","        #Examine Individual Feature that lead to most negative shap values Contributions using shap dependence plot\n","        #self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n","\n","\n","\n","\n","        ###################################### Overprediction Error Shapley Analysis #######################################\n","\n","        ## filter out instance where underpreidction is happening, then identfy the features that are causing the overprediction\n","        ## do not identfiy the features on the entire dataset.\n","        top_n_feat = 50\n","        overprediction = prediction['target'] > groundtruth['target']\n","        bad_feat_op_idx = (prediction['target'] > groundtruth['target']) & (prediction['target'] < shap_values_all.base_values)\n","        good_feat_op_idx = (prediction['target'] > groundtruth['target']) & (prediction['target'] > shap_values_all.base_values)\n","\n","\n","        # abs_summed_shap = np.abs(shap_values_all.values[overprediction]).sum(axis=0)\n","        # sorted_summed_shap = np.sort(abs_summed_shap)[::-1]\n","        # labels = np.array(shap_values_all.feature_names)[np.argsort(abs_summed_shap)[::-1]]\n","        # fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on summed absolute SHAP values\", height=800)\n","        # fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        # fig.show()\n","        # print(f'\\nTop {top_n_feat} features that have high absolute shapley values over all instances/samples on {set_name} set for OVERprediction instances')\n","        # print(list(labels[:top_n_feat]))\n","\n","        #simple_summed_shap = shap_values_all.values[overprediction].sum(axis=0)\n","        simple_summed_shap = shap_values_all.values[good_feat_op_idx].sum(axis=0)\n","        sorted_summed_shap = np.sort(simple_summed_shap)\n","        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)]\n","        # fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on simply summed SHAP values, Asc. order\", height=800)\n","        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances with prediction > base value, Truly GOOD Feature importance based on simply summed SHAP values, Asc. order\", height=800)\n","        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        fig.show()\n","        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for OVERprediction instances, Asc. order')\n","        print(list(labels[:top_n_feat]))\n","\n","        #simple_summed_shap = shap_values_all.values[overprediction].sum(axis=0)\n","        simple_summed_shap = shap_values_all.values[bad_feat_op_idx].sum(axis=0)\n","        sorted_summed_shap = np.sort(simple_summed_shap)[::-1]\n","        labels = np.array(shap_values_all.feature_names)[np.argsort(simple_summed_shap)[::-1]]\n","        # fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on simply summed SHAP values,  Desc. order\", height=800)\n","        fig = px.bar(x=sorted_summed_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances with prediction < base value, Truly BAD Feature importance based on simply summed SHAP values,  Desc. order\", height=800)\n","        fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        fig.show()\n","        print(f'\\nTop {top_n_feat} features that have high simply summed shapley values over all instances/samples on {set_name} set for OVERprediction instances, Desc. order')\n","        print(list(labels[:top_n_feat]))\n","\n","        # max_shap_values = shap_values_all.values[overprediction].max(axis=0)\n","        # sorted_max_shap = np.sort(max_shap_values)[::-1]\n","        # labels = np.array(shap_values_all.feature_names)[np.argsort(max_shap_values)[::-1]]\n","        # fig = px.bar(x=sorted_max_shap[:top_n_feat], y=labels[:top_n_feat], orientation='h', title=f\"{set_name} set's OVERprediction instances, Feature importance based on max SHAP values\", height=800)\n","        # fig.update_layout(yaxis=dict(tickfont=dict(size=10)))  # Reduce the font size of the labels\n","        # fig.show()\n","        # print(f'\\nTop {top_n_feat} features that have high max shapley values over all instances/samples on {set_name} set for OVERprediction instances')\n","        # print(list(labels[:top_n_feat]))\n","\n","        #Examine Individual Feature that lead to most negative shap values Contributions using shap dependence plot\n","        #self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n","\n","        print(f\"\\n\\nFraction of underprediction instances: {sum(underprediction) / len(prediction)} on {set_name} set\")\n","        print(f\"Underprediction RMSPE: {self.rmspe(groundtruth['target'][underprediction], prediction['target'][underprediction])} on {set_name} set\")\n","        print(f\"Fraction of overprediction instances: {sum(overprediction) / len(prediction)} on {set_name} set\")\n","        print(f\"Overprediction RMSPE: {self.rmspe(groundtruth['target'][overprediction], prediction['target'][overprediction])} on {set_name} set\")\n","        print(f\"OVERALL RMSPE: {self.rmspe(groundtruth['target'], prediction['target'])} on {set_name} set\")\n","\n","\n","\n","        # del shap_values_all,groundtruth, prediction, error, underprediction, overprediction, abs_summed_shap, sorted_summed_shap,\\\n","        #     labels, feat_error_corr_dict, sorted_feat_error_corr_dict, min_shap_values, max_shap_values, sorted_min_shap, sorted_max_shap\n","        del shap_values_all,groundtruth, prediction, error, underprediction, overprediction, abs_summed_shap, sorted_summed_shap,\\\n","            labels, feat_error_corr_dict, sorted_feat_error_corr_dict\n","        return\n","\n","\n","\n","\n","    def compute_shapley_heatmap(self,shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #print(' NOTE: Heatmap is sorted with f(X) from smallest values to biggest value !! (picture is wrong)')\n","        # fig,ax = plt.subplots(figsize=(13.5,2))\n","        # y_asc = np.sort( all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values )\n","        # ax.plot( range(len(y_asc)), y_asc, color='g')\n","        # ax.axhline(y_asc.mean(),color='r', linestyle='dashed')\n","        # ax.set_ylabel('Correct f(x) in Asc. order')\n","        # ax.set_yticks(np.arange(0,max(y_asc),0.002))\n","        # fig.show()\n","\n","        fig,ax = plt.subplots()\n","        # order = np.argsort(all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values)\n","        shap.plots.heatmap(shap_values,instance_order=shap_values.sum(1))\n","        #ax.title(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values,all_stock_train_pred_df\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_decision(self,model_base_value,shap_values,feature_names,stock_id,view_time_ids_start,view_time_ids_end,set_name):\n","\n","\n","\n","        # Create decision plot\n","        shap.decision_plot(model_base_value, shap_values, X_test, feature_names=feature_names)\n","\n","\n","        #### ONLY for Explainer\n","        print(f'\\nDECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.decision(model_base_value,shap_values=shap_values.values,features=shap_values.data,feature_names=feature_names, show=True) #matplotlib=True,\n","        ax.set_title(f'\\n DECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_force(self,model_base_value,shap_values,X,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # ### ONLY for Explainer\n","        # print(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig,ax = plt.subplots()\n","        # shap.plots.force(model_base_value,shap_values=shap_values[0],features=X[0],feature_names=feature_names, show=True) #matplotlib=True,\n","        # ax.title(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig.show()\n","\n","        # example_index = 0  # You can change this index to any other example\n","        # example = X[example_index]\n","        # # Explain the prediction of the example\n","        # shap.force_plot(explainer.expected_value, shap_values[example_index], example, feature_names=data.feature_names)\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","\n","    # def compute_train_avg_target_rvol(self, unique_stock_ids, y_train):\n","    #     # unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","    #     unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","    #     train_target_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","    #     for s in unique_stock_ids:\n","    #         st_index = self.train_stock_id == s\n","    #         t_index = self.train_time_id[st_index]\n","    #         train_target_df.loc[t_index, s] = y_train[st_index].values\n","    #     train_avg_target_rvol = train_target_df.ffill().bfill().mean(axis=1)\n","    #     return train_avg_target_rvol\n","\n","    # def compute_test_avg_target_rvol(self, unique_stock_ids, y_test):\n","    #     #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","    #     unique_test_time_ids = self.test_time_id\n","    #     test_target_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","    #     for s in unique_stock_ids:\n","    #         st_index = self.test_stock_id == s\n","    #         t_index = self.test_time_id[st_index]\n","    #         test_target_df.loc[t_index, s] = y_test[st_index].values\n","    #     test_avg_target_rvol = test_target_df.ffill().bfill().mean(axis=1)\n","    #     return test_avg_target_rvol\n","\n","\n","    def fraction_above_average(self,signal1, avg):\n","        # Count the fraction of times when signal1 is above signal2\n","        fraction_above_avg = (signal1 > avg).mean()\n","        return fraction_above_avg\n","\n","\n","    def compute_all_stock_v1tr_df(self, unique_stock_ids, v1tr):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_v1tr_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_v1tr_df.loc[t_index, s] = v1tr[st_index].values\n","        all_stock_v1tr_df = all_stock_v1tr_df.ffill().bfill()\n","        return all_stock_v1tr_df\n","\n","    def compute_all_stock_train_pred_df(self, unique_stock_ids, train_pred):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_train_pred_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_train_pred_df.loc[t_index, s] = train_pred[st_index].values\n","        all_stock_train_pred_df = all_stock_train_pred_df.ffill().bfill()\n","        return all_stock_train_pred_df\n","\n","    def compute_all_stock_test_pred_df(self, unique_stock_ids, test_pred):\n","        # unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        all_stock_test_pred_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_test_pred_df.loc[t_index, s] = test_pred[st_index].values\n","        all_stock_test_pred_df = all_stock_test_pred_df.ffill().bfill()\n","        return all_stock_test_pred_df\n","\n","\n","    def compute_all_stock_y_train_df(self, unique_stock_ids, y_train):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_y_train_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_y_train_df.loc[t_index, s] = y_train[st_index].values\n","        all_stock_y_train_df = all_stock_y_train_df.ffill().bfill()\n","        return all_stock_y_train_df\n","\n","    def compute_all_stock_y_test_df(self, unique_stock_ids, y_test):\n","        #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        all_stock_y_test_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_y_test_df.loc[t_index, s] = y_test[st_index].values\n","        all_stock_y_test_df = all_stock_y_test_df.ffill().bfill()\n","        return all_stock_y_test_df\n","\n","\n","\n","    ######## Identify stocks belonging to clusters based on clusterings in dataset\n","    ######## find stock ids of clusters having same feature values\n","    ######## This is reverse-engineering cluster labels of already clustered stocks\n","    def calculate_cluster_fraction(self, column, n_clusters, stock_list):\n","        \"\"\" This function computes the fraction of stock ids in stock_list inside a cluster in the clustering feature.\n","        The fraction is between 0 - 1. 1 indicates all the stock ids in stock_list are in a particular cluster.\n","        \"\"\"\n","\n","        # self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        # self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","\n","        # unique_stock_ids = self.train_stock_id.unique()\n","        # time_id_order = df2.loc[:3829,'time_id'].values\n","        # train_time_id_ind = int(len(time_id_order)*0.7)\n","\n","        # train_time_ids = time_id_order[:train_time_id_ind]\n","        # train_stock_id = df2[df2['time_id'].isin(train_time_ids)]['stock_id']\n","        # train_time_id = df2[df2['time_id'].isin(train_time_ids)]['time_id']\n","\n","        unique_stock_ids = self.train_stock_id.unique()\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","\n","        train_col_df = self.df[self.df['time_id'].isin(train_time_ids)][column]\n","\n","        ## reshape the dataframe\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_column_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_column_df.loc[t_index, s] = train_col_df[st_index].values\n","        all_stock_column_df = all_stock_column_df.ffill().bfill()\n","\n","        features = all_stock_column_df.T.to_numpy()\n","\n","        ## kmeans\n","        kmeans = KMeans(n_clusters=n_clusters,n_init=10)\n","        kmeans.fit(features)\n","        cluster_labels = kmeans.labels_\n","        cluster_labels\n","\n","        clusters_dict = {}\n","        unique_labels = np.unique(cluster_labels)\n","        for label in unique_labels:\n","            indices = np.where(cluster_labels == label)[0]\n","            stocks_in_cluster = unique_stock_ids[indices]\n","            clusters_dict[label] = stocks_in_cluster.tolist()\n","\n","        for c in clusters_dict.keys():\n","            cnt=0\n","            for s in stock_list:\n","                if s in clusters_dict[c]:\n","                    cnt+=1\n","            print(f'cluster: {c}, # stock ids in cluster: {cnt}, clustering fraction: {cnt/len(clusters_dict[c])}')\n","\n","        return\n","\n","\n","\n","    def check_stock_list_in_all_clustering_features(self, stock_list):\n","\n","        clustering_features_list = [    \"log_target_vol_corr_32_clusters_stnd\",\n","                                        \"log_target_vol_sum_stats_16_clusters_stnd\",\n","                                        \"sum_stats_4_clusters_labels\",\n","                                        \"sum_stats_10_clusters_labels\",\n","                                        \"sum_stats_16_clusters_labels\",\n","                                        \"sum_stats_30_clusters_labels\",\n","                                        \"pear_corr_32_clusters_labels\",\n","                                        \"pear_corr_4_clusters_labels\",\n","                                        \"pear_corr_49_clusters_labels\",\n","                                        \"pear_corr_90_clusters_labels\",]\n","\n","        print('stock_list: ' , stock_list)\n","        for feature in clustering_features_list:\n","            n_clusters = int(re.findall(r'\\d+', feature)[0])\n","            print('Feature: ', feature)\n","            print('Cluster Fractions: ')\n","            print(self.calculate_cluster_fraction( feature, n_clusters, stock_list))\n","\n","        return\n","\n","\n","    def compute_acf_pacf(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        plt.close('all')\n","        for s in unique_stock_ids[0:1]:#[0:40]:\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            plot_acf(stock_residual, lags=200,ax=ax[0])\n","            plot_pacf(stock_residual, lags=200,ax=ax[1])\n","            ax[0].set_title(f'Autocorrelation of stock {s} Residuals on train set')\n","            ax[1].set_title(f'Partial Autocorrelation of stock {s} Residuals on train set')\n","            ax[0].set_xticks(range(0,200,5))\n","            ax[1].set_xticks(range(0,200,5))\n","            ax[0].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_xlabel('lags')\n","            ax[0].set_ylabel('ACF')\n","            ax[1].set_ylabel('PACF')\n","            ax[0].grid(True)\n","            ax[1].grid(True)\n","            fig.show()\n","        return\n","\n","\n","\n","    def compute_IFFT(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","\n","        ##### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        ##### IFFT plot of reconstructed time series ######\n","        plt.close('all')\n","        for s in unique_stock_ids[100:]:#[40:112]:\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            x = stock_residual.values\n","            limit = 0.00001\n","\n","            n=len(x)\n","            fhat = np.fft.fft(x,n)\n","            PSD = fhat*np.conj(fhat) / n\n","            freq = (1/n)*np.arange(n)\n","            start=1 #ignore dc component\n","            L = np.arange(start,np.floor(n/2),dtype='int')\n","            # fig,ax = plt.subplots(figsize=(30,6))\n","            # #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            # ax.axhline(limit,  color='k', linestyle='-')\n","            # ax.plot(freq[L],PSD[L])\n","            # ax.set_xlabel('freq')\n","            # ax.set_ylabel('mag')\n","            # ax.set_title(f'mag plot of stock: {s} residual')\n","            # fig.show()\n","\n","            indices = PSD > limit\n","            num_freqs = len(np.where(indices>0)[0])\n","            print('# of frequencies in residual = ',num_freqs)\n","\n","            fhat = fhat*indices\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            ffilt = np.fft.ifft(fhat)\n","            ax[0].plot(np.arange(0,len(x)),ffilt.real,label='top '+str(num_freqs)+' frequencies in residual (train set)',c='g',alpha=1)\n","            ax[0].plot(np.arange(0,len(x)),x,label='original residual',c='r',alpha=0.2)\n","            ax[0].legend()\n","            ax[0].grid()\n","            ax[0].set_xlabel('time id')\n","            ax[0].set_ylabel('residual')\n","            ax[0].set_title(f'IFFT of stock: {s} residual')\n","\n","\n","            x1 = all_stock_y_train_df[s].values\n","            limit1 = 0.00001\n","            n1=len(x1)\n","            fhat1 = np.fft.fft(x1,n1)\n","            PSD1 = fhat1*np.conj(fhat1) / n1\n","            freq1 = (1/n1)*np.arange(n1)\n","            start1=1 #ignore dc component\n","            L1 = np.arange(start1,np.floor(n1/2),dtype='int')\n","            fig1,ax1 = plt.subplots(figsize=(30,6))\n","            #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            ax1.axhline(limit1,  color='k', linestyle='-')\n","            ax1.plot(freq1[L],PSD1[L])\n","            ax1.set_xlabel('freq')\n","            ax1.set_ylabel('mag')\n","            ax1.set_title(f'mag plot of stock: {s} rvol.')\n","            fig1.show()\n","\n","            indices1 = PSD1 > limit1\n","            num_freqs1 = len(np.where(indices1>0)[0])\n","            print('# of frequencies in rvol. = ',num_freqs1)\n","            fhat1 = fhat1*indices1\n","            ffilt1 = np.fft.ifft(fhat1)\n","            ax[1].plot(np.arange(0,len(x1)),ffilt1.real,label='top '+str(num_freqs1)+' frequencies in true rvol. (train set)',c='g',alpha=1)\n","            ax[1].plot(np.arange(0,len(x1)),x1,label='original true rvol.',c='r',alpha=0.2)\n","            ax[1].legend()\n","            ax[1].grid()\n","            ax[1].set_xlabel('time id')\n","            ax[1].set_ylabel('rvol.')\n","            ax[1].set_title(f'IFFT of stock: {s} rvol.')\n","            fig.show()\n","        return\n","\n","\n","\n","\n","    def calculate_total_gap(self,data):\n","        # 1. Order the ground truth in ascending order\n","        sorted_data = np.sort(data)\n","\n","        # 2. Take up to the 75th percentile (remove upper outliers)\n","        percentile_75 = np.percentile(sorted_data, 75)\n","        filtered_data = sorted_data[sorted_data <= percentile_75]\n","\n","        # 3. Take first differences\n","        first_differences = np.diff(filtered_data)\n","\n","        # 4. Order the first differences in descending order\n","        sorted_differences = np.sort(first_differences)[::-1]  # Sort in descending order\n","\n","        # 5. Take the sum of the first differences up to the 50th percentile\n","        length = len(sorted_differences)\n","        cutoff_index = int(0.5 * length)  # 50th percentile index\n","        total_gap = np.sum(sorted_differences[:cutoff_index])\n","\n","        return total_gap\n","\n","\n","\n","\n","    def overall_stock_id_analysis(self,unique_stock_ids,all_stock_pred_df,all_stock_y_GroundTruth_df,residuals,train_flag):\n","\n","        if train_flag:\n","            set_name = 'train'\n","        else:\n","            set_name = 'test'\n","\n","        all_stock_pred_pivot = all_stock_pred_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_pred_pivot = all_stock_pred_pivot.reindex(self.time_id_order)\n","\n","        all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_pivot.reindex(self.time_id_order)\n","\n","\n","        rmspe_per_stock = []\n","        total_gap_per_stock = []\n","        for s in unique_stock_ids:\n","            rmspe_per_stock.append( np.nanmean( ((all_stock_pred_pivot[s]-all_stock_y_GroundTruth_pivot[s])/all_stock_y_GroundTruth_pivot[s])**2 )**0.5  )\n","            total_gap_per_stock.append( self.calculate_total_gap(all_stock_y_GroundTruth_pivot[s].values) )\n","\n","\n","        ###### Bar plot of RMSPE for all stocks in the training set\n","        all_stock_rmspe = pd.Series(rmspe_per_stock,index=unique_stock_ids)\n","        smallest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index.values[:10]\n","        largest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n","        # fig, ax = plt.subplots(figsize=(40,10))\n","        fig, ax = plt.subplots(figsize=(10,10))\n","        # ax.text(0,0.52,f'10 largest RMSPE stocks: {largest_10_rmspe_stocks}')\n","        # ax.text(0,0.62,f'10 smallest RMSPE stocks: {smallest_10_rmspe_stocks}')\n","        # ax.text(0,0.2,f'10 largest RMSPE stocks: {largest_10_rmspe_stocks}')\n","        # ax.text(0,0.2,f'10 smallest RMSPE stocks: {smallest_10_rmspe_stocks}')\n","        ax.bar(unique_stock_ids, rmspe_per_stock)\n","        ax.set_xticks(unique_stock_ids)\n","        ax.tick_params(axis='x', rotation=45)\n","        ax.set_yticks(np.arange(0, max(rmspe_per_stock) + 0.01, 0.04))\n","        ax.grid()\n","        ax.set_title(f'RMSPE of Real. Vol. for each stock on {set_name} set')\n","        ax.set_xlabel('Stock ID')\n","        ax.set_ylabel('RMSPE')\n","        plt.show()\n","        plt.close()\n","        ## check if the largest and smallest fall into a cluster of a clustering feature\n","        print('\\n 10_largest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = largest_10_rmspe_stocks)\n","        print('\\n 10_smallest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = smallest_10_rmspe_stocks)\n","        print(f'30_largest_rmspe_stocks in {set_name} set: ',all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:30])\n","\n","\n","        ####### plot total_gap in descending order for all stocks  ######\n","        all_stock_total_gap = pd.Series(total_gap_per_stock,index=unique_stock_ids)\n","        smallest_10_total_gap_stocks = all_stock_total_gap.sort_values(ascending=True).index.values[:10]\n","        largest_10_total_gap_stocks = all_stock_total_gap.sort_values(ascending=True).index[::-1].values[:10]\n","        fig, ax = plt.subplots(figsize=(40,10))\n","        # ax.text(4, max(total_gap_per_stock),f'10 largest total gap stocks: {largest_10_total_gap_stocks}')\n","        # ax.text(4, max(total_gap_per_stock)-0.003,f'10 smallest total gap stocks: {smallest_10_total_gap_stocks}')\n","        sorted_indices = np.argsort(total_gap_per_stock)[::-1]\n","        sorted_unique_stock_ids = unique_stock_ids[sorted_indices]\n","        sorted_total_gap_per_stock = np.array(total_gap_per_stock)[sorted_indices]\n","        ax.bar(range(len(sorted_unique_stock_ids)), sorted_total_gap_per_stock,tick_label=sorted_unique_stock_ids)\n","        ax.tick_params(axis='x', rotation=45)\n","        ax.set_yticks(np.arange(0, max(sorted_total_gap_per_stock) + 0.001, 0.001))\n","        ax.grid()\n","        ax.set_title(f'Total Gap of Real. Vol. for each stock on GroundTruth {set_name} set')\n","        ax.set_xlabel('Stock ID')\n","        ax.set_ylabel('Total Gap')\n","        plt.show()\n","        plt.close()\n","        print(f'30_largest_total_gap_stocks in {set_name} set: ',all_stock_total_gap.sort_values(ascending=True).index[::-1].values[:30])\n","\n","\n","        groundtruth = all_stock_y_GroundTruth_df['target'].values\n","        prediction = all_stock_pred_df['target'].values\n","        residuals = residuals['target'].values # copy is made, its not modified inplace\n","\n","        ####### scatter plot of True Real. Vol. vs. Pred Real. Vol.\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(groundtruth, prediction, c='b', alpha=0.1)\n","        ax.plot(groundtruth, groundtruth, c='r',linestyle='solid' )\n","        ax.set_title(f'Scatter Plot of True vs Predicted Values on {set_name} set')\n","        ax.set_xlabel(f'True {set_name} rvol. Values')\n","        ax.set_ylabel(f'Predicted {set_name} rvol. Values')\n","        plt.show()\n","        plt.close()\n","\n","        ## y_train and train_pred Distributions Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( groundtruth,bins=1000, color='green', alpha=0.9, histtype='bar', rwidth=0.8, label='GroundTruth')\n","        plt.hist( prediction,bins=1000, color='red', alpha=0.3, ec='r', label='Prediction')\n","        ax.set_title(f'Distribution of GroundTruth (skew: {stats.skew(groundtruth)} , kurt:{stats.kurtosis(groundtruth)}) and Prediction (skew: {stats.skew(prediction)} , kurt:{stats.kurtosis(prediction)}) on {set_name} set')\n","        ax.set_xlabel(f' GroundTruth and Prediction on {set_name} set')\n","        ax.set_ylabel('frequency')\n","        plt.legend(loc=\"upper left\")\n","        plt.show()\n","        plt.close()\n","\n","        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(groundtruth, residuals, c='c',alpha=0.1 )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n","        ax.set_title(f' True R.V. Vs. Residuals Values Plot on {set_name} set')\n","        ax.set_xlabel(f'True {set_name} Values')\n","        ax.set_ylabel(f'{set_name} residuals')\n","        plt.show()\n","        plt.close()\n","\n","\n","        ####### scatter plot of Fitted rvol. Values Vs. train residuals Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(prediction, residuals, c='m',alpha=0.1 )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n","        ax.set_title(f' fitted R.V. Vs. Residuals Values Plot on {set_name} set')\n","        ax.set_xlabel(f'fitted {set_name} Values')\n","        ax.set_ylabel(f'{set_name} residuals')\n","        plt.show()\n","        plt.close()\n","\n","        ## Normal Q-Q Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        sm.qqplot(residuals, line='q', ax=ax)\n","        ax.set_title(f'QQ Plot of Residuals on {set_name} set')\n","        ax.set_xlabel('Theoretical Quantiles')\n","        ax.set_ylabel('Sample Quantiles')\n","        plt.show()\n","        plt.close()\n","\n","        ## Residuals Distribution Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( residuals,bins=1000)\n","        ax.set_title(f'Distribution of Residuals on {set_name} set')\n","        ax.set_xlabel(f'{set_name} Residuals')\n","        ax.set_ylabel('frequency')\n","        plt.show()\n","        plt.close()\n","\n","        del all_stock_pred_df,all_stock_y_GroundTruth_df,residuals,all_stock_y_GroundTruth_pivot,all_stock_pred_pivot,groundtruth,prediction\n","        gc.collect()\n","        return pd.DataFrame({f'{set_name}_rmspe_per_stock':rmspe_per_stock}, index=unique_stock_ids)\n","\n","\n","\n","\n","    def individual_stock_id_analysis(self,picked_stock_id,unique_stock_ids,all_stock_y_GroundTruth_df,all_stock_pred_df,residuals,avg_target_rvol,train_flag):\n","\n","        if train_flag:\n","            set_name = 'train'\n","        else:\n","            set_name = 'test'\n","\n","        all_stock_pred_pivot = all_stock_pred_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_pred_pivot = all_stock_pred_pivot.reindex(self.time_id_order)\n","\n","        all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_pivot.reindex(self.time_id_order)\n","\n","        groundtruth = all_stock_y_GroundTruth_pivot[picked_stock_id].values\n","        prediction = all_stock_pred_pivot[picked_stock_id].values\n","        picked_st_residuals = groundtruth - prediction\n","\n","        ## 1. scatter plot\n","        plt.figure(figsize=(10,10))\n","        plt.scatter(all_stock_y_GroundTruth_pivot[picked_stock_id],all_stock_pred_pivot[picked_stock_id], c='blue',label=picked_stock_id, alpha=0.4)\n","        plt.plot(all_stock_y_GroundTruth_pivot[picked_stock_id],all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='red',label=picked_stock_id, alpha=1 )\n","        plt.grid()\n","        plt.xlabel(f'GroundTruth')\n","        plt.ylabel(f'Prediction')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s scatter plot of y_{set_name} vs. {set_name}_pred on {set_name} set\")\n","        plt.show()\n","        plt.close()\n","\n","        ## 2. Line plot of true vs average real. vol.\n","        fraction_above_avg = self.fraction_above_average(all_stock_y_GroundTruth_pivot[picked_stock_id], avg_target_rvol)\n","        plt.figure(figsize=(30,5))\n","        plt.text(1,all_stock_y_GroundTruth_pivot[picked_stock_id].max(),f\"fraction of times this stock's values are above all stocks' avg_target_rvol = {fraction_above_avg}\")\n","        plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.4 )\n","        #plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),[avg_target_rvol]*len(all_stock_y_GroundTruth_pivot[picked_stock_id]),linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n","        plt.plot(range(len(avg_target_rvol)),avg_target_rvol,linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n","        plt.grid()\n","        plt.xlabel('index')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s line plot of True y_{set_name} vs. {set_name}_avg_target_rvol on {set_name} set\")\n","        plt.show()\n","        plt.close()\n","\n","        # ## 3. Line plot of pred vs true real. vol.\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.7 )\n","        # plt.plot(range(len(all_stock_pred_pivot[picked_stock_id])),all_stock_pred_pivot[picked_stock_id],linestyle='solid', c='red',label='Pred stock id: '+str(picked_stock_id), alpha=0.4 )\n","        # plt.grid()\n","        # plt.xlabel('index')\n","        # plt.ylabel(f'{set_name} rvol.')\n","        # plt.legend()\n","        # plt.title(f\"stock {picked_stock_id}'s line plot of True y_{set_name} vs {set_name}_pred on {set_name} set\")\n","        # plt.show()\n","        # plt.close()\n","\n","        ## y_train and train_pred Distributions Histogram:\n","        fig, ax = plt.subplots(2, 1, figsize=(30, 10))\n","        max_val = max(groundtruth.max(), prediction.max())\n","        ax[0].hist(groundtruth, bins=1000, color='green', alpha=1, histtype='bar', rwidth=0.8, label='GroundTruth')\n","        ax[0].set_xlim(-0.001, max_val)\n","        ax[0].set_title(f\"stock {picked_stock_id}'s Distribution of GroundTruth (skew: {stats.skew(groundtruth)} , kurt:{stats.kurtosis(groundtruth)}) and Prediction (skew: {stats.skew(prediction)} , kurt:{stats.kurtosis(prediction)}) on {set_name} set\")\n","        ax[0].set_xlabel(f' GroundTruth on {set_name} set')\n","        ax[0].set_ylabel('frequency')\n","        ax[1].hist(prediction, bins=1000, color='red', alpha=1, ec='r', label='Prediction')\n","        ax[1].set_xlim(-0.001, max_val)\n","        ax[1].set_xlabel(f' Prediction on {set_name} set')\n","        ax[1].set_ylabel('frequency')\n","        plt.legend(loc=\"upper left\")\n","        plt.show()\n","        plt.close()\n","\n","        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n","        fig, ax = plt.subplots(2,1,figsize=(30,10))\n","        max_val = max(groundtruth.max(), prediction.max())\n","        ax[0].scatter(groundtruth, picked_st_residuals, c='c',alpha=0.1 )\n","        ax[0].set_xlim(-0.001, max_val)\n","        ax[0].axhline(y=0, color='g', linestyle='-')\n","        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n","        ax[0].set_title(f\"stock {picked_stock_id}'s True R.V. Vs. Residuals Values Plot on {set_name} set\")\n","        ax[0].set_xlabel(f'True {set_name} Values')\n","        ax[0].set_ylabel(f'{set_name} picked_st_residuals')\n","        ax[1].scatter(prediction, picked_st_residuals, c='m',alpha=0.1 )\n","        ax[1].set_xlim(-0.001, max_val)\n","        ax[1].axhline(y=0, color='g', linestyle='-')\n","        ax[1].set_title(f\"stock {picked_stock_id}'s fitted R.V. Vs. Residuals Values Plot on {set_name} set\")\n","        ax[1].set_xlabel(f'fitted {set_name} Values')\n","        ax[1].set_ylabel(f'{set_name} picked_st_residuals')\n","        plt.legend(loc=\"upper left\")\n","        plt.show()\n","        plt.close()\n","\n","        ## Normal Q-Q Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        sm.qqplot(picked_st_residuals, line='q', ax=ax)\n","        ax.set_title(f\"stock {picked_stock_id}'s QQ Plot of Residuals on {set_name} set\")\n","        ax.set_xlabel('Theoretical Quantiles')\n","        ax.set_ylabel('Sample Quantiles')\n","        plt.show()\n","        plt.close()\n","\n","        ## Residuals Distribution Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( picked_st_residuals,bins=1000)\n","        ax.set_title(f\"stock {picked_stock_id}'s Distribution of Residuals on {set_name} set\")\n","        ax.set_xlabel(f'{set_name} Residuals')\n","        ax.set_ylabel('frequency')\n","        plt.show()\n","        plt.close()\n","\n","        ## box plot of residuals, groundtruth and prediction\n","        fig, ax = plt.subplots(figsize=(5,5))\n","        ax.boxplot([picked_st_residuals,groundtruth,prediction],labels=['residuals','groundtruth','prediction'])\n","        ax.set_title(f\" stock {picked_stock_id}'s Box Plot of Residuals, GroundTruth and Prediction on {set_name} set\")\n","        ax.set_ylabel('value')\n","        #ax.set_yticks(np.arange(-0.01, 0.025, 0.001))\n","        ax.grid()\n","        plt.show()\n","        plt.close()\n","\n","        # ###### Autocorrelation Plot\n","        # fig, ax = plt.subplots(figsize=(10,3))\n","        # plot_acf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        # ax.set_xlabel('Lag')\n","        # ax.set_ylabel('Autocorrelation')\n","        # ax.set_yticks(np.arange(-1, 1, 0.1))\n","        # ax.grid()\n","        # ax.set_title(f'Autocorrelation of {set_name} Residuals')\n","        # fig.show()\n","\n","        # ###### Partial Autocorrelation Plot\n","        # fig, ax = plt.subplots(figsize=(10,3))\n","        # plot_pacf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        # ax.set_xlabel('Lag')\n","        # ax.set_ylabel('Partial Autocorrelation')\n","        # ax.set_yticks(np.arange(-1, 1, 0.1))\n","        # ax.grid()\n","        # plt.title(f'Partial Autocorrelation of {set_name} Residuals')\n","        # plt.show()\n","\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        #self.compute_acf_pacf(unique_stock_ids,all_stock_pred_df,all_stock_y_GroundTruth_df)\n","\n","\n","        # #### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        # #### IFFT plot of reconstructed time series ######\n","        # self.compute_IFFT(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n","\n","        del picked_stock_id,unique_stock_ids,all_stock_y_GroundTruth_df,all_stock_pred_df,residuals\n","        gc.collect()\n","        return groundtruth\n","\n","\n","\n","    def overall_time_id_analysis(self, all_stock_y_GroundTruth_df,all_stock_pred_df,avg_target_rvol,train_flag):\n","\n","        # Precompute variables\n","        set_name = 'train' if train_flag else 'test'\n","\n","        # Pivot with 'stock_id' as index and 'time_id' as columns\n","        if train_flag:\n","            all_stock_pred_pivot = all_stock_pred_df.pivot(index='stock_id', columns='time_id', values='target').reindex(columns=self.time_id_order)\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='stock_id', columns='time_id', values='target').reindex(columns=self.time_id_order)\n","        else:\n","            all_stock_pred_pivot = all_stock_pred_df.pivot(index='stock_id', columns='time_id', values='target')\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='stock_id', columns='time_id', values='target')\n","\n","        unique_time_ids = all_stock_y_GroundTruth_pivot.columns.values\n","        # Vectorized RMSPE calculation along the 'stock_id' axis\n","        rmspe_per_time_id = np.sqrt(\n","            np.mean(\n","                ((all_stock_pred_pivot - all_stock_y_GroundTruth_pivot) / all_stock_y_GroundTruth_pivot) ** 2, axis=0\n","            )\n","        )\n","\n","        # print('all_stock_pred_pivot.index',all_stock_pred_pivot)\n","        # display_n_time_ids = 10\n","        # sorted_rmspe_per_time_id = rmspe_per_time_id.sort_values(ascending=False).head(display_n_time_ids)\n","        # for t in sorted_rmspe_per_time_id.index:\n","        #     plt.figure(figsize=(30,5))\n","        #     plt.plot(all_stock_pred_pivot.index,all_stock_pred_pivot[t],linestyle='solid', c='red',label='pred', alpha=0.4 )\n","        #     plt.plot(all_stock_pred_pivot.index,all_stock_y_GroundTruth_pivot[t],linestyle='solid', c='green',label='true', alpha=0.4 )\n","        #     plt.title(f'time index {t}, rmspe {np.sqrt(np.mean(((all_stock_pred_pivot[t]-all_stock_y_GroundTruth_pivot[t])/all_stock_y_GroundTruth_pivot[t])**2))}')\n","        #     plt.xticks(all_stock_pred_pivot.index,rotation=90)\n","        #     plt.grid()\n","        #     plt.show()\n","        #     plt.close()\n","\n","        # plot the RMSPE for all time ids\n","        display_n_time_ids = 200\n","        sorted_rmspe_per_time_id = rmspe_per_time_id.sort_values(ascending=False).head(display_n_time_ids)\n","        fig, ax = plt.subplots(figsize=(40, 10))\n","        ax.bar(range(len(sorted_rmspe_per_time_id)), sorted_rmspe_per_time_id, tick_label=sorted_rmspe_per_time_id.index)\n","        ax.set_xticks(range(len(sorted_rmspe_per_time_id)))\n","        ax.tick_params(axis='x', rotation=90)\n","        ax.set_yticks(np.arange(0, max(sorted_rmspe_per_time_id) + 0.01, 0.04))\n","        ax.grid()\n","        ax.set_title(f'RMSPE of Real. Vol. for each time id on {set_name} set')\n","        ax.set_xlabel('Time ID')\n","        plt.show()\n","        plt.close()\n","\n","        all_time_id_rmspe = rmspe_per_time_id\n","\n","        fig, ax = plt.subplots(figsize=(40, 10))\n","        largest_20_rmspe_time_ids = all_time_id_rmspe.nlargest(20).index.values\n","        # Precompute the maximum RMSPE value for the largest 20 time IDs\n","        max_large_val = all_time_id_rmspe.loc[largest_20_rmspe_time_ids].max()\n","        # Create the second bar plot for the largest 20 RMSPE time IDs\n","        ax.text(5, max_large_val + 0.001, f'20 largest RMSPE time ids: {largest_20_rmspe_time_ids}', fontsize=12)\n","        # Convert time ids to string once and plot the data\n","        ax.bar(largest_20_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[largest_20_rmspe_time_ids])\n","        # Set y-ticks efficiently, using the precomputed max value\n","        ax.set_yticks(np.arange(0, max_large_val + 0.001, 0.08))\n","        ax.set_ylabel('RMSPE')\n","        ax.set_title(f'20 largest RMSPE time ids on {set_name} set')\n","        ax.grid()\n","        plt.show()\n","        plt.close()\n","\n","\n","        all_time_id_rmspe = rmspe_per_time_id\n","        # Use `nsmallest` and `nlargest` to directly get the top 10 smallest and largest RMSPE values without full sorting\n","        smallest_10_rmspe_time_ids = all_time_id_rmspe.nsmallest(10).index.values\n","        # Precompute max value for y-ticks range in a single operation\n","        max_small_val = all_time_id_rmspe.loc[smallest_10_rmspe_time_ids].max()\n","        # Create the bar plot for RMSPE with precomputed values\n","        fig, ax = plt.subplots(figsize=(40, 10))\n","        # Set text once, and avoid converting indices to strings multiple times\n","        ax.text(5, max_small_val + 0.001, f'10 smallest RMSPE time ids: {smallest_10_rmspe_time_ids}', fontsize=12)\n","        # Convert time ids to string once and avoid recalculating max_small_val in yticks\n","        ax.bar(smallest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[smallest_10_rmspe_time_ids])\n","        # Set y-ticks with precomputed values (adjusting the range once)\n","        ax.set_yticks(np.arange(0, max_small_val + 0.001, 0.04))\n","        ax.set_ylabel('RMSPE')\n","        ax.set_title(f'10 smallest RMSPE time ids on {set_name} set')\n","        ax.grid()\n","        plt.show()\n","        plt.close()\n","\n","\n","        ###### visualize the time ids with largest and smallest RMSPE on the average rvol. plot on training set\n","        plt.figure(figsize=(30,5))\n","        plt.plot(range(len(avg_target_rvol)),avg_target_rvol,linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n","        large_idx = np.where(np.isin(unique_time_ids,largest_20_rmspe_time_ids))[0]\n","        red_colors = colors = ['black', 'maroon', 'darkred', 'firebrick', 'crimson', 'indianred', \\\n","          'tomato', 'lightcoral', 'salmon', 'hotpink', 'palevioletred', \\\n","          'mediumvioletred', 'orchid', 'fuchsia', 'magenta', 'violet', \\\n","          'plum', 'mediumorchid', 'lavender', 'thistle']\n","        for i,s in enumerate(large_idx):\n","            plt.axvline(x=s, ymin=0, ymax=1,color=red_colors[i],linestyle='-',label=str(i))\n","        small_idx = np.where(np.isin(unique_time_ids,smallest_10_rmspe_time_ids))[0]\n","        green_colors = ['gold', 'yellow', 'lightyellow', 'khaki', 'greenyellow', \\\n","                'chartreuse', 'lime', 'lawngreen', 'darkgreen', 'forestgreen', \\\n","                'seagreen', 'mediumseagreen', 'springgreen', 'aquamarine', \\\n","                'turquoise', 'lightgreen', 'mediumspringgreen', 'cyan', 'skyblue', 'deepskyblue']\n","        for j,l in enumerate(small_idx):\n","            plt.axvline(x=l, ymin=0, ymax=1,color=green_colors[j],linestyle='-',label=str(j))\n","        plt.grid()\n","        plt.yticks(np.arange(0, 0.04, 0.01))\n","        plt.xlabel('sequential time id index')\n","        plt.ylabel(f'{set_name} rvol.')\n","        plt.title(f'Average {set_name} rvol. Darker reddish lines for 20 largest RMSPE time ids and greenish lines for 10 smallest RMSPE time ids')\n","        plt.legend()\n","        plt.show()\n","\n","        del all_stock_y_GroundTruth_df,all_stock_pred_df,avg_target_rvol\n","        gc.collect()\n","        return all_stock_y_GroundTruth_pivot[largest_20_rmspe_time_ids],  all_stock_pred_pivot[largest_20_rmspe_time_ids]\n","\n","\n","    def compute_model_bias_variance(self, y_test, y_train, X_train, best_mlxtend_rf_params):\n","\n","        ## model bias and variance measurement\n","        # estimate bias and variance\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","\n","        #full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","\n","        #X_train = full_train_df[self.feat_cols_list]\n","        X_train = X_train[self.feat_cols_list]\n","        #y_train = full_train_df[self.target_name] #target\n","        y_train = y_train[self.target_name]\n","        X_test = self.test_df[self.feat_cols_list]\n","        #y_test = self.test_df[self.target_name] #target\n","\n","        # Assuming best_mlxtend_rf_params contains the hyperparameters\n","        n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features = best_mlxtend_rf_params\n","\n","        # Create RandomForestRegressor model\n","        rf_model = RandomForestRegressor(\n","            n_estimators=n_estimators,\n","            max_depth=max_depth,\n","            min_samples_split=min_samples_split,\n","            min_samples_leaf=min_samples_leaf,\n","            max_features=max_features,\n","            random_state=42,\n","            n_jobs=-1\n","        )\n","\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp(self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train ** -2 * v1tr ** 2\n","        #w_test = y_test ** -2 * v1ts ** 2\n","\n","        # Train RandomForestRegressor model\n","        rf_model.fit(X_train.values, y_train.values / v1tr.values, sample_weight=w_train)\n","\n","        # Now you can use bias_variance_decomp\n","        mse, bias, var = bias_variance_decomp(rf_model, X_train.values, y_train.values / v1tr.values, X_test.values, y_test['target'].values / v1ts.values, loss='mse', num_rounds=30, random_seed=1)\n","        print('\\nMSE: %.3f' % mse)\n","        print('Bias: %.3f' % bias)\n","        print('Variance: %.3f' % var)\n","\n","        return\n","\n","\n","\n","    ############################################################################################################\n","    \"\"\"  TRAINING and TESTING SET COMPARISON functions START\"\"\"\n","    ############################################################################################################\n","\n","    def train_n_test_set_st_RMSPE_comparison(self,train_rmspe_per_stock,test_rmspe_per_stock):\n","        # Combine train and test RMSPE into a single DataFrame for easier plotting\n","        rmspe_df = pd.DataFrame({\n","            'Stock ID': train_rmspe_per_stock.index,\n","            'Train RMSPE': train_rmspe_per_stock['train_rmspe_per_stock'],\n","            'Test RMSPE': test_rmspe_per_stock['test_rmspe_per_stock']\n","        })\n","        fig = px.line(rmspe_df, x='Stock ID', y=['Train RMSPE', 'Test RMSPE'], \\\n","                      title=f'Train vs Test RMSPE per Stock,\\n Corrrelation between train and test RMSPE: {rmspe_df[\"Train RMSPE\"].corr(rmspe_df[\"Test RMSPE\"])}')\n","        fig.update_layout(xaxis=dict(tickvals=rmspe_df['Stock ID'], tickangle=90), yaxis_title='RMSPE', xaxis_title='Stock ID')\n","        fig.show()\n","        return\n","\n","\n","\n","    def train_n_test_set_picked_st_distribution_comparison(self,train_groundtruth, test_groundtruth , picked_stock_id):\n","\n","        # perform Kolmogorov-Smirnov test\n","        ks_stat, ks_pval = stats.ks_2samp(train_groundtruth, test_groundtruth)\n","        print(f\"Kolmogorov-Smirnov test: KS Statistic: {ks_stat}, P-Value: {ks_pval}\")\n","        # Check if the distributions are the same\n","        alpha = 0.05  # Significance level\n","        if ks_pval < alpha:\n","            print(\"The null hypothesis is rejected. The test set does not come from the same distribution as the train set.\")\n","            ks_test_stats = f\"K-S TEST: Train and Test sets are DIFFERENT\"\n","        else:\n","            print(\"The null hypothesis is accepted. The test set may come from the same distribution as the train set.\")\n","            ks_test_stats = f\"K-S TEST: Train and Test sets are SAME\"\n","\n","        # violin plot train_groundtruth and test_groundtruth on the same plot\n","        fig = go.Figure()\n","        fig.add_trace(go.Violin(x=['Train']*len(train_groundtruth), y=train_groundtruth, name='Train', box_visible=True, meanline_visible=True))\n","        fig.add_trace(go.Violin(x=['Test']*len(test_groundtruth), y=test_groundtruth, name='Test', box_visible=True, meanline_visible=True))\n","        train_stats = f\"Train - Mean: {np.mean(train_groundtruth):.4f}, Median: {np.median(train_groundtruth):.4f}, Std: {np.std(train_groundtruth):.4f}, Skew: {stats.skew(train_groundtruth):.4f}, Kurtosis: {stats.kurtosis(train_groundtruth):.4f}, 75th Percentile: {np.percentile(train_groundtruth, 75):.4f}\"\n","        test_stats = f\"Test - Mean: {np.mean(test_groundtruth):.4f}, Median: {np.median(test_groundtruth):.4f}, Std: {np.std(test_groundtruth):.4f}, Skew: {stats.skew(test_groundtruth):.4f}, Kurtosis: {stats.kurtosis(test_groundtruth):.4f}, 75th Percentile: {np.percentile(test_groundtruth, 75):.4f}\"\n","        fig.update_layout(\n","            title=f\"Stock {picked_stock_id}'s Train vs Test GroundTruth Distribution<br>{train_stats}<br>{test_stats}<br>   {ks_test_stats}\",\n","            yaxis=dict(tickmode='linear', tick0=0, dtick=0.002)  # Increase y tick resolution\n","        )\n","        fig.show()\n","        return\n","\n","\n","\n","    def each_time_id_RMSPE_across_stocks_comparison(self,gt_time_id_df, pred_time_id_df,set_name,y_train_df):\n","\n","        y_train_pivot = y_train_df.pivot(index='time_id', columns='stock_id', values='target')\n","\n","        for time_id in gt_time_id_df.columns.values:\n","            gt_df = pd.DataFrame({'gt':gt_time_id_df[time_id],'pred':pred_time_id_df[time_id]}, index=gt_time_id_df.index)\n","            plt.figure(figsize=(40, 10))\n","            plt.title(f'Time ID {time_id} GroundTruth vs Prediction on {set_name} set')\n","            plt.plot(gt_df.index, gt_df['gt'].values, label='GroundTruth', color='blue')\n","            plt.plot(gt_df.index, gt_df['pred'].values, label='Prediction', color='red')\n","            # ## Add box plot of y_train_pivot for each stock\n","            plt.boxplot([y_train_pivot[stock_id].dropna().values for stock_id in gt_df.index], positions=gt_df.index)\n","            plt.xticks(ticks=gt_df.index, labels=gt_df.index, rotation=90)\n","            plt.yticks(np.arange(0, 0.04, 0.002))\n","            plt.ylim(0, 0.04)\n","            plt.grid()\n","            plt.legend()\n","            plt.xlabel('Stock ID')\n","            plt.ylabel('Real. Vol.')\n","            plt.show()\n","\n","        del gt_time_id_df, pred_time_id_df, y_train_df,y_train_pivot\n","        return\n","\n","\n","    def val_set_n_test_set_representation_plot(self,mean_val_set_rmspe_error, test_set_rmspe_error, train_set_rmspe_error):\n","        ## check if average walk-forward validation set error and test set error are positively correlated.\n","        val_test_corr = np.corrcoef(mean_val_set_rmspe_error,test_set_rmspe_error)[0,1]\n","        fig, ax1 = plt.subplots()\n","        # Original scatter plot\n","        ax1.scatter(test_set_rmspe_error, mean_val_set_rmspe_error, alpha=0.4, color='blue', label='Mean Validation RMSPE')\n","        ax1.set_xlabel('Test Set RMSPE Error')\n","        ax1.set_ylabel('Mean Validation Set RMSPE Error', color='blue')\n","        ax1.tick_params(axis='y', labelcolor='blue')\n","        # Create twin axis\n","        ax2 = ax1.twinx()\n","        # Add train_set_rmspe_error to the twin axis\n","        ax2.scatter(test_set_rmspe_error, train_set_rmspe_error, alpha=0.4, color='red', label='Train RMSPE')\n","        ax2.set_ylabel('Train Set RMSPE Error', color='red')\n","        ax2.tick_params(axis='y', labelcolor='red')\n","        # Add correlation to the title\n","        train_test_corr = np.corrcoef(test_set_rmspe_error, train_set_rmspe_error)[0, 1]\n","        plt.title(f'Test Set RMSPE Error vs. Mean Validation/Train Set RMSPE Error \\n : val_test_corr: {val_test_corr:.3f} \\n train_test_corr {train_test_corr:.3f}')\n","        # Add legend\n","        plt.legend(loc='upper left')\n","        plt.show()\n","\n","        return\n","\n","\n","    ############################################################################################################\n","    \"\"\"  TRAINING and TESTING SET COMPARISON functions END\"\"\"\n","    ############################################################################################################\n","\n","\n","\n","    def evaluate_predictions(self,final_reg,test_pred, y_test,train_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,w_test,best_mlxtend_RF_params,mean_val_set_rmspe_error,raw_train_pred,raw_test_pred,raw_train_gn,raw_test_gn):\n","\n","        y_true = y_test\n","        y_pred = test_pred\n","        test_residuals = pd.DataFrame(y_true['target'] - y_pred['target'])\n","        test_residuals[['stock_id','time_id']] = y_true[['stock_id','time_id']]\n","        train_residuals = pd.DataFrame(y_train['target'] - train_pred['target'])\n","        train_residuals[['stock_id','time_id']] = y_train[['stock_id','time_id']]\n","        unique_stock_ids = self.train_stock_id.unique()\n","\n","        #all_stock_train_pred_df = self.compute_all_stock_train_pred_df(unique_stock_ids, train_pred)\n","        all_stock_train_pred_df = train_pred\n","        all_stock_raw_train_pred_df = raw_train_pred\n","        #all_stock_v1tr_df = self.compute_all_stock_v1tr_df(unique_stock_ids, v1tr)\n","        all_stock_v1tr_df = v1tr\n","        #all_stock_y_train_df = self.compute_all_stock_y_train_df(unique_stock_ids, y_train)\n","        all_stock_y_train_df = y_train\n","        all_stock_raw_train_gn_df = raw_train_gn\n","        #train_avg_target_rvol = self.compute_train_avg_target_rvol(unique_stock_ids, y_train)\n","        train_avg_target_rvol = y_train.groupby('time_id')['target'].mean().reindex(self.time_id_order)\n","        test_avg_target_rvol = y_test.groupby('time_id')['target'].mean() #.reindex(self.time_id_order)\n","        # train_avg_target_rvol = y_train.groupby('time_id')['target'].median().reindex(self.time_id_order) ## median\n","        # test_avg_target_rvol = y_test.groupby('time_id')['target'].median() #.reindex(self.time_id_order) ## median\n","\n","        #all_stock_test_pred_df = self.compute_all_stock_test_pred_df( unique_stock_ids, test_pred)\n","        all_stock_test_pred_df = test_pred\n","        all_stock_raw_test_pred_df = raw_test_pred\n","        #all_stock_y_test_df = self.compute_all_stock_y_test_df( unique_stock_ids, y_test)\n","        all_stock_y_test_df = y_test\n","        all_stock_raw_test_gn_df = raw_test_gn\n","\n","        all_stock_v1ts_df = v1ts\n","\n","\n","        print('\\n####################################### PREDICTION #################################################')\n","\n","        #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        #v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        print('corr(y_pred/v1ts, y_true/v1ts)',self.nancorr(       y_pred['target'].values/v1ts['wap1_log_price_ret_vol'] ,        y_true['target'].values/v1ts['wap1_log_price_ret_vol'] ))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred['target'].values/v1ts['wap1_log_price_ret_vol']), np.log(y_true['target'].values/v1ts['wap1_log_price_ret_vol'])))\n","        print('corr(y_pred, y_true)',self.nancorr(y_pred['target'].values, y_true['target'].values))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred['target'].values), np.log(y_true['target'].values)))\n","        train_score = np.mean( ((train_pred['target'].values-y_train['target'].values)/y_train['target'].values)**2 )**0.5\n","        print(f'RMSPE train score: ',  train_score)\n","        #train_set_rmspe_error.append(train_score)\n","        test_score = np.mean( ((y_pred['target'].values-y_true['target'].values)/y_true['target'].values)**2 )**0.5\n","        print(f'RMSPE test score: ',  test_score  )\n","        #test_set_rmspe_error.append(test_score)\n","\n","        ############################ SET PARAMETERS HERE ##############################\n","        ##### individual stock id analysis parameters START #####\n","        train_set_rmspe_error = [0.190656,0.190647,0.195972]\n","        test_set_rmspe_error = [0.230367,0.227526,0.232057]\n","        high_rmspe_stocks = ['31','37', '18','33', '112', '88','60','110','27', '9','16','30',  '103',  '5', '58', '89', '66', '40',  '0', '4', '75',  '90', '98']\n","        start_index = 9\n","        end_index = 10\n","        good_pdp_feature_name = ['tlog_target_vol_pcorr_3_clusters', 'v1proj_29', 'vol1_mean', 'wap1_log_price_ret_volstock_mean_from_25'][3]\n","        bad_up_pdp_feature_name = ['wap1_log_price_ret_volstock_mean_from_10',\n","                                'v1spprojt10f29','pear_corr_90_clusters_labels',\\\n","                                'robust_sum_stats_60_clusters_labels','pear_corr_3_clusters_labels',\\\n","                                'tlog_tlog1p_target_vol_robust_sum_stats_2_clusters',]\n","        bad_op_pdp_feature_name = ['root_book_delta_count',\\\n","                                'soft_stock_mean_tvpl2_:10','soft_stock_mean_tvpl2_:20' ,\\\n","                                'root_trade_count_smean']\n","        \n","        good_up_pdp_feature_name = ['soft_stock_mean_tvpl2_:20', 'tlog_tlinear_sad_ask_size2', 'tlog_tlinear_sad_bid_size1', 'tlog_tlinear_sad_size1', 'trade_count_15_15'][4]\n","        good_op_pdp_feature_name = ['tlog_eps523_trade_price_n_wap_eqi_price0_dev', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20', 'tvpl2_rmed2v1',\\\n","                                     'wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:0'][3]\n","\n","        top_RF_n_shap_feature_names = ['wap1_log_price_ret_volstock_mean_from_25', 'v1spprojt15f25', 'v1spprojt15f25_q1', 'soft_stock_mean_tvpl2_:20',\n","                                        'tlog_target_vol_pcorr_3_clusters', 'v1proj_29_15_q3', 'v1spprojt15f29', 'root_trade_count_smean',\n","                                        'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20',\n","                                        'wap1_log_price_ret_volstock_mean_from_20', 'pear_corr_90_clusters_labels',\n","                                        'robust_sum_stats_60_clusters_labels'][11]\n","\n","        decision_plot_feature_names = ['wap1_log_price_ret_volstock_mean_from_25', 'v1spprojt15f25', 'v1spprojt15f25_q1', 'soft_stock_mean_tvpl2_:20', 'tlog_target_vol_pcorr_3_clusters', 'pear_corr_90_clusters_labels', 'root_trade_count_smean', 'robust_sum_stats_60_clusters_labels', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_vol_ast_per_wap1_lprice_ret_vol_:20', 'v1proj_29_15_q3', 'log_liq2_ret_*_wap_eqi_price1_ret_vol_15_ratio_median_stock', 'wap1_log_price_ret_volstock_mean_from_20', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:0', 'vol1_mean', 'texp_wap1_log_price_ret_vol_from_20', 'v1spprojt15f29_q3', 'liqvol1_smean', 'soft_stock_mean_tvpl2_liqf_volf20', 'tvpl2', 'v1proj_29_q3', 'v1spprojt15f29', 'tvpl2_rmed2v1', 'tlog_eps523_trade_price_n_wap_eqi_price0_dev', 'root_trade_count', 'tvpl2_smean_vol', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:10', 'v1liq2sprojt5f25', 'wap1_log_price_ret_vol_from_25', 'tlog_target_vol_pcorr_10_clusters', 'v1proj_25_15_std', 'v1proj_29', 'tlog_wap1_log_price_ret_normalized*mean_centered_per_wap1_lprice_ret_vol_:20', 'tlog_tlog1p_target_vol_robust_sum_stats_2_clusters', 'pear_corr_3_clusters_labels', 'tlog_wap1_lprice_ret_vol_ati_*_wap1_lprice_ret_avg_ast_per_wap1_lprice_ret_vol_:20', 'v1liq2sprojt10f25', 'root_book_delta_count', 'wap1_log_price_ret_volstock_mean_from_10', 'wap1_log_price_ret_per_spread_sqr_vol_15_ratio_median_stock', 'pear_corr_10_clusters_labels', 'wap1_log_price_ret_volstock_mean_from_0', 'texp_wap1_log_price_ret_vol_from_10', 'v1spprojt10f29', 'wap1_log_price_ret_per_liq2_vol_15_ratio', 'log_wap1_log_price_ret_vol', 'v1proj_25', 'v1proj_25_std', 'tvpl2_rmed2v1lf25', 'tlog_tlog1p_target_vol_robust_sum_stats_4_clusters', 'tlog_eps523_trade_price_n_wap1_dev']\n","        ##### individual stock id analysis parameters END #####\n","\n","        ############################ SET PARAMETERS HERE ##############################\n","\n","        \"\"\" TRAINING SET PREDICTIONS START \"\"\"\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET #########################################################\n","        print('\\n########################################################################################################################')\n","        print('\\n####################################### TRAINING SET predictions START #################################################')\n","        print('\\n########################################################################################################################')\n","        ###################################################################################################################\n","\n","        ################################################################################################\n","        ############################## OVERALL STOCK ANALYSIS START ######################################\n","        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n","        #train_rmspe_per_stock = self.overall_stock_id_analysis(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals,train_flag=True)\n","        print('\\n####################################### OVERALL STOCK ANALYSIS END ######################################')\n","        ############################## OVERALL STOCK ANALYSIS END ######################################\n","        ################################################################################################\n","\n","\n","        ################################################################################################\n","        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n","        #### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n","        # for picked_stock_id in high_rmspe_stocks[start_index:end_index]:\n","        #     train_groundtruth = self.individual_stock_id_analysis(int(picked_stock_id),unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,train_residuals,train_avg_target_rvol,train_flag=True)\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n","        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS START ##################################\n","        ################################################################################################\n","        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n","        #train_gt_time_id_df, train_pred_time_id_df = self.overall_time_id_analysis(all_stock_y_train_df,all_stock_train_pred_df,train_avg_target_rvol, train_flag=True)\n","        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY START #########################################\n","        ###################################################################################################################\n","        print('\\n###################################### Feature importance & SHAPLEY START #########################################')\n","        print('\\n#---------------------------------------------compute_global_SHAP_values----------------------------------------------------------------#')\n","        train_shap_values_all = self.compute_global_SHAP_values(final_reg,X_train,raw_train_gn,raw_train_pred,v1tr,set_name='train')\n","        os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n","        ##os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n","        with open('train_shap_values_all_st31_rf_model.pkl', 'wb') as file:\n","            pickle.dump(train_shap_values_all, file)\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","\n","        print('\\n#---------------------------------------------analyze_global_SHAP_values----------------------------------------------------------------#')\n","        # os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n","        # #os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n","        # with open('train_shap_values_all_st31_rf_model.pkl', 'rb') as file:\n","        #     train_shap_values_all = pickle.load( file)\n","        # self.analyze_global_SHAP_values(train_shap_values_all,all_stock_raw_train_gn_df,all_stock_raw_train_pred_df,set_name='train')\n","        # top_n_feats = 50\n","        # self.compute_shapley_barplot(train_shap_values_all,top_n_feats,X_train,y_train['target'],stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='train')\n","        # self.compute_shapley_PDP_n_Scatter(top_RF_n_shap_feature_names,train_shap_values_all,all_stock_raw_train_gn_df,all_stock_raw_train_pred_df,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='train')\n","        # self.compute_shapley_decision(train_shap_values_all.base_values,train_shap_values_all,decision_plot_feature_names,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='train')\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","\n","        print('\\n#---------------------------------------------compute_individual_stock_SHAP_values----------------------------------------------------------------#')\n","        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n","        stock_id = 0\n","        view_time_ids_start = 0\n","        view_time_ids_end = 500\n","        #self.compute_individual_stock_SHAP_values(final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","\n","        print('\\n#---------------------------------------------compute_individual_instance_SHAP_values----------------------------------------------------------------#')\n","        # ovearll_error_idxs, up_error_idxs, op_error_idxs = self.identiy_largest_overall_n_under_n_over_prediction_errors(all_stock_y_train_df,all_stock_train_pred_df,set_name='train')\n","        # self.shapley_analysis_of_large_error_instances(ovearll_error_idxs, up_error_idxs, op_error_idxs,train_shap_values_all,set_name='train')\n","        # print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","        # print('\\n###################################### Feature importance & SHAPLEY END #########################################')\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY END ###########################################\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE START ################################################\n","        ###################################################################################################################\n","\n","        ### Plot top 30 feature importances\n","        print('\\n###################################### MODEL BIAS VARINANCE START #################################################')\n","        # fig, axx = plt.subplots(figsize=(10, 10))\n","        # importances = final_reg.feature_importances_\n","        # indices = np.argsort(importances)[-30:]\n","        # plt.title('Feature Importances')\n","        # plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","        # plt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\n","        # plt.xlabel('Relative Importance')\n","        # plt.show()\n","\n","        # # Corrected feature importance extraction\n","        # feature_importance = pd.DataFrame({'feature': X_train.columns, 'importance': importances})\n","        # feature_importance = feature_importance.sort_values('importance', ascending=False)\n","        # print('\\n ------ Random Forest Feature Importance ------\\n')\n","        # print(feature_importance)\n","        # print('\\n Top 25 Random Forest features\\n')\n","        # print(list(feature_importance['feature'][:25].values))\n","        # print('\\n Bottom 25 Random Forest features\\n')\n","        # print(list(feature_importance['feature'][-25:].values))\n","        # self.compute_model_bias_variance(y_test, y_train, X_train, best_mlxtend_RF_params)\n","        print('\\n###################################### MODEL BIAS VARINANCE END #################################################')\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE END #################################################\n","        ##################################################################################################################\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","        print('\\n####################################### TRAINING SET predictions END #################################################\\n')\n","        print('\\n######################################################################################################################\\n')\n","\n","\n","\n","        \"\"\" TESTING SET PREDICTIONS START \"\"\"\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS START ########################################\n","        print('\\n#######################################################################################################################')\n","        print('\\n####################################### TESTING SET predictions START #################################################')\n","        print('\\n#######################################################################################################################')\n","        ###################################################################################################################\n","\n","        ################################################################################################\n","        ############################## OVERALL STOCK ANALYSIS START ######################################\n","        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n","        # test_rmspe_per_stock = self.overall_stock_id_analysis(unique_stock_ids,all_stock_test_pred_df,all_stock_y_test_df,test_residuals,train_flag=False)\n","        print('\\n####################################### OVERALL STOCK ANALYSIS END ######################################')\n","        ############################## OVERALL STOCK ANALYSIS END ######################################\n","        ################################################################################################\n","\n","\n","\n","        ################################################################################################\n","        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n","        ##### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n","        # for picked_stock_id in high_rmspe_stocks[start_index:end_index]:\n","        #     test_groundtruth = self.individual_stock_id_analysis(int(picked_stock_id),unique_stock_ids,all_stock_y_test_df,all_stock_test_pred_df,test_residuals,test_avg_target_rvol,train_flag=False)\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n","        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS START ##################################\n","        ################################################################################################\n","        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n","        #test_gt_time_id_df, test_pred_time_id_df = self.overall_time_id_analysis(all_stock_y_test_df,all_stock_test_pred_df,test_avg_target_rvol, train_flag=False)\n","        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### TEST Feature importance & SHAPLEY START #########################################\n","        ###################################################################################################################\n","\n","        print('\\n###################################### TEST Feature importance & SHAPLEY START #########################################')\n","        print('\\n#---------------------------------------------compute_global_SHAP_values----------------------------------------------------------------#')\n","        test_shap_values_all = self.compute_global_SHAP_values(final_reg,X_test,raw_test_gn,raw_test_pred,v1ts,set_name='test')\n","        os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n","        with open('test_shap_values_all_st31_rf_model.pkl', 'wb') as file:\n","            pickle.dump(test_shap_values_all, file)\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","\n","        print('\\n#---------------------------------------------analyze_global_SHAP_values----------------------------------------------------------------#')\n","        # os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/Final_submission_data/shapley_values')\n","        # # #os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n","        # with open('test_shap_values_all_st31_rf_model.pkl', 'rb') as file:\n","        #     test_shap_values_all = pickle.load( file)\n","        # self.analyze_global_SHAP_values(test_shap_values_all,all_stock_raw_test_gn_df,all_stock_raw_test_pred_df,set_name='test')\n","        # self.compute_shapley_barplot(test_shap_values_all,top_n_feats,X_test,y_test['target'],stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='test')\n","        # self.compute_shapley_PDP_n_Scatter(top_RF_n_shap_feature_names,test_shap_values_all,all_stock_raw_test_gn_df,all_stock_raw_test_pred_df,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='test')\n","        # self.compute_shapley_decision(test_shap_values_all.base_values,test_shap_values_all,decision_plot_feature_names,stock_id=None,view_time_ids_start=None,view_time_ids_end=None,set_name='test')\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","\n","        print('\\n#---------------------------------------------compute_individual_stock_SHAP_values-------------------------------------------------------#')\n","        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n","        stock_id = 0\n","        view_time_ids_start = 0\n","        view_time_ids_end = 500\n","        #self.compute_individual_stock_SHAP_values(final_reg,X_test,all_stock_test_pred_df,all_stock_v1tr_df,all_stock_y_test_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","\n","\n","        print('\\n#---------------------------------------------compute_individual_instance_SHAP_values-------------------------------------------------------#')\n","        #ovearll_error_idxs, up_error_idxs, op_error_idxs = self.identiy_largest_overall_n_under_n_over_prediction_errors(all_stock_y_test_df,all_stock_test_pred_df,set_name='test')\n","        #self.shapley_analysis_of_large_error_instances(ovearll_error_idxs, up_error_idxs, op_error_idxs,test_shap_values_all,set_name='test')\n","        print('\\n#-----------------------------------------------------------------------------------------------------------------#')\n","        print('\\n###################################### TEST Feature importance & SHAPLEY END #########################################')\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY END #########################################\n","        ###################################################################################################################\n","\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","        print('##################################################################################################')\n","        print('\\n####################################### TESTING SET predictions END #################################################\\n')\n","\n","\n","\n","\n","        \"\"\" TRAINING AND TESTING SET PERFORMANCE COMPARISON START \"\"\"\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING AND TESTING SET PERFORMANCE COMPARISON START ##########################################\n","        ###################################################################################################################\n","        print('\\n####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON START #################################################')\n","        #self.train_n_test_set_st_RMSPE_comparison(train_rmspe_per_stock,test_rmspe_per_stock)\n","        print('##################################################################################################')\n","        print('\\n####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON END #################################################\\n')\n","\n","        ###################################################################################################################\n","        print('\\n####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON START #################################################')\n","        #self.train_n_test_set_picked_st_distribution_comparison(train_groundtruth, test_groundtruth,high_rmspe_stocks[start_index:end_index])\n","        print('##################################################################################################')\n","        print('\\n####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON END #################################################\\n')\n","        ###################################################################################################################\n","\n","\n","        ###################################################################################################################\n","        print('##################################################################################################')\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on train set START ########################')\n","        #self.each_time_id_RMSPE_across_stocks_comparison(train_gt_time_id_df, train_pred_time_id_df,set_name='train',y_train_df = all_stock_y_train_df)\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on train set END ########################')\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on test set START ##########################\\n')\n","        #self.each_time_id_RMSPE_across_stocks_comparison(test_gt_time_id_df, test_pred_time_id_df,set_name='test',y_train_df = all_stock_y_train_df)\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on test set END ##########################\\n')\n","        print('##################################################################################################')\n","        ###################################################################################################################\n","\n","\n","        ###################################################################################################################\n","        print('\\n####################################### Representativeness of validation set in test set START #################################################')\n","        ###################################################################################################################\n","        #self.val_set_n_test_set_representation_plot(mean_val_set_rmspe_error, test_set_rmspe_error, train_set_rmspe_error)\n","        ###################################################################################################################\n","        print('\\n####################################### Representativeness of validation set in test set END #################################################')\n","        ###################################################################################################################\n","\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING AND TESTING SET COMPARISON END ##########################################\n","        ###################################################################################################################\n","\n","\n","        del X_train,y_train, all_stock_train_pred_df, all_stock_v1tr_df ,  all_stock_test_pred_df, all_stock_y_train_df,  all_stock_y_test_df\n","        del y_true, y_pred, test_residuals, train_residuals, unique_stock_ids, all_stock_v1ts_df, train_avg_target_rvol, test_avg_target_rvol\n","        gc.collect()\n","        return train_shap_values_all, test_shap_values_all\n","\n","\n","\n","    def visualize_tree(self,):\n","        # feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n","        # feature_importances.to_csv('feature_importances.csv')\n","        # plt.figure(figsize=(16, 12))\n","        # sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(20), x='average', y='feature')\n","        # plt.title('20 TOP feature importance over {} folds average'.format(folds.n_splits));\n","\n","        # importances = pd.DataFrame({'Feature': model.feature_name(),\n","        #                             'Importance': sum( [model.feature_importance(importance_type='gain') for model in models] )})\n","        # importances2 = importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n","        # importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n","\n","        #TODO: #plot decision tree for interpretability\n","\n","        return\n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"id":"8e78ed9d","metadata":{"id":"8e78ed9d"},"outputs":[],"source":["\n","\n","def objective_st31(trial):\n","\n","  t_v_t = train_validate_n_test(df_train_reordered_for_stock_31, df_test_for_stock_31)\n","\n","  ######  SET Hyperparameter's range for tuning ######\n","\n","  # #Hyperparameters and algorithm parameters are described here\n","  params = {\n","      'n_estimators' : trial.suggest_int('n_estimators', 1000, 4000, step=50),  # Range of trees\n","      'max_depth' : trial.suggest_int('max_depth', 20, 60),  # Control tree depth\n","      'max_leaves' : trial.suggest_int('max_leaves', 20, 200) , # Maximum leaves\n","      'max_features' : trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']) , # Feature selection\n","      'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10) , # Minimum samples in leaf\n","      'min_samples_split' : trial.suggest_int('min_samples_split', 1, 20),  # Minimum samples to split\n","  }\n","\n","\n","  # # #hyperparameter shortlisting\n","  # params = {\n","  #     'n_estimators' : trial.suggest_int('n_estimators', 1700, 4000, step=50),  # Range of trees\n","  #     'max_depth' : trial.suggest_int('max_depth', 30, 60),  # Control tree depth\n","  #     'max_leaves' : trial.suggest_int('max_leaves', 90, 200) , # Maximum leaves\n","  #     'max_features' : trial.suggest_categorical('max_features', [ 'sqrt', 'log2']) , # Feature selection\n","  #     'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 5) , # Minimum samples in leaf\n","  #     'min_samples_split' : trial.suggest_int('min_samples_split', 10, 20),  # Minimum samples to split\n","  # }\n","\n","\n","\n","  ######  SET Hyperparameter's range for tuning ######\n","\n","  val_avg_error = t_v_t.RF_train_validate(params, trial)\n","  print(f\"val_avg_error: {val_avg_error}\")\n","\n","\n","  del t_v_t\n","  gc.collect()\n","  return val_avg_error\n"]},{"cell_type":"code","execution_count":null,"id":"59525d4a","metadata":{"id":"59525d4a"},"outputs":[],"source":["\n","\n","#if __name__ == \"__main__\":\n","\n","#optuna.logging.set_verbosity(optuna.logging.WARNING)\n","# study_name= 'Correct_residual_autocorrrelation_HAR_n_target_lag_feat_n_target_pred'\n","\n","mean_val_set_rmspe_error = [ 0.226999,0.226498,0.226883]\n","\n","study = optuna.create_study(study_name ='Random Forest' ,direction=\"minimize\")\n","study.optimize(objective_st31, n_trials=75) # 50\n","\n","pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"Trial no.: \",trial.number)\n","print(\"  Value: \", trial.value)\n","mean_val_set_rmspe_error.append(trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))\n","\n","#print(\"Best hyperparameters:\", study.best_params)\n","\n","fig = optuna.visualization.plot_parallel_coordinate(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_optimization_history(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_slice(study)\n","fig.show()\n","\n","fig = optuna.visualization.plot_param_importances(study)\n","fig.show()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6d620dba","metadata":{"id":"6d620dba"},"outputs":[],"source":["\n","seed1 = 11\n","missing_value = -np.inf  # Replace with a suitable value\n","\n","\n","############ Best parameters Manual Start ############\n","# n_estimators=\n","# max_depth= \n","# min_samples_split= \n","# min_samples_leaf= \n","############ Best parameters Manual End ############\n","\n","############ Best parameters Automatic Start ############\n","best_trial = study.best_trial\n","seed1 = 11\n","\n","n_estimators =  best_trial.params['n_estimators']\n","max_depth = best_trial.params['max_depth']\n","min_samples_split =  best_trial.params['min_samples_split']\n","min_samples_leaf =  best_trial.params['min_samples_leaf']\n","max_features = best_trial.params['max_features']\n","max_leaves = best_trial.params['max_leaves']\n","############ Best parameters Automatic End ############\n","\n","\n","\n","\n","\n","best_mlxtend_RF_params = [n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features,max_leaves]\n","\n","best_params = { 'n_estimators': n_estimators,\n","              \"max_depth\": max_depth,\n","            \"min_samples_split\": min_samples_split,\n","            \"min_samples_leaf\" : min_samples_leaf,\n","            \"max_features\": max_features,\n","            \"max_leaves\": max_leaves,\n","               }\n","\n","\n","t_v_t = train_validate_n_test(df_train_reordered_for_stock_31, df_test_for_stock_31)\n","final_reg,train_pred,test_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,target_name,raw_train_pred, raw_test_pred,raw_train_gn,target_shift = t_v_t.make_predictions(best_params)\n","#target_name = 'target'\n","\n","train_pred = pd.DataFrame(train_pred.to_pandas()).rename(columns={0:'target'})\n","train_pred['time_id'] = df_train_reordered_for_stock_31['time_id'].astype(int)\n","train_pred['stock_id'] = df_train_reordered_for_stock_31['stock_id'].astype(int)\n","\n","raw_train_pred = pd.DataFrame(raw_train_pred.to_pandas()).rename(columns={0:'target'})\n","raw_train_pred['time_id'] = df_train_reordered_for_stock_31['time_id'].astype(int)\n","raw_train_pred['stock_id'] = df_train_reordered_for_stock_31['stock_id'].astype(int)\n","\n","y_train = pd.DataFrame(y_train).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n","y_train['time_id'] = df_train_reordered_for_stock_31['time_id'].astype(int)\n","y_train['stock_id'] = df_train_reordered_for_stock_31['stock_id'].astype(int)\n","\n","\n","raw_train_gn = pd.DataFrame(raw_train_gn.to_pandas()).rename(columns={0:'target'})\n","raw_train_gn['time_id'] = df_train_reordered_for_stock_31['time_id'].astype(int)\n","raw_train_gn['stock_id'] = df_train_reordered_for_stock_31['stock_id'].astype(int)\n","\n","v1tr = pd.DataFrame(v1tr.to_pandas())\n","v1tr['time_id'] = df_train_reordered_for_stock_31['time_id'].astype(int)\n","v1tr['stock_id'] = df_train_reordered_for_stock_31['stock_id'].astype(int)\n","v1tr['wap1_log_price_ret_vol'] = v1tr['log_wap1_log_price_ret_vol']\n","v1tr.drop(columns=['log_wap1_log_price_ret_vol'], inplace=True)\n","\n","test_pred = pd.DataFrame(test_pred.to_pandas()).rename(columns={0:'target'})\n","test_pred['time_id'] = df_test_for_stock_31['time_id'].astype(int)\n","test_pred['stock_id'] = df_test_for_stock_31['stock_id'].astype(int)\n","\n","raw_test_pred = pd.DataFrame(raw_test_pred.to_pandas()).rename(columns={0:'target'})\n","raw_test_pred['time_id'] = df_test_for_stock_31['time_id'].astype(int)\n","raw_test_pred['stock_id'] = df_test_for_stock_31['stock_id'].astype(int)\n","\n","## # Merge the DataFrames on 'time_id' and 'stock_id' columns\n","os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data')\n","#os.chdir('/content/drive/MyDrive/optiver_real_vol/Final Submission/kaggle_submission_versions')\n","\n","v1ts = pd.DataFrame(v1ts.to_pandas())\n","v1ts['time_id'] = df_test_for_stock_31['time_id'].astype(int)\n","v1ts['stock_id'] = df_test_for_stock_31['stock_id'].astype(int)\n","v1ts['wap1_log_price_ret_vol'] = v1ts['log_wap1_log_price_ret_vol']\n","v1ts.drop(columns=['log_wap1_log_price_ret_vol'], inplace=True)\n","\n","train = pd.read_csv('train.csv')\n","X_test['time_id'] = df_test_for_stock_31['time_id'].astype('int64')\n","X_test['stock_id'] = X_test['stock_id'].astype('int64')\n","y_test_df = pd.merge(X_test[['time_id', 'stock_id']], train[['time_id', 'stock_id', 'target']], on=['time_id', 'stock_id'], how='left')\n","y_test_df['time_id'] = y_test_df['time_id'].astype(int)\n","y_test_df['stock_id'] = y_test_df['stock_id'].astype(int)\n","w_test = y_test_df[target_name] **-2 * v1ts['wap1_log_price_ret_vol']**2\n","X_test.drop(columns=['time_id'], inplace=True)\n","\n","raw_test_gn = pd.DataFrame()\n","raw_test_gn['target'] = np.log(y_test_df['target'].values / v1ts['wap1_log_price_ret_vol'].values) + target_shift\n","raw_test_gn['time_id'] = y_test_df['time_id'].astype(int)\n","raw_test_gn['stock_id'] = y_test_df['stock_id'].astype(int)\n"]},{"cell_type":"code","execution_count":null,"id":"87731b00","metadata":{},"outputs":[],"source":["train_pred_stock_31 = train_pred[train_pred['stock_id'] == 31].copy()\n","test_pred_stock_31 = test_pred[test_pred['stock_id'] == 31].copy()\n","\n","y_train_stock_31 = y_train[y_train['stock_id'] == 31].copy()\n","y_test_stock_31 = y_test_df[y_test_df['stock_id'] == 31].copy()\n","\n","rmspe_train = np.sqrt(np.mean((((train_pred_stock_31['target'].values - y_train_stock_31['target'].values)/y_train_stock_31['target'].values) ** 2)))\n","print(rmspe_train)\n","rmspe_test = np.sqrt(np.mean((((test_pred_stock_31['target'].values - y_test_stock_31['target'].values)/y_test_stock_31['target'].values) ** 2)))\n","print(rmspe_test)\n","\n","\n","print('Train RMSPE for st 31 only:', rmspe_train)\n","print('Test RMSPE  for st 31 only:', rmspe_test)\n"]},{"cell_type":"code","execution_count":null,"id":"a082387f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":14593,"status":"ok","timestamp":1728511916091,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"a082387f","outputId":"553214b9-a4f6-4ad6-d6a1-dc08eeaa9095"},"outputs":[],"source":["mean_val_set_rmspe_error = [ 0.226999,0.226498,0.226883]\n","\n","train_shap_values_all, test_shap_values_all = t_v_t.evaluate_predictions(final_reg,test_pred, y_test_df,train_pred,y_train,X_train,X_test,v1tr,v1ts,w_train,w_test,best_mlxtend_RF_params,mean_val_set_rmspe_error,raw_train_pred,raw_test_pred,raw_train_gn,raw_test_gn)\n"]},{"cell_type":"code","execution_count":null,"id":"92056f84","metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":2344753,"sourceId":27233,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"cuml_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"},"papermill":{"default_parameters":{},"duration":15331.990222,"end_time":"2024-09-16T06:04:13.265071","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-16T01:48:41.274849","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
