{"cells":[{"cell_type":"code","execution_count":1,"id":"3c4fec69","metadata":{"executionInfo":{"elapsed":6578,"status":"ok","timestamp":1727078808790,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"3c4fec69"},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","import matplotlib.pyplot as plt\n","import statsmodels.api as sm\n","import plotly.subplots as sub_plots\n","import plotly.graph_objects as go\n","import statsmodels.api as sm\n","import scipy.stats as stats\n","\n","from sklearn.cluster import KMeans\n","import re\n","\n","import warnings\n","#warnings.filterwarnings(\"ignore\")\n","from sklearn.metrics import confusion_matrix\n","#from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","\n","import glob\n","import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","from numba import jit, njit\n","import numba as nb\n","import plotly_express as px\n","from itertools import combinations, permutations, product, combinations_with_replacement\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from scipy.signal import find_peaks\n","import pickle\n","from joblib import Parallel, delayed\n","import seaborn as sns\n","from sklearn import model_selection\n","from sklearn.metrics import r2_score\n","import gc\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","import scipy as sp\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from sklearn.cluster import SpectralClustering, MiniBatchKMeans, MeanShift, AgglomerativeClustering\n","from sklearn.mixture import GaussianMixture\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","from scipy.spatial.distance import squareform\n","from scipy.stats import skew, kurtosis\n","import shap\n","from datetime import datetime\n","import ipywidgets as widgets\n","from matplotlib.patches import Rectangle\n","import xgboost as xgb\n","from sklearn.preprocessing import OneHotEncoder\n","from xgboost import plot_tree, plot_importance\n","from sklearn.model_selection import RepeatedKFold, cross_val_score, TimeSeriesSplit\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.genmod.generalized_linear_model import GLM\n","import warnings\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.utils import class_weight\n","import optuna\n","from optuna.trial import TrialState\n","from xgboost import XGBRegressor\n","from mlxtend.evaluate import bias_variance_decomp\n","import re\n","\n","from matplotlib.pyplot import cm\n","\n","\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import minmax_scale\n","\n"]},{"cell_type":"markdown","id":"4ab61745","metadata":{"id":"4ab61745"},"source":["## **Model Training**"]},{"cell_type":"code","execution_count":2,"id":"Oj0U2qtRrYr3","metadata":{"executionInfo":{"elapsed":2938,"status":"ok","timestamp":1727078811723,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"Oj0U2qtRrYr3"},"outputs":[],"source":["os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data/Final_submission_data/partial_train_n_full_test')\n","\n","with open('train_feat_df_reordered.pkl','rb') as f:\n","  train_feat_df_reordered = pickle.load(f)\n","\n","with open('test_feat_df.pkl','rb') as f:\n","  test_feat_df = pickle.load(f)\n","\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/kaggle/input/optiver-realized-volatility-prediction')\n","\n","##### remove test from training data #####\n","\n","df_train_reordered = train_feat_df_reordered.copy()\n","del train_feat_df_reordered\n","\n","df_test = test_feat_df.copy()\n","del test_feat_df"]},{"cell_type":"code","execution_count":264,"id":"b29976a7","metadata":{"executionInfo":{"elapsed":1434,"status":"ok","timestamp":1727078820208,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"b29976a7"},"outputs":[],"source":["class train_validate_n_test(object):\n","\n","    def __init__(self,df_train_reordered, df_test) -> None:\n","\n","        #self.time_id_order = df.loc[:3829,'time_id'].values # select ordered unique time_ids\n","        #self.train_time_id_ind = int(len(self.time_id_order)*0.7)\n","\n","        largest_num_time_id_stock = df_train_reordered.groupby('stock_id')['time_id'].apply(lambda x: x.nunique()).argmax()\n","        self.time_id_order = df_train_reordered[df_train_reordered['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n","        self.n_folds = 10\n","        folds = TimeSeriesSplit(n_splits=self.n_folds,)# max_train_size=None, gap=10)\n","        #self.splits = folds.split( range( self.train_time_id_ind ) ) # split 70% train time_ids into n_fold splits\n","        nunique_train_time_ids = df_train_reordered['time_id'].nunique()\n","        self.splits = folds.split( range( nunique_train_time_ids ) )\n","\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        #self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        #self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","        self.train_stock_id = df_train_reordered['stock_id']\n","        self.train_time_id = df_train_reordered['time_id']\n","\n","        # test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        # self.test_df = df[df['time_id'].isin(test_time_ids)]\n","        self.test_time_id = df_test[df_test['stock_id'] == largest_num_time_id_stock]['time_id'].values # select reordered unique time_ids\n","        self.test_df = df_test\n","        self.test_stock_id = self.test_df['stock_id']\n","        self.test_time_id = self.test_df['time_id']\n","\n","        #self.df = df\n","        self.df_train_reordered = df_train_reordered\n","\n","        # feature_importances = pd.DataFrame()\n","        cols = list(df_train_reordered.columns)\n","        cols.remove('tlog_target')\n","        cols.remove('target')\n","        cols.remove('time_id')\n","        self.feat_cols_list =  cols #cat_feat_labels+float32_feat_labels+float64_feat_labels # int32_feat_labels+int64_feat_labels+float32_feat_labels+float64_feat_labels\n","        # feature_importances['feature'] = self.feat_cols_list\n","\n","        self.target_name = 'target' # _standardized' log target is easier to transform back than log_target_standardized\n","\n","        #del df\n","        del df_train_reordered\n","        gc.collect()\n","\n","    # def onehotencode_cat_var(self,full_set):\n","    #     full_set = cat_feat_labels #full_set.astype({\"stn_id\":str,\"block_id\":str,\"ts_of_day\":str,\"hr_of_day\":str,\"day_of_wk\":str,\"day_of_mn\":str,\"wk_of_mon\":str })\n","    #     full_set = pd.get_dummies(full_set, prefix_sep=\"_\",columns =cat_feat_labels,drop_first=True)\n","    #     #ds_df = ds_df.drop('rem_blk_outf_'+self.stn,axis=1)\n","    #     return full_set\n","\n","    #### RMSPE cost function\n","    def rmspe(self,y_true, y_pred):\n","        return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n","\n","\n","    # Custom RMSPE objective function\n","    def rmspe_objective(self,preds, dtrain):\n","        labels = dtrain.get_label()\n","        errors = (preds - labels) / labels\n","        gradient = 2 * errors / (1 + errors**2)\n","        hessian = 2 * (1 - errors**2) / (1 + errors**2)**2\n","        return gradient, hessian\n","\n","\n","    def xgb_RMSPE(self,preds, train_data):\n","        labels = train_data.get_label()\n","        return 'RMSPE', round(self.rmspe(y_true = labels, y_pred = preds),5)\n","\n","\n","    def nancorr(self,a, b):\n","        v = np.isfinite(a)*np.isfinite(b) > 0\n","        return np.corrcoef(a[v], b[v])[0,1]\n","\n","\n","    def xgb_train_validate(self,params_xgb,n_rounds,esr,trial):\n","        rmspe_val_score = []\n","        models= []\n","        test_y_preds = np.zeros(len(self.test_df))\n","        best_iterations = []\n","        learning_train_rmspe = []\n","        learning_val_rmspe = []\n","\n","        for fold_n, (train_index, valid_index) in enumerate(self.splits):\n","            print('Fold:',fold_n+1)\n","            # print('train_index',train_index)\n","            # print('valid_index',valid_index)\n","            train_time_ids = self.time_id_order[train_index]\n","            val_time_ids = self.time_id_order[valid_index]\n","            train_df = self.df_train_reordered[self.df_train_reordered['time_id'].isin(train_time_ids)]\n","            val_df = self.df_train_reordered[self.df_train_reordered['time_id'].isin(val_time_ids)]\n","\n","            X_train = train_df[self.feat_cols_list]\n","            y_train = train_df[self.target_name] # target\n","            X_valid = val_df[self.feat_cols_list]\n","            y_val = val_df[self.target_name] # target\n","\n","            v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n","            v1v = np.exp(  X_valid['log_wap1_log_price_ret_vol']) # double exponential to nullify the log\n","\n","            # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n","            # v1v = np.exp(np.exp(  X_valid['log_wap1_log_price_ret_vol'])) # double exponential to nullify the log\n","            #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","\n","            w_train = y_train **-2 * v1tr**2\n","            w_val = y_val **-2 * v1v**2\n","\n","            print('Training....')\n","            dtrain = xgb.DMatrix(X_train, label=y_train/v1tr,weight=w_train,enable_categorical=True)\n","            dvalid = xgb.DMatrix(X_valid,   label=  y_val/v1v,weight=w_val,enable_categorical=True)\n","            watchlist  = [(dtrain,'train_loss_fold_'+str(fold_n+1)), (dvalid, 'val_loss_fold_'+str(fold_n+1))]\n","            evals_result = {}\n","            reg = xgb.train(params=params_xgb, dtrain=dtrain, num_boost_round=n_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE,  evals_result=evals_result,maximize=False,  early_stopping_rounds=esr,verbose_eval=False)\n","            learning_train_rmspe.append(evals_result['train_loss_fold_'+str(fold_n+1)])\n","            learning_val_rmspe.append(evals_result['val_loss_fold_'+str(fold_n+1)])\n","\n","            models.append(reg)\n","            best_iterations.append(reg.best_iteration)\n","\n","            p = reg.predict(dvalid)*v1v\n","            val_score =  np.mean( ((p-y_val)/y_val)**2 )**0.5\n","\n","            # full_score += y_val.shape[0]*score**2\n","\n","            print(f'fold: {fold_n+1}, val rmspe score is {val_score}')\n","            print('corr(p/v1v, y_val/v1v)',self.nancorr(       p/v1v ,        y_val/v1v ))\n","            print('log(corr( ))',self.nancorr(np.log(p/v1v), np.log(y_val/v1v)))\n","            print('corr(p, y_val)',self.nancorr(p, y_val))\n","            print('log(corr( ))',self.nancorr(np.log(p), np.log(y_val)))\n","\n","            #test_pred = reg.predict(self.test_df[self.feat_cols_list] )*v1ts ## this method is not suitable for Timeseries cross validation because initial splits are too far from test set.\n","            #test_y_preds += test_pred/self.n_folds\n","\n","            rmspe_val_score.append(val_score)\n","\n","        mean_rmspe_val_score = np.mean(rmspe_val_score)\n","        print(f'mean rmspe val score over {self.n_folds} splits is',mean_rmspe_val_score)\n","        #print(f'mean rmspe test score: ',  np.mean( ((test_y_preds-self.test_df[self.target_name])/self.test_df[self.target_name])**2 )**0.5  ) # target\n","\n","        # Plot learning curves\n","        fig,ax = plt.subplots(2,1,figsize=(10,6))\n","        for fold_n in range(len(rmspe_val_score)):\n","            ax[0].plot(learning_train_rmspe[fold_n]['RMSPE'], label=f'Fold {fold_n+1} Train RMSPE')\n","            ax[0].plot(learning_val_rmspe[fold_n]['RMSPE'],linestyle='dashed', label=f'Fold {fold_n+1} Validation RMSPE')\n","        last_fold = len(rmspe_val_score) - 1\n","        ax[1].plot(learning_val_rmspe[last_fold]['RMSPE'],linestyle='dashed', label=f'Fold {last_fold+1} Validation RMSPE')\n","        ax[1].set_xlabel('Boosting Round')\n","        ax[0].set_ylabel('RMSPE')\n","        ax[1].set_ylabel('RMSPE')\n","        ax[0].legend()\n","        ax[1].legend()\n","        ax[0].grid(True)\n","        ax[1].grid(True)\n","        fig.suptitle(f'Learning Curves, Trial: {trial.number}')\n","        fig.show()\n","\n","        del self.df_train_reordered, X_train, X_valid, y_train, y_val,train_df,val_df,dtrain,dvalid, v1tr, v1v\n","        gc.collect()\n","        return mean_rmspe_val_score,best_iterations[-1]\n","\n","\n","    def manual_shapley_addivity_check(self,model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name):\n","\n","        y_train_true = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","                #### ONLY for Explainer\n","        shap_pred = ( shap_values.base_values + shap_values.values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","                #### ONLY for TreeExplainer\n","        #shap_pred = ( model_base_value + shap_values.sum(axis=1) )* all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","\n","        #print('shap_values.sum(axis=1)',shap_values.sum(axis=1))\n","        # print('shap_values.base_values',shap_values.base_values[0])\n","        # print('shap_values.values',shap_values.values[0].sum())\n","        #print('len(shap_values.values.sum(axis=1))',len(shap_values.values.sum(axis=1)))\n","\n","        model_shap_rmspe = self.rmspe(model_pred, shap_pred)\n","\n","        fig, ax = plt.subplots(2,1,figsize=(30,10))\n","        ax[0].plot(np.arange(0,len(y_train_true)),y_train_true,label='true rvol.',linestyle='dashed',c='g',marker='*',alpha=0.2)\n","        ax[0].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.6)\n","        ax[0].set_title(f'True Rvol. Vs. model predicted Rvol.' )\n","        ax[0].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[0].set_ylabel('rvol.')\n","        ax[0].legend()\n","        ax[0].grid(True)\n","\n","        ax[1].plot(np.arange(0,len(model_pred)),model_pred,label='model prediction',linestyle='dashed',c='b',marker='*',alpha=0.4)\n","        ax[1].plot(np.arange(0,len(shap_pred)),shap_pred,label='summed shap values prediction',linestyle='dashed',c='r',marker='*',alpha=0.4)\n","        ax[1].set_title(f'Check additivity of shap values, RMSPE:{model_shap_rmspe} between model and shap values prediction' )\n","        ax[1].text(0,0.01,f\"stock_id: {stock_id}, view_time_ids_start: {view_time_ids_start}, view_time_ids_end:{view_time_ids_end}\")\n","        ax[1].set_ylabel('rvol.')\n","        ax[1].legend()\n","        ax[1].grid(True)\n","        fig.tight_layout()\n","        fig.show()\n","\n","\n","        del all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_PDP_n_Scatter(self,feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df):\n","        ####### compute partial dependence plot of most important features\n","\n","        ###### Partial dependence plot\n","        #fig,ax = plt.subplots()\n","        #shap.plots.partial_dependence(feature_name, model.predict, xgb.DMatrix(X_train,enable_categorical=True), model_expected_value=True, feature_expected_value=True)\n","        #fig.show()\n","\n","        ##### scatter plot\n","        print(f'\\n scatter plot of {feature_name} vs. shap values')\n","        print(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #fig,ax = plt.subplots()\n","        shap.plots.scatter(shap_values[:,feature_name])\n","        #fig.show()\n","\n","\n","        ##### scatter plot of feature vs. True target rvol. on trianing set\n","        fig,ax = plt.subplots()\n","        yval = all_stock_y_train_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        xval = X[feature_name]\n","        ax.scatter(xval,yval)\n","        ax.plot([min(xval), max(xval)], [min(yval),max(yval)], color = 'red', linewidth = 1)\n","        ax.set_xlabel(feature_name)\n","        ax.set_ylabel('True target rvol.')\n","        ax.grid\n","        ax.set_title(f'scatter plot of {feature_name} Vs. True Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        ##### scatter plot of feature vs. predicted target rvol.on trianing set\n","        fig,ax = plt.subplots()\n","        yval1 = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        ax.scatter(xval,yval1)\n","        ax.plot([min(xval), max(xval)], [min(yval1),max(yval1)], color = 'red', linewidth = 1)\n","        ax.set_xlabel(feature_name)\n","        ax.set_ylabel('Predicted target rvol.')\n","        ax.grid\n","        ax.set_title(f'scatter plot of {feature_name} Vs. Predicted Rvol. for stock_id: {stock_id}, from {view_time_ids_start} to {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","    def compute_shapley_beeswarm(self,X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # shap.summary_plot(shap_values, X)\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.beeswarm(shap_values)\n","        #ax.set_title(f' stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","    def compute_shapley_barplot(self,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for TreeExplainer\n","        # plt.figure()\n","        # stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # shap_values = np.multiply(shap_values.T ,stock_v1tr_df).T\n","        # plt.bar(shap_values.abs().sum(axis=1))\n","        # plt.title(f'stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # plt.show()\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values,)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del all_stock_v1tr_df,shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_individual_stock_SHAP_values(self,final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        stock_id = stock_id\n","        view_time_ids_start = view_time_ids_start\n","        view_time_ids_end = view_time_ids_end\n","        X = X_train[X_train['stock_id'].isin([stock_id])].iloc[view_time_ids_start:view_time_ids_end]\n","\n","        ###### Explainer #######\n","        explainer = shap.Explainer(final_reg,X)\n","        shap_values = explainer(np.array(X),check_additivity=False)\n","        shap_values.feature_names = final_reg.feature_names\n","\n","        ###### TreeExplainer #######\n","        # explainer = shap.TreeExplainer(final_reg,feature_perturbation='interventional')\n","        # shap_values = explainer.shap_values(np.array(X),check_additivity=False)\n","        # shap_values.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","        # print(f'Model base value: {model_base_value} before scaling by v1tr')\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.manual_shapley_addivity_check(model_base_value,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,shap_values,stock_id,view_time_ids_start,view_time_ids_end,feature_name )\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values.base_values = shap_values.base_values * all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        stock_v1tr_df = all_stock_v1tr_df[stock_id].values[view_time_ids_start:view_time_ids_end] # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values.values = np.multiply(shap_values.values.T ,stock_v1tr_df).T\n","        ###### check correctness of shap_values\n","        # sp = shap_values.base_values + shap_values.values.sum(axis=1)\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(len(sp)),sp)\n","        # model_pred = all_stock_train_pred_df[stock_id].values[view_time_ids_start:view_time_ids_end]\n","        # plt.plot(range(len(sp)), model_pred )\n","        # plt.show()\n","\n","        self.compute_shapley_beeswarm(X,shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        #shap_values = final_reg.predict(dtrain, pred_contribs=True)\n","        ### Calculate SHAP values for a specific instance (e.g., the first test instance)\n","        ### shap_values = explainer.shap_values(X_test.iloc[0])\n","\n","        self.compute_shapley_barplot(shap_values,all_stock_v1tr_df,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        self.compute_shapley_heatmap(shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df)\n","\n","        ####### INDIVIDUAL feature contributions ##############################\n","        ####### compute partial dependence plot of most important features\n","        self.compute_shapley_PDP_n_Scatter(feature_name,shap_values,stock_id,view_time_ids_start,view_time_ids_end,X,all_stock_y_train_df,all_stock_train_pred_df)\n","\n","        #self.compute_shapley_decision(model_base_value,shap_values.data,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        ##### force plot has some error\n","        #self.compute_shapley_force(model_base_value,shap_values.data,X,shap_values.feature_names,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","\n","        del final_reg,X_train\n","        gc.collect()\n","        return\n","\n","\n","    def compute_overall_SHAP_values(self,final_reg,X_train,y_train,train_pred,v1tr):\n","\n","        print(f'\\nGround-Truth Rvol. grand average on train set: {y_train.values.mean()}')\n","        print(f'\\nModel Prediction Rvol. grand average on train set: {train_pred.values.mean()}')\n","\n","        # plot shapley feature importances for all samples\n","        final_reg.set_param({\"device\": \"cuda\"})\n","        shap.initjs()\n","\n","        X = X_train\n","\n","        ###### Explainer #######\n","        explainer = shap.Explainer(final_reg,X)\n","        shap_values_all = explainer(np.array(X),check_additivity=False)\n","        shap_values_all.feature_names = final_reg.feature_names\n","\n","        model_base_value = explainer.expected_value\n","\n","        ####### GLOBAL ALL feature contributions ##############################\n","        ###### Do manual additivity check because it fails\n","        self.overall_manual_shapley_addivity_check(train_pred,v1tr,shap_values_all)\n","\n","        ####### Manually correct the shap values to accomodate v1tr scaling\n","        shap_values_all.base_values = shap_values_all.base_values * v1tr.values  # all_stock_v1tr_df.iloc[view_time_ids_start:view_time_ids_end,stock_id].values\n","        shap_values_all.values = np.multiply(shap_values_all.values.T ,v1tr.values).T\n","\n","        ###### Beeswarm plot\n","        #### ONLY for Explainer\n","        print(f'\\n Overall Beeswarm plot for all stock ids and time ids')\n","        shap.plots.beeswarm(shap_values_all)\n","        #ax.set_title(f'\\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","\n","        ###### Bar plot MEAN Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all)# clustering=clustering)\n","        #ax.title(f'MEAN ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        ###### Bar plot MAXIMUM Absolute value of features\n","        #### ONLY for Explainer\n","        print(f'\\nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","        #clustering = shap.util.hclust(X,y)\n","        shap.plots.bar(shap_values_all.abs.max(0), )#clustering=clustering)\n","        #ax.title(f'nMAXIMUM ABSOLUTE of feature bar plot for all stock ids and time ids')\n","\n","        del final_reg,X_train,y_train,train_pred,v1tr\n","        gc.collect()\n","        return\n","\n","\n","    def overall_manual_shapley_addivity_check(self,train_pred,v1tr,shap_values_all):\n","\n","        #### ONLY for Explainer\n","        shap_pred_all = ( shap_values_all.base_values + shap_values_all.values.sum(axis=1) ) * v1tr #pd.DataFrame(all_stock_v1tr_df.values.ravel() , columns=['v1tr_all'])['v1tr_all'].values\n","\n","        model_shap_rmspe_all = self.rmspe(train_pred, shap_pred_all)\n","        print(f'\\n Check Additivity of shap values in all stock and time ids, model_shap_rmspe_all: {model_shap_rmspe_all}')\n","\n","        del train_pred,v1tr,shap_values_all\n","        gc.collect()\n","        return\n","\n","\n","\n","    def compute_shapley_heatmap(self,shap_values,stock_id,view_time_ids_start,view_time_ids_end,all_stock_train_pred_df):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        #print(' NOTE: Heatmap is sorted with f(X) from smallest values to biggest value !! (picture is wrong)')\n","        # fig,ax = plt.subplots(figsize=(13.5,2))\n","        # y_asc = np.sort( all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values )\n","        # ax.plot( range(len(y_asc)), y_asc, color='g')\n","        # ax.axhline(y_asc.mean(),color='r', linestyle='dashed')\n","        # ax.set_ylabel('Correct f(x) in Asc. order')\n","        # ax.set_yticks(np.arange(0,max(y_asc),0.002))\n","        # fig.show()\n","\n","        fig,ax = plt.subplots()\n","        # order = np.argsort(all_stock_train_pred_df.iloc[ view_time_ids_start : view_time_ids_end ,stock_id].values)\n","        shap.plots.heatmap(shap_values,instance_order=shap_values.sum(1))\n","        #ax.title(f'\\nHEAT MAP \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values,all_stock_train_pred_df\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_decision(self,model_base_value,shap_values,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        #### ONLY for Explainer\n","        print(f'\\nDECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig,ax = plt.subplots()\n","        shap.plots.decision(model_base_value, shap_values,feature_names=feature_names)\n","        #ax.title(f'\\n DECISION PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        fig.show()\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","    def compute_shapley_force(self,model_base_value,shap_values,X,feature_names,stock_id,view_time_ids_start,view_time_ids_end):\n","\n","        # ### ONLY for Explainer\n","        # print(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig,ax = plt.subplots()\n","        # shap.plots.force(model_base_value,shap_values=shap_values[0],features=X[0],feature_names=feature_names, show=True) #matplotlib=True,\n","        # ax.title(f'\\n FORCE PLOT (IGNORE Magnitude, only see relative magnitude) \\n stock id: {stock_id}, view_time_ids_start:  {view_time_ids_start}, view_time_ids_end: {view_time_ids_end}')\n","        # fig.show()\n","\n","        # example_index = 0  # You can change this index to any other example\n","        # example = X[example_index]\n","        # # Explain the prediction of the example\n","        # shap.force_plot(explainer.expected_value, shap_values[example_index], example, feature_names=data.feature_names)\n","\n","        del shap_values\n","        gc.collect()\n","        return\n","\n","\n","\n","\n","    def make_predictions(self,best_params,num_rounds ):\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        #full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","        full_train_df = self.df_train_reordered\n","\n","        X_train = full_train_df[self.feat_cols_list]\n","        y_train = full_train_df[self.target_name] # target\n","        X_test = self.test_df[self.feat_cols_list]\n","        #y_test = self.test_df[self.target_name] # target\n","\n","        # v1tr = np.exp(np.exp(X_train['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        # v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train **-2 * v1tr**2\n","        #w_test = y_test **-2 * v1ts**2\n","\n","        print('Final model')\n","        dtrain = xgb.DMatrix(X_train, label=y_train/v1tr,weight=w_train,enable_categorical=True )\n","        #dtest = xgb.DMatrix(X_test, label=y_test/v1ts,weight=w_test,enable_categorical=True )\n","        dtest = xgb.DMatrix(X_test,enable_categorical=True )\n","        watchlist  = [(dtrain,'train_loss')]\n","        evals_result = {}\n","        final_reg = xgb.train(params=best_params, dtrain=dtrain, num_boost_round=num_rounds, evals=watchlist, obj=self.rmspe_objective,custom_metric=self.xgb_RMSPE, evals_result=evals_result,maximize=False, verbose_eval=False)\n","        #test_error = evals_result['test_loss']\n","        train_pred = final_reg.predict(dtrain)*v1tr\n","        test_pred = final_reg.predict( dtest )*v1ts\n","\n","\n","\n","\n","        del full_train_df#,X_train,X_test #,feat_names\n","        gc.collect()\n","\n","        return final_reg,test_pred,train_pred,y_train,X_train,X_test,v1tr,w_train\n","\n","\n","    # def compute_train_avg_target_rvol(self, unique_stock_ids, y_train):\n","    #     # unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","    #     unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","    #     train_target_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","    #     for s in unique_stock_ids:\n","    #         st_index = self.train_stock_id == s\n","    #         t_index = self.train_time_id[st_index]\n","    #         train_target_df.loc[t_index, s] = y_train[st_index].values\n","    #     train_avg_target_rvol = train_target_df.ffill().bfill().mean(axis=1)\n","    #     return train_avg_target_rvol\n","\n","    # def compute_test_avg_target_rvol(self, unique_stock_ids, y_test):\n","    #     #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","    #     unique_test_time_ids = self.test_time_id\n","    #     test_target_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","    #     for s in unique_stock_ids:\n","    #         st_index = self.test_stock_id == s\n","    #         t_index = self.test_time_id[st_index]\n","    #         test_target_df.loc[t_index, s] = y_test[st_index].values\n","    #     test_avg_target_rvol = test_target_df.ffill().bfill().mean(axis=1)\n","    #     return test_avg_target_rvol\n","\n","\n","    def fraction_above_average(self,signal1, avg):\n","        # Count the fraction of times when signal1 is above signal2\n","        fraction_above_avg = (signal1 > avg).mean()\n","        return fraction_above_avg\n","\n","\n","    def compute_all_stock_v1tr_df(self, unique_stock_ids, v1tr):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_v1tr_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_v1tr_df.loc[t_index, s] = v1tr[st_index].values\n","        all_stock_v1tr_df = all_stock_v1tr_df.ffill().bfill()\n","        return all_stock_v1tr_df\n","\n","    def compute_all_stock_train_pred_df(self, unique_stock_ids, train_pred):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_train_pred_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_train_pred_df.loc[t_index, s] = train_pred[st_index].values\n","        all_stock_train_pred_df = all_stock_train_pred_df.ffill().bfill()\n","        return all_stock_train_pred_df\n","\n","    def compute_all_stock_test_pred_df(self, unique_stock_ids, test_pred):\n","        # unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        all_stock_test_pred_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_test_pred_df.loc[t_index, s] = test_pred[st_index].values\n","        all_stock_test_pred_df = all_stock_test_pred_df.ffill().bfill()\n","        return all_stock_test_pred_df\n","\n","\n","    def compute_all_stock_y_train_df(self, unique_stock_ids, y_train):\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_y_train_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_y_train_df.loc[t_index, s] = y_train[st_index].values\n","        all_stock_y_train_df = all_stock_y_train_df.ffill().bfill()\n","        return all_stock_y_train_df\n","\n","    def compute_all_stock_y_test_df(self, unique_stock_ids, y_test):\n","        #unique_test_time_ids = self.time_id_order[self.train_time_id_ind:]\n","        unique_test_time_ids = self.test_time_id\n","        all_stock_y_test_df = pd.DataFrame(index=unique_test_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.test_stock_id == s\n","            t_index = self.test_time_id[st_index]\n","            all_stock_y_test_df.loc[t_index, s] = y_test[st_index].values\n","        all_stock_y_test_df = all_stock_y_test_df.ffill().bfill()\n","        return all_stock_y_test_df\n","\n","\n","\n","    ######## Identify stocks belonging to clusters based on clusterings in dataset\n","    ######## find stock ids of clusters having same feature values\n","    ######## This is reverse-engineering cluster labels of already clustered stocks\n","    def calculate_cluster_fraction(self, column, n_clusters, stock_list):\n","        \"\"\" This function computes the fraction of stock ids in stock_list inside a cluster in the clustering feature.\n","        The fraction is between 0 - 1. 1 indicates all the stock ids in stock_list are in a particular cluster.\n","        \"\"\"\n","\n","        # self.train_stock_id = df[df['time_id'].isin(train_time_ids)]['stock_id']\n","        # self.train_time_id = df[df['time_id'].isin(train_time_ids)]['time_id']\n","\n","        # unique_stock_ids = self.train_stock_id.unique()\n","        # time_id_order = df2.loc[:3829,'time_id'].values\n","        # train_time_id_ind = int(len(time_id_order)*0.7)\n","\n","        # train_time_ids = time_id_order[:train_time_id_ind]\n","        # train_stock_id = df2[df2['time_id'].isin(train_time_ids)]['stock_id']\n","        # train_time_id = df2[df2['time_id'].isin(train_time_ids)]['time_id']\n","\n","        unique_stock_ids = self.train_stock_id.unique()\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","\n","        train_col_df = self.df[self.df['time_id'].isin(train_time_ids)][column]\n","\n","        ## reshape the dataframe\n","        #unique_train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        unique_train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","        all_stock_column_df = pd.DataFrame(index=unique_train_time_ids, columns=unique_stock_ids)\n","        for s in unique_stock_ids:\n","            st_index = self.train_stock_id == s\n","            t_index = self.train_time_id[st_index]\n","            all_stock_column_df.loc[t_index, s] = train_col_df[st_index].values\n","        all_stock_column_df = all_stock_column_df.ffill().bfill()\n","\n","        features = all_stock_column_df.T.to_numpy()\n","\n","        ## kmeans\n","        kmeans = KMeans(n_clusters=n_clusters,n_init=10)\n","        kmeans.fit(features)\n","        cluster_labels = kmeans.labels_\n","        cluster_labels\n","\n","        clusters_dict = {}\n","        unique_labels = np.unique(cluster_labels)\n","        for label in unique_labels:\n","            indices = np.where(cluster_labels == label)[0]\n","            stocks_in_cluster = unique_stock_ids[indices]\n","            clusters_dict[label] = stocks_in_cluster.tolist()\n","\n","        for c in clusters_dict.keys():\n","            cnt=0\n","            for s in stock_list:\n","                if s in clusters_dict[c]:\n","                    cnt+=1\n","            print(f'cluster: {c}, # stock ids in cluster: {cnt}, clustering fraction: {cnt/len(clusters_dict[c])}')\n","\n","        return\n","\n","\n","\n","    def check_stock_list_in_all_clustering_features(self, stock_list):\n","\n","        clustering_features_list = [    \"log_target_vol_corr_32_clusters_stnd\",\n","                                        \"log_target_vol_sum_stats_16_clusters_stnd\",\n","                                        \"sum_stats_4_clusters_labels\",\n","                                        \"sum_stats_10_clusters_labels\",\n","                                        \"sum_stats_16_clusters_labels\",\n","                                        \"sum_stats_30_clusters_labels\",\n","                                        \"pear_corr_32_clusters_labels\",\n","                                        \"pear_corr_4_clusters_labels\",\n","                                        \"pear_corr_49_clusters_labels\",\n","                                        \"pear_corr_90_clusters_labels\",]\n","\n","        print('stock_list: ' , stock_list)\n","        for feature in clustering_features_list:\n","            n_clusters = int(re.findall(r'\\d+', feature)[0])\n","            print('Feature: ', feature)\n","            print('Cluster Fractions: ')\n","            print(self.calculate_cluster_fraction( feature, n_clusters, stock_list))\n","\n","        return\n","\n","\n","    def compute_acf_pacf(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        plt.close('all')\n","        for s in unique_stock_ids[0:1]:#[0:40]:\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            plot_acf(stock_residual, lags=200,ax=ax[0])\n","            plot_pacf(stock_residual, lags=200,ax=ax[1])\n","            ax[0].set_title(f'Autocorrelation of stock {s} Residuals on train set')\n","            ax[1].set_title(f'Partial Autocorrelation of stock {s} Residuals on train set')\n","            ax[0].set_xticks(range(0,200,5))\n","            ax[1].set_xticks(range(0,200,5))\n","            ax[0].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_yticks(np.arange(-1, 1, 0.1))\n","            ax[1].set_xlabel('lags')\n","            ax[0].set_ylabel('ACF')\n","            ax[1].set_ylabel('PACF')\n","            ax[0].grid(True)\n","            ax[1].grid(True)\n","            fig.show()\n","        return\n","\n","\n","\n","    def compute_IFFT(self,unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df):\n","\n","        ##### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        ##### IFFT plot of reconstructed time series ######\n","        plt.close('all')\n","        for s in unique_stock_ids[100:]:#[40:112]:\n","            stock_residual = all_stock_train_pred_df[s]-all_stock_y_train_df[s]\n","            x = stock_residual.values\n","            limit = 0.00001\n","\n","            n=len(x)\n","            fhat = np.fft.fft(x,n)\n","            PSD = fhat*np.conj(fhat) / n\n","            freq = (1/n)*np.arange(n)\n","            start=1 #ignore dc component\n","            L = np.arange(start,np.floor(n/2),dtype='int')\n","            # fig,ax = plt.subplots(figsize=(30,6))\n","            # #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            # ax.axhline(limit,  color='k', linestyle='-')\n","            # ax.plot(freq[L],PSD[L])\n","            # ax.set_xlabel('freq')\n","            # ax.set_ylabel('mag')\n","            # ax.set_title(f'mag plot of stock: {s} residual')\n","            # fig.show()\n","\n","            indices = PSD > limit\n","            num_freqs = len(np.where(indices>0)[0])\n","            print('# of frequencies in residual = ',num_freqs)\n","\n","            fhat = fhat*indices\n","            fig,ax = plt.subplots(2,1,figsize=(30,6))\n","            ffilt = np.fft.ifft(fhat)\n","            ax[0].plot(np.arange(0,len(x)),ffilt.real,label='top '+str(num_freqs)+' frequencies in residual (train set)',c='g',alpha=1)\n","            ax[0].plot(np.arange(0,len(x)),x,label='original residual',c='r',alpha=0.2)\n","            ax[0].legend()\n","            ax[0].grid()\n","            ax[0].set_xlabel('time id')\n","            ax[0].set_ylabel('residual')\n","            ax[0].set_title(f'IFFT of stock: {s} residual')\n","\n","\n","            x1 = all_stock_y_train_df[s].values\n","            limit1 = 0.00001\n","            n1=len(x1)\n","            fhat1 = np.fft.fft(x1,n1)\n","            PSD1 = fhat1*np.conj(fhat1) / n1\n","            freq1 = (1/n1)*np.arange(n1)\n","            start1=1 #ignore dc component\n","            L1 = np.arange(start1,np.floor(n1/2),dtype='int')\n","            fig1,ax1 = plt.subplots(figsize=(30,6))\n","            #ax.plot(freq[L],np.array([15]*len(freq[L]))) # line at 15\n","            ax1.axhline(limit1,  color='k', linestyle='-')\n","            ax1.plot(freq1[L],PSD1[L])\n","            ax1.set_xlabel('freq')\n","            ax1.set_ylabel('mag')\n","            ax1.set_title(f'mag plot of stock: {s} rvol.')\n","            fig1.show()\n","\n","            indices1 = PSD1 > limit1\n","            num_freqs1 = len(np.where(indices1>0)[0])\n","            print('# of frequencies in rvol. = ',num_freqs1)\n","            fhat1 = fhat1*indices1\n","            ffilt1 = np.fft.ifft(fhat1)\n","            ax[1].plot(np.arange(0,len(x1)),ffilt1.real,label='top '+str(num_freqs1)+' frequencies in true rvol. (train set)',c='g',alpha=1)\n","            ax[1].plot(np.arange(0,len(x1)),x1,label='original true rvol.',c='r',alpha=0.2)\n","            ax[1].legend()\n","            ax[1].grid()\n","            ax[1].set_xlabel('time id')\n","            ax[1].set_ylabel('rvol.')\n","            ax[1].set_title(f'IFFT of stock: {s} rvol.')\n","            fig.show()\n","        return\n","\n","\n","\n","\n","    def calculate_total_gap(self,data):\n","        # 1. Order the ground truth in ascending order\n","        sorted_data = np.sort(data)\n","        \n","        # 2. Take up to the 75th percentile (remove upper outliers)\n","        percentile_75 = np.percentile(sorted_data, 75)\n","        filtered_data = sorted_data[sorted_data <= percentile_75]\n","        \n","        # 3. Take first differences\n","        first_differences = np.diff(filtered_data)\n","        \n","        # 4. Order the first differences in descending order\n","        sorted_differences = np.sort(first_differences)[::-1]  # Sort in descending order\n","        \n","        # 5. Take the sum of the first differences up to the 50th percentile\n","        length = len(sorted_differences)\n","        cutoff_index = int(0.5 * length)  # 50th percentile index\n","        total_gap = np.sum(sorted_differences[:cutoff_index])\n","        \n","        return total_gap\n","\n","\n","\n","\n","    def overall_stock_id_analysis(self,unique_stock_ids,all_stock_pred_df,all_stock_y_GroundTruth_df,residuals,train_flag):\n","        \n","        if train_flag:\n","            set_name = 'train'\n","        else:\n","            set_name = 'test'\n","\n","        all_stock_pred_pivot = all_stock_pred_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_pred_pivot = all_stock_pred_pivot.reindex(self.time_id_order)\n","\n","        all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_pivot.reindex(self.time_id_order)\n","\n","\n","        rmspe_per_stock = []\n","        total_gap_per_stock = []\n","        for s in unique_stock_ids:\n","            rmspe_per_stock.append( np.nanmean( ((all_stock_pred_pivot[s]-all_stock_y_GroundTruth_pivot[s])/all_stock_y_GroundTruth_pivot[s])**2 )**0.5  )\n","            total_gap_per_stock.append( self.calculate_total_gap(all_stock_y_GroundTruth_pivot[s].values) )\n","\n","\n","        ###### Bar plot of RMSPE for all stocks in the training set\n","        all_stock_rmspe = pd.Series(rmspe_per_stock,index=unique_stock_ids)\n","        smallest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index.values[:10]\n","        largest_10_rmspe_stocks = all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:10]\n","        fig, ax = plt.subplots(figsize=(40,10))\n","        ax.text(0,0.52,f'10 largest RMSPE stocks: {largest_10_rmspe_stocks}')\n","        ax.text(0,0.62,f'10 smallest RMSPE stocks: {smallest_10_rmspe_stocks}')\n","        ax.bar(unique_stock_ids, rmspe_per_stock)\n","        ax.set_xticks(unique_stock_ids)\n","        ax.tick_params(axis='x', rotation=45)\n","        ax.set_yticks(np.arange(0, max(rmspe_per_stock) + 0.01, 0.04))\n","        ax.grid()\n","        ax.set_title(f'RMSPE of Real. Vol. for each stock on {set_name} set')\n","        ax.set_xlabel('Stock ID')\n","        ax.set_ylabel('RMSPE')\n","        plt.show()\n","        plt.close()\n","        ## check if the largest and smallest fall into a cluster of a clustering feature\n","        print('\\n 10_largest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = largest_10_rmspe_stocks)\n","        print('\\n 10_smallest_rmspe_stocks in clustering feature')\n","        #self.check_stock_list_in_all_clustering_features(stock_list = smallest_10_rmspe_stocks)\n","        print(f'30_largest_rmspe_stocks in {set_name} set: ',all_stock_rmspe.sort_values(ascending=True).index[::-1].values[:30])\n","        \n","        \n","        ####### plot total_gap in descending order for all stocks  ######\n","        all_stock_total_gap = pd.Series(total_gap_per_stock,index=unique_stock_ids)\n","        smallest_10_total_gap_stocks = all_stock_total_gap.sort_values(ascending=True).index.values[:10]\n","        largest_10_total_gap_stocks = all_stock_total_gap.sort_values(ascending=True).index[::-1].values[:10]\n","        fig, ax = plt.subplots(figsize=(40,10))\n","        ax.text(4, max(total_gap_per_stock),f'10 largest total gap stocks: {largest_10_total_gap_stocks}')\n","        ax.text(4, max(total_gap_per_stock)-0.003,f'10 smallest total gap stocks: {smallest_10_total_gap_stocks}')\n","        sorted_indices = np.argsort(total_gap_per_stock)[::-1]\n","        sorted_unique_stock_ids = unique_stock_ids[sorted_indices]\n","        sorted_total_gap_per_stock = np.array(total_gap_per_stock)[sorted_indices]\n","        ax.bar(range(len(sorted_unique_stock_ids)), sorted_total_gap_per_stock,tick_label=sorted_unique_stock_ids)\n","        ax.tick_params(axis='x', rotation=45)\n","        ax.set_yticks(np.arange(0, max(sorted_total_gap_per_stock) + 0.001, 0.001))\n","        ax.grid()\n","        ax.set_title(f'Total Gap of Real. Vol. for each stock on GroundTruth {set_name} set')\n","        ax.set_xlabel('Stock ID')\n","        ax.set_ylabel('Total Gap')\n","        plt.show()\n","        plt.close()\n","        print(f'30_largest_total_gap_stocks in {set_name} set: ',all_stock_total_gap.sort_values(ascending=True).index[::-1].values[:30])\n","        \n","\n","        groundtruth = all_stock_y_GroundTruth_df['target'].values\n","        prediction = all_stock_pred_df['target'].values\n","        residuals = residuals['target'].values # copy is made, its not modified inplace\n","\n","        ####### scatter plot of True Real. Vol. vs. Pred Real. Vol.\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(groundtruth, prediction, c='b', alpha=0.1)\n","        ax.plot(groundtruth, groundtruth, c='r',linestyle='solid' )\n","        ax.set_title(f'Scatter Plot of True vs Predicted Values on {set_name} set')\n","        ax.set_xlabel(f'True {set_name} rvol. Values')\n","        ax.set_ylabel(f'Predicted {set_name} rvol. Values')\n","        plt.show()\n","        plt.close()\n","\n","        ## y_train and train_pred Distributions Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( groundtruth,bins=1000, color='green', alpha=0.9, histtype='bar', rwidth=0.8, label='GroundTruth')\n","        plt.hist( prediction,bins=1000, color='red', alpha=0.3, ec='r', label='Prediction')\n","        ax.set_title(f'Distribution of GroundTruth (skew: {stats.skew(groundtruth)} , kurt:{stats.kurtosis(groundtruth)}) and Prediction (skew: {stats.skew(prediction)} , kurt:{stats.kurtosis(prediction)}) on {set_name} set')\n","        ax.set_xlabel(f' GroundTruth and Prediction on {set_name} set')\n","        ax.set_ylabel('frequency')\n","        plt.legend(loc=\"upper left\")\n","        plt.show()\n","        plt.close()\n","\n","        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(groundtruth, residuals, c='c',alpha=0.1 )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n","        ax.set_title(f' True R.V. Vs. Residuals Values Plot on {set_name} set')\n","        ax.set_xlabel(f'True {set_name} Values')\n","        ax.set_ylabel(f'{set_name} residuals')\n","        plt.show()\n","        plt.close()\n","\n","\n","        ####### scatter plot of Fitted rvol. Values Vs. train residuals Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        ax.scatter(prediction, residuals, c='m',alpha=0.1 )\n","        ax.axhline(y=0, color='g', linestyle='-')\n","        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n","        ax.set_title(f' fitted R.V. Vs. Residuals Values Plot on {set_name} set')\n","        ax.set_xlabel(f'fitted {set_name} Values')\n","        ax.set_ylabel(f'{set_name} residuals')\n","        plt.show()\n","        plt.close()\n","\n","        ## Normal Q-Q Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        sm.qqplot(residuals, line='q', ax=ax)\n","        ax.set_title(f'QQ Plot of Residuals on {set_name} set')\n","        ax.set_xlabel('Theoretical Quantiles')\n","        ax.set_ylabel('Sample Quantiles')\n","        plt.show()\n","        plt.close()\n","\n","        ## Residuals Distribution Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( residuals,bins=1000)\n","        ax.set_title(f'Distribution of Residuals on {set_name} set')\n","        ax.set_xlabel(f'{set_name} Residuals')\n","        ax.set_ylabel('frequency')\n","        plt.show()\n","        plt.close()\n","\n","        del all_stock_pred_df,all_stock_y_GroundTruth_df,residuals,all_stock_y_GroundTruth_pivot,all_stock_pred_pivot,groundtruth,prediction\n","        gc.collect()\n","        return pd.DataFrame({f'{set_name}_rmspe_per_stock':rmspe_per_stock}, index=unique_stock_ids)\n","\n","\n","\n","\n","    def individual_stock_id_analysis(self,picked_stock_id,unique_stock_ids,all_stock_y_GroundTruth_df,all_stock_pred_df,residuals,avg_target_rvol,train_flag):\n","\n","        if train_flag:\n","            set_name = 'train'\n","        else:\n","            set_name = 'test'\n","\n","        all_stock_pred_pivot = all_stock_pred_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_pred_pivot = all_stock_pred_pivot.reindex(self.time_id_order)\n","\n","        all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='time_id', columns='stock_id', values='target')\n","        if train_flag:\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_pivot.reindex(self.time_id_order)\n","\n","        groundtruth = all_stock_y_GroundTruth_pivot[picked_stock_id].values\n","        prediction = all_stock_pred_pivot[picked_stock_id].values\n","        picked_st_residuals = groundtruth - prediction\n","\n","        ## 1. scatter plot\n","        plt.figure(figsize=(10,10))\n","        plt.scatter(all_stock_y_GroundTruth_pivot[picked_stock_id],all_stock_pred_pivot[picked_stock_id], c='blue',label=picked_stock_id, alpha=0.4)\n","        plt.plot(all_stock_y_GroundTruth_pivot[picked_stock_id],all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='red',label=picked_stock_id, alpha=1 )\n","        plt.grid()\n","        plt.xlabel(f'GroundTruth')\n","        plt.ylabel(f'Prediction')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s scatter plot of y_{set_name} vs. {set_name}_pred on {set_name} set\")\n","        plt.show()\n","        plt.close()\n","\n","        ## 2. Line plot of true vs average real. vol.\n","        fraction_above_avg = self.fraction_above_average(all_stock_y_GroundTruth_pivot[picked_stock_id], avg_target_rvol)\n","        plt.figure(figsize=(30,5))\n","        plt.text(1,all_stock_y_GroundTruth_pivot[picked_stock_id].max(),f\"fraction of times this stock's values are above all stocks' avg_target_rvol = {fraction_above_avg}\")\n","        plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.4 )\n","        plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),[avg_target_rvol]*len(all_stock_y_GroundTruth_pivot[picked_stock_id]),linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n","        plt.grid()\n","        plt.xlabel('index')\n","        plt.ylabel('train rvol.')\n","        plt.legend()\n","        plt.title(f\"stock {picked_stock_id}'s line plot of True y_{set_name} vs. {set_name}_avg_target_rvol on {set_name} set\")\n","        plt.show()\n","        plt.close()\n","        \n","        # ## 3. Line plot of pred vs true real. vol.\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(len(all_stock_y_GroundTruth_pivot[picked_stock_id])),all_stock_y_GroundTruth_pivot[picked_stock_id],linestyle='solid', c='green',label='True stock id: '+str(picked_stock_id), alpha=0.7 )\n","        # plt.plot(range(len(all_stock_pred_pivot[picked_stock_id])),all_stock_pred_pivot[picked_stock_id],linestyle='solid', c='red',label='Pred stock id: '+str(picked_stock_id), alpha=0.4 )\n","        # plt.grid()\n","        # plt.xlabel('index')\n","        # plt.ylabel(f'{set_name} rvol.')\n","        # plt.legend()\n","        # plt.title(f\"stock {picked_stock_id}'s line plot of True y_{set_name} vs {set_name}_pred on {set_name} set\")\n","        # plt.show()\n","        # plt.close()\n","\n","        ## y_train and train_pred Distributions Histogram:\n","        fig, ax = plt.subplots(2, 1, figsize=(30, 10))\n","        max_val = max(groundtruth.max(), prediction.max())\n","        ax[0].hist(groundtruth, bins=1000, color='green', alpha=1, histtype='bar', rwidth=0.8, label='GroundTruth')\n","        ax[0].set_xlim(-0.001, max_val)\n","        ax[0].set_title(f\"stock {picked_stock_id}'s Distribution of GroundTruth (skew: {stats.skew(groundtruth)} , kurt:{stats.kurtosis(groundtruth)}) and Prediction (skew: {stats.skew(prediction)} , kurt:{stats.kurtosis(prediction)}) on {set_name} set\")\n","        ax[0].set_xlabel(f' GroundTruth on {set_name} set')\n","        ax[0].set_ylabel('frequency')\n","        ax[1].hist(prediction, bins=1000, color='red', alpha=1, ec='r', label='Prediction')\n","        ax[1].set_xlim(-0.001, max_val)\n","        ax[1].set_xlabel(f' Prediction on {set_name} set')\n","        ax[1].set_ylabel('frequency')\n","        plt.legend(loc=\"upper left\")\n","        plt.show()\n","        plt.close()\n","\n","        ####### scatter plot of True rvol. Values Plot Vs. Train Residuals\n","        fig, ax = plt.subplots(2,1,figsize=(30,10))\n","        max_val = max(groundtruth.max(), prediction.max())\n","        ax[0].scatter(groundtruth, picked_st_residuals, c='c',alpha=0.1 )\n","        ax[0].set_xlim(-0.001, max_val)\n","        ax[0].axhline(y=0, color='g', linestyle='-')\n","        #ax.axhline(y=np.mean(groundtruth), color='r', linestyle='-')\n","        ax[0].set_title(f\"stock {picked_stock_id}'s True R.V. Vs. Residuals Values Plot on {set_name} set\")\n","        ax[0].set_xlabel(f'True {set_name} Values')\n","        ax[0].set_ylabel(f'{set_name} picked_st_residuals')\n","        ax[1].scatter(prediction, picked_st_residuals, c='m',alpha=0.1 )\n","        ax[1].set_xlim(-0.001, max_val)\n","        ax[1].axhline(y=0, color='g', linestyle='-')\n","        ax[1].set_title(f\"stock {picked_stock_id}'s fitted R.V. Vs. Residuals Values Plot on {set_name} set\")\n","        ax[1].set_xlabel(f'fitted {set_name} Values')\n","        ax[1].set_ylabel(f'{set_name} picked_st_residuals')\n","        plt.legend(loc=\"upper left\")\n","        plt.show()\n","        plt.close()\n","\n","        ## Normal Q-Q Plot:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        sm.qqplot(picked_st_residuals, line='q', ax=ax)\n","        ax.set_title(f\"stock {picked_stock_id}'s QQ Plot of Residuals on {set_name} set\")\n","        ax.set_xlabel('Theoretical Quantiles')\n","        ax.set_ylabel('Sample Quantiles')\n","        plt.show()\n","        plt.close()\n","\n","        ## Residuals Distribution Histogram:\n","        fig, ax = plt.subplots(figsize=(30,10))\n","        plt.hist( picked_st_residuals,bins=1000)\n","        ax.set_title(f\"stock {picked_stock_id}'s Distribution of Residuals on {set_name} set\")\n","        ax.set_xlabel(f'{set_name} Residuals')\n","        ax.set_ylabel('frequency')\n","        plt.show()\n","        plt.close()\n","\n","        ## box plot of residuals, groundtruth and prediction\n","        fig, ax = plt.subplots(figsize=(5,5))\n","        ax.boxplot([picked_st_residuals,groundtruth,prediction],labels=['residuals','groundtruth','prediction'])\n","        ax.set_title(f\" stock {picked_stock_id}'s Box Plot of Residuals, GroundTruth and Prediction on {set_name} set\")\n","        ax.set_ylabel('value')\n","        #ax.set_yticks(np.arange(-0.01, 0.025, 0.001))\n","        ax.grid()\n","        plt.show()\n","        plt.close()\n","\n","        # ###### Autocorrelation Plot\n","        # fig, ax = plt.subplots(figsize=(10,3))\n","        # plot_acf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        # ax.set_xlabel('Lag')\n","        # ax.set_ylabel('Autocorrelation')\n","        # ax.set_yticks(np.arange(-1, 1, 0.1))\n","        # ax.grid()\n","        # ax.set_title(f'Autocorrelation of {set_name} Residuals')\n","        # fig.show()\n","\n","        # ###### Partial Autocorrelation Plot\n","        # fig, ax = plt.subplots(figsize=(10,3))\n","        # plot_pacf(residuals, lags=20, ax=ax)  # You can adjust the number of lags as needed\n","        # ax.set_xlabel('Lag')\n","        # ax.set_ylabel('Partial Autocorrelation')\n","        # ax.set_yticks(np.arange(-1, 1, 0.1))\n","        # ax.grid()\n","        # plt.title(f'Partial Autocorrelation of {set_name} Residuals')\n","        # plt.show()\n","\n","        ##### Autocorrelation and Partial Autocorrelation Plot EVERY individual stock\n","        #self.compute_acf_pacf(unique_stock_ids,all_stock_pred_df,all_stock_y_GroundTruth_df)\n","\n","\n","        # #### FAST FOURIER TRANSFORM plot of EVERY individual stock\n","        # #### IFFT plot of reconstructed time series ######\n","        # self.compute_IFFT(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df)\n","\n","        del picked_stock_id,unique_stock_ids,all_stock_y_GroundTruth_df,all_stock_pred_df,residuals\n","        gc.collect()\n","        return groundtruth\n","\n","\n","\n","    def overall_time_id_analysis(self, all_stock_y_GroundTruth_df,all_stock_pred_df,avg_target_rvol,train_flag):\n","\n","        # Precompute variables\n","        set_name = 'train' if train_flag else 'test'\n","\n","        # Pivot with 'stock_id' as index and 'time_id' as columns\n","        if train_flag:\n","            all_stock_pred_pivot = all_stock_pred_df.pivot(index='stock_id', columns='time_id', values='target').reindex(columns=self.time_id_order)\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='stock_id', columns='time_id', values='target').reindex(columns=self.time_id_order)\n","        else:\n","            all_stock_pred_pivot = all_stock_pred_df.pivot(index='stock_id', columns='time_id', values='target')\n","            all_stock_y_GroundTruth_pivot = all_stock_y_GroundTruth_df.pivot(index='stock_id', columns='time_id', values='target')\n","\n","        unique_time_ids = all_stock_y_GroundTruth_pivot.columns \n","        # Vectorized RMSPE calculation along the 'stock_id' axis\n","        rmspe_per_time_id = np.sqrt(\n","            np.mean(\n","                ((all_stock_pred_pivot - all_stock_y_GroundTruth_pivot) / all_stock_y_GroundTruth_pivot) ** 2, axis=0\n","            )\n","        )\n","\n","\n","        # plot the RMSPE for all time ids\n","        display_n_time_ids = 1000\n","        sorted_rmspe_per_time_id = rmspe_per_time_id.sort_values(ascending=False).head(display_n_time_ids)\n","        fig, ax = plt.subplots(figsize=(40, 10))\n","        ax.bar(range(len(sorted_rmspe_per_time_id)), sorted_rmspe_per_time_id, tick_label=sorted_rmspe_per_time_id.index)\n","        ax.set_xticks(range(len(sorted_rmspe_per_time_id)))\n","        ax.tick_params(axis='x', rotation=90)\n","        ax.set_yticks(np.arange(0, max(sorted_rmspe_per_time_id) + 0.01, 0.04))\n","        ax.grid()\n","        ax.set_title(f'RMSPE of Real. Vol. for each time id on {set_name} set')\n","        ax.set_xlabel('Time ID')\n","        plt.show()\n","        plt.close()\n","\n","        all_time_id_rmspe = rmspe_per_time_id\n","\n","        fig, ax = plt.subplots(figsize=(40, 10))\n","        largest_20_rmspe_time_ids = all_time_id_rmspe.nlargest(20).index.values\n","        # Precompute the maximum RMSPE value for the largest 20 time IDs\n","        max_large_val = all_time_id_rmspe.loc[largest_20_rmspe_time_ids].max()\n","        # Create the second bar plot for the largest 20 RMSPE time IDs\n","        ax.text(5, max_large_val + 0.001, f'20 largest RMSPE time ids: {largest_20_rmspe_time_ids}', fontsize=12)\n","        # Convert time ids to string once and plot the data\n","        ax.bar(largest_20_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[largest_20_rmspe_time_ids])\n","        ax.axhline(y=avg_target_rvol, color='g', linestyle='-')\n","        # Set y-ticks efficiently, using the precomputed max value\n","        ax.set_yticks(np.arange(0, max_large_val + 0.001, 0.08))\n","        ax.set_ylabel('RMSPE')\n","        ax.set_title(f'20 largest RMSPE time ids on {set_name} set')\n","        ax.grid()\n","        plt.show()\n","        plt.close()\n","\n","\n","        all_time_id_rmspe = rmspe_per_time_id\n","        # Use `nsmallest` and `nlargest` to directly get the top 10 smallest and largest RMSPE values without full sorting\n","        smallest_10_rmspe_time_ids = all_time_id_rmspe.nsmallest(10).index.values\n","        # Precompute max value for y-ticks range in a single operation\n","        max_small_val = all_time_id_rmspe.loc[smallest_10_rmspe_time_ids].max()\n","        # Create the bar plot for RMSPE with precomputed values\n","        fig, ax = plt.subplots(figsize=(40, 10))\n","        # Set text once, and avoid converting indices to strings multiple times\n","        ax.text(5, max_small_val + 0.001, f'10 smallest RMSPE time ids: {smallest_10_rmspe_time_ids}', fontsize=12)\n","        # Convert time ids to string once and avoid recalculating max_small_val in yticks\n","        ax.bar(smallest_10_rmspe_time_ids.astype(str), all_time_id_rmspe.loc[smallest_10_rmspe_time_ids])\n","        # Set y-ticks with precomputed values (adjusting the range once)\n","        ax.set_yticks(np.arange(0, max_small_val + 0.001, 0.04))\n","        ax.set_ylabel('RMSPE')\n","        ax.set_title(f'10 smallest RMSPE time ids on {set_name} set')\n","        ax.grid()\n","        plt.show()\n","        plt.close()\n","\n","\n","        # ###### visualize the time ids with largest and smallest RMSPE on the average rvol. plot on training set\n","        # plt.figure(figsize=(30,5))\n","        # plt.plot(range(0,10),[avg_target_rvol]*10,linestyle='solid', c='blue',label=f'{set_name}_avg_target_rvol', alpha=0.4 )\n","        # large_idx = np.where(np.isin(unique_time_ids,largest_10_rmspe_time_ids))[0]\n","        # red_colors = ['black','darkred','crimson','lightcoral','indianred','orchid','hotpink','palevioletred','violet','plum']\n","        # for i,s in enumerate(large_idx):\n","        #     plt.axvline(x=s, ymin=0, ymax=1,color=red_colors[i],linestyle='-',label=str(i))\n","        # small_idx = np.where(np.isin(unique_time_ids,smallest_10_rmspe_time_ids))[0]\n","        # green_colors = ['gold','yellow','blue','darkgreen','lime','seagreen','mediumseagreen','springgreen','aquamarine','turquoise','lightgreen']\n","        # for j,l in enumerate(small_idx):\n","        #     plt.axvline(x=l, ymin=0, ymax=1,color=green_colors[j],linestyle='-',label=str(j))\n","        # plt.grid()\n","        # plt.yticks(np.arange(0, 0.04, 0.01))\n","        # plt.xlabel('sequential time id index')\n","        # plt.ylabel(f'{set_name} rvol.')\n","        # plt.legend()\n","        # plt.show()\n","\n","        del all_stock_y_GroundTruth_df,all_stock_pred_df,avg_target_rvol\n","        gc.collect()\n","        return all_stock_y_GroundTruth_pivot[largest_20_rmspe_time_ids],  all_stock_pred_pivot[largest_20_rmspe_time_ids]\n","\n","\n","\n","    def compute_model_bias_variance(self,y_test,y_train,X_train,best_mlxtend_xgb_params):\n","\n","        ## model bias and variance measurement\n","        # estimate bias and variance\n","        #train_time_ids = self.time_id_order[:self.train_time_id_ind]\n","        train_time_ids = self.time_id_order #[:self.train_time_id_ind]\n","\n","        #full_train_df = self.df[self.df['time_id'].isin(train_time_ids)]\n","\n","        #X_train = full_train_df[self.feat_cols_list]\n","        X_train = X_train[self.feat_cols_list]\n","        #y_train = full_train_df[self.target_name] #target\n","        y_train = y_train[self.target_name]\n","        X_test = self.test_df[self.feat_cols_list]      \n","        #y_test = self.test_df[self.target_name] #target\n","\n","\n","        # Assuming best_mlxtend_xgb_params contains the hyperparameters\n","        max_depth, eta, subsample, colsample_bytree, gamma, reg_alpha, reg_lambda, min_child_weight, num_rounds = best_mlxtend_xgb_params\n","\n","        # Create XGBRegressor model\n","        xgb_model = XGBRegressor(\n","            max_depth=max_depth,\n","            learning_rate=eta,\n","            subsample=subsample,\n","            colsample_bytree=colsample_bytree,\n","            gamma=gamma,\n","            reg_alpha=reg_alpha,\n","            reg_lambda=reg_lambda,\n","            min_child_weight=min_child_weight,\n","            n_estimators=num_rounds,\n","            objective='reg:squarederror',\n","            tree_method = \"hist\",\n","            device = \"cuda\"\n","        )\n","\n","        v1tr = np.exp(X_train['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        w_train = y_train **-2 * v1tr**2\n","        #w_test = y_test **-2 * v1ts**2\n","\n","        # Train XGBRegressor model\n","        xgb_model.fit(X_train.values, y_train.values/v1tr.values, sample_weight=w_train)\n","\n","        # Now you can use bias_variance_decomp\n","        mse, bias, var = bias_variance_decomp(xgb_model, X_train.values, y_train.values/v1tr.values, X_test.values, y_test['target'].values/v1ts.values, loss='mse', num_rounds=30, random_seed=1)\n","        print('\\nMSE: %.3f' % mse)\n","        print('Bias: %.3f' % bias)\n","        print('Variance: %.3f' % var)\n","\n","        return\n","\n","\n","\n","    ############################################################################################################\n","    \"\"\"  TRAINING and TESTING SET COMPARISON functions START\"\"\"\n","    ############################################################################################################\n","\n","    def train_n_test_set_st_RMSPE_comparison(self,train_rmspe_per_stock,test_rmspe_per_stock):\n","        # Combine train and test RMSPE into a single DataFrame for easier plotting\n","        rmspe_df = pd.DataFrame({\n","            'Stock ID': train_rmspe_per_stock.index,\n","            'Train RMSPE': train_rmspe_per_stock['train_rmspe_per_stock'],\n","            'Test RMSPE': test_rmspe_per_stock['test_rmspe_per_stock']\n","        })\n","        fig = px.line(rmspe_df, x='Stock ID', y=['Train RMSPE', 'Test RMSPE'], \\\n","                      title=f'Train vs Test RMSPE per Stock,\\n Corrrelation between train and test RMSPE: {rmspe_df[\"Train RMSPE\"].corr(rmspe_df[\"Test RMSPE\"])}')\n","        fig.update_layout(xaxis=dict(tickvals=rmspe_df['Stock ID'], tickangle=90), yaxis_title='RMSPE', xaxis_title='Stock ID')\n","        fig.show()\n","        return\n","    \n","\n","\n","    def train_n_test_set_picked_st_distribution_comparison(self,train_groundtruth, test_groundtruth , picked_stock_id):\n","        \n","        # perform Kolmogorov-Smirnov test\n","        ks_stat, ks_pval = stats.ks_2samp(train_groundtruth, test_groundtruth)\n","        print(f\"Kolmogorov-Smirnov test: KS Statistic: {ks_stat}, P-Value: {ks_pval}\")\n","        # Check if the distributions are the same\n","        alpha = 0.05  # Significance level\n","        if ks_pval < alpha:\n","            print(\"The null hypothesis is rejected. The test set does not come from the same distribution as the train set.\")\n","            ks_test_stats = f\"K-S TEST: Train and Test sets are DIFFERENT\"\n","        else:\n","            print(\"The null hypothesis is accepted. The test set may come from the same distribution as the train set.\")\n","            ks_test_stats = f\"K-S TEST: Train and Test sets are SAME\"\n","\n","        # violin plot train_groundtruth and test_groundtruth on the same plot\n","        fig = go.Figure()\n","        fig.add_trace(go.Violin(x=['Train']*len(train_groundtruth), y=train_groundtruth, name='Train', box_visible=True, meanline_visible=True))\n","        fig.add_trace(go.Violin(x=['Test']*len(test_groundtruth), y=test_groundtruth, name='Test', box_visible=True, meanline_visible=True))\n","        train_stats = f\"Train - Mean: {np.mean(train_groundtruth):.4f}, Median: {np.median(train_groundtruth):.4f}, Std: {np.std(train_groundtruth):.4f}, Skew: {stats.skew(train_groundtruth):.4f}, Kurtosis: {stats.kurtosis(train_groundtruth):.4f}, 75th Percentile: {np.percentile(train_groundtruth, 75):.4f}\"\n","        test_stats = f\"Test - Mean: {np.mean(test_groundtruth):.4f}, Median: {np.median(test_groundtruth):.4f}, Std: {np.std(test_groundtruth):.4f}, Skew: {stats.skew(test_groundtruth):.4f}, Kurtosis: {stats.kurtosis(test_groundtruth):.4f}, 75th Percentile: {np.percentile(test_groundtruth, 75):.4f}\"\n","        fig.update_layout(\n","            title=f\"Stock {picked_stock_id}'s Train vs Test GroundTruth Distribution<br>{train_stats}<br>{test_stats}<br>   {ks_test_stats}\",\n","            yaxis=dict(tickmode='linear', tick0=0, dtick=0.002)  # Increase y tick resolution\n","        )\n","        fig.show()\n","        return\n","\n","\n","\n","    def each_time_id_RMSPE_across_stocks_comparison(self,gt_time_id_df, pred_time_id_df,set_name,y_train_df):\n","\n","        y_train_pivot = y_train_df.pivot(index='time_id', columns='stock_id', values='target')\n","        print('len',len(y_train_pivot.columns.values))\n","        for time_id in gt_time_id_df.columns.values:\n","            gt_df = pd.DataFrame({'gt':gt_time_id_df[time_id],'pred':pred_time_id_df[time_id]}, index=gt_time_id_df.index)\n","            plt.figure(figsize=(40, 10))\n","            plt.title(f'Time ID {time_id} GroundTruth vs Prediction on {set_name} set')\n","            plt.plot(gt_df.index, gt_df['gt'].values, label='GroundTruth', color='blue')\n","            plt.plot(gt_df.index, gt_df['pred'].values, label='Prediction', color='red')\n","            # ## Add box plot of y_train_pivot for each stock\n","            plt.boxplot([y_train_pivot[stock_id] for stock_id in gt_df.index], positions=gt_df.index)\n","            plt.xticks(ticks=gt_df.index, labels=gt_df.index, rotation=90)\n","            plt.yticks(np.arange(0, 0.04, 0.002))\n","            plt.ylim(0, 0.04)\n","            plt.grid()\n","            plt.legend()\n","            plt.xlabel('Stock ID')\n","            plt.ylabel('Real. Vol.')\n","            plt.show()\n","        \n","        del gt_time_id_df, pred_time_id_df, y_train_df,y_train_pivot\n","        return\n","\n","    ############################################################################################################\n","    \"\"\"  TRAINING and TESTING SET COMPARISON functions END\"\"\"\n","    ############################################################################################################\n","\n","\n","\n","    def evaluate_predictions(self,final_reg,test_pred, y_test,train_pred,y_train,X_train,v1tr,w_train,best_mlxtend_xgb_params):\n","\n","        y_true = y_test\n","        y_pred = test_pred\n","        test_residuals = pd.DataFrame(y_true['target'] - y_pred['target'])\n","        test_residuals[['stock_id','time_id']] = y_true[['stock_id','time_id']]\n","        train_residuals = pd.DataFrame(y_train['target'] - train_pred['target'])\n","        train_residuals[['stock_id','time_id']] = y_train[['stock_id','time_id']]\n","        unique_stock_ids = self.train_stock_id.unique()\n","\n","        #all_stock_train_pred_df = self.compute_all_stock_train_pred_df(unique_stock_ids, train_pred)\n","        all_stock_train_pred_df = train_pred\n","        #all_stock_v1tr_df = self.compute_all_stock_v1tr_df(unique_stock_ids, v1tr)\n","        all_stock_v1tr_df = v1tr\n","        #all_stock_y_train_df = self.compute_all_stock_y_train_df(unique_stock_ids, y_train)\n","        all_stock_y_train_df = y_train\n","        #train_avg_target_rvol = self.compute_train_avg_target_rvol(unique_stock_ids, y_train)\n","        train_avg_target_rvol = y_train['target'].mean()\n","        test_avg_target_rvol = y_test['target'].mean()\n","\n","        #all_stock_test_pred_df = self.compute_all_stock_test_pred_df( unique_stock_ids, test_pred)\n","        all_stock_test_pred_df = test_pred\n","        #all_stock_y_test_df = self.compute_all_stock_y_test_df( unique_stock_ids, y_test)\n","        all_stock_y_test_df = y_test\n","\n","        print('\\n####################################### PREDICTION #################################################')\n","\n","        #v1ts = np.exp(np.exp( self.test_df['log_wap1_log_price_ret_vol'])) # double exponential to nullify log\n","        v1ts = np.exp( self.test_df['log_wap1_log_price_ret_vol']) # double exponential to nullify log\n","        print('corr(y_pred/v1ts, y_true/v1ts)',self.nancorr(       y_pred['target'].values/v1ts ,        y_true['target'].values/v1ts ))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred['target'].values/v1ts), np.log(y_true['target'].values/v1ts)))\n","        print('corr(y_pred, y_true)',self.nancorr(y_pred['target'].values, y_true['target'].values))\n","        print('log(corr( ))',self.nancorr(np.log(y_pred['target'].values), np.log(y_true['target'].values)))\n","        print(f'RMSPE train score: ',  np.mean( ((train_pred['target'].values-y_train['target'].values)/y_train['target'].values)**2 )**0.5  )\n","        print(f'RMSPE test score: ',  np.mean( ((y_pred['target'].values-y_true['target'].values)/y_true['target'].values)**2 )**0.5  )\n","\n","\n","        ############################ SET PARAMETERS HERE ##############################\n","        ##### individual stock id analysis parameters START #####\n","        high_rmspe_stocks = ['31','37', '18','33', '112', '88','60','110','27', '9','16','30',  '103',  '5', '58', '89', '66', '40',  '0', '4', '75',  '90', '98']\n","        start_index = 9\n","        end_index = 10\n","        ##### individual stock id analysis parameters END #####\n","\n","        ############################ SET PARAMETERS HERE ##############################\n","\n","        \"\"\" TRAINING SET PREDICTIONS START \"\"\"\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET #########################################################\n","        print('\\n####################################### TRAINING SET predictions START #################################################')\n","        ###################################################################################################################\n","\n","        ################################################################################################\n","        ############################## OVERALL STOCK ANALYSIS START ######################################\n","        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n","        # train_rmspe_per_stock = self.overall_stock_id_analysis(unique_stock_ids,all_stock_train_pred_df,all_stock_y_train_df,train_residuals,train_flag=True)\n","        print('\\n####################################### OVERALL STOCK ANALYSIS END ######################################')\n","        ############################## OVERALL STOCK ANALYSIS END ######################################\n","        ################################################################################################\n","\n","\n","        ################################################################################################\n","        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n","        ##### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n","        # for picked_stock_id in high_rmspe_stocks[start_index:end_index]:\n","        #     train_groundtruth = self.individual_stock_id_analysis(int(picked_stock_id),unique_stock_ids,all_stock_y_train_df,all_stock_train_pred_df,train_residuals,train_avg_target_rvol,train_flag=True)\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n","        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS START ##################################\n","        ################################################################################################\n","        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n","        train_gt_time_id_df, train_pred_time_id_df = self.overall_time_id_analysis(all_stock_y_train_df,all_stock_train_pred_df,train_avg_target_rvol, train_flag=True)\n","        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY START #########################################\n","        ###################################################################################################################\n","        print('\\n###################################### Feature importance & SHAPLEY START #########################################')\n","\n","        #self.compute_overall_SHAP_values(final_reg,X_train,y_train,train_pred,v1tr)\n","\n","        feature_name = \"log_first_10_min_vol_stnd\" ## see impact of a feature in more detail\n","        stock_id = 0\n","        view_time_ids_start = 0\n","        view_time_ids_end = 500\n","        #self.compute_individual_stock_SHAP_values(final_reg,X_train,all_stock_train_pred_df,all_stock_v1tr_df,all_stock_y_train_df,feature_name,stock_id,view_time_ids_start,view_time_ids_end)\n","\n","        print('\\n###################################### Feature importance & SHAPLEY END #########################################')\n","        ###################################################################################################################\n","        ###################################### Feature importance & SHAPLEY END #########################################\n","        ###################################################################################################################\n","\n","\n","\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE START ################################################\n","        ###################################################################################################################\n","\n","         #### Plot top 30 feature importances\n","        # fig, ax = plt.subplots(figsize=(10, 10))\n","        # xgb.plot_importance(final_reg, importance_type='gain', max_num_features=30, height=0.8, show_values=False)\n","        # self.compute_model_bias_variance(y_test,y_train,X_train,best_mlxtend_xgb_params)\n","\n","\n","        ###################################################################################################################\n","        ###################################### MODEL BIAS VARINANCE END #################################################\n","        ##################################################################################################################\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","        print('\\n####################################### TRAINING SET predictions END #################################################\\n')\n","\n","\n","\n","\n","        \"\"\" TESTING SET PREDICTIONS START \"\"\"\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS START ########################################\n","        print('\\n####################################### TESTING SET predictions START #################################################')\n","        ###################################################################################################################\n","\n","        ################################################################################################\n","        ############################## OVERALL STOCK ANALYSIS START ######################################\n","        print('\\n####################################### OVERALL STOCK ANALYSIS START ######################################')\n","        # test_rmspe_per_stock = self.overall_stock_id_analysis(unique_stock_ids,all_stock_test_pred_df,all_stock_y_test_df,test_residuals,train_flag=False)\n","        print('\\n####################################### OVERALL STOCK ANALYSIS END ######################################')\n","        ############################## OVERALL STOCK ANALYSIS END ######################################\n","        ################################################################################################\n","\n","\n","\n","        ################################################################################################\n","        ############################## INDIVIDUAL STOCK ANALYSIS START #################################\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS START #################################')\n","        ##### Analyze Single/ INDIVIDUAL stocks with high RMSPE in train set\n","        # for picked_stock_id in high_rmspe_stocks[start_index:end_index]:\n","        #     test_groundtruth = self.individual_stock_id_analysis(int(picked_stock_id),unique_stock_ids,all_stock_y_test_df,all_stock_test_pred_df,test_residuals,test_avg_target_rvol,train_flag=False)\n","        print('\\n############################## INDIVIDUAL STOCK ANALYSIS END #################################')\n","        ############################## INDIVIDUAL STOCK ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS START ##################################\n","        ################################################################################################\n","        print('\\n############################## OVERALL TIME ANALYSIS START #################################')\n","        test_gt_time_id_df, test_pred_time_id_df = self.overall_time_id_analysis(all_stock_y_test_df,all_stock_test_pred_df,test_avg_target_rvol, train_flag=False)\n","        print('\\n############################## OVERALL TIME ANALYSIS END #################################')\n","        ################################################################################################\n","        ############################## OVERALL TIME ID ANALYSIS END #################################\n","        ################################################################################################\n","\n","\n","\n","        ###################################################################################################################\n","        ############################################ TESTING SET PREDICTIONS END ##########################################\n","        ###################################################################################################################\n","        print('##################################################################################################')\n","        print('\\n####################################### TESTING SET predictions END #################################################\\n')\n","\n","\n","\n","\n","        \"\"\" TRAINING AND TESTING SET PERFORMANCE COMPARISON START \"\"\"\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING AND TESTING SET PERFORMANCE COMPARISON START ##########################################\n","        ###################################################################################################################\n","        print('\\n####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON START #################################################')\n","        #self.train_n_test_set_st_RMSPE_comparison(train_rmspe_per_stock,test_rmspe_per_stock)\n","        print('##################################################################################################')\n","        print('\\n####################################### TRAINING AND TESTING SET stocks RMSPE COMPARISON END #################################################\\n')\n","\n","        ###################################################################################################################\n","        print('\\n####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON START #################################################')\n","        #self.train_n_test_set_picked_st_distribution_comparison(train_groundtruth, test_groundtruth,high_rmspe_stocks[start_index:end_index])\n","        print('##################################################################################################')\n","        print('\\n####################################### TRAINING AND TESTING SET stocks DISTRIBUTION COMPARISON END #################################################\\n')\n","        ###################################################################################################################\n","\n","\n","        ###################################################################################################################\n","        print('##################################################################################################')\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on train set START ########################')\n","        print('all_stock_y_train_df',all_stock_y_train_df)\n","        self.each_time_id_RMSPE_across_stocks_comparison(train_gt_time_id_df, train_pred_time_id_df,set_name='train',y_train_df = all_stock_y_train_df)\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on train set END ########################')\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on test set START ##########################\\n')\n","        self.each_time_id_RMSPE_across_stocks_comparison(test_gt_time_id_df, test_pred_time_id_df,set_name='test',y_train_df = all_stock_y_train_df)\n","        print('\\n################## time_id RMSPE COMPARISON between groundtruth and prediction on test set END ##########################\\n')\n","        print('##################################################################################################')\n","        ###################################################################################################################\n","\n","\n","        ###################################################################################################################\n","        ############################################ TRAINING AND TESTING SET COMPARISON END ##########################################\n","        ###################################################################################################################\n","\n","\n","        del X_train,y_train, all_stock_train_pred_df, all_stock_v1tr_df ,  all_stock_test_pred_df, all_stock_y_train_df,  all_stock_y_test_df\n","        del y_true, y_pred, test_residuals, train_residuals, unique_stock_ids\n","        gc.collect()\n","        return\n","\n","\n","\n","    def visualize_tree(self,):\n","        # feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\n","        # feature_importances.to_csv('feature_importances.csv')\n","        # plt.figure(figsize=(16, 12))\n","        # sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(20), x='average', y='feature')\n","        # plt.title('20 TOP feature importance over {} folds average'.format(folds.n_splits));\n","\n","        # importances = pd.DataFrame({'Feature': model.feature_name(),\n","        #                             'Importance': sum( [model.feature_importance(importance_type='gain') for model in models] )})\n","        # importances2 = importances.nlargest(40,'Importance', keep='first').sort_values(by='Importance', ascending=True)\n","        # importances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,6), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)\n","\n","        #TODO: #plot decision tree for interpretability\n","\n","        return\n","\n","\n","\n"]},{"cell_type":"code","execution_count":265,"id":"8e78ed9d","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1727078820731,"user":{"displayName":"Jose Peeterson","userId":"16489366788214429501"},"user_tz":-480},"id":"8e78ed9d"},"outputs":[],"source":["num_trees = 3000\n","\n","def objective(trial):\n","\n","\n","    t_v_t = train_validate_n_test(df_train_reordered, df_test)\n","\n","    ######  SET Hyperparameter's range for tuning ######\n","    early_stopping_rounds = 25\n","    num_round= num_trees\n","    seed1=11\n","    missing_value = -np.inf   # Replace with a suitable value\n","\n","    # # Hyperparameters and algorithm parameters are described here\n","    # params = {'disable_default_eval_metric': 1,\n","    #           \"max_depth\": trial.suggest_int('max_depth', 2, 25),\n","    #         \"eta\": trial.suggest_float(name='eta', low=0.0001, high=1,log=True),\n","    #         \"subsample\" : round(trial.suggest_float(name='subsample', low=0.3, high=1.0,step=0.1),1),\n","    #         \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.05, high=0.8,step=0.05),1),\n","    #         'gamma': trial.suggest_int('gamma', 1, 15),\n","    #         'reg_alpha': trial.suggest_int('reg_alpha', 1, 15),\n","    #         'reg_lambda': trial.suggest_int('reg_lambda', 1, 15),\n","    #         'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n","    #         \"tree_method\": 'hist',\n","    #         \"device\": \"cuda\",\n","    #         \"seed\":seed1,\n","    #         #'missing': missing_value\n","    #         }\n","\n","\n","    # Hyperparameters and algorithm parameters are described here\n","    params = {'disable_default_eval_metric': 1,\n","              \"max_depth\": trial.suggest_int('max_depth', 15, 40),\n","            \"eta\": trial.suggest_float(name='eta', low=0.00001, high=0.2,log=True),\n","            \"subsample\" : round(trial.suggest_float(name='subsample', low=0.7, high=1.0,step=0.05),1),\n","            \"colsample_bytree\": round(trial.suggest_float(name='colsample_bytree', low=0.5, high=1,step=0.05),1),\n","            'gamma': trial.suggest_int('gamma', 0, 6),\n","            'reg_alpha': trial.suggest_int('reg_alpha', 8, 14),\n","            'reg_lambda': trial.suggest_int('reg_lambda', 8, 14),\n","            'min_child_weight': trial.suggest_int('min_child_weight', 0, 10),\n","            \"tree_method\": 'hist',\n","            \"device\": \"cuda\",\n","            \"seed\":seed1,\n","            #'missing': missing_value\n","            }\n","\n","    ######  SET Hyperparameter's range for tuning ######\n","\n","    val_avg_error,best_iteration = t_v_t.xgb_train_validate(params,num_round,early_stopping_rounds,trial)\n","    print(f\"val_avg_error: {val_avg_error}, best_iteration: {best_iteration}\")\n","    trial.set_user_attr(\"best_iteration\", best_iteration)\n","\n","    del t_v_t\n","    gc.collect()\n","    return val_avg_error\n","\n"]},{"cell_type":"code","execution_count":266,"id":"59525d4a","metadata":{},"outputs":[],"source":["\n","\n","# #if __name__ == \"__main__\":\n","\n","# #optuna.logging.set_verbosity(optuna.logging.WARNING)\n","# # study_name= 'Correct_residual_autocorrrelation_HAR_n_target_lag_feat_n_target_pred'\n","\n","# study = optuna.create_study(study_name ='Correct_residual_autocorrrelation_HAR_feat' ,direction=\"minimize\")\n","# study.optimize(objective, timeout=18000, n_trials=75) # 50\n","\n","# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n","# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n","\n","# print(\"Study statistics: \")\n","# print(\"  Number of finished trials: \", len(study.trials))\n","# print(\"  Number of pruned trials: \", len(pruned_trials))\n","# print(\"  Number of complete trials: \", len(complete_trials))\n","\n","# print(\"Best trial:\")\n","# trial = study.best_trial\n","\n","# print(\"Best number of iteration/boosting rounds: \",study.trials[trial.number].user_attrs['best_iteration'])\n","\n","# print(\"Trial no.: \",trial.number)\n","# print(\"  Value: \", trial.value)\n","\n","# print(\"  Params: \")\n","# for key, value in trial.params.items():\n","#     print(\"    {}: {}\".format(key, value))\n","\n","# #print(\"Best hyperparameters:\", study.best_params)\n","\n","# fig = optuna.visualization.plot_parallel_coordinate(study)\n","# fig.show()\n","\n","# fig = optuna.visualization.plot_optimization_history(study)\n","# fig.show()\n","\n","# fig = optuna.visualization.plot_slice(study)\n","# fig.show()\n","\n","# fig = optuna.visualization.plot_param_importances(study)\n","# fig.show()\n","\n"]},{"cell_type":"code","execution_count":267,"id":"95948e2e","metadata":{},"outputs":[],"source":["# df = study.trials_dataframe()\n","\n","# plt.figure(figsize=(10, 5))\n","# df['value'].plot(yticks=np.arange(0.2, 0.5, 0.01), ylim=(0.15, 0.3))\n","# plt.xlabel('Trial Number')\n","# plt.ylabel('Validation Error')\n","# plt.show()\n","# for cols in ['params_colsample_bytree','params_eta','params_gamma','params_max_depth','params_min_child_weight','params_reg_alpha','params_reg_lambda','params_subsample']:\n","#     print(cols)\n","#     plt.figure(figsize=(10, 5))\n","#     kernel = stats.gaussian_kde([df[cols], df['value']])\n","#     df['z'] = kernel([df[cols], df['value']])\n","#     fig = px.scatter(df, x=cols, y='value', color='z', color_continuous_scale=px.colors.sequential.Bluered)\n","#     fig.show()"]},{"cell_type":"code","execution_count":null,"id":"6d620dba","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6d620dba","outputId":"8ec60e6f-83d5-4ec2-d4a0-a5bd0ad553e8"},"outputs":[],"source":["\n","seed1 = 11\n","missing_value = -np.inf  # Replace with a suitable value\n","\n","\n","############ Best parameters Manual Start ############\n","num_rounds = 1135 #838 #study.trials[trial.number].user_attrs['best_iteration']\n","max_depth = 17\n","eta =  0.012302593098587278\n","subsample =  0.75\n","colsample_bytree =  0.8\n","gamma = 0\n","reg_alpha =  11\n","reg_lambda = 8\n","min_child_weight =  6\n","############ Best parameters Manual End ############\n","\n","############ Best parameters Automatic Start ############\n","# best_trial = study.best_trial\n","# num_rounds = study.best_trial.user_attrs['best_iteration']\n","# print('final best iteration: ',num_rounds )\n","# seed1 = 11\n","# missing_value = -np.inf  # Replace with a suitable value\n","# max_depth = best_trial.params['max_depth']\n","# eta =  best_trial.params['eta']\n","# subsample =  best_trial.params['subsample']\n","# colsample_bytree =  best_trial.params['colsample_bytree']\n","# gamma =  best_trial.params['gamma']\n","# reg_alpha =  best_trial.params['reg_alpha']\n","# reg_lambda = best_trial.params['reg_lambda']\n","# min_child_weight = best_trial.params['min_child_weight']\n","############ Best parameters Automatic End ############\n","\n","\n","\n","\n","\n","best_mlxtend_xgb_params = [max_depth,eta,subsample,colsample_bytree,gamma,reg_alpha,reg_lambda,min_child_weight,num_rounds]\n","\n","best_params = { 'disable_default_eval_metric': 1,\n","              \"max_depth\": max_depth,\n","            \"eta\": eta,\n","            \"subsample\" : subsample,\n","            \"colsample_bytree\":  colsample_bytree,\n","            'gamma':gamma,\n","            'reg_alpha': reg_alpha,\n","            'reg_lambda': reg_lambda,\n","            'min_child_weight': min_child_weight,\n","            \"tree_method\": 'hist',\n","            \"device\": \"cuda\",\n","            \"seed\":seed1,\n","            #'missing': missing_value\n","               }\n","\n","t_v_t = train_validate_n_test(df_train_reordered, df_test)\n","#final_reg,test_pred,train_pred,y_train,X_train,X_test,v1tr,w_train = t_v_t.make_predictions(best_params,num_rounds)\n","\n","\n","train_pred = pd.DataFrame(train_pred).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n","train_pred['time_id'] = df_train_reordered['time_id']\n","train_pred['stock_id'] = df_train_reordered['stock_id']\n","\n","y_train = pd.DataFrame(y_train).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n","y_train['time_id'] = df_train_reordered['time_id']\n","y_train['stock_id'] = df_train_reordered['stock_id']\n","\n","v1tr = pd.DataFrame(v1tr)\n","v1tr['time_id'] = df_train_reordered['time_id']\n","v1tr['stock_id'] = df_train_reordered['stock_id']\n","\n","test_pred = pd.DataFrame(test_pred).rename(columns={'log_wap1_log_price_ret_vol':'target'})\n","test_pred['time_id'] = df_test['time_id'].astype(int)\n","test_pred['stock_id'] = df_test['stock_id'].astype(int)\n","\n","## # Merge the DataFrames on 'time_id' and 'stock_id' columns\n","os.chdir('/home/optimusprime/Desktop/peeterson/optiver/Optiver-Realized-Volatility-Prediction/data')\n","train = pd.read_csv('train.csv')\n","X_test['time_id'] = df_test['time_id']\n","y_test_df = pd.merge(X_test[['time_id', 'stock_id']], train[['time_id', 'stock_id', 'target']], on=['time_id', 'stock_id'], how='left')\n","y_test_df['time_id'] = df_test['time_id'].astype(int)\n","y_test_df['stock_id'] = df_test['stock_id'].astype(int)\n","\n","t_v_t.evaluate_predictions(final_reg,test_pred, y_test_df,train_pred,y_train, X_train,v1tr,w_train,best_mlxtend_xgb_params)\n","\n","\n","\n","# del final_reg,test_pred, y_test,train_pred,y_train,X_train,X_test,v1tr,w_train\n","# gc.collect()\n","\n","# ## save the best model with timestamp for future use.\n","# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","# filename = f'xgb_gpu_{timestamp}.pkl'\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Xgboost_gpu_models/xgb_gpu_model_registry')\n","# with open(filename, 'wb') as file:\n","#     pickle.dump(final_reg, file)\n","# print(f'Model saved to: {filename}')\n","# os.chdir('/content/drive/MyDrive/optiver_real_vol/Datasets/liquidity_features')\n","\n","\n","# del study, trial,t_v_t,final_reg,test_pred, y_test,train_pred,y_train\n","# gc.collect()\n"]},{"cell_type":"code","execution_count":null,"id":"8ac64f6f","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"aa8c6dc4","metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":2344753,"sourceId":27233,"sourceType":"competition"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"papermill":{"default_parameters":{},"duration":15331.990222,"end_time":"2024-09-16T06:04:13.265071","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-16T01:48:41.274849","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
